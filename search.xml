<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Going Vertical in Memory Management</title>
      <link href="/2023/09/15/Going-Vertical-in-Memory-Management/"/>
      <url>/2023/09/15/Going-Vertical-in-Memory-Management/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自International Symposium on Computer Architecture，ISCA，2014</li><li>Going Vertical in Memory Management: Handling Multiplicity by Multi-policy</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Lei Liu，Zehan Cui，Yungang Bao，Mingyu Chen，Chengyong Wu，中国科学院计算机研究所计算机体系结构国家重点实验室</li><li>Yong Li匹兹堡大学ECE系</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>来自不同领域的许多新兴应用程序通常表现出异构内存特征。<span class="label primary">当在并行平台上组合运行时，这些应用程序会呈现出令人畏惧的各种工作负载行为</span>，这对任何<span class="label primary">内存分配策略的有效性</span>提出了挑战。先前的基于分区或随机内存分配方案通常仅管理内存层次结构的一级，并且通常针对特定工作负载。</p><p><strong>共享内存资源</strong>的有效管理对于应用程序性能和系统吞吐量非常重要。然而，商业化并行机中使用的大多数现有内存和缓存管理机制都采用通用地址交错或调度/分区方法，这些方法忽视了当今异构环境中不同的内存利用率特征和不同的资源需求。这通常会导致程序间扰动、资源颠簸、内存/缓存利用率低下，从而导致性能下降。</p><p>当时架构允许所有应用程序共享LLC（末级缓存）和DRAM组，从而在许多情况下导致严重的争用。一般用页面着色解决，存在两种基于页面着色的分区技术，即高速缓存分区和DRAM存储体分区。如图 1 所示，可以通过使用操作系统物理页地址中表示LLC集索引（LLC颜色位）的位作为颜色位来实现缓存分区。当为应用程序分配页面时，操作系统可以为物理页面分配特定的颜色，以便应用程序只能访问指定颜色的缓存集。</p><div class="note info no-icon">说人话：运行的特定某个程序只能使用颜色A的DRAM分区，或者只能缓存到颜色A'的LLC缓存分区中</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/510d5e5c024448c78ccbc62c9e047875.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>处理各种动态变化的内存和缓存分配需求。需要设计一种能够通过区分内存特性来选择合适的分配策略的内存管理系统。</p><p>（为了实现这一目标，简单地集成最佳性能的机制是不切实际的，因为几乎所有最先进的方案都需要对内存控制器/分配器或缓存层次结构进行昂贵的更改，更不用说检测和预测应用程序需求和冲突方面的挑战了。）</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>最近的几种解决方案尝试通过将主内存（DRAM 组）[10,16,17,29] 或缓存 [15,24,30,31,32] 水平分区为独占片来隔离具有不同内存资源需求的应用程序。这些方法避免了对内存占用较小的程序的干扰，但可能会通过有效减少容量来影响较大工作负载的性能。操作系统级别的分区和其他内存分配优化更加灵活，并且在许多案例方面表现良好。</p><p>先前的研究工作[12,29]表明，LLC和DRAM争用会显着降低整体系统性能，并且已经提出了许多解决方案来缓解争用问题。最有效的优化之一是基于页面着色的软件分区，它允许操作系统内核利用底层架构信息，例如 LLC 和 DRAM 的物理地址映射。通过页面着色，可以通过修改内核伙伴系统来缓解争用问题 [4,10,15,17,21,22,24,26]，同时避免对内存控制器或缓存层次结构进行昂贵的硬件更改。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>为了处理多样化且动态变化的内存和缓存分配需求，我们通过垂直分区增强现有的“水平”缓存/DRAM 存储体分区，并探索由此产生的多策略空间。着色位分为三类：bank-only、仅高速缓存位（C位）和重叠位（O位在图 1 中同时索引bank和高速缓存）。特别是，O位启用垂直分区（VP），通过内存层次结构垂直地对高速缓存和bank组进行分区。结合水平和垂直分区形成了以前未研究过的分区策略空间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d82182ffc00c4216b76bef9a55d7bd4a.png" alt="从操作系统和典型多核机器上三类颜色位的角度来看的地址映射"></p><p>水平内存和缓存分区的好处是否可以累积（即，我们应该进行垂直分区吗？）所以作者测了测如下表所示的几种搭配<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/26bbd7193c5f467c9dbd3405d6959b7c.png"></p><p>把214个workload根据性能提升的原因做了可视化<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/93ea1c4e302d44109902c0078a399e3f.png"></p><p><strong>从上述定量研究中可以得出一个明显的结论：内存分配策略的有效性取决于特定的应用程序特性，特别是缓存需求。实际上，工作负载可能包含多个同时运行的应用程序，这些应用程序具有不同特征的任意组合，这使得确定适当的内存分配任务具有挑战性。</strong>然后总结了一下其他因素影响不大，缓存分区性能表现出的性能差异更大。为了验证缓存利用率特征对缓存分区策略的潜在影响，我们收集了当缓存配额从8/8（使用整个缓存）减少到1/8时各种应用程序的性能下降情况。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d6f56fdc5eca4311851b27a93bc66713.png"></p><p>每个应用程序都会执行八次，每次都会通过基于页面着色的缓存分区分配不同数量的 LLC。根据结果​​，我们将应用程序的缓存行为分为四类：Core Cache Fitting（CCF）、LLC High （LLCH）、LLC Middle（LLCM） 和LLC Thrashing（LLCT）。图 4 报告了SPEC2006基准测试套件中各种基准测试的分类 [1]。CCF应用程序（表示为绿色曲线），例如hmmer和namd，在使用较少的LLC资源时不会显着降低性能，因为它们的工作集大小足够小，可以容纳L1和L2每核专用缓存。LLCT应用程序（黑色曲线），例如libquantum，也对缓存配额不敏感，但这是由于缓存抖动行为而不是较小的工作集大小。LLCH应用程序（红色曲线）（例如mcf）由于其资源匮乏的特性，因缓存配额减少而遭受最严重的性能下降。与LLCH相比，LLCM（蓝色曲线）应用程序使用更少的缓存资源，因此速度减慢没有LLCH应用程序那么多。例如，gcc和bzip2是LLCM，因为当缓存从8/8减少到4/8时，它们不会遭受明显的降级。然而，当缓存配额降至3/8以下时，性能会急剧下降。</p><p>但是要动态分类：做图4时的静态分析发现热页面的数量在很多情况下可以反映应用程序的LLC需求。图5显示了多个基准测试的热门页数量和缓存需求之间的相关性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e10451bd578c4c58bbc3ceb565996949.png"></p><p>研究了这些策略针对<strong>2000</strong>多个工作负载的性能，并通过<strong>数据挖掘方法将结果与应用程序特征相关联</strong>。基于这种相关性，我们得出了几种实用的内存分配规则，并将其集成到统一的多策略框架中，以指导动态和多样化的多编程/线程工作负载的资源分区和合并。（生成了一套实用的分区和聚合规则以及一棵策略决策树，帮助HVR自动选择策略、动态资源分区和聚合。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c95f6f8820ec410285d3dadd9678cb5c.png"></p><p>（置信度和支持度是数据挖掘中的术语。在我们的工作中，支持被定义为规则中包含特定类型应用程序的工作负载的比例；置信度表明该规则的准确性。）</p><div class="note info no-icon">其实就是先采样热页，制定一些阈值，得到分类算法。然后将workload名称和分类绑定成一个键值对，再将分类和着色位的性能（就是最初的散点图）建立相关规则（通过观察得到的，比如：“包含 LLCT 和其他应用程序（LLCH、LLCM、CCF）的工作负载应使用 C-VP 或 A-VP（37.1% 支持，94.4% 置信度）”）。**总是一开始使用bank-only然后迁移** </div><p>在 Linux 内核 2.6.32 中将我们的方法实现为重构的页面索引系统加上一系列内核模块。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>大量实验表明，在实践中，我们的框架可以选择适当的内存分配策略，并始终优于未修改的Linux内核，与现有技术相比，性能提升高达11%。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>运行时采样得到分类，再和着色位绑定太晚了吧。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Chip on </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dancing in the Dark Profiling for Tiered Memory</title>
      <link href="/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/"/>
      <url>/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Parallel and Distributed Processing Symposium, (IPDPS), 2021</li><li>Dancing in the Dark: Profiling for Tiered Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jinyoung Choi, University of California, Riverside</li><li>Sergey Blagodurov, Advanced Micro Devices, Inc.</li><li>Hung-Wei Tseng, University of California, Riverside</li></ul><h2 id="2-Introduction-amp-Background"><a href="#2-Introduction-amp-Background" class="headerlink" title="2. Introduction &amp; Background"></a>2. Introduction &amp; Background</h2><p>多种内存技术协同-&gt;更多内存、更少延迟、更大带宽<br>tiered memory architectures （TMAs）<br>缓存会给更多设计给硬件，平面寻址会有更多软件策略<br>对延迟的优化，可以通过命中更多的热页面，常用的优化内存延迟的方式。<br>但是软件优化通过TLB条目和内存页：</p><ol><li>页面访问不可见只有缺页时才知道，识别被多次访问的代码段很难。</li><li>硬件监控的多样性，是由供应商提供，没有标准。</li><li>创建一个新的分析工具，组合硬件监控器的信息做出热度排名策略。</li></ol><p>与硬件控制相比，软件控制的4个优点：</p><ol><li>首先，分层内存允许直接从数据所在的层进行就地内存访问。使用硬件缓存时，请求的内存块将从第 2 层可寻址内存引入第 1 层缓存，从而导致流量增加。而且缺页导致的迁移开销大。<div class="note primary">所以有的数据可能缓存更合适，有的就平面寻址。PM的延迟和带宽的落后是什么造成的，XCL呢？？？为什么CPU那里是三层缓存，那么DRAM的缓存多少合适？</div> </li><li>其次，在TMA中，内存页面可以在任一层中找到；缓存会在内存中创建重复的、可能不一致的页面副本，并且需要机制来保持数据一致。</li><li>第三，分层内存解决方案允许缓存策略微调（通过工作负载混合、服务级别协议等）以适应高层策略决策并消除过度迁移。（不懂）</li><li>第四，由于与根深蒂固的NUMA架构相似，分层内存可以利用NUMA和异构内存管理（HMM）[5]系统基础设施。 Linux 社区中关于如何将NVM暴露给操作系统一直存在争论。当前的提案围绕将NVM配置为无CPU的NUMA节点以及使用AutoNUMA或其他现有的管理NUMA方法[11]、[12]。这里再次强调的是内存如何分配和移动，而不是如何分析热度（例如，AutoNUMA 中的定期取消映射和页错误处理如何产生开销 [13]）。</li></ol><p>内存分析方法：</p><ol><li><span class="label info">页表项PTE位跟踪。PTE包括已访问A位和脏D位。操作系统可以清除这些PTE位，并且硬件页表遍历器PTW将设置它们。</span>PTW在TLB未命中时设置A位。A位不区分在分析间隔期间访问一次的页面和多次访问的页面。更频繁的A位检查可以提高分析的信息量，但也会增加开销。</li><li><span class="label info">TBP能够从加载或存储指令收集地址跟踪</span>。在AMD系统上，基于指令的采样IBS[18]、[19] 使CPU指令能够在穿过管道时被标记，允许在指令执行时收集数据，并在指令退出时引发中断。</li><li>轻量级分析 (LWP) [20] 是 AMD64 系列 15h AMD 处理器的硬件扩展，与 IBS 的不同之处在于 LWP 在生成中断之前收集大量数据。</li><li>英特尔的处理器基于事件的采样（PEBS）是一种基于跟踪的功能，类似于IBS/LWP，其中处理器在指定的内存区域中记录标记的样本。PEBS样本可以根据许多事件（例如缓存未命中）来选择，每个PEBS记录包含<span class="label info">时间戳、线性地址和物理地址</span>等[21]</li><li>硬件性能计数器HWPC是大多数现代CPU和GPU上可用的特殊硬件寄存器，作为性能监控单元PMU的一部分。通过事件复用，perf 和 pfmon 等软件工具可以监控比物理寄存器更多的事件。HWPC 是粗粒度的（所有进程页面的一个指标），并且不能用于获取内存访问跟踪 [25]</li><li>BadgerTrap[6]拦截TLB Miss，并且对TLB做相应修改，用采样页预估总体未命中次数。</li></ol><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>这篇文章评估了很多内存监视器，提出一种统一的方法，为分层内存提供详细、低开销的可见性。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>表 I</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>figure1系统的架构图，说明了每个部分的功能，其实就是将以上提到的组合了一下，取长补短。TMP 使用多种互补的监测方法，最大限度地提高信息量并最大限度地减少开销（表 I）</p><p>实施方案，非常细节可以对着代码看。</p><div class="note primary">cache是相当于备份了的数据，所以这里统计热度和他没有关系，但是，被cache备份的数据有多大呢？相反可以放在慢速吧。</div> <p>TMP在分层内存中的使用还是识别页面冷热，并且排序，然后确定哪些页面是可以迁移的，迁移时虚拟地址不变，页面物理地移动（这一段介绍也很细节，可以对着代码看）作者figure6的分析表明，是想要去提高第一层的命中率的。</p><div class="notedanger no-icon">Figure 2的I-TLB Miss，D-TLB Miss是虚拟页到物理页转化时的miss，这个时候cache miss应该已经发生了，这两种情况的共同点就是会去DRAM取页面。但是这和作者的策略将冷热页面放在不同内存层的关系有多大呢？也就是在慢速和快速内存的那一点点区别嘛。前面提到“对延迟的优化，可以通过命中更多的热页面，常用的优化内存延迟的方式。”（在讨论尾延迟时是因为整个差距太大了其他操作us？一次尾延迟5s）  另一方面，有什么方法可以减少这个miss吗？（以前考虑的大页面算一种，但是效果不好）</div><div class="note primary">以前的迁移都是根据热度来的，但是同上，既然这种缓存不命中对内存来说不是内存策略可以改变的，而且访问第二级内存和访问第一级内存并不是慢很多个数量级。那么为什么不从其他角度考虑，比如根据容量需求，但是容量需求的背后应该是延迟敏感带宽敏感的数据或者读写延迟敏感的部分数据，更应该被放在那些地方！！！但是站在其他论文角度，层级结构就是为了更好的利用局部性，但是PM和DRAM和传统的层级并不一样。从我自己采样的热力图来看就是会存在局部性很差的情况；从YCSB设置的访问模式来看，这样的规定也会受损。他们本质的区别还是带宽、读写延迟、持久性、能耗！但是造成延迟不同、带宽不同的本质原因又是什么?</div> <div class="notedanger no-icon">迁移会涉及TLB shootdown这个开销大？</div><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>感觉这方面还是缺失一些。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>但是整个组合会变得复杂，到底在收集信息时内核负担会不会变大反而效率更低。文章说：保持工作负载开销低于应用程序开销的 5%。<br>有的对硬件的采样是不允许的，那么这个可能有的地方收集不了数据。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> B </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Center Ethernet and RDMA Issues at Hyperscale</title>
      <link href="/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/"/>
      <url>/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE Computer Volume: 56, Issue: 7, July 2023</li><li>Data Center Ethernet and Remote Direct Memory Access: Issues at Hyperscale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Torsten Hoefler , ETH Zürich</li><li>Duncan Roweth, Keith Underwood, and Robert Alverson, Hewlett Packard Enterprise</li><li>Mark Griswold, Vahid Tabatabaee, Mohan Kalkunte, and Surendra Anubolu, Broadcom</li><li>Siyuan Shen, ETH Zürich</li><li>Moray McLaren, Google</li><li>Abdul Kabbani and Steve Scott, Microsoft</li></ul><p>使服务器应用程序可以直接操作远程服务器的内存,不需要经过操作系统和CPU。</p><p>Fabric，就是支持RDMA的局域网(LAN)。</p><p>怎样才能对内存进行传输，注册。 因为RDMA硬件对用来做数据传输的内存是有特殊要求的。</p><p>在数据传输过程中，应用程序不能修改数据所在的内存。<br>操作系统不能对数据所在的内存进行page out操作————物理地址和虚拟地址的映射必须是固定不变的。<br>注意无论是DMA或者RDMA都要求物理地址连续，这是由DMA引擎所决定的。</p><p>几十年来，以太网一直在有线局域网领域占据主导地位，范围从私人住宅的部署到最大的数据中心。数据中心经历了巨大的增长在过去的十年中，联网机器的数量超过了当今最大的超级计算机的规模。尽管仍存在一些差异，但此类超大规模数据中心和超级计算机的网络要求非常相似。[1]然而，超级计算机传统上使用专用互连进行连接，而数据中心则建立在以太网上。由于类似需求和规模经济，随着每一代新技术的产生，两者的结合不断紧密。我们认为现在是重新思考融合互连的基本假设和架构的最佳时机。</p><h3 id="数据中心以太网的新环境"><a href="#数据中心以太网的新环境" class="headerlink" title="数据中心以太网的新环境"></a>数据中心以太网的新环境</h3><p>多种技术趋势正在加速高性能互连的融合。首先，不断增长的网络性能要求推动了可支持TB级带宽的更高效的主机堆栈的发展，每秒数亿美元的交易以及新兴数据密集型应用程序（例如人工智能（AI））所需的单位微秒延迟。[2]这些极端要求迫使所有协议和硬件都尽可能高效，以至于尽可能有效排除许多传统上驱动数据中心网络的TCP/ IP状堆栈远程直接内存访问（RDMA）是大约三十年前用于高性能计算（HPC）工作负载的远程内存访问（RDMA），后来又将其扩展到InfiniBand(IB) verbs RDMA的目标存储。RDMA可以通过网络启用CPU释放的硬件加速DMA。在过去的10年中，它事实上已成为低开销和高速网络的标准。</p><blockquote><p>InfiniBand verbs 是InfiniBand架构中定义的一套编程接口(verbs),用于应用程序访问InfiniBand网络资源并利用RDMA(远程直接内存访问)技术进行通信。</p></blockquote><blockquote><p>在数据中心网络中排除传统TCP/IP栈的主要原因有:</p></blockquote><ol><li>TCP/IP栈开销大。TCP/IP栈在主机内核中运行,每次网络IO都需要经过完整的网络协议处理,包括缓冲拷贝、上下文切换、校验计算等,增加延迟和CPU使用。</li><li>不必要的内存复制。数据在应用、内核、NIC间多次缓冲拷贝,影响吞吐量和延迟。</li><li>CPU利用率高。网络栈处理占用大量CPU资源,对计算密集型应用影响很大。</li><li>通信堆栈长。TCP/IP通信堆栈包含过多层,每个层在主机内核和NIC都要处理,增大延迟。</li><li>难以实现kernel bypass。 TCP/IP难以实现应用直接控制网络硬件,无法实现kernel bypass架构。</li><li>可扩展性差。TCP难以实现应用级的可扩展性和负载均衡。 </li><li>缺乏数据中心网络特性。TCP/IP缺乏RDMA、RoCE、sr-iov、overlay网络等数据中心特性。）所以数据中心采用RDMA、userspace network stack、overlay network等技术,可以获得更高性能、弹性和效率）。</li></ol><p>如今，几乎所有超级计算机架构以及领先的数据中心提供商都在生产中使用 RDMA。几十年前对负载平衡、拥塞控制和错误处理的简单假设并不适用于当今带宽提高100倍、消息速率提高10倍以上的网络。此外，简单的RDMA网络接口卡（NIC）通常会通过附加功能进行增强。由此产生的“智能网卡”通常会减轻大量负载。服务并实施专门的网络协议。现代网络交换机还具有改进的功能，从先进的网内遥测和网内计算功能到网内负载平衡和拥塞控制。3我们认为，当前现有的标准和部署的基础设施存在根本差距必须在不久的将来解决这个问题，以支持高效的高性能网络。</p><blockquote><p>遥测数据(Telemetry Data)是指从网络设备中采集、汇报的与网络运行状态相关的数据。它通过实时反映网络的运行情况来帮助网络管理和运维。常见的网络遥测数据包括:</p></blockquote><ul><li>基础信息:设备型号、配置、软件版本等静态信息。</li><li>流量数据:接口流量速率、总流量、流量方向等。</li><li>QoS数据:接口队列长度、延迟、丢包、拥塞情况等。 </li><li>路由信息:路由表、下一跳等动态路由信息。</li><li>会话信息:活动的会话连接数、来源/目的地等。</li><li>资源利用率:CPU使用率、内存使用率、链接利用率等。</li><li>环境数据:设备温度、风扇速度等环境信息。</li><li>事件和警报:设备故障、链路中断等事件信息。<br>遥测数据可以通过SNMP、NETCONF等管理协议获取,也可以通过流式遥测技术像gRPC Streaming、 Kafka等机制订阅获取。收集到的遥测数据可以用于网络状态分析、事件检测、容量规划、流量工程等。</li></ul><blockquote><p>网内遥测(In-band Telemetry, INT)是一种网络遥测技术,它可以在网络数据包中携带遥测数据,并随着数据包传递通过网络。网内遥测的主要特征和优势包括:</p></blockquote><ul><li>遥测数据直接嵌入数据包中,不需要额外的控制消息,更加高效。</li><li>可以提供数据包在网络中的实时状态,如延迟、丢包等信息。</li><li>可以细粒度地反映网络状态,每一个数据包都是探针。</li><li>可以快速发现网络热点,进行负载均衡和故障定位。</li><li>无需专用监控网络,不会占用额外带宽。</li><li>可以配合软件定义网络(SDN)实现可编程的遥测控制。<blockquote><p>网内遥测通常需要数据平面支持,在交换机或网卡中实现遥测头插入和解析。通过网内遥测技术,可以极大地增强网络的可观测性, 一些主流的网内遥测技术包括: </p></blockquote></li><li>INT: IETF正在推进的INT标准。</li><li>IOAM: In-situ OAM,由Cisco推出。</li><li>P4INT: 基于P4语言的可编程INT实现。</li><li>iOAM: IOAM的衍生协议。</li></ul><h3 id="RoCE：融合还是胶带"><a href="#RoCE：融合还是胶带" class="headerlink" title="RoCE：融合还是胶带"></a>RoCE：融合还是胶带</h3><p>RoCE（RDMA over Converged Ethernet）是可以在Ethernet网络上运行RDMA的网络协议。其主要特点如下:</p><ul><li>RoCE在Ethernet上实现了RDMA功能,使RDMA不再只局限在专用的InfiniBand网络上。</li><li>在IP/Ethernet网络基础设施上,通过对数据平面进行改进来实现RDMA。</li><li>支持两种传输方式:RoCEv1使用UDP封装;RoCEv2使用一种特殊的以太网帧格式。</li><li>RoCEv1依赖数据中心级别的Lossless Ethernet技术来实现可靠传输。RoCEv2新增了自身的流控机制。</li><li>RoCE可以获得与InfiniBand接近的低延迟和高吞吐性能。</li><li>与iWARP相比,RoCE更加依赖硬件卸载,实现CPU利用率更低。<br>RoCE的优势在于兼容现有的以太网网络,使RDMA应用更易于部署,不再需要专门的IB交换机网络。RoCE已得到广泛支持,是数据中心采用RDMA的主流选择之一。</li></ul><p>传统上，当交换机缓冲区已满时，以太网会丢弃数据包，并依赖于端到端重传。为了支持RoCE，CE引入优先流控制（PFC）来实现链路级无损操作。PFC重新利用以太网中存在的以太网暂停帧来支持具有不同链路传输速率的网络。PFC增强暂停帧以停止（或限制）特定优先级上的流量，以避免数据包丢失。不幸的是，这套复杂的协议会干扰网络中的不同层，并降低当今一些最重要工作负载的效率。</p><p>RoCE的语义、负载平衡和拥塞控制机制继承自InfiniBand。这意味着所有消息都应该按顺序出现在目的地，就好像它们是通过静态路由传输一样，本质上不允许许多数据包级负载平衡机制。对于人工智能训练工作负载（长期存在的流），多路径机制可以大大缩短作业完成时间。此外，RoCE v2使用基于IP显式拥塞通知（ECN）的简单拥塞控制机制。当检测到拥塞时，兼容 ECN 的交换机会对数据包进行标记，接收方会将该信息转发回发送方，从而在单个参数的引导下降低其注入率。无拥塞期后，使用第二个配置参数再次自动提高速率。</p><h3 id="下一代高性能网络"><a href="#下一代高性能网络" class="headerlink" title="下一代高性能网络"></a>下一代高性能网络</h3><p>对于某些工作负载，消息延迟（有时是消息速率）起着核心作用。其中一些属于 OBS 类别，但其他一些具有复杂的数据相关消息链，形成应用程序中的关键性能路径。这些通常是强大的扩展工作负载，解决问题的时间很重要，并且必须容忍低效的执行。具有严格期限的大规模模拟（例如天气预报和石油勘探）属于这一类，但一些事务处理和搜索/推理工作负载也属于这一类。在这里，通常具有严格的（个位数微秒）延迟要求。</p><p>除了流量类型之外，部署环境也在发生变化。新出现的机密计算理念要求所有流量在线路上进行加密。理想情况下，流量在安全飞地中进行端到端加密和解密，并且没有网络设备（NIC 或交换机）值得信任。此外，相关的新兴多租户场景需要管理来自单个主机的数万个连接。这些通常由智能 NIC 提供支持，通过管理资源（例如带宽和安全性）虽然有速率限制和过滤。此外，需要更先进的负载平衡和路由的新的经济高效、小直径和专用拓扑成为极端带宽部署的必要条件。2,8 这些要求的许多组合对下一代高性能网络。</p><h3 id="RoCE存在的问题"><a href="#RoCE存在的问题" class="headerlink" title="RoCE存在的问题"></a>RoCE存在的问题</h3><p> 关于论文中提出RoCE需要改进的8个方面的问题,我总结如下:</p><ol><li> congestion control (拥塞控制)。RoCEv1没有拥塞控制机制,需要依赖DCQCN。RoCEv2虽有拥塞控制但需要进一步完善。</li><li>physical layer (物理层) 。RoCE对PHY层时钟同步和链路断开检测还需改进。</li><li>path MTU (路径MTU)。RoCE需要更好处理不同MTU路径的情况。</li><li> flow steering (流导向)。需要更好的QoS和流量工程能力来导向不同优先级的RoCE流量。</li><li> resilience (弹性) 。如何改善RoCE的故障恢复能力需要进一步研究。</li><li> labeling (标签)。RoCE当前还不支持MPLS等标签交换技术。</li><li> standards (标准)。需要更多针对RoCE在数据中心使用的标准化工作。</li><li> debugging (调试) 。RoCE网络故障定位和性能诊断工具需要加强。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards an Adaptable Systems Architecture for Memory Tiering at WarehouseScale</title>
      <link href="/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/"/>
      <url>/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS, 2022</li><li>Towards an Adaptable Systems Architecture for Memory Tiering<br>at Warehouse-Scale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>all Google. Padmapriya Duraisamy, Wei Xu, Scott Hare etc.</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>当前的系统实现使用英特尔傲腾持久内存的变体在速度较慢、成本较低的第 2 层中提供 25% 的总内存 [2]。该设计点在性能下降的限制下最大限度地降低成本，并进一步受到离散 DIMM 容量产品的限制。即使是 25%，考虑到部署规模，上行空间也是巨大的。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>介绍了非常规分层内存系统的设计和实现及其管理，该系统在生产中实现了 &lt;5% 性能影响的目标，在多样化、高动态、多租户中用速度慢得多的层替换 25% 的 DRAM仓库规模设置。</p><p>定义内存分层的机器级优化指标，用于自适应地平衡复杂的全机群应用性能和利用率目标（第 2 节）。</p><p>引入了用于大规模实时复杂系统评估的强大 A/B 测试方法（第 4 节）。</p><p>首次对生产仓库规模环境中可直接访问的分层内存系统进行全面分析，成功服务于不同的应用程序类别（第 5 节和第 7 节）。</p><p>评估一系列政策并证明硬件辅助事件分析满足性能要求（第 6 节）的有效性，确定主动降级和快速检测晋升候选人的重要性。</p><p>揭示了地址转换开销、干扰影响和页面大小问题如何在布局优化后成为关键挑战，并呼吁采用新的 malloc 级技术来减少 “访问碎片”，尤其是在使用 hugepages 时。</p><p>利用实时 A/B 测试机制，以实证为导向开发自适应分层感知集群调度，在最大限度提高利用率的同时减少对性能的影响，以及应用指导下的分层感知巨大页面管理，以减少访问碎片并提高第 2 层的利用率（第 8 节）。</p><p>我们的经验凸显了大规模管理内存层的复杂性，其中工作负载行为可能每天都会发生变化，或者稳定数周后突然发生变化。我们相信，这种系统设计和用于捕获大规模实时多样化工作负载的复杂交互影响的方法将开辟新的、日益重要的研究途径。</p><p>与内存分层相关的应用服务可分为两类–高重要性延迟敏感型（HILS）和其他（非 HILS）。HILS 包括对响应时间有严格要求的面向用户的应用、处于其他 HILS 应用关键路径上的缓存应用以及数据处理任务（Production Tier，见 [48 ]）。非 HILS 包括面向吞吐量的应用、批处理、ML 训练管道和其他 SLO 要求较弱的应用。这些级别通常位于同一台机器上。Borg 调度器会根据观察到的性能在集群中主动管理作业，内存分层也会对性能产生影响。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>为了更好地分析分层堆栈，我们定义了两个直接连接到分层架构本身的代理指标： • 次要层驻留率(STRR)是驻留在第2层的已分配内存的比例。它提供了有关层使用情况的标准化视角。 STRR充当衡量对利用率影响的代理。<br>• 次要层访问率(STAR)是针对驻留在第2层的页面的应用程序的所有内存访问的比例。较低的STAR意味着较低的性能影响。STAR充当应用程序性能下降的代理。</p><p>从用户空间驱动策略决策，并利用内核机制执行策略决策</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Merchandiser Data Placement on Heterogeneous Memory for TaskParallel HPC Applications with LoadBalance Awareness</title>
      <link href="/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/"/>
      <url>/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自PPoPP，2022</li><li>Merchandiser: Data Placement on Heterogeneous Memory for Task-Parallel HPC Applications with Load-Balance Awareness </li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Zhen Xie，University of California, Merced Argonne National Laboratory</li><li>Jie Liu，University of California, Merced</li><li>Jiajia Li，North Carolina State University</li><li>Dong Li，University of California, Merced</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ea9867b9f36043af9a699eb42ec35696.png"></p><p>图1.a给出了一个基于MPI的任务并行应用的例子。在DMRG中，一个哈密顿矩阵首先被分割成多个块，每个块被分配给一个MPI 进程(1-3行) 。然后每个MPI进程运行一个计算循环，作为输入(第5-7行)循环的一次迭代被认为是1个任务实例。因此，MPI进程中的任务是重复执行的。在每个迭代结束时，有一个全局的MPI进程之间的同步。</p><p>图1.b给出了1个基于OpenMP的任务并行应用的例子，一个主循环运行了许多SpGEMM (C=A*B) 。在主循环的每一次迭代中，A首先被分割成若干个bins，部分 A*B 得到部分C</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2d6f15b49475435ba52e45722d3e969f.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>Input-Aware Memory Access Quantification<br>既然是从任务的角度来解决问题，那么如何确定什么样的数据放入快速内存中？<br>连串的、一定步长的，需要考虑左右两边数据的、随机的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519a7d4c0e2149bf9fd8ddbdd870afcc.png"></p><p>阿尔法的值，根据步长和数据类型来确定，由作者提前枚举好了的。综上给了两种方法来判断输入数据的将来被访问量，一种是用户直接通过API指定，如果没有指定的情况就是使用公式计算，依据具体计算方式设置阿尔法。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b690c63eb3a3414991abdf491177a583.png"></p><p>迁移的决策模型应该是，作者的想法就是怎么把数据放置在不同PM的敏感性展现出来，区分出数据在不同异构放置的差别。<br>时间上的预测完全参考13年的一篇论文<br>梯度提升回归：每个学习算法准确率都不高。但是它们集成起来可以获得很好的准确率。这些学习算法依次应用。也就是说每个学习算法都是在前一个学习算法的错误中学习.</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/89e360308c2b400b8d7318f8e4b2efc6.png"></p><p><strong>性能模型的设计</strong><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/03757318787c480091970cfc3d2290a2.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transparent Page Placement for CXL Enabled Tiered Memory</title>
      <link href="/2023/08/19/Transparent-Page-Placement-for-CXL-Enabled-Tiered-Memory/"/>
      <url>/2023/08/19/Transparent-Page-Placement-for-CXL-Enabled-Tiered-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS，2023，CCFA</li><li>TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hasan Al Maruf，Mosharaf Chowdhury – University of Michigan密歇根大学</li><li>Hao Wang，Niket Agarwal，Pallab Bhattacharya – NVIDIA</li><li>Abhishek Dhanotia，Johannes Weiner，Chris Petersen，Shobhit Kanaujia，Prakash Chauhan – Meta Inc.脸书</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><ul><li>CXL [7] 是一种基于 PCI Express（PCIe）接口的开放式、业界支持的互连。<br>它支持主机处理器和设备（例如加速器、内存缓冲区、智能 I/O 设备等）之间的高速、低延迟通信。</li><li>CXL 在同一物理地址空间中提供字节可寻址内存，并允许使用标准内存分配 API 进行透明内存分配。</li><li>CXL-Memory 访问延迟也与 NUMA 访问延迟类似。</li></ul><p>这篇文章将直接附加到 CPU 的内存称为本地内存，将 CXL 附加内存称为CXL-内存。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5aac1cd90eef40299199fc6d08e7b1f0.png"></p><blockquote><p><strong>补充：</strong>现在CXL同时支持多种内存的应用案例都是DRAM和HBM（是一种高带宽内存 High Bandwidth Memory）。HBM更适用于对带宽要求极高的场景,如高端GPU、AI加速卡等。HBM单片容量较小(4-16GB),但带宽巨大(超过500GB/s)。</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>现实实践中面对的问题：<br><strong>数据中心应用程序的内存需求增加</strong>在META公司中每一代硬件内存消耗的能源和总花费占比都在不停增长。</p><p><strong>同类服务器设计中的扩展挑战</strong>内存控制器仅支持单代内存技术，这限制了不同技术的混合搭配，具有不同的每 GB 成本和带宽与延迟配置。大内存容量都是2的几次方。限制了细粒度；每一代服务器的带宽和容量都有限制。图4</p><p><strong>数据中心应用程序的轻量级表征</strong>现有的工具会导致较高的 CPU 开销（每个核心超过 15%），并且通常会减慢应用程序的速度。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/149d075453ee4bfb9e5e3e7214a92947.png"></p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="5-1采样数据工具"><a href="#5-1采样数据工具" class="headerlink" title="5.1采样数据工具"></a>5.1采样数据工具</h3><p>Chameleon 的主要用例是了解应用程序的内存访问行为，即应用程序内存的哪一部分保持热-温-冷状态、页面在特定温度层上存活多长时间、访问它们的频率等等。长期内存。</p><p>收集器做两类采样，放入hash表，定时唤醒。<br>收集器唤醒工作线程以处理当前哈希表中的数据，并移动到另一个哈希表以存储下一个间隔的采样数据<br>工作器处理哈希表里的数据，统计每个页面的位图大小是64位，生成报告后休眠。</p><blockquote><p>load是从内存读取数据到处理器的寄存器中的指令,而store则是将寄存器中的数据写入内存的指令。两者都是与内存打交道的指令,但方向不同,load是内存到寄存器,而store是寄存器到内存。</p></blockquote><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5c874a20c38e4191b939a5ef0ca59536.png"></p><p>我们使用 Chameleon 来分析我们生产中运行的跨不同服务域的各种大型内存绑定应用程序，并进行以下观察。 </p><ol><li>访问的内存的很大一部分在几分钟内保持冷状态。我们可以将其卸载到慢速层内存，而不会对性能产生重大影响。 </li><li>大部分匿名内存（为程序的堆栈、堆和/或 mmap 调用创建）往往更热，而大部分文件支持的内存往往相对更冷。</li><li>页面访问模式在有意义的持续时间（分钟到小时）内保持相对稳定。这足以观察应用程序行为并在内核空间中做出页面放置决策。 </li><li>工作负载对不同页面类型（文件和匿名页面）具有不同程度的敏感度，并且随着时间的推移而变化。</li><li>冷页重新访问时间因工作负载而异。分层内存系统上的页面放置应该意识到这一点，并主动将热页移动到较低的内存节点，以避免高内存访问延迟。于Web而言，几乎80%的页面会在 10 分钟内被重新访问。</li></ol><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ec7f0938b1844787b423dbe944b29744.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/927aa0606137423b80bc79092b10fdca.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f7c7b158461d442ea37c3eb4d5b9d7f9.png"></p><h3 id="5-2架构设计"><a href="#5-2架构设计" class="headerlink" title="5.2架构设计"></a>5.2架构设计</h3><p>TPP 将“较热”页面放置在本地内存中，并将“较冷”页面移动到 CXL 内存中。<br>TPP 的设计空间可分为四个主要区域：</p><ul><li>(a) 轻量级降级到 CXL-Memory，</li><li>(b) 解耦分配和回收路径，</li><li>(c) 热页升级到本地节点，</li><li>(d) 页面类型感知内存分配。</li></ul><p> 一般来所NUMA系统的回收顺序是：首先回收本地节点上不活跃的页面；然后回收远程节点上不活跃的页面；如果还不够,才考虑回收本地节点上最近最少使用的页面。所以在图的上面①会将本地回收候选页面迁移到XCL节点的降级列表中，除非本地节点的容量小于工作集大小的热部分，否则在回收期间热页面迁移到 CXL 节点的机会非常低。如果降级期间的迁移失败（例如，由于 CXL 节点上的内存不足），我们将回退到该失败页面的默认回收机制。</p><p> Linux为节点内的每个内存区域维护三个watermakes（最小、低、高）。如果节点的空闲页面总数低于low_watermark，Linux会认为该节点面临内存压力并启动该节点的页面回收。在我们的例子中，TPP将它们降级为CXL节点。对本地节点的新分配将停止，直到回收器释放足够的内存以满足high_watermark。<span class="label info">由于分配率较高，回收可能无法跟上</span>，本地内存分配频繁停止，<span class="label info">更多页面最终出现在CXL节点中</span>，最终降低应用程序性能。所以现在将回收和分配化为不同的门槛②，回收和驱逐会提早发生，而禁止页面分配会在内存饱和度更高一步的时候。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d51196b572b849d48550379551aa0801.png"></p><p>NUMA系统中CPU访问另一节点页面时,生成缺页异常提醒内核进行页面迁移。不常访问的页面产生的升级流量很容易填满本地节点的空闲空间，并为CXL节点产生更高的降级流量。于是通过页面在LRU列表中的位置来检查页面的年龄。如果故障页面处于非活动LRU状态，我们不会立即考虑该页面进行升级，因为它可能是不经常访问的页面。仅当在活动 LRU 中找到故障页面（图13中的①）时，我们才将其视为升级候选页面。这大大减少了升级流量。</p><p>然而，操作系统使用LRU列表进行回收。如果内存节点没有压力并且回收没有启动，则非活动LRU列表中的页面不会自动移动到活动LRU列表。由于CXL节点可能并不总是处于压力之下，因此经常可以在非活动LRU列表中找到缺页的页面，又会没法向上迁移。为了解决这个问题，每当我们在非活动LRU列表中发现缺页异常时，我们都会将该页面标记为已访问，并立即将其移至活动LRU列表（图13中的②）。如果在下一次NUMA提示缺页异常，则它将处于活动LRU中，并提升到本地节点（图13中的③）。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>因为工作负载就这些特性，那就直接对症下药，效果都挺明显的。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>但是换了workload就没这个效果了。按照这种方式，每家都得自己去设计啰。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>是根据工作负载来设计架构的，关注的问题点比较小，也就容易做到很细致出色吧。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>一个奇怪的现象，虽然大家可能观察值不同，但是最后系统设计都比较像，好像这两部分可以分裂一样。<br>那个Chameleon工具，说是开源但是并没有，之后可以再去看看。<br>TMO: Transparent memory offloading in datacenters. In ASPLOS, 2022.可以看看。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</title>
      <link href="/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/"/>
      <url>/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><p><strong>文章来自23年7月TechRxiv</strong><br>Performance, Energy and NVM Lifetime-Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</p></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Manolis Katsaragakis,微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院,鲁汶大学(KU Leuven)比利时</li><li>Christos Baloukas, Lazaros Papadopoulos, 微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院</li><li>Francky Catthoor,鲁汶大学微电子研究中心</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>为了有效利用 DRAM/NVM 异构内存系统，多年来已经提出了几种数据放置算法 [15]、[16]。尽管数据放置算法通常很复杂，并且能够有效地利用正在执行的应用程序的复杂内存层次结构，但它们的结果通常受到以下事实的限制：<span class="label info">原始应用程序通常是为 DRAM 而不是为异构内存设计的</span></p><p>作者通过一些实验展示了数据组织改进的应用程序级方法对异构 DRAM/NVM 系统上数据放置算法结果的影响（作为整个论文的动机）。其中有两个新的概念（这两个概念也对后续设计有很大影响，但是这两个概念并不常见）：</p><ul><li><span class="label danger">动态数据类型细化方法 Dynamic Data Type Refinement methodology（DDTR）</span>（D. A. Alonso, S. Mamagkakis, C. Poucet etc. Dynamic memory management for embedded systems. Springer, 2015）</li><li><span class="label danger">帕累托最优 Pareto Optimal</span></li></ul><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>异构内存中可以从数据组织的角度改进数据放置，所提出的方法旨在满足三个主要目标：<br>(i) 展示通过应用程序级优化实现的数据放置算法的改进结果。<br>(ii) 基于多个目标（性能、能耗、对 NVM 寿命的影响）评估布局解决方案。<br>(iii) 提供可用性、可扩展性和可扩展性特征（例如，即使对于相对较大的设计空间，也有合理的探索时间，并支持各种应用领域、数据放置算法和真实或模拟的 NVM 技术）。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><ul><li>几种数据放置算法 [15]、[16]</li><li>应用级数据优化方法[18]、[19]</li><li>库支持 [13]、[17] </li><li>根据布局粒度级别进行分类：数据结构、内存对象或内存页 [23] [15] [24] [25][23] 的作者提出了一种在线配置文件引导的数据分层解决方案，涉及异构内存系统的页面粒度放置，旨在提高 HPC 应用程序的性能。其他指示性方法包括静态代码检测工具，用于自动执行内存对象放置以实现性能和能源优化[15]。在数据结构放置粒度上，[24]中提出了缓存和非缓存 NVM 的写感知数据结构放置。 [25] 中提出了基于人工智能的页面粒度数据放置。</li><li>异构 DRAM/NVM 系统上的应用程序域特定布局：最近还研究了应用程序域特定布局方法，目标是与领域无关的布局算法相比获得更好的结果。这些领域包括与数据库相关的工作负载[26]、大数据应用程序[27]、[28]、基于图的[29]、[30]、[31]和基于深度神经网络的应用程序[32]、[33]。</li></ul><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/85597fd358cb4e0882741b4808f25738.png"></p><p>图2显示了所提出的方法的概述。该方法的输入是目标应用程序的源代码。它由三个步骤组成：</p><ol><li><p><strong>数据组织的优化</strong>，此步骤的主要目标是有效选择应用程序的数据结构实现，以最大限度地减少数据访问次数和内存占用。实现方式是源代码的原始数据结构被库的数据结构替换。（仅限于基于列表的数据结构实现的 C++ 标准模板库 (STL) 兼容变体。）<span class="label default">这一步生成一组帕累托最优解</span>,每一个都对应于正在优化的应用程序的数据结构实现的不同组合。</p></li><li><p><strong>内存对象分析</strong>，基于内存跟踪和分析工具，<span class="label default">分析上一步提供的应用程序的帕累托最优版本</span>。通过平台感知采样和分析 (2b)，我们收集每个对象的内存跟踪，包括：加载和存储操作、对象大小和 LLC 未命中。还可以根据所选数据放置算法的输入要求来监视其他指标。</p></li><li><p><strong>异构内存系统上的放置和评估</strong>，模型接收（i）每个内存页、对象或数据结构的分析信息和（ii）内存规格作为输入，例如读/写延迟、读/写能耗和内存容量（3b）。每个应用程序版本都根据所选的数据放置算法部署在异构内存系统上，并执行。为了监控执行时间、能耗和NVM写入次数，需要实时监控（3e）。应用程序版本在真实或模拟的异构存储器系统上放置和执行。开发人员可以在性能、能耗和 NVM 写入访问次数之间进行权衡，并选择满足设计约束的帕累托最优解决方案。</p></li></ol><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>所提出的方法根据以下标准进行评估：<br>● DDTR 等高级数据组织优化方法对数据放置结果的影响，包括性能、能耗和对 NVM 寿命的影响。<br>● 该方法可以在多大程度上有效地集成具有不同优化目标的各种数据放置算法以及各种内存技术（模拟的或真实的硬件）。<br>● 当应用于具有相对大量数据结构的应用程序时，该方法在探索时间方面的可扩展性。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ul><li>相当于需要人工干预去做这个优化。</li><li>而且优化过程依赖帕累托（来自财经领域）最优和DDTR（15年一篇论文），这是以前没有过的应该，那么到底有多可靠呢？审稿方对这些态度咋样呢？</li><li>评估方面，没有和其他的工作比较，也看不出什么优势emmm。</li></ul><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>没有任何相关工作研究应用级数据优化和异构内存系统上的数据放置之间的相互作用，以提高性能、能耗和对 NVM 寿命结果的影响。提出了一种内存管理方法，该方法结合了动态数据结构细化步骤以及异构内存系统上的放置算法。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><ul><li>最开始对程序数据组织优化应该是离线完成的，<a href="https://github.com/mkatsa/DDTR-DRAM-NVM">可能可以作为参考</a>。</li><li>可以考虑的workload：从 Shark ML 库 [42] 和 Chrono 物理引擎 [43] 中选择了五个代表性应用程序。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Rxiv </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>P2Cache An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</title>
      <link href="/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/"/>
      <url>/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li><strong>文章来自HotStorage 23</strong> </li><li>P2Cache:An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Dusol Lee, Inhyuk Choi, Chanyoung Lee, Jihong Kim, 首尔大学Seoul National University</li><li>Sungjin Lee, 大邱庆北科学技术院DGIST</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p><span class="label info">大多数操作系统在主机DRAM内存中使用页面缓存，以利用I/O访问的局部性。</span>但页面缓存的高速缓存管理策略可能与应用程序的特定I/O特征不太匹配。如果应用<span class="label info">程序随机访问I/O地址空间，则页面缓存使用的标准读取策略可能无效。</span></p><p>现有缓存策略的局限性——评估三种常见技术：</p><h3 id="2-1操作系统级缓存"><a href="#2-1操作系统级缓存" class="headerlink" title="2.1操作系统级缓存"></a>2.1操作系统级缓存</h3><p>采用 LRU 替换策略与预读算法相结合，如果应用程序具有中等局部性，通常可以实现较高的命中率 [11]。现有的数据密集型应用程序通过<span class="label info">高度定制的算法处理大量数据</span>，从而导致复杂的 I/O 模式。不幸的是，由于其通用设计，操作系统级页面缓存通常<span class="label info">无法捕获各个应用程序的独特行为</span>，从而即使可以实现更高的命中率，也无法提供次优的性能。</p><p>下图是Lumos [2]，执行图形处理算法 - Pagerank [15]的IO模式和性能趋势（它使用专门的数据结构和优化技术来优化图形处理引擎，因此生成的 I/O 访问模式非常复杂。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2cc466ecd6cb409bb417d9e68a446c3b.png" alt="数据集大小超过系统内存后，应用程序开始遭受缓存抖动，导致性能严重下降（参见图 1(b) 中的基线）"></p><ul><li>性能低下是由于操作系统级页面缓存的内存管理无效，该缓存使用 LRU 和预读策略，而不考虑输入工作负载模式。操作系统级页面缓存优先驱逐最近最少引用的页面，<span class="label danger">但这些页面实际上很快就会再次被引用，特别是在循环 I/O 模式下。</span></li><li>可以观察到，使用具有小数据固定的 MRU 策略（而不是 LRU）会带来更高的性能，如图 1 中的 MRU+PIN 所示，性能提高了 25%。然而，很难改变内部的情况。适应输入工作负载的内核缓存替换策略。</li></ul><h3 id="2-2基于提示的操作系统级缓存"><a href="#2-2基于提示的操作系统级缓存" class="headerlink" title="2.2基于提示的操作系统级缓存"></a>2.2基于提示的操作系统级缓存</h3><p>作为一种替代方案，一些应用程序（例如 GridGraph [1] 或 SQLite [7]）尝试通过 fadvise [16] 和 madvise [17] 向内核提供应用程序级提示来更好地管理缓存数据。在保留内核级缓存管理相同优点（强大的数据保护和高效的数据共享）的同时，它能够<span class="label info">通过在应用程序代码中嵌入重要的缓存管理决策</span>（例如，WILLNEED、SEQUENTIAL、DONTNEED）来实现更高的缓存命中率。</p><p>然而，它也有缺点。首先，它需要在修改现有应用程序代码方面付出巨大的努力。其次，仅通过注入提示很难精细控制内核级页面缓存。我们在代码中精心添加了提示信息，以MRU方式管理内核缓存。修改后的版本表现出更高的命中率，但性能仍然比使用 MRU 慢得多（参见图 1 中的 FADV）。这是因为缓存抖动。 </p><h3 id="2-3用户级缓存"><a href="#2-3用户级缓存" class="headerlink" title="2.3用户级缓存"></a>2.3用户级缓存</h3><p>为了减轻I/O特征（应用程序）和（内核级页缓存）策略之间的不匹配问题，数据密集型应用程序经常在应用程序级别[3-7]上实现自己的页面缓存。</p><ul><li>受内核缓存策略干扰，驱逐了有用的页面，违反了应用程序的意图。</li><li>无法利用内核的保护和共享功能。<br>下图使用 Simrank [19] 进行了实验，这是一个具有自己的用户级缓存的图形应用程序。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2439afe15dd740a6917ea14962b24687.png"><br>如图 2（a）所示，Simrank 的 I/O 参考模式大多是随机的，因为它将流行数据缓存在用户级缓存中。由于这种特定于应用程序的管理，对于相同的数据集，Graph-Walker 表现出比 Lumos 更高的性能（见图 2(b)）。然而，当内存不足时，GraphWalker 的性能下降幅度比 Lumos 更高。</li></ul><h2 id="3-解决的问题"><a href="#3-解决的问题" class="headerlink" title="3. 解决的问题"></a>3. 解决的问题</h2><p>内存密集型程序的I/O更加具有特性，内核级页面缓存由于无法考虑特定于应用程序的 I/O 模式而无法提供高性能。应用程序级提示可以缓解该问题，但与最佳效果相比，效果有限。虽然用户级自定义缓存可以高效工作，但它无法利用内核的基础设施，并且会因内核干预而导致性能下降。于是做了一个<strong>允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE</strong>。 </p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>如果直接实现了用户级页面缓存（例如，如Jaydio [8]或RockSDB的Direct-io [9]），则可能无法对某些内核功能进行介入，例如用于确保数据保护和数据一致性的功能。更重要的是，如果SSD或主机存储系统发生重大变化，则需要重新实现用户级缓存。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1392a78bce7340b886bd89b864b1e283.png"><br>图 3 显示了 P2Cache 的操作概览。要为应用程序创建自定义页面缓存，可能需要应用程序的特定于应用程序的数据来开发新的缓存策略。如果需要，数据会被移动到内核的受保护内存(1)。为了避免其他应用程序对数据进行未经授权的访问，为每个自定义页面缓存分配了一个密码(passwd)。使用P2C API函数以及应用程序的内核数据，为P2Cache的每个探测点实现一个eBPF程序 (2)。将eBPF程序加载到各自的探测点(3)后，只要内核的执行流到达这些点，就会执行eBPF程序，应用程序的自定义页面缓存就会生效。在执行eBPF程序之前，扩展的eBPF VM会验证程序是否有权访问应用程序拥有的内核数据以及特定于应用程序的数据。这是通过将eBPF程序的passwdprobe与从相应应用程序传递来的passwd进行比较来实现的。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，使用我们的P2Cache实施的Cusmom Page缓存可在数据密集型图应用中提高32％的性能，内存容量低于数据集容量时也是相比其他方案性能下降更缓慢。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>做了一个允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE，别人的都是应用层级的。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><blockquote><p>数据密集型工作负载：Lumos [2] and GraphWalker [4]。目前使用的工作负载，容量如果内存不能容下，则会杀死进程，这里使用这两个工作过负载还可以使得Memory Size / Dataset Size成比例。Lumos 维护多个文件并同时扫描它们，将混合 I/O 模式发送到磁盘。 Lumos 还使用多个元数据文件并重复读取它们，从而产生高度本地化的 I/O 模式。</p></blockquote><blockquote><p>2.2节的实现手段可以模仿。eBPF程序也能去影响内核。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> O </tag>
            
            <tag> Cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Coalescing for Multi-Grained Page Migration</title>
      <link href="/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/"/>
      <url>/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>Coalescing for Multi-Grained Page Migration</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>华中科技大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>观察结果1，一些热页在虚拟地址和物理地址上都有连续性。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>Tamp使用DRAM作为数据缓冲器来缓存NVM中多种尺寸的热页(所谓的多粒度页)。相应地使用分裂的超级页TLB和多粒度TLB来分别加速NVM和DRAM的地址转换。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>每一个idea都有对应的问题要解决，最后嵌套太多了，所以投的刊物可能会不好。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> O </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ACM Transactions on Architecture and Code Optimization, (TACO), 2019</li><li>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>XIAOYUAN WANG, HAIKUN LIU, XIAOFEI LIAO, JI CHEN, HAI JIN, YU ZHANG, and LONG ZHENG, 华中科技大学</li><li>BINGSHENG HE, 新加坡国立大学</li><li>SONG JIANG, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>在大内存系统中，<strong>超级页一直被用来减轻地址转换开销</strong>。 然而，在由DRAM和NVM组成的混合存储系统中，<strong>超页面往往会阻碍轻量级页面迁移，而轻量级页面迁移对性能和能量效率至关重要</strong>。 </p><blockquote><p>Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and NVM.</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><ol><li>如果大多数内存引用<strong>分布在超级页的一个小区域中</strong>，那么以超级页粒度（例如2MB）进行的页迁移会导致DRAM容量和带宽的巨大浪费，从而导致无法承受的性能开销。 成本可能比超级页面迁移的好处还要大。 这给超级页面的使用带来了一个困境，因为<strong>轻量级页面迁移可能会超过扩展TLB覆盖的好处</strong>。 </li></ol><blockquote><p>However, page migration at the superpage granularity (e.g., 2MB) can incur unbearable performance overhead due to a vast waste of DRAM capacity and bandwidth if most memory references are distributed in a small region ofthe superpage (see Section 2.2). The cost may be even larger than the benefit of superpage migration. This presents a dilemma for the use of superpages,<br>since the lightweight page migration can outweigh the benefits of extended TLB coverage.</p></blockquote><p><a href="https://www.jianshu.com/p/bea989a85a31">累积分布函数图怎么看</a><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d3b808800554ed3b2d0a153353c4e26.png" alt="2MB超级页的累积分布函数与给定区间(108个周期)内一个超级页中被触及的4KB小页的数量"><br>从图中可以看到，很大一部分工作负载有80%以上的概率：2MB的页面中被访问的4kb页面只有12.5%。<br>还有一张表格来说明问题：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/abbbcb642622478cb35fd087610a7f69.png" alt="4kb热页访问统计"><br>使用的工作负载：</p><ul><li><a href="https://www.spec.org/cpu2006">SPEC CPU2006</a></li><li><a href="http://parsec.cs.princeton.edu/index.htm">Parsec</a></li><li><a href="http://www.cs.cmu.edu/%E2%88%BCpbbs/">Problem Based Benchmarks Suit（PBBS）</a></li><li><a href="http://whitedb.org/">WhiteDB</a></li><li><a href="https://redis.io/">Redis</a></li><li><a href="http://graph500.org/">Graph500</a></li><li><a href="http://www.netlib.org/benchmark/">Linpack</a></li><li><a href="https://www.nas.nasa.gov/publications/npb.html">NPB-CG</a></li><li><a href="http://icl.cs.utk.edu/hpcc/">HPC Challenge Benchmark GUPS</a></li></ul><ol start="2"><li><strong>轻量级热页的标识</strong>：为了支持轻量级页迁移，大量工作提倡通过内存控制器监视内存访问。 然而，当<strong>主存容量变大时</strong>，以每页粒度（即4KB）使用<strong>访问计数器会导致高得令人望而却步的存储开销</strong>。</li></ol><blockquote><p>Identification oflightweight hot pages: to support lightweight page migration, a large body of work advocates monitoring memory accesses through the memory controller [55, 63]. However, using access counters at per-page granularity (i.e., 4KB) leads to prohibitively high storage overhead when the capacity of main memory becomes large.</p></blockquote><ol start="3"><li>轻量级页面迁移对TLB覆盖率的影响：页面迁移通常会<strong>分割超级页面</strong>，从而<strong>破坏物理地址的连续性</strong>。 </li></ol><blockquote><p>Impact oflightweight page migration on TLB coverage: page migrations often fragment superpages and thus break the physical address continuity.</p></blockquote><ol start="4"><li>热页寻址效率：由于热页占应用程序内存引用的主要部分，因此必须进一步<strong>减少DRAM中那些热页的地址转换开销</strong>。 </li></ol><blockquote><p>Efficiency of hot pages addressing:ashot pages<br>contribute to a major portion of applications’ memory references, it is essential to further reduce<br>the overhead of address translation for those hot pages in the DRAM.</p></blockquote><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的工作主张分割超级页面以实现轻量级内存管理，如页面迁移和共享，同时牺牲地址转换的性能[37,58]当<strong>超级页面中的热小页面迁移到DRAM时，保持改进的TLB覆盖率</strong>仍然是一个挑战。</p><blockquote><p>Previous work has advocated splintering superpages to enable lightweight memory management such as page migration and sharing, while sacrificing the performance of address translation [37,58]. It is still a challenge to retain the improved TLB coverage when the hot small pages within superpages are migrated to the DRAM.</p></blockquote><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>针对上述问题，提出了一种新的内存管理机制Rainbow, Rainbow在Superpage粒度上管理NVM，并使用DRAM在每个Superpage内缓存频繁访问（热）的小页面。相应地，Rainbow利用拆分TLB[2,7,30,52]的可用硬件特性来支持不同的页面大小，其中一个TLB用于寻址超级页面，另一个TLB用于寻址小页面。 Rainbow将SuperPage中的热小页迁移到DRAM中，而不会损害SuperPage TLB的完整性。 因此，Rainbow实际上将DRAM架构为NVM的缓存。</p><ul><li>为了减少细粒度页面访问计数的存储开销，分两个阶段进行计数。在<strong>给定的时间间隔内</strong>，Rainbow<strong>首先计算Superpage粒度下的NVM内存访问</strong>，然后选择<strong>前N个热门Superpage作为目标</strong>。 在第二阶段，我们<strong>只监视那些小页面</strong>(4KB)粒度的热点超页面，以识别热点小页面。 这种基于历史的策略避免了监视大量冷超页中的子块（4KB页），从而显著降低了热页识别的开销。 </li><li>我们采用<strong>拆分TLB</strong>来加速DRAM和NVM引用的地址转换性能。当一些小页迁移到DRAM时，为了保持SuperPages TLB的完整性，我们在内存控制器中使用位图来识别迁移的热页，而<strong>不会分裂SuperPages</strong>。</li><li>我们提出了一种<strong>物理地址重映射机制来访问DRAM中迁移的热页</strong>，而不必为寻址DRAM页而遭受昂贵的页表遍历。为了实现这一目标，我们将迁移的热点页面的目的地址存储在其原始住所（超级页面）中。 一旦热页对应的TLB未命中，DRAM页寻址应求助于对超级页的间接访问。这种设计在逻辑上利用了SuperPage TLBS作为4KB页面TLB的下一级缓存。 因为Superpage TLB命中率通常很高，所以Rainbow可以显著加快DRAM页面寻址的速度。 </li></ul><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，与现有的内存迁移策略相比，在没有Superpage支持的情况下，Rainbow可以将应用程序的TLB丢失率降低99.9%，并将应用程序的性能（以OFIPC为标准）平均提高2.9×(45.3%)。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA, 2021</li><li>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Mazen Alwadi1, Vamsee Reddy Kommareddy1, Clayton Hughes2, Simon David Hammond2, Amro Awad3<br>University of Central Florida1, Sandia National Laboratories2, North Carolina State University3</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>它们存在着高写延迟和有限的写持久性。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。对于大容量的NVM快速和持久的缓存能力是有限的。利用DRAM作为NVM的一个快速持久的缓存，受到能源支持的限制。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新DRAM缓存内容到NVM</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>实现NVM的快速持久的DRAM缓存，我们利用选择性的NVM镜像对缓存在DRAM中的持久页面进行了新的内存控制器。</p><p>在DRAM中缓存时将持久区域的更新镜像到NVM。 Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>写次数不变，对于写耐久性的破坏</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>虽然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作。仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA 2022属于CCF-A</li><li>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Haikun Liu, Renshan Liu, Xiaofei Liao, Hai Jin, and Yu ZhangBig Data Technology and<br>System, Service Computing Technology and System Lab, Cluster and Grid<br>Computing Lab, Huazhong<br>University of Science and Technology.<br>Bingsheng He is National University of Singapore</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>充分利用NVM和DRAM混合系统的优势，主要目标是将应用程序数据正确放置在混合存储器上。观察发现很多都是小粒度的热数据：</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>于是提出混合内存系统编程接口，<strong>对象粒度迁移，减少迁移开销</strong>。同时提高应用性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的研究主要集中在页面迁移方案上，以实现更高的性能和能源效率。但是，这些方案都依赖于在线页面访问监控成本高。并且还会由于多核时维护缓存/TLB一致性和dram带宽争用产生更多页粒度迁移的开销。</p><p>依赖于页面访问的recency and frequency来决定数据在DRAM或NVM上的位置。1一些用硬件辅助页迁移的方案由于目前硬件不支持页面访问技术，需要对硬件架构进行大量修改[5][6][11]。2用操作系统监控内存访问只能引用1个访问位不足以表达页面冷热度[12][Thermostat13]。3页面迁移通常需要一段时间来检测热页。4预测的页面访问模式可能与未来的访问行为不一致，导致不必要的页面迁移。5大页（huge page）已被越来越多地用于大数据应用和虚拟化平台[13][14]，由于对DRAM容量和带宽的低效利用，粗粒度的页面迁移甚至会降低系统性能。</p><p>采用离线剖析工具，应用对象的粒度来描述内存访问模式，然后指导它们在DRAM或NVM上的静态放置[15]。这个方案只考虑了内存访问行为的整体观点，而忽略了内存访问模式的潜在变化，即对象的访问频率(热度)可能在不同的执行阶段动态变化。</p><p>另一项工作将静态对象放置与选定页面的动态迁移相结合[16]，以处理这些波动。然而，它仍然依赖于内存控制器的硬件扩展来执行在线页面访问监控和迁移。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>OAM利用离线剖析方法来捕获对象级内存访问模式，并采用性能/能量模型来指导对象内存分配和迁移。更具体地说，我们分析每个对象的细粒度时间段的内存痕迹，并采用我们的性能/能量模型来识别对象内存访问模式变化的不同执行阶段。我们在应用程序的源代码中找出未来执行阶段会发生变化的位置，并通过静态代码工具自动注入对象迁移指令。当修改后的程序运行时，它通过考虑DRAM的使用和数据迁移的净收益，自己执行对象迁移。</p><p>![]<a href="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1560f56c3c8c4c2bac8a3b2f22c9ece4.png">https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1560f56c3c8c4c2bac8a3b2f22c9ece4.png</a></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>“有效的静态内存分配：在于2pp执行时间和性能差不多的情况下，与只使用DRAM的系统相比，””OAM w/o migration “”能够平均减少51%的内存能量消耗。这些结果表明，我们最初的OAM数据放置策略对提高混合内存系统的性能和能源效率是有效的。</p><p>在线内存迁移的有效性：与静态内存分配方案相比，对象迁移可以进一步提高应用性能，平均提高11%。没有迁移的OAM平均可以实现51%的EDP减少，而有迁移的OAM平均可以进一步减少10%的EDP</p><p>与一些页面迁移算法相比较：与CLOCK-DWF和2PP相比，OAM可以显著减少迁移流量，平均分别为42%和22%。开销也比他们都小。</p><p>适应不同数据和规模：执行时间都比不使用该方案要节省时间。</p><p>对不同NVM性能的敏感性：测试了目前的产品，对于Optance是由读密集引起的迁移，因为写延迟是差不多的。”</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>聚焦于更低的迟延和能耗。减少迁移性能开销</p><p>而我们的机制只为用C++编写的应用程序提供对象级迁移接口。</p><p>page-interleaving貌似是模拟和算法的基础。</p><p>也没有考虑断电后数据怎么办，而且会存在一致性问题。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>提供了一个离线剖析工具来详细描述应用程序的内存访问模式，并提出了一个<strong>性能/能源模型</strong>来指导应用程序对象的初始内存分配和动态迁移<strong>从能源消耗上解决了读写不均衡</strong>，而不需要任何硬件修改和操作系统干预的在线内存监控。一个静态代码工具，用于自动转换应用程序源代码中的对象级内存分配和迁移，而不会给应用程序的程序员带来负担。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> 细粒度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/05/01/hello-world/"/>
      <url>/2023/05/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-Create-a-new-post"><a href="#1-Create-a-new-post" class="headerlink" title="1.Create a new post"></a>1.Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行命令会在/source/_posts下创建新文章，之后需要使用MarkDown语法编写该文章。  </p><p><code>---</code>包括起来的内容称之为<code>Front-matter</code>有很多配置选项可以添加。<br>更多的简单语法可以参考<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程</a><br>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="2-清除旧数据"><a href="#2-清除旧数据" class="headerlink" title="2.清除旧数据"></a>2.清除旧数据</h3><p>文章写好之后，首先清除掉旧的数据</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo clean <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会清除掉之前生成的网页，即站点根目录下的public文件夹</p><h3 id="3-Generate-static-files"><a href="#3-Generate-static-files" class="headerlink" title="3.Generate static files"></a>3.Generate static files</h3><p>然后使用如下命令生成新的页面：More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate 或者简写 hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会将source文件夹下所有的md文件进行渲染，生成HTML页面，存放在public文件夹下</p><h3 id="4-Run-server"><a href="#4-Run-server" class="headerlink" title="4.Run server"></a>4.Run server</h3><p>在本地开启服务器，预览一下文章是否满意</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server  <span class="token string">'hexo s'</span> <span class="token keyword">for</span> short<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="5-Deploy-to-remote-sites"><a href="#5-Deploy-to-remote-sites" class="headerlink" title="5.Deploy to remote sites"></a>5.Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy <span class="token string">'hexo d'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="Blog-Template"><a href="#Blog-Template" class="headerlink" title="Blog Template"></a>Blog Template</h2><p>更高阶更详尽的Hexo Markdown教程参考<a href="https://blog.17lai.site/posts/cf0f47fd/#%E5%B8%B8%E7%94%A8%E6%A0%87%E8%AE%B0">夜法之书的博客</a><br>一些可以用到的LeTax数学公式编辑方式<a href="http://t.csdn.cn/VivVj">超详细 LaTex数学公式</a> || <a href="http://t.csdn.cn/iPVFt">LaTeX数学公式-详细教程</a></p><p>图床就是用csdn了 <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 还有好多文章慢慢搬运过来，不急。<br>但是需要在每个csdn的图片的链接上加上<a href="https://images.weserv.nl/?url=%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5LRU%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%89%8D%E7%BC%80%E3%80%82">https://images.weserv.nl/?url=真正的图片链接LRU，这个前缀。</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span>https://images.weserv.nl/?url<span class="token operator">=</span><span class="token punctuation">)</span><span class="token punctuation">{</span>% raw %<span class="token punctuation">}</span>在这之间写LaTex或者其他造成的符号转义冲突之类的报错<span class="token punctuation">{</span>% endraw %<span class="token punctuation">}</span>toc: <span class="token boolean">true</span>mathjax: <span class="token boolean">true</span>hide: <span class="token boolean">true</span>categories: Paper<span class="token comment"># password: 4dea5c7cb70f50322ec9d734aa4aa078be9227c05251e18991c596f387552370</span>tags:  - A - Hybrid Memory Systemsimg: https://images.weserv.nl/?url<span class="token operator">=</span>summary: 。---<span class="token comment">## 1. 论文信息</span><span class="token operator">&lt;</span>div <span class="token assign-left variable">class</span><span class="token operator">=</span><span class="token string">"note primary"</span><span class="token operator">&gt;</span>- 文章来自IEEE International Symposium on High-Performance Computer Architecture, <span class="token punctuation">(</span>HPCA<span class="token punctuation">)</span>, <span class="token number">2022</span>- 名字<span class="token operator">&lt;</span>/div<span class="token operator">&gt;</span> <span class="token comment">### 所有作者及单位</span> - A, 佛罗里达国际大学<span class="token punctuation">(</span>FIU<span class="token punctuation">)</span><span class="token comment">## 2. Background</span><span class="token comment">## 3. 解决了什么问题</span><span class="token comment">## 4. 其他学者解决这个问题的思路和缺陷</span><span class="token comment">## 5. 围绕该问题作者如何构建解决思路</span><span class="token comment">## 6. 从结果看，作者如何有力证明他解决了问题</span><span class="token comment">## 7. 缺陷和改进思路</span><span class="token comment">## 8. 创新点</span><span class="token comment">## 9. 积累</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个标签结束后必须空一行:</p><div class="note info">这里是 info 标签样式</div> <div class="note info no-icon">这里是不带符号的 info 标签样式</div> <div class="note primary">这里是 primary 标签样式</div> <div class="note primary no-icon">这里是不带符号的 primary 标签样式</div> <div class="note warning">这里是 warning 标签样式</div> <div class="note warning no-icon">这里是不带符号的 warning 标签样式</div> <div class="note danger">这里是 danger 标签样式</div> <div class="note danger no-icon">这里是不带符号的 danger 标签样式</div><p>然后是行内标签，比加粗更能显示重点，Fulid移植的。<br><span class="label primary">Label primary</span></p><p><span class="label default">Label default</span></p><p><span class="label info">Label info</span></p><p><span class="label success">Label success</span></p><p><span class="label warning">Label warning</span></p><p><span class="label danger">Label danger</span></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>this is a test for my first blog</title>
      <link href="/2023/04/30/this-is-a-test-for-my-first-blog/"/>
      <url>/2023/04/30/this-is-a-test-for-my-first-blog/</url>
      
        <content type="html"><![CDATA[<h2 id="詹青云2018年华语辩论世界杯决赛结辩"><a href="#詹青云2018年华语辩论世界杯决赛结辩" class="headerlink" title="詹青云2018年华语辩论世界杯决赛结辩"></a>詹青云2018年华语辩论世界杯决赛结辩</h2><p>大家好，我们今天和对方有三个根本的分歧。一是成功路径不同。我方承认，聚焦没有问题。如果一个年轻人在年轻的时代完全知道一生要什么，一生走下去从不后悔。没问题，挺幸福的。可是现实是，这个决定对于大多数人来说不应该在青年时代做。这个时候你的大脑没有发育完全、你的人生还在不停地变动、你的智识还有限，而这个世界在飞快的变化。很有可能你想要聚焦的东西有一天是你不适应、不喜欢或者被时代淘汰的东西。</p><p>这时候您方跳到了第二点告诉我说没关系，我只要坚定自己的内心我就没有问题了。这就是我们双方第二点分歧：幸福观的不同。您方的幸福观是一种妥协的幸福观，而我放的幸福观是进取的幸福观。您方的意思是不管我人生发挥得怎样，社会如何对待我，不用在乎！我妥协、我看开、我豁达，就可以幸福。对方辩友，那些历史上真正收获了豁达心态的人，杨慎“是非成败转头空”，王维“行到水穷处，坐看云起时”的时候，他们是在什么时候收获这种豁达，是在遍历人生的沧桑，经历了繁华，经过了奋斗，见识了人世中更深刻的道理，他可以领悟到繁华。就算我退一步，俗一点讲，我多读一点书，多看一点世界，对这个世界的理解和思考方式丰富一点，这种做加法的方法您才能收获真的豁达。</p><p>最后我们双方最根本的分歧是对这个时代理解不同。您告诉我说这个世界纷繁复杂，已经把太多选择推到年轻人的面前，所以我选择加就是在随大流。不是。我们仔细想一想，这个时代给我们多的选择不过是您方说的商品、营销课、成功学。可是人生加减法上，那些人生重大关头的选择是什么，这个社会真的在逼我们做加法吗？不是。我到了这个年纪就应该结婚生子，成家立业。在人生重大关头的选择上，这个社会是要求青年人割舍那些不切实际的幻想，割去那些错误的观念，回归一套社会范式，一套人生范式，是要求你做减法的。</p><p>这个时候真正追随自己内心是应该不顾这套范式的束缚，冲破束缚去追寻自己心中所爱，活出一个真正多元的世界。更重要的是，我们今天不是在替一个年轻人的幸福说话，是一代青年人。青年人拓宽人生边界的可能是在拓宽这个社会价值判断的可能。</p><p>既然这个世界号称它是多元而包容的，我们就应该去试，去让他实现这个诺言。</p><p>既然这个社会多元而包容，既然这个世界告诉我们“人不轻狂枉少年”，就没有人应该天然地觉得“轻狂”是一个贬义词。对方辩友一直在劝我们：人生选到自己最幸福的东西才是快乐的。对方辩友，各位，我们都是年轻人。在我们人生的这个阶段，有什么东西是唯一珍贵的？什么叫“欲买桂花同载酒，终不似，少年游”，什么叫“旧游无处不堪寻。无寻处，惟有少年心”。那个唯一带不走的东西，不就是青春本身吗？这一份机会你不珍惜，这一份可能你不珍惜，您跟我谈的是什么？是那一份安顿了的幸福，是那一份成熟了的幸福。可是这不是年轻人的幸福。因为年轻人不是在替你一个人谋幸福，不是你一个人看开了就可以。他是要为这个世界拓宽边界，是让所有的人都有机会把道路越走越宽，是</p><p>趁着年轻，我偏要勉强。</p><p>谢谢大家。</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Perspective </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts, Practices, and Future Direction</title>
      <link href="/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/"/>
      <url>/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>虽然这篇的有些引用也是十多年前的数据，但是作为学习一个阶段的总结，和大佬对比一下在知识结构上的完整度，还有什么是不清楚的。还是挺有用的。</p></blockquote><div class="note primary"><ul><li>文章来自Journal of Computer Science and Technology, (JCST), 2021</li><li>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts,Practices, and Future Direction</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hai-Kun Liu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Di Chen, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Hai Jin, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Xiao-Fei Liao, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Binsheng He, 新加坡国立大学</li><li>Kan Hu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Yu Zhang, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li></ul><hr><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>非易失性主存储器（NVMM）最近已成为未来存储系统的一种有前途的技术。通常，NVMM具有许多理想的属性，例如高密度、字节寻址、非易失性、低成本和能效，但代价是高写入延迟、高写入功耗和有限的写入耐用性（写寿命短）。NVMM已经成为动态随机存取存储器（DRAM）的强有力的替代品，并将从根本上改变内存系统的格局。它们在系统架构设计、操作系统内存管理以及混合内存系统的编程模型方面带来了许多研究机会和挑战。在本文中，我们首先回顾了新兴NVMM技术的概况，然后对NVMM技术的最新研究进展进行了综述。我们根据不同的维度（如内存架构、数据持久性、性能改进、节能和磨损均衡）对这些研究进行分类。其次，为了展示构建NVMM系统的最佳实践，我们从架构、系统和应用的维度介绍了我们最近的混合存储系统设计工作。最后，我们对NVMM的未来研究方向提出了展望，并对设计挑战和机遇进行了阐述。</p><p><strong>关键词</strong>：非易失性存储器、持久性存储器、混合存储器系统、存储器层次结构<br>non-volatile memory, persistent memory, hybrid memory systems, memory hierarchy</p><hr><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在大数据时代，内存计算越来越受到数据密集型应用的青睐。内存子系统对现代计算系统的功能和性能具有越来越大的影响。使用DRAM (动态随机存取存储器)的传统大内存系统[1 ,2]在功率和密度方面面临严峻的可扩展性挑战[3]。尽管DRAM规模从2013年的28nm持续到2016年的10+ nm[4，5] ，扩展已经放缓，变得越来越困难。此外,最近的研究[6-10]表明，基于DRAM的主存储器约占物理服务器总能耗的30%-40%。新兴的非易失性主存储器（NVMM）技术，如相变存储器（PCM）、自旋转移扭矩存储器（STT-RAM）和3D XPoint[11]通常提供比DRAM更高的内存密度、更低的每的比特成本和待机功耗。NVMM技术的出现有可能弥合慢速持久存储（即磁盘和SSD）与DRAM之间的差距，并将从根本上改变存储系统的格局。</p><p>表1显示了闪存SSD、DRAM、PCM、STT-RAM、ReRAM和Intel Optane DC持久内存模块（DCPMM）的不同内存特点，包括读/写延迟、写耐久性和待机功耗[7，12，13]。尽管NVMM在密度和能耗方面具有各种优势，但其写入延迟比DRAM高约6倍-30倍， 写入功耗比DRAM高约5倍- 10倍。此外，NVMM的写入耐久性非常有限(约 $10^8$ 倍) ，而DRAM能够承受约 $10^{16}$ 次的写入操作。这些缺点使得很难直接替代DRAM。使用NVMM的一种更实用的方法是混合内存架构，由DRAM和NVMM组成[15,16]。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>为了充分发挥两者在混合内存系统中的优势，在性能提升、节能降耗、磨损均衡、数据持久性等方面存在许多开放的研究问题。为了解决这些问题，已经有许多关于内存层次结构的设计[15-18]、内存管理[19-21]和内存分配方案[22-24]的研究。这些研究成果导致了混合内存架构、操作系统和编程模型的创新。尽管学术界和工业界已经做出了大量工作来将新兴NVMM集成到存储器层次结构中，但仍有许多挑战需要解决。</p><p>另一方面，先前对NVMM技术的研究大多基于模拟/仿真NVMM器件，与真正非易失性（双列直插式内存模块Dual In-line Memory Modules）DIMM相比，NVMM设备承诺的性能可能存在各种偏差。最近宣布推出的Intel Optane DCPMM终于将NVMM DIMM商业化。真正的Intel Optane DCPMM与之前的研究预期承诺的功能相比，表现明显不同[18，20，26，28]。例如，如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3GB/s，而DRAM的读写带宽之间的差距要小得多（1.3x）。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。由于Optane DCPMM DIMM的这些关键特性，以前关于持久性内存系统的研究应该重新审视和优化，以适应真正的NVMM DIMM。</p><p><strong>贡献</strong>。本文首先回顾关于混合内存架构、操作系统级混合内存管理和混合内存编程模型的最新研究现状。表2显示了NVMM技术的最新研究分类。我们根据不同维度对这些研究进行分类，包括内存架构、持久内存(PM)管理、性能改进、节能、磨损均衡、编程模型和应用程序。我们还讨论了它们的相似性和差异，以突出设计挑战和机遇。其次，为了展示构建NVMM系统的最佳实践，我们<strong>从架构、系统和应用的维度展示了我们在混合存储系统设计方面的努力</strong>。最后，我们提出了在实际应用场景中使用NVMM的未来研究方向，并对研究领域的设计挑战和机遇进行了一些说明。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9239e045ee11414cb228f4b752b13f86.png" alt="将相关工作分类"></p><p>尽管有其他关于NVMM的研究，但鉴于NVMM的快速发展，这篇综述从一个独特的角度对NVMM进行了回顾。在[97]中, 作者介绍了PCM技术的最新研究。以解决有限的写入耐久性、潜在的长延迟、高能量写入、功耗等问题以及一些对内存隐私的担忧。在[98]中，作者对PCM设备及其架构和软件进行了全面的调查和回顾。其他一些有趣的调查侧重于在架构上将四种NVM技术(PCM、MRAM、FeRAM和ReRAM )集成到现有存储层次结构[99]中，或将NVM用于存储和主存储器系统的软件优化[100]。我们的调查与那些调查有三个不同之处。首先，先前的研究[97 ,98]从计算机架构的角度关注PCM设计。相比之下，我们的论文主要从存储器层次、系统软件和应用的维度来研究使用混合存储器的系统。其次，我们的论文包含了更多新发表的期刊、会议论文的评论。特别是，我们对新发布的Intel Optane DCPMM设备进行了更多研究。第三，我们介绍了最近关于存储器系统的近期经验，以阐明未来混合存储器系统的挑战和机遇。</p><p>本文的其余部分组织如下。第2节描述了现有的由DRAM和NVMM组成的混合内存架构。第3节介绍了NVMM中数据持久性保证的挑战和当前解决方案。4节介绍了混合存储器系统中性能优化和节能的最新研究。第5节介绍NVMM写耐久性的研究。第6节介绍了研究NVMM技术所做的努力和实践。在第7节中，我们讨论了NVMM的未来研究方向。我们在第8节结束本文。</p><h1 id="2-Hybrid-Memory-Architectures混合内存架构"><a href="#2-Hybrid-Memory-Architectures混合内存架构" class="headerlink" title="2. Hybrid Memory Architectures混合内存架构"></a>2. Hybrid Memory Architectures混合内存架构</h1><p>已经有很多关于混合存储器架构的研究。通常，主要有两种混合存储器架构，即水平和分层[18]，如图1所示。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e565e90afda244f6a8dcfbe3081068c1.png" alt="常见的架构，最近华科的工作也有混合的架构出现"></p><h2 id="2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构"><a href="#2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构" class="headerlink" title="2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构"></a>2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构</h2><p>许多DRAM-NVMM混合存储器系统[14 ,15 ,31]通过OS在平面(单个)存储器地址空间中管理DRAM和NVMM，并将它们两者用作主存储器[31 ,32]。为了提高数据访问性能，这些混合内存系统需要通过将频繁访问的热NVMM页面迁移到DRAM来克服NVMM的缺点，如图1（a）所示。需要开发内存访问监控机制来指导页面迁移。</p><p><strong>内存访问监控</strong>。Zhang和Li[31]使用多队列算法对页面的热度进行分类，并将热页面和冷页面分别放置在DRAM和NVMM中。Park等人[32]也主张用于管理DRAM和NVMM的水平混合存储架构。此外，他们还提出了三种优化策略来降低混合存储系统的能耗。他们以非常细的DRAM行粒度监视内存数据，并定期检查每个DRAM行中的访问计数器。根据计数器将数据写回NVMM，以减少DRAM刷新的能耗。在再次访问数据之前，数据不会从NVMM缓存到DRAM。脏数据尽可能长时间地保存在DRAM中，以减少DRAM和NVMM之间的数据交换开销以及NVMM的昂贵写操作。</p><div class="note info no-icon"> 听起来监控成本比较高 </div><p><strong>页面迁移</strong>。针对不同的优化目标，已经提出了许多页面迁移算法。Soyoon等人[33]认为，在识别热页时，NVMM写入的频率比数据访问的最近性更重要，并提出了一种称为CLOCK with Dirty bits and Write frequency（CLOCK-DWF）的页替换算法。对于每个NVMM写入操作, CLOCK-DWF需要首先将相应的页面提取到DRAM，然后在DRAM中执行写入。这种方法可能会导致许多不必要的页面迁移，从而给NVMM带来更多的能耗和写回操作。Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面，并使用两个最近最少使用的(LRU)队列分别选择DRAM和NVM中的要被驱逐的页面。Yoon[17]等人基于行缓冲区局部性进行了页面迁移，其中行缓冲区命中率低的页面被迁移到DRAM，而行缓冲区点击率高的页面仍保留在NVMM中。Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性。Khouzani[35]等人考虑了程序的内存布局和内存级并行性，以迁移混合内存系统中的页面。</p><p><strong>架构限制</strong>。在水平混合存储器架构中管理NVMM和DRAM有几个挑战。</p><p>首先,<span class="label primary">页面级内存监控成本高昂</span>。一方面，由于当今的商品x86系统不支持页面粒度的内存访问监控，因此硬件支持的页面迁移方案需要大量的硬件修改来监控内存访问统计[14,15,33]。另一方面, OS层的内存访问监控通常会导致显著的性能开销。许多操作系统在页面表条目(PTE)中为每个页面维护一个“已访问”位，以标识该页面是否被访问。然而，该位不能真实地反映页面访问的最近性和频率。因此，一些基于软件的方法将禁用Translation Lookaside Buffer（TLB）[102]来跟踪每个内存引用。这种页面访问监控机制通常会导致显著的性能开销，甚至抵消混合内存系统中页面迁移的好处。</p><p>第二，页面迁移成本也很高。<span class="label primary">一次页迁移可能导致多次页读/写操作（代价高昂）</span>。页面可能只包含一小部分热数据，因此由于内存带宽和DRAM容量的浪费，页面粒度的迁移成本相对较高。</p><p>第三，热页面检测机制可能需要很长时间来预热页面，从而降低页面迁移的收益。此外，<span class="label primary">对于某些不规则的内存访问模式，热页面预测可能不准确，从而导致不必要的页面迁移</span>。</p><h2 id="2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构"><a href="#2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构" class="headerlink" title="2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构"></a>2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构</h2><p>许多研究建议通过分层的缓存-内存架构来组织DRAM和NVMM[16,38,39]。他们使用DRAM作为NVMM的缓存，如图1（b）所示。DRAM缓存对操作系统和应用程序是不可见的，完全由硬件管理。</p><p>Qureshi等人[16]提出了一种由大尺寸PCM和小尺寸DRAM组成的分层混合存储系统。DRAM缓存包含最近访问的数据，以减少最昂贵的NVMM访问，而大容量NVMM存储大部分所需数据，以避免在应用程序执行期间进行昂贵的I/O操作。类似地，Mladenov[38]设计了一个具有小容量DRAM缓存和大容量NVMM的混合存储系统，并基于应用程序数据的空间局部性对其进行管理。DRAM作为按需缓存进行管理，并通过LRU算法进行替换。Loh和Hill[39]以缓存行的粒度管理DRAM，以提高DRAM缓存的效率，并使用组连接方式将NVMM数据映射到DRAM缓存。他们将元数据tag和数据放在同一个存储行中，以便可以快速访问缓存命中的数据，并减少标记查询的性能开销。</p><div class="note info no-icon"> 缓存是得把这个DRAM100%用起来的，水平就不好说了 </div><p>在这种内存结构中，由于DRAM被组织为N路集合关联缓存，因此需要额外的硬件来管理DRAM缓存。例如，需要SRAM存储器来存储DRAM高速缓存中数据块的元数据tag，并且需要硬件查找电路来查找DRAM高速缓冲存储器中所请求的数据。因此，为了访问DRAM缓存中的数据，需要两个内存引用，一个用于访问元数据，另一个用于实际数据。为了加速元数据访问，Qureshi[16]等人使用了高速SRAM来存储元数据。Meza等人[40]通过将元数据放在同一DRAM行中的数据块旁边，降低了标记存储的硬件成本。他们还建议使用片上元数据缓冲区将频繁访问的元数据缓存在小型SRAM中。</p><p><strong>架构限制</strong>。尽管分层混合存储器架构通常比单独访问NVMM中的数据的场景提供更好的性能，但在运行具有较差局部性的工作负载时，它可能会导致性能显著下降[103]。原因是大多数硬件管理的分层DRAM-NVMM系统为了简化而利用基于按需的数据预取策略，因此DRAM缓存位于存储器分层的关键数据路径中。如果数据块未命中DRAM缓存，则无论页面热度如何，都必须将其从NVMM提取到DRAM。这种缓存填充策略可能会导致DRAM和NVMM之间频繁的数据交换（类似于缓存抖动问题）。另一方面，硬件管理的缓存架构不能充分利用DRAM容量。由于DRAM缓存被设置为关联的，因此每个NVMM数据块被映射到一个固定的集合。当集合已满时,它必须在将新的NVMM数据块提取到DRAM之前驱逐数据块，即使其他缓存集合为空。</p><h2 id="2-3-Intel-Optane-DCPMM的体系结构"><a href="#2-3-Intel-Optane-DCPMM的体系结构" class="headerlink" title="2.3 Intel Optane DCPMM的体系结构"></a>2.3 Intel Optane DCPMM的体系结构</h2><p>最近发布的Intel Optane DCPMM与DRAM结合使用时支持水平和分层混合内存结构。OptaneDCPMM DIMM目前有两种操作模式:内存模式和应用程序直接模式[25]。这些模式中的每一种对于特定的用例都有其优点。</p><p><strong>内存模式</strong>。在这种模式下，DCPMM充当<strong>大容量的主存储器</strong>。操作系统将DCPMM识别为传统DRAM，<strong>并禁用DCPMM的持久性功能</strong>。如果将传统DRAM与DCPMM结合使用，它将隐藏在操作系统中，并充当DCPMM的缓存层。因此，DCPMM和DRAM实际上被组织在分层混合存储器架构中。内存模式的主要优点是提供在内存总线通道上提供优越内存容量。这种模式强烈强调在不修改上层系统的情况下围绕内存空间构建大容量存储环境以及应用程序。推荐的用例是扩展主内存容量，以实现更好的基础设施扩展，例如用于大数据应用程序的并行计算平台（MapReduce、图形计算）。</p><div class="note info no-icon"> 这种用例应该是指DRAM很少很少，PM特别多的，数据仓库那种级别的缓存吧？通常一般服务器的比例拿去做缓存真的很浪费容量咦 </div><p><strong>AD模式</strong>。在这种模式下，DCPMM为操作系统和应用程序提供了所有持久性特性。操作系统将DRAM和DCPMM分别作为主存储器和持久存储向应用程序公开。与DCPMM混合的传统DRAM仍然充当应用程序的标准DRAM，而DCPMM也被分配到存储器总线以实现更快的存储器访问。DCPMM用作两种namespace之一:直接访问存取（DAX）和块存储。前者的namespace是字节可寻址的持久存储，应用程序通过特殊的apis直接访问。因此，DCPMM和DRAM在这种模式下被逻辑地组织在一个水平混合内存架构中。后一个命名空间将DCPMM 作为区块存储设备提供给应用程序，类似于SSD，但是可以通过更快的内存总线访问。应用程序直接模式强调减少延迟和提高带宽的优势，比NVMe快2.7倍。推荐的用例适用于大型内存数据库，这些数据库需要满足数据持久性的要求。</p><p>还有一种结合了内存模式和应用程序直接模式的混合内存模式。DCPMM的一部分容量用于内存模式操作,DCPMM剩余的容量用于应用程序直接模式操作。这种混合内存模式为管理不同应用场景的混合内存系统提供了一种更灵活的方法。</p><div class="note info no-icon"> 这里没有说通过热插拔的dvdax模式，变成易失内存 </div><h2 id="2-4-Summary总结"><a href="#2-4-Summary总结" class="headerlink" title="2.4  Summary总结"></a>2.4  Summary总结</h2><p>上述两种混合存储器架构对于不同的场景有各自的优缺点。<span class="label primary">通常，分层架构更适合具有良好数据局部性的应用程序，而平面可寻址架构更适用于延迟不敏感或占空间较大的应用程序。</span>关于哪种架构比另一种架构更好，目前尚无定论。实际上，Intel Optane DCPMM支持分层和平面可寻址混合内存架构。当前DCPMM的一个限制是，在重新配置DCPMM模式后，系统需要重新启动。如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。</p><h1 id="3-Persistent-Memory-Management-持久内存管理"><a href="#3-Persistent-Memory-Management-持久内存管理" class="headerlink" title="3 Persistent Memory Management 持久内存管理"></a>3 Persistent Memory Management 持久内存管理</h1><p>数据持久性是NVMM的一个重要设计考虑因素。下面，我们首先介绍持久内存（PM）管理所面临的技术挑战，然后介绍有关持久内存管理的最新研究，包括持久内存的使用、持久内存访问模式、容错机制和持久对象。</p><h1 id="3-1-Technical-Challenges-技术挑战"><a href="#3-1-Technical-Challenges-技术挑战" class="headerlink" title="3.1 Technical Challenges 技术挑战"></a>3.1 Technical Challenges 技术挑战</h1><p>在混合内存系统中，NVMM可以在运行应用程序时充当主内存，并在应用程序完成时充当持久存储。NVMM的字节寻址能力和非易失性特性消除了内存和外部存储的区别。然而，当NVMM中的数据需要持久化时，就需要对NVMM中的数据进行重新组织和重新定位。</p><p>图2显示了持久内存（PM）的管理操作。NVMM区域是物理PM设备。NVMM区域可以像DRAM一样用作工作内存，也可以像磁盘一样用作持久存储。当程序完成时，工作内存中的数据应该刷新到持久存储中。此外，为了保证高可靠性，检查点机制被广泛利用来从电源故障或系统崩溃中恢复系统。Gao等人[104]开发了一种利用NVMM在混合内存系统中进行实时检查点的新颖方法。</p><div class="note info no-icon">Checkpointing（检查点）用于在运行过程中定期保存系统的状态和数据快照。这些检查点允许系统在发生故障或需要恢复时，从之前的状态继续执行，而不是从头开始重新计算。</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ba021f22f99b414582492752b9d4377a.png" alt="没看懂这个图"></p><p>有效管理PM面临多项挑战。首先，持久存储以文件系统的形式被广泛管理。作为字节可寻址NVMM提供比传统块设备更好的随机访问性能，基于PM的文件系统的性能瓶颈已经从硬件转移到系统软件堆栈。缩短软件堆栈中的数据路径至关重要。其次，由于许多CPU使用回写式缓存来实现写操作的高性能，因此末级缓存（LLC）可能会改变写回PM的数据顺序。如果出现断电或者系统崩溃的情况，可能会导致数据不一致的问题。因此，为了保证PM中数据的一致性，需要写操作的顺序和写原子性模型来保证PM中数据的一致性。第三，与基于PM的文件系统相比，持久对象和数据结构对于PM编程更有前景，因为它们消除了文件系统中的复杂数据结构，包括i节点、元数据和数据。然而，这些持久对象和数据结构仍然面临着保证数据一致性的挑战。接下来，我们将回顾试图解决这些挑战的研究<br>（这后面就跳过了，大部分在讨论文件系统，不是我目前考虑的，下次一定）</p><h1 id="4-Performance-Improvement-and-Energy-Saving性能改进和节能"><a href="#4-Performance-Improvement-and-Energy-Saving性能改进和节能" class="headerlink" title="4 Performance Improvement and Energy Saving性能改进和节能"></a>4 Performance Improvement and Energy Saving性能改进和节能</h1><p>由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。这些研究可分为三类:减少NVMM写入次数、减少NVM写入本身的能耗以及通过页面迁移减少DRAM的能耗。</p><h2 id="4-1-NVMM-Write-Reduction-NVMM写入减少"><a href="#4-1-NVMM-Write-Reduction-NVMM写入减少" class="headerlink" title="4.1 NVMM Write Reduction NVMM写入减少"></a>4.1 NVMM Write Reduction NVMM写入减少</h2><p>为了减少NVMM写入,分层结构显然更合适,因为DRAM缓存减少了大量NVMM写入。为此,开发了两种主要技术,即页面迁移和绕过NVMM写入。</p><p><strong>页面迁移</strong>。页面迁移[14 ,15 ,33 ,58 ,59]策略主要基于写入次数和每个页面的最近访问频率来选择要迁移的页面。它们的主要区别在于触发页面迁移的条件。</p><p>PDRAM[15]根据写入次数将PCM页迁移到DRAM。在PDRAM中,存储器控制器维护一个表以记录每个PCM页的访问计数。如果写入PCM页的次数超过给定阈值,则触发页故障，然后将该页从PCM页迁移到DRAM。</p><p>CLOCK-DWF[33]将页面的写入历史集成到CLOCK算法中。当发生页面错误时，虚拟页面将从磁盘提取到PCM。否则, 该页将在DRAM中分配,因为该页可能是写密集型页。</p><p>RaPP[14]根据页面的等级在DRAM和PCM之间迁移页面。在RaPP中, 页面按访问频率和最近度排序。排名靠前的页面从PCM迁移到DRAM。因此，频繁写入的页面被放置在DRAM中,而很少写入的页面则被放置在PCM中。此外, RaPP还将任务关键页面放置在DRAM中，以提高应用程序性能。通过监视LLC中每个页面的写回操作的数量,存储器控制器能够跟踪每个页面的访问频率和最近性。RaPP根据多队列（MQ）算法对页面进行排序[118]。传统MQ定义了多个最近最少使用（LRU）队列。每个LRU队列是一个页面描述符队列, 其中包括参考计数器和逻辑过期时间。当第1次访问页面时,页面将移动到队列0的尾部。如果页面的引用计数达到 $2^{i+1}$ ,则提示页面排队i+1。一旦PCM页面被移动到队列5,它就被迁移到DRAM。</p><p><strong>缓冲NVMM写入</strong>。在混合内存系统中, 缓存能够减少对NVMM的大量写入。适当的缓存替换策略不仅可以提高应用程序性能, 还可以降低NVMM的能耗。先前的研究[7,18]发现, 缓存中的许多块在被从缓存中逐出之前不会被再次使用。这些块称为死块，并消耗宝贵的缓存容量。DASCA[7]提出了一种死块预测方法,以减少STT-RAM缓存的能耗。驱逐这些死块将减少对STT-RAM缓存的写入,并且不会影响缓存命中率。WADE[62]进一步利用了NVMM读取和NVMM写入之间的能耗不对称性。由于NVMM写入操作比NVMM读取操作消耗更多的能量,因此频繁写入的块应保留在缓存中。WADE将缓存中的块分为两类:频繁回写的块和非频繁回写块。非频繁写回的块被替换,以提供更多机会将其他数据块保留在缓存中。</p><h2 id="4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低"><a href="#4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低" class="headerlink" title="4.2 NVMM Energy Consumption Reduction NVMM能耗降低"></a>4.2 NVMM Energy Consumption Reduction NVMM能耗降低</h2><p>由于NVMM写入显示的能耗是NVMM读取的能耗的几倍，因此在降低NVMM写入的能耗方面已经做出了许多努力。这些方法可以分为两类:差分写入（仅写入脏位而不是整行）和在单个写入期间并行多个写入。</p><p>如果要写入的位数超过缓存行中总位数的一半，则Flip-N-Write[64]尝试通过翻转位来减少PCM写入能耗。在一次写入期间, 如果行中超过一半的位被写入 ,则每个位被翻转,因此位翻转不超过总位的50%。同时,设置标记位以识别行中的位是否被翻转。当读取行时,标记位用于确定行中的位是否应该翻转。与Flip-N-Write类似, Andrew等人[73]提倡细粒度写入。它只监视脏位而不是一行中的所有位。一个叫做PCM的新术语引入功率令牌以指示单次写入期间的电源。假设为每个芯片分配Plimit Watts功率, 并且每个位写入需要Pbit Watts , Plimitpbit可以同时写入位。在芯片内,可以同时写入bank。在单个写入期间,如果多个写入请求位于不同的存储库中,并且总功耗不超过Plimit ,则可以同时执行这些写入。因此，细粒度写入不仅减少了NVMM写入，而且通过实现更高的存储体并行性来提高系统性能。</p><p>一些研究[65 ,66]通过分离SET和RESET操作来提高NVMM的能量效率。由于NVMM写入1比写入0消耗更多的能量和时间，如果以正确的方式执行这些写入,则可以减少写入延迟和能耗。三阶段写入[65]将写入操作分为比较阶段、写入零阶段和写入一阶段。在比较阶段,利用Flip-N-Write机制来减少写入次数。零位和一位分别在写零级和写一级中被分别写入。因为在大多数工作负载中,零写操作占了大部分写操作，所以Tetris write[66]进一步考虑了SET和RESET操作的不对称性, 并行调度代价高昂的写操作。在功率约束下, 写零操作被插入到写操作的剩余间隔中。</p><p>CompEx[67]提出了一种压缩扩展编码机制,以减少MLC/TLC NVMM的能耗。为了提高MLC/TLC单元的寿命,首先压缩数据以减少数据冗余。然后将扩展码应用于压缩数据并写入物理NVMM单元。对于具有8个状态的TLC单元，状态0、1、6和7称为终端能量状态，而状态2、3、4.和5称为中心能量状态。中心能量状态消耗更多的时间和能量，因为它们需要更多的编程和验证迭代。CompEx 利用扩展代码仅使用NVMM单元的终端能量状态。由于在编程MLC/TCL单元时，终端能量状态需要比中心能量状态更少的能量和时间，所以这一想法起效。混合片上缓存也被提出以减少CPU的功耗。RHC[68]构建了一个混合缓存，其中SRAM和NVMM中的每种方式都可以独立地打开或关闭。如果一行很长时间没有被访问，该行将被关闭，而其标签仍处于打开状态，以跟踪该行的访问。当对标签的访问超过阈值时，该行将通电。为了更好地利用高性能SRAM和低动态功耗NVMM，RHC对SRAM和NVMM采用不同的阈值。</p><h2 id="4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低"><a href="#4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低" class="headerlink" title="4.3 DRAM Energy Consumption Reduction DRAM能耗降低"></a>4.3 DRAM Energy Consumption Reduction DRAM能耗降低</h2><p>在只有DRAM的存储系统中, 静态能耗可以占存储系统总能耗的一半以上[69-71]。在混合存储器系统中,页面迁移技术被广泛用于减轻DRAM的能耗。非活动页面可以从DRAM迁移到NVMM，以便空闲的DRAM组可以断电。当页面稍后变为活动时, 它将再次迁移到DRAM。然而, 如果页面迁移没有正确执行,DRAM列组可能会频繁断电并重新激活。额外的能耗可能会抵消页面迁移带来的好处。</p><p>为了减少混合存储器系统的能耗，RAMZzz[8]揭示了高能耗的两个主要根源。一个是活动页面的稀疏分布,另一个是页面迁移可能不有效,因为DRAM的多能量状态之间的传输会引入额外的能量消耗。为了解决前一个问题, RAMZzz使用多个队列将具有类似活动的页面收集到同一个DRAM列中，从而避免频繁的能量状态转移。多个队列具有L个LRU队列来记录页面描述符。页面描述符包含一段时间内页面的ID和访问（读和写）计数。为了减少数据迁移的能量开销，将具有类似内存访问行为的页面重新组合在一起。这样，需要将页面分配给新的bank。RAMZzz在banks间并行迁移这些页面。</p><p>Refree[72]通过避免DRAM刷新，进一步降低了混合存储器系统中的DRAM能耗。当DRAM行需要刷新时,这意味着该行很长时间没有被访问。行中的数据已过时,不久以后不太可能再次访问。Refree将这些行逐出PCM，而不是在DRAM中刷新它们。在Refree中,所有行都会定期监视。此周期的间隔等于DRAM行自上次刷新以来的保留时间的一半。因此，行分为活动行和非活动行。激活行在访问。非活动行被逐出PCM，从而消除DRAM刷新。</p><h1 id="5-Write-Endurance-Improvement-写入耐久性改进"><a href="#5-Write-Endurance-Improvement-写入耐久性改进" class="headerlink" title="5. Write Endurance Improvement 写入耐久性改进"></a>5. Write Endurance Improvement 写入耐久性改进</h1><p>在混合存储器系统中,主要有两种策略来克服NVMM的有限写入耐久性。一个是减少NVMM写入，另一个是磨损均衡,它在所有NVMM单元之间均匀分布写入流量。</p><h2 id="5-1-Write-Reduction写入减少"><a href="#5-1-Write-Reduction写入减少" class="headerlink" title="5.1 Write Reduction写入减少"></a>5.1 Write Reduction写入减少</h2><p>已经提出了许多用于改善NVM寿命的写减少策略，包括数据迁移[8、14、15]、 缓存或缓冲[16]和内部NVM写减少[64、73、74]。</p><p>提出了一种延迟写入机制[16],以减少对PCM的写入。在分层混合存储器系统中,DRAM缓冲器用于隐藏高延迟PCM访问。当发生页面错误时, 数据将直接从磁盘提取到DRAM缓存中。在从DRAM高速缓存中逐出页面之前,页面不会写入PCM。行级写入还可以减轻NVMM上的写入操作, 从而减少NVMM的磨损[16]。对于内存密集型工作负载,写操作可能集中在几行中。通过跟踪DRAM中的缓存行，只有脏行被写回PCM,而不是页面的所有行。提出了内存压缩机制[67 ,75] ,以提高MLC/TLC NVMM的寿命。在写入NVMM单元之前, 首先压缩数据。因此,只有一小部分NVMM单元被写入。然而, 耐久性的提高是以性能适度下降为代价的。如果NVMM单元以较低的功耗写入, 则该单元可以以较高的写入延迟为代价维持更多的写入。具体地, 当写入NVMM单元的速度下降N倍时,单元的耐久性可以提高N到N3倍。Mellow Write[76]探索了这一功能，以提高NVMM的寿命。为了减轻性能下降, Mellow Write只采用只有一次写入操作的缓慢的存储体写入。</p><h2 id="5-2-Wear-Leveling磨损均衡"><a href="#5-2-Wear-Leveling磨损均衡" class="headerlink" title="5.2 Wear-Leveling磨损均衡"></a>5.2 Wear-Leveling磨损均衡</h2><p>与减少写入的方法不同, 磨损均衡在所有NVMM页面之间均匀分布写入。尽管写入总数没有减少,磨损均衡技术可以防止某些页面被高强度写入而快速磨损。</p><p>对于NVMM ,我们可以记录每行的写入计数,以指导磨损均衡策略。但是,不能忽略外部存储开销。Start Gap[77]提出了一种细粒度磨损均衡方案。PCM页的行以旋转方式存储。在0和15之间随机生成旋转值,以指示移位的位置。对于具有16行的PCM页面,旋转值的范围可以从0到15。当旋转值为0时,页面存储在其原始地址中。如果旋转值为1 ,则第0行存储在第1行的物理地址中,并且每一行的地址都被旋转值移位。</p><p>在PDRAM[15]中,磨损均衡由写入计数阈值触发。当页面的写入计数超过给定阈值时,将触发页面交换中断以将页面迁移到DRAM。交换的PCM页面被添加到列表中，这些页面将再次重新定位。</p><p>Zombie[78]为实现weal-leveling(这是不是多写了一个l)提供了另一个方向,并进一步延长了PCM的整体寿命。与在PCM单元之间均匀分配写入的Start-Gap之外, Zombie利用禁用页面中的空闲块为工作内存提供更多的纠错资源。当PCM单元磨损时,它变得不可用。由于从软件的角度来看,内存占用的空间是按照页来组织的,因此包含故障单元的整个页面将被禁用。但是，如果提供了一些备用单元格来替换出现故障的单元,则可以再次使用该页面。这些备用单元称为纠错资源。当所有备用单元耗尽时, 最终放弃包含失败单元的页面。通常，当页面被禁用时,大约有99%的位可用。Zombie利用禁用页面中的大量好比特作为备用纠错资源,其中好比特被组织在细粒度块中。通过将工作页面与纠错资源配对, Zombie可以延长NVMM的使用寿命。</p><p>DRM[79]在虚拟地址空间和物理NVMM地址空间之间添加了中间映射层。在中间地址空间中，一个页面可能映射到PCM中的一个好页面或两个有故障的兼容PCM页面。兼容页面意味着一对具有错误字节的页面,但这些错误字节都不位于两个页面的同一位置。因此，两个兼容的页面可以被组合成一个新的好页面通过这种方式, DRM将PCM寿命显著提高了40倍。</p><h1 id="6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践"><a href="#6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践" class="headerlink" title="6 Practices of Hybrid Memory System Designs混合存储系统设计实践"></a>6 Practices of Hybrid Memory System Designs混合存储系统设计实践</h1><p>在本节中，我们从内存架构、OS支持的混合内存管理和NVMM支持的应用程序的角度介绍了我们最近在NVMM系统设计和优化方面的努力和实践，如图5所示。在下文中，我们将简要介绍我们的实践。</p><h2 id="6-1-Memory-Architectural-Designs内存架构设计"><a href="#6-1-Memory-Architectural-Designs内存架构设计" class="headerlink" title="6.1 Memory Architectural Designs内存架构设计"></a>6.1 Memory Architectural Designs内存架构设计</h2><p>在本小节中，我们将介绍我们对混合内存模拟和仿真、硬件/软件协同混合内存架构、细粒度NVM压缩和磨损均衡以及混合内存感知片上缓存管理的研究。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/37d5195a01334fcb9a70359bfca5ba53.png"></p><h3 id="6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真"><a href="#6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真" class="headerlink" title="6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真"></a>6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真</h3><p>混合存储器体系结构仿真是研究混合存储器系统的先决条件。我们将zsim[27]与NVMain[20]集成, 以构建全系统架构模拟器。Zsim是用于x86-64多核架构的快速处理器模拟器。它能够对多核、片上缓存层次结构、缓存-致性协议(如MESI)、片上互连拓扑网络和物理内存接口进行建模。<strong>Zsim</strong>使用Intel Pin工具包收集进程的内存跟踪,然后回放内存跟踪以表征内存访问行为。<strong>NVMain</strong>是用于NVMM的架构级主存储器模拟器。它能够模拟不同的内存配置文件, 如读/写延迟、带宽、功耗等。它还支持子阵列级内存并行性和不同的内存地址编码方案。此外，NVMain还可以对混合存储器（如DRAM和存储器层次结构中的不同NVMM）进行建模。由于操作系统级内存管理不是由zsim模拟的,因此我们通过添加Translation Lookaside Buffer（TLB）和内存管理模块（如伙伴内存分配器和页表）来扩展zsim , 以支持全系统模拟。实施细节参考我们的开源软件。我们的工作为研究界提供了一个快速、完整的体系结构仿真框架。它可以帮助研究人员了解不同的NVMM特性,设计混合存储系统,并以简单高效的方式评估各种系统设计对应用程序性能的影响。</p><h3 id="6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器"><a href="#6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器" class="headerlink" title="6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器"></a>6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器</h3><p>当前基于仿真的NVMM技术研究方法太慢,或者无法运行复杂的工作负载,例如并行和分布式应用程序。我们提出HME[28]，一种轻量级NVMM使用非统一内存访问（NUMA）架构的性能仿真器。HME利用商品Intel CPU中可用的硬件性能计数器来模拟较慢NVMM的性能特性。为了模拟NVMM的访问延迟,HME定期向远程NUMA节点上的DRAM访问注入软件生成的延迟。为了模拟NVMM带宽,HME利用DRAM热控制接口在短时间内限制对DRAM通道的内存请求量。不同于另一个NVMM仿真器Quartz[29] ,它不模拟NVMM的写入延迟,HME识别写直通和写回缓存逐出操作, 以分别模拟它们的延迟。通过这种方式,与Quartz相比, HME能够显著减少NVMM访问延迟的平均仿真误差[29]。在真正的NVMM设备Intel Optane DCPMM问世之前, 这项工作可以帮助研究人员和程序员评估NVMM性能特性对应用程序的影响, 并指导混合内存系统的系统设计和优化。</p><h3 id="6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存"><a href="#6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存" class="headerlink" title="6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存"></a>6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存</h3><p>基于我们的混合存储器模拟器， 我们提出了一种称为HSCC[18]的硬件/软件协同混合存储器架构。在HSCC中,DRAM和NVMM在物理上组织在单个存储器地址空间中，并且都用作主存储器。然而,DRAM在逻辑上可以用作NVMM的缓存,也可以由OS管理。图6显示了HSCC的系统架构。我们扩展了页表和TLB，以维护NVMM到DRAM的物理地址映射, 从而以缓存/内存层次结构的形式管理DRAMNVMM。通过这种方式, HSCC能够像虚拟到NVMM地址转换一样高效地执行NVMM到DRAM地址转换。此外, 我们在每个TLB条目和页表条目中.添加一个访问计数器，以监视内存引用。与以前在内存控制器或操作系统中监视内存访问的方法不同,我们的设计可以精确地跟踪所有数据访问, 而无需额外的存储(SRAM)和性能开销。我们通过动态阈值调整策略识别频繁访问的(热)页面，以适应不同的应用程序,然后将NVMM中的热页面迁移到DRAM缓存，以获得更高的性能和能效。此外,我们开发了一种基于实用程序的DRAM缓存填充方案,以平衡DRAM缓存的效率和DRAM利用率。由于软件管理的DRAM顶面能够映射到任何NVMM页面,因此DRAM实际.上用作完全关联的缓存。这种方法可以显著提高DRAM缓存的利用率,并且还提供了根据应用程序的动态内存访问行为重新配置混合内存架构的机会。由于CPU可以绕过DRAM缓存直接访问NVMM中的冷数据，因此DRAM既可以用作平面可寻址混合存储器架构中的主存储器,也可以用作分层混合存储器架构。因此,与最先进的工作相比,HSCC可以将系统性能显著提高9.6倍，能耗降低34.3%[16]。我们的工作为实现可重构混合存储器系统提供了第一个架构解决方案,该系统可以在水平和分层存储器架构之间动态改变DRAMNVMM管理。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c8216069dd1e4b42b6526c54ad69b35c.png"></p><p>我们进一步在HSCC上提出了以下技术,以提高缓存性能并改进磨损均衡机制。</p><p>由于NVMM块的缓存未命中惩罚是DRAM块的几倍,因此在平面可寻址混合存储器体系结构中，缓存命中率不是唯一需要改进的性能指标。为了最好地利用昂贵的LLC，我们提出了一种新的度量，即平均存储器访问时间（AMAT）,以评估混合存储器系统的总体性能。我们考虑了DRAM块和NVMM块的非对称缓存未命中惩罚，并提出了一种LLC未命中惩罚感知替换算法,称为MALRU[36,37] ,以改进混合存储器系统中的AMAT。MALRU动态地将LLC划分为保留区域和正常替换区域。MALRU优先替换LLC中的死DRAM块和冷DRAM块，使得NVMM块和热DRAM块保持在保留区域中。通过这种方式, 与LRU算法相比,MALRU实现了高达228%的应用程序性能改进。这项工作展示了混合存储器系统如何影响片上缓存的架构设计。</p><p>为了提高NVMM的写入耐久性，我们提出了一种新的NVMM架构,以支持空间无关数据压缩和磨损均衡[119]。由于许多应用程序的内存块通常包含大量零字节和频繁值, 我们提出了零重复数据消除和频繁值压缩机制（称为ZD-FVC[119]）, 以减少NVMM上的位写入。ZD-FVC可以集成到NVMM模块中，并完全由硬件实现, 无需任何操作系统的干预。我们在Gem5和NVMain模拟器中实现了ZD-FVC[119]，并使用SPEC CPU2006中的几个程序对其进行了评估。实验结果表明,ZD-FVC比几种最先进的方法要好得多。特别是, 与频繁值压缩相比, DZ-FVC可以将数据压缩比提高1.5倍。与数据比较写入相比，ZD-FVC能够将NVMM上的位写入减少30%，并将NVMM的寿命平均提高5.8倍。相应地, ZD-FVC还平均减少了43%的NVMM写入延迟和21%的能耗。我们的设计以简单高效的方式为NVMM提供了细粒度数据压缩和磨损均衡解决方案。它是其他磨损均衡方案的补充，以进一步提高NVMM寿命。</p><h1 id="6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件"><a href="#6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件" class="headerlink" title="6.2 System Software for Hybrid Memories混合存储器的系统软件"></a>6.2 System Software for Hybrid Memories混合存储器的系统软件</h1><p>在本小节中,我们介绍了软件层混合内存系统的实践，包括对象级混合内存分配和迁移、NUMA感知页面迁移、超级页面支持和NVMM虚拟化机制。</p><h3 id="6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移"><a href="#6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移" class="headerlink" title="6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移"></a>6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移</h3><p>页面迁移技术已被广泛用于改善混合存储器系统中的系统性能和能量效率。然而,以前的页面迁移方案都依赖于OS层中昂贵的在线页面访问监控方案来跟踪页面访问的最近性或频率。此外,由于额外的内存带宽消耗和缓存/TLB一致性保证机制,页面粒度上的数据迁移通常会导致非平凡的性能开销。</p><p>为了减轻混合内存系统中数据迁移的性能开销，我们提出了更轻量级的面向对象内存分配和迁移机制,称为OAM[120]。OAM的框架如图7所示。与之前的研究[44 ,121]不同, 我们进一步分析了细粒度时隙中的对象访问模式, 这些研究仅在静态对象放置的全局视图中描述了内存访问行为。OAM利用编译框架LLVM以对象粒度描述应用程序内存访问模式，然后将应用程序的执行分为不同阶段。OAM利用性能能量集成模型来指导不同执行阶段的初始内存分配和运行时对象迁移, 而无需对硬件和操作系统进行侵入性修改以进行在线页面访问监控。我们通过扩展Glibc库和Linux内核开发了新的内存分配和迁移API。基于这些API ,程序员能够将DRAM或NVMM显式分配给不同的对象, 然后迁移访问模式在DRAM和NVMM。我们开发了一个静态代码插入工具,可以自动修改遗留应用程序的源代码,而无需程序员重新设计应用程序。与最先进的页面迁移方法CLOCK-DWF[33]和2PP[44]相比, 实验结果表明,OAM可以分别显著降低83%和69%的数据迁移成本,并实现约22%和10%的应用程序性能改进。以前的持久内存管理方案通常依赖内存访问评测来指导静态数据放置,以及页面迁移（代价高昂）技术来适应运行时的动态内存访问模式。OAM提供了一种更轻量级的混合内存管理方案,支持细粒度对象级内存分配和迁移。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/257cf52418304fce863269f59094a936.png"></p><h3 id="6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理"><a href="#6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理" class="headerlink" title="6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理"></a>6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理</h3><p>在非统一内存访问（NUMA）架构中，不同NUMA节点中应用程序观察到的内存访问延迟通常是不对称的。由于NVMM比DRAM慢几倍，混合存储器系统可以进一步扩大不同NUMA节点之间的性能差距。NUMA系统的传统内存管理机制在混合内存系统中不再有效,甚至可能降低应用程序性能。例如,自动NUMA平衡（ANB）策略总是将远程NUMA节点中的应用程序数据迁移到运行应用程序线程或进程的NUMA节点。然而，由于远程DRAM的访问性能可能甚至高于本地NVMM,ANB可能会错误地将应用数据移动到较慢的位置。为了解决这个问题，我们提出了HiNUMA[60]，这是一种用于混合内存管理的新NUMA抽象。当应用程序数据首次放置在混合存储器系统中时，HiNUMA将应用程序数据放置在NVMM和DRAM上，以分别平衡带宽敏感应用程序和延迟敏感应用程序的内存带宽利用率。总访问延迟。初始数据放置基于NUMA拓扑和混合内存访问性能。对于运行时混合内存管理,我们提出了一个新的NUMA平衡策略,名为HANB[60]，用于页面迁移。HANB能够通过考虑数据访问频率和内存带宽利用率来降低混合内存访问的总成本。我们在Linux内核中实现HiNUMA ,无需对硬件和应用程序进行任何修改。与NUMA架构中的传统内存管理策略和其他最先进的工作相比, HiNUMA可以通过有效利用混合内存来显著提高应用程序性能。从HiNUMA[60]中学到的经验教训也适用于配备真正IntelOptaneDCPMM设备的混合内存系统。</p><h3 id="6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储"><a href="#6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储" class="headerlink" title="6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储"></a>6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储</h3><p>随着应用程序占地面积和相应内存容量的快速增长,虚拟到物理地址转换已成为混合内存系统的新的性能瓶颈。在大内存系统中, 超页已被广泛用于减轻地址转换开销。然而,使用超级页面的副作用是，它们通常会阻碍轻量级内存管理,例如页面迁移,而页面迁移在混合内存系统中被广泛用于提高系统性能和能效。不幸的是,同时拥有超级页面和轻量级页面迁移是一个挑战。</p><p>为了解决这个问题,我们提出了一种新的混合内存管理系统Rainbow[41]以弥合超级页面和轻量级页面迁移之间的根本冲突。如图8所示，Rainbow以超页（2MB）的粒度管理NVMM,并将DRAM作为缓存管理以基本页的粒度（4KB）将热数据块存储在超级页中。为了加快地址转换, Rainbow使用了拆分TLB的现有硬件功能来支持超级页面和普通页面。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/13d8d4f2c76f4dc28be42a4424d5faee.png"></p><p>我们提出了一种两阶段页面访问监控机制来识别超级页面中的热基页面。在第一阶段，Rainbow记录所有超级页面的访问计数以识别前N个热超级页面。在第二阶段,我们逻辑上将这些热超页分割成基本页（4KB） , 并进一步监视它们以识别热基本页。这些方案显著减少了页访问计数器的SRAM存储开销和由于对热基页进行排序而导致的运行时性能开销。通过新的NVMM到DRAM地址重新映射机制, Rainbow能够将热基页迁移到DRAM,同时仍能保证超级页TLB的完整性。拆分的超页TLB和基本页TLB是并行查阅的。我们的地址重映射机制在逻辑上使用超页TLB作为基本页TLB的缓存。由于超级页TLB的命中率通常很高,Rainbow能够显著加快基本页地址转换。为了进一步提高TLB命中率, 我们还扩展Rainbow以支持多个页面大小,并一起迁移相邻的热基页面[42]。与不支持超级页面的最先进混合内存系统[18]相比,Rainbow通过同时使用超级页面和轻量级页面迁移的优势，可以将应用程序性能显著提高最多2.9倍。</p><p>这项工作提供了硬件/软件协同设计，以弥合超级页面和轻量级页面迁移技术之间的根本冲突。这可能是减轻大容量混合存储器系统中不断增加的虚拟到物理地址转换开销的一个有前途的解决方案。</p><h3 id="6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理"><a href="#6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理" class="headerlink" title="6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理"></a>6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理</h3><p>NVMM有望在云和数据中心环境中更受欢迎。然而,关于将NVMM用于虚拟机（VM）的研究很少。我们提出了HMvisor[61],一种管理程序/虚拟机协同混合内存管理系统, 以有效利用DRAM和NVMM。如图9所示, HMvisor利用伪NUMA机制来支持VM中的混合内存分配。由于VM中的虚拟NUMA节点可以映射到不同的物理NUMA节点, HMvisor可以将不同的内存区域映射到单个VM ,从而向VM暴露内存异质性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1402aec0f84e49268d98793dc3a37f4c.png"></p><p>为了支持VM中的轻量级页面迁移,HMvisor监控页面访问计数并进行虚拟机管理程序中的热页和冷页分类，然后VM通过域间通信机制周期性地收集热页面的信息。我们在VM中实现了一个可加载的驱动程序,以在DRAM和NVMM之间执行进程级页面迁移。由于HMvisor由VM本身执行页面迁移,因此HMvisor无需暂停VM进行页面迁移。HMvisor还提倡混合内存资源交易策略,以动态调整VM中NVMM和DRAM的大小。通过这种方式,HMvisor可以满足多样化应用程序的不同内存需求（容量或性能） ,同时保持VM的总货币成本不变。</p><p>HMvisor的原型在QEMU/KVM平台上实现。我们的评估表明,HMvisor能够以仅5%的性能开销为代价将NVMM写入流量减少50%。此外, 动态内存调整策略可以在VM承受高内存压力时显著减少VM中的主要页面错误,因此甚至可以将应用程序性能提高30倍。</p><p>这是一项在虚拟化环境中管理混合内存的早期系统工作。所提出的方案完全由软件实现, 因此也适用于新Intel Optane DCPMM设备的混合存储系统。</p><h2 id="6-3-NVMM-Supported-Applications-NVMM支持的应用程序"><a href="#6-3-NVMM-Supported-Applications-NVMM支持的应用程序" class="headerlink" title="6.3 NVMM-Supported Applications NVMM支持的应用程序"></a>6.3 NVMM-Supported Applications NVMM支持的应用程序</h2><p>由于混合存储器系统可以提供非常大容量的主存储器,因此它们已被广泛用于大数据应用,例如内存中的关键值KV存储和图形计算。在本小节中,我们介绍了NVMM支持的针对这些应用程序的系统优化实践。</p><p>具有大容量内存的内存KV存储可以在主内存中缓存更多的热数据,从而为应用程序提供更高的性能。然而,在混合内存系统中直接部署传统的KV存储（如memcached）存在若干挑战。例如，如何有效地识别热KV对象?如何重新设计NVMM友好的KV索引以减少NVMM写入?如何重新设计缓存替换算法以平衡混合内存系统中的对象访问频率和最近性?如何解决slab calcification问题[122] ,以在混合存储器系统中最佳地利用DRAM资源?</p><p>为了解决上述问题，我们提出HMCached[80] ,这是混合DRAM/NVMM系统的KV缓存（memcached）的扩展。图10显示了HMCached的系统架构。HM缓存跟踪KV对象访问并记录每个KV对的元数据结构中的进程计数, 因此HMCached可以轻松识别NVMM中频繁访问的对象,并将它们迁移到DRAM。这样, 我们逻辑上将DRAM用作NVMM的专用缓存,以避免更昂贵的NVMM访问。此外, 我们通过拆分基于哈希的KV索引来重新设计NVMM友好的KV数据结构, 以进一步减少NVMM访问。我们将KV对象的频繁更新元数据（例如,引用计数、时间戳和访问计数）放在DRAM中,其余部分（例如，键和值）放在NVMM中。我们利用多队列算法[118]来考虑DRAM缓存替换的对象访问频率和最近性。此外，我们建立了一个基于效用的性能模型来评估板类重新分配的效益。我们的动态slab重新分配策略能够有效解决slab calcification问题,并在数据访问模式发生变化时显著提高应用程序性能。与普通memcached相比HMCached可以显著减少70%的NVMM访问, 并实现大约50%的性能改进。此外，HMCached能够降低75%的DRAM成本,同时性能下降不到10%。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/76462968db584a9c8136d9c88d89346e.png"></p><p>据我们所知,我们是第一个探索混合存储系统中KV存储的对象级数据管理的。我们基于Memcached实现HMCached并开放源代码。我们发现，后来的研究（如flatstore[90]）也有类似的想法来解耦KV存储的数据结构。</p><p>今天,我们已经看到了许多内存中的图形处理系统,其中应用程序的性能与主内存的容量高度相关。高密度和低成本NVMM技术对于降低图形处理的I/O成本至关重要。如图11所示, 与基于SSD的存储系统相比,混合存储系统可以显著提高应用程序性能。图12显示了混合存储器系统和仅DRAM系统之间的应用程序性能差距。我们提出了NGraph，一种新的图形处理框架，专门设计用于更好地利用混合存储器。我们基于不同图形数据的访问模式开发混合内存感知数据放置策略,以减轻对NVMM的随机和频繁访问。通常，图形结构数据占总图形数据的大部分。NGraph根据目标顶点划分图形数据,并采用任务分解机制来避免多个处理器之间的数据争用。此外,NGraph采用了工作窃取机制，以最小化多核系统上并行图形数据处理的最大时间。我们称为ReRAM技术的。</p><p>基于图形处理框架Ligra[123]实现NGraph。与最先进的Ligra相比,NGraph可以将应用程序性能提高48%。从这项工作中获得的经验教训[91]可用于在配备真实PM设备的图形处理平台中进一步提高大规模图形分析的性能。</p><div class="note info no-icon"> 华科的工作关注的问题方向有的还挺小的，如果专注于某个问题的话，最后系统设计出来也是比较偏向于适合某个场景的。从后面第7章节来看，这个发展好像是走向越来越专业化的，不同的需求不同的架构和系统设计。 </div> <h1 id="7-Research-Directions研究方向"><a href="#7-Research-Directions研究方向" class="headerlink" title="7 Research Directions研究方向"></a>7 Research Directions研究方向</h1><p>NVMM技术的出现在材料、微电子、计算机架构、系统软件、编程模型和大数据应用领域引起了许多有趣的研究课题。随着IntelOptaneDCPMM等真正的NVMM设备越来越多地应用于数据中心环境,NVMM可能会改变数据中心的存储环境。我们的经<br>验和做法进行了一些初步和有趣的研究。在下文中,我们分享了NVMM未来研究方向的愿景,并分析了研究挑战和新机遇。图13说明了不同维度NVMM技术的未来趋势。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/21935e2720af4afeae966b3ce6e3724d.png"></p><ol><li><p>3D堆叠NVMM技术的发展仍在继续。NVMM有望提供更高的集成密度以降低成本。目前, 高端NVDIMM对于企业应用来说仍然过于昂贵。NVMM与传统DRAM和NAND闪存竞争的关键挑战是存储密度或每字节成本。NVMM技术主要有两种单片3D集成机制[124]。一种是将例如Intel/Micron 3DX点。另一种是垂直3D堆叠结构。然而, 3D集成技术尚未成熟。仍然存在许多挑战,如制造成本、柱电极电阻和潜路径问题。</p></li><li><p>NVMM越来越多地用于分布式共享存储器系统。随着NVMM的密度不断增加，单个服务器中的主内存容量可以达到数百TB。为了提高大容量NVMM的利用率,必须通过远程直接内存访问（RDMA）技术在多个服务器之间共享它们。使用NVMM的典型方法是将来自多个服务器的所有可共享内存聚合到混合共享内存资源池中, 例如Hotpot[49,125,126]。所有内存资源在全局内存空间中共享。有一些关于在数据中心和云环境中使用NVMM的初步研究[49,126,127]。使用PM的一个新趋势是将其作为分类内存进行管理,就像传统的分类存储系统一样。此型号与以前的共享PM系统不同, 在该系统中，PMDIMM分布在多个服务器中，由用户级应用程序共享。这些计算内存紧密耦合的体系结构在可管理性、可扩展性和资源利用率方面有几个缺点。相比之下，在少数存储器节点中配备有大量PM的分解PM系统可以由计算节点通过高速结构连接。这种计算/内存分类架构可以更容易地减轻数据中心环境中的上述挑战。然而,仍然存在许多挑战。例如,NVMM的持久性特性也应在分布式环境中得到保证。传统的PM管理指令（如clflush和mfence） 只能保证数据在单个服务器中持久化，但不能保证数据通过RDMA网络持久化到远程服务器。对于每个RDMA操作,一旦数据到达远程服务器中的网络接口卡（NIC），它就会向数据发送方发出确认。由于NIC中有数据缓冲区，数据不会立即存储到远程NVMM。如果此时发生电源故障,则无法保证数据持久性。因此,必须重新设计RDMA协议以支持flushing原语。此外,计算节点应支持对用户级应用程序透明的远程页面交换。为了支持这种机制应该重新设计传统的虚拟内存管理策略。另一方面,由于PM表现出类似内存的性能, 并且是字节可寻址的，因此需要对内存调度和管理进行新的设计，以适应分解的PM。</p></li></ol><p>3)基于NVMM的计算存储器集成计算机体系结构正在兴起。例如,新兴NVMM在存储器内处理（PIM）[95,96]和近数据处理（NDP）[128,129]架构中的应用正在兴起。PIM和NDP近年来已成为新的计算范式。NDP是指将处理器与存储器集成在单个芯片上,以便计算能够尽可能接近地访问存储器中的数据。NDP能够显著降低数据移动的成本。实现这一目标主要有两种方法。一种是将小型计算逻辑（如FPGA/ASIC）集成到存储器芯片中，以便在数据最终被提取到CPU之前对其进行预处理。另一种方法是将内存单元（HBM/HMC）集成到计算（CPU/GPGPU/FPGA）中。该模型通常用于许多处理器架构, 如IntelO Xeon PhiTM Knights Landing系列、NVIDIAO tesla V100和Google Tensor处理单元（TPU）。PIM指的是完全在计算机内存中处理数据。它通过在主存储器中执行计算,提供了高带宽、大规模并行性和高能量效率。使用NVMM（如ReRAM）的PIM通常可以并行计算两个或多个内存行的位逻辑, 并支持一步多行操作。该范例对于模拟计算方式中的矩阵向量乘法特别有效，并且可以实现极大程度的性能加速和节能。因此, PIM在加速机器学习算法（如卷积神经网络）中得到了广泛的研究（CNN）和深度神经网络（DNN）。尽管在PIM架构中.使用NVMM技术的兴趣越来越大[94-96 ,130]， 但目前的研究主要基于电模拟,没有一项研究可用于中型原型设计。</p><p>4)除了传统应用之外, 一些使用NVMM的新应用正在出现。尽管NVMM技术已初步应用于许多大数据应用, 如KV存储、图形计算和机器学习,但大多数编程框架/模型和运行时系统都是为磁盘设备和基于DRAM的主存储器而设计的, 它们在混合存储系统中并不有效。例如, 这些系统中广泛使用缓冲和延迟写入机制来隐藏I/O操作的高延迟。然而,混合存储器系统中可能不需要这些机制,甚至可能会损害应用程序性能。应重新设计Hadoop/Spark/GraphChi/Tensorflow等大数据处理平台,以适应NVMM技术的特点。除了这些传统应用之外,一些基于NVMM的新型应用正在出现。例如,有一些建议通过利用NVMM的切换过程的内在变化,将NVMM用作硬件安全原语, 如物理不可克隆函数（PUF）[131]。 PUF通常用于具有高安全性要求的应用,例如密码学。最近,已经提出了许多基于NVMM技术的逻辑电路并将其原型化[132-134]。例如，ReRAM技术被提议用作基于ReRAM的FPGA的可重构开关[133]。此外，STT-RAM技术被提出用于设计非易失性缓存或寄存器[135]。</p><h1 id="8-Conclusions结论"><a href="#8-Conclusions结论" class="headerlink" title="8 Conclusions结论"></a>8 Conclusions结论</h1><p>与传统DRAM技术相比, 新兴的NVMM技术具有许多良好的特性。它们有可能从根本上改变存储系统的面貌，甚至为计算机系统添加新的功能和特性。现在有很多机会重新思考当今计算机系统的设计,以实现系统性能和能耗的数量级改进。本文从内存体系结构、操作系统级内存管理和应用程序优化的角度全面介绍了最新的工作和我们的实践。我们还分享了我们对NVMM技术未来研究方向的展望。通过利用NVMM的独特特性, 有巨大的机会来创新未来的计算范式, 开发NVMM的多种新颖应用。</p><h2 id="积累"><a href="#积累" class="headerlink" title="积累"></a>积累</h2><p>一些可以进一步看一看的文献：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3 GB/s，而DRAM的读写带宽之间的差距要小得多(1.3x)。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。</p><div class="note danger"> 但是这个可能不是我想要的CPU和内存的延迟带宽之类的，可能是针对做持久性时的数据 </div> <p>Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面。</p><p>Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性.</p><p>“如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。”看看华科廖小飞团队在这个方面最近的工作。</p><p>“由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。”看一看这些有没有做内存的延迟和能耗的统计。</p><p>文中提到和写密集有关的（但是这个是store还是write还是要看看，有的需求write挺少的）CLOCK-DWF[33]，PDRAM[15]，RaPP[14]</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> B </tag>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</title>
      <link href="/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/"/>
      <url>/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Adnan Maruf, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Ashikee Ghosh, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Janki Bhimani, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Daniel Campello, Google</li><li>Andy Rudoff, 英特尔公司</li><li>Raju Rangaswami, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>将PM作为第二级内存直接暴露给CPU是目前比较有希望的一个做法：如何把数据在正确时间放入正确分层中去。于是面临一个是大家关注的问题。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>动机：通过四个工作负载，统计归类50个采样页面的访问模式，得出层级友好页面是需要迁移的对象（空间局部性）。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b9a6c5150a4c455c9eca0fbbcd1aaf99.png" alt="热力图"><br>同时一段时间访问过的页面在下一段时间被访问概率也很大（时间局部性）<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cb7d1ef346b84032b6cae36568adf862.png" alt="相线图表示被访问的概率在不同时间窗口的变化"><br>然后说明了用frequency&amp;recency识别到的页面也具有层级友好的特征。相比于静态分层，说明了动态分层的必要。最后把整个比较模糊的大问题转化为：试图解决分层系统中：如何根据frequency&amp;recency来识别升级的热点页?如何在内核中设计一个简单、低开销而又高效的系统？</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>这些都是分层技术上的对比：<br>静态分层即一个内存页一旦被映射到一个分层，在其生命周期内就不会被重新分配到不同的分层。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9b9a94f135544a11a3dfdb7aca8ed615.png" alt="现有内存分层技术的比较"><br>[11]Nimble：只根据recency来选择页面，这篇的作者为了解决frequency决定把工作负载执行期分为观察窗口和性能窗口。得出观察窗口频率高的，在性能窗口概率也高。专注于透明大页（THP）迁移。应用程序需要通过Nimble的启动器运行以利用其页面迁移技术。MC在内置内核实现的功能。Nimble需要一个额外的启动器来运行内核上的任何工作负载。</p><p>[12]AutoTieringhint page fault的缺页异常来跟踪页面访问，并使用recency来识别热点页进行升级。尽管缺页异常可以提供高准确度的页面访问跟踪，但跟踪所有页面的成本很高，因为每一个页面故障都必须在访问页面之前进行处理。这是因为基于缺页的软件页面访问跟踪成本很高，而且跟踪页面历史位以识别冷页面的开销也很大。所以在后面工作负载测试中表现很差。</p><p>[19]Thermostat源代码不可用没有评估，通过poisoning页表项（PTE）和触发缺页异常来跟踪巨大的页面，并将冷页面迁移到较低的内存层。</p><p>[22]AMP（非统一内存访问架构NUMA，主板会分成不同的插槽，每一个插槽一组cpu，以及和这组cpu离得近的内存。）这在两个插座的NUMA机器中是不现实的，因为每个节点通常有自己的DRAM、PM和CPU。AMP使用一个节点，只用于DRAM的分配，其他节点只用于PM的分配。AMP是在Linux内核4.15版本上实现的，它不支持所需的KMEM DAX驱动（从内核v5.1开始提供），以便PM作为主内存在分层系统中使用。AMP的核心设计原则要求它扫描和剖析来自DRAM和PM层的所有内存页，这在实际系统的内核中是不现实的，因为在我们评估的工作负载中，内存页的数量可以增长到数亿。<br>对比于PM当前这款硬件的两种用法。</p><p>[7]Memory-mode：数据不能持久化，dram作为缓存不透明。缓存是需要从高层获取数据的，而这里提出的分层是两层都能被直接访问的。</p><p>[44]对象级需要改变应用程序API，而内核级别的修改不需要应用程序有何变化。</p><p>[32]提出了一种有效使用持久性内存作为NUMA节点的设计。这个分层设计同时意识到了DRAM和PM节点 ,并且只通过NUMA平衡处理匿名页面的升级/移动。</p><p>[33-36]不需要硬件，而且都是主存没有缓冲。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>设计MULTI-CLOCK的主要假设是，最近被访问过一次以上的页面，在不久的将来更有可能被访问。</p><p>具体升级降级要求门槛就是那张图。（但是频繁程度不够吧？）及时更新页面引用状态的方式根据对内存页的访问模式不同而不同。无监督式：CPU在进程的页表入口中设置的页面引用位。 <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ca0046254e904f9991e9b0242696b89f.png" alt="文章的idea"></p><p>与CLOCK类似，升级是由每次系统守护程序kpromoted去完成的。它定期被唤醒，以扫描列表，更新它们，并将最近由无监督访问产生的升级列表中页迁移到更高层的页。</p><p>降级机制基于今天的虚拟内存系统中的页面驱逐技术。当内存达到内存压力时（该层与系统总内存量来计算）会去扫描每个列表。活动列表中的页面相对于非活动列表的比例超过了一个与该层可用内存量相关的阈值时，那么在活动列表中没有被标记为引用的页面将被移至非活动列表。最后，非活动列表被扫描，以寻找未被标记为引用的页面，并将其迁移到较低的层级。当没有更低层级列表可以迁移，就写回块存储。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>评估的目的是确定MULTI-CLOCK是否、何时以及如何能够提高应用工作负载的性能。</p><p>使用雅虎云服务基准（YCSB）[13]的六个不同的工作负载和GAP基准套件(GAPBS) [14] 的六个工作负载来讨论我们的结果。Memcached[3]，一个使用大量主内存来维护其数据的内存缓存服务，作为YCSB的键值存储后端。 配置时内存要被全部消耗，并且消耗一部分PM。</p><p>MULTI-CLOCk在所有YSCB工作负载上都优于静态分层、Nimble、AT-CPM和AT-OPM。对GAPBS的执行时间也比其他方案减少。</p><p>分析了MULT-CLOCK和Nimble所升级的页面数量。MULTI-CLOCK每次扫描平均升级758页，最多扫描1024。而N是把1024作为固定值。如果那些将来不会再被重新访问的页面被提升到DRAM中，那么提升这些页面的开销会损害系统性能。第二次再次被访问的百分比比N高15%</p><p>实现了目标，低开销，特别是密集型应用。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ol><li>能耗方面摘要提了一下，后面也没说呀。</li><li>作者源码有一些问题，在运行高性能计算用MIP的时候会内核崩溃。</li><li>后面的性能评估对比了几种相关的工作，但是选择的workload也是比较局限的，GAPBS的论文都没有正式发出来。</li></ol><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><ol><li>在内核上修改代码，不需要其他程序有什么修改。</li><li>使用升级列表和频率可以很好的利用层级友好页面的局部性。</li></ol><h2 id="9-相关链接"><a href="#9-相关链接" class="headerlink" title="9. 相关链接"></a>9. 相关链接</h2><ul><li><a href="https://github.com/sylab/multi-clock">作者公布的源码</a></li><li><a href="https://docs.pmem.io/ndctl-user-guide/">ndctl用户手册</a></li><li><a href="https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/">PM的AD模式下的kmem模式设置</a></li><li><a href="https://github.com/brianfrankcooper/YCSB">YCSB工作负载源码</a></li><li><a href="https://cloud.tencent.com/developer/article/1004637">YCSB介绍与相关运行配置介绍</a></li><li><a href="https://github.com/pmem/ndctl/tree/main/Documentation/daxctl">ndctl下的daxctl安装源码下载</a></li><li><a href="https://memark.io/index.php/2021/04/09/pmem_intro/">持久内存开发资料汇总</a></li><li><a href="https://blog.csdn.net/qq_37858386/article/details/78444168">Linux驱动编程中EXPORT_SYMBOL介绍（因为作者公布的源码里有这个报错）</a></li><li><a href="https://github.com/pmem/ndctl/issues/108">daxctl fails to reconfigure to system-ram when DAX modules built-in</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/guide/qsg-intro-to-provisioning-pmem.html">持久内存配置简介，快速入门</a></li><li><a href="https://github.com/sbeamer/gapbs">GAPBS工作负载下载和使用</a></li></ul><h2 id="10-积累"><a href="#10-积累" class="headerlink" title="10. 积累"></a>10. 积累</h2><p> 当使用PM作为主存储器时，其持久性能力变得无关紧要（这里作者遵循原先的主存的设计，即易失性，抛弃了NVM数据持久的特点，后面提到的该产品Memory Mode，并且文献[10]说PMEP想要持久性靠的是<code>clflush</code>指令，这个指令由于要排序开销很大），从而完全避免了其最大的性能开销[10]</p><p> 有开源内核好像有一个选项可以计数随时间变化页面在层级间迁移的数量。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time Complexity Calculation</title>
      <link href="/2021/06/06/Time-Complexity-Calculation/"/>
      <url>/2021/06/06/Time-Complexity-Calculation/</url>
      
        <content type="html"><![CDATA[<h2 id="一、常见阶大小比较"><a href="#一、常见阶大小比较" class="headerlink" title="一、常见阶大小比较"></a>一、常见阶大小比较</h2><p>从大到小：  </p><ul><li>超指数阶：$n^n$，$n!$</li><li>指数阶：$9^{n/2}$,  $2^n$</li><li>多项式阶：$n^3$,   $n*log(n)$, $n^{1/2}$</li><li>对数阶：$log^2(n)$, $log(n)$, $log(log(n))$</li><li>常数阶：100, 1<br>下题需要保留阶最高的部分：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220748744.jpg"></li></ul><h2 id="二、算法复杂性估计函数"><a href="#二、算法复杂性估计函数" class="headerlink" title="二、算法复杂性估计函数"></a>二、算法复杂性估计函数</h2>$$\lim_{n \to \infty} \frac{f(n)}{g(n)}  =\begin{cases}(大于0的常数或)0       &amp;&amp;&amp; f(n)=O(g(n))上界&amp;-----f(n)\le cg(n)\\(大于0的常数或)无穷    &amp;&amp;&amp; f(n)= \Omega(g(n))下界&amp;-----f(n)\ge cg(n)\\大于0的常数            &amp;&amp;&amp; f(n)= \Theta(g(n))确切界&amp;-----f(n)=cg(n)\\0      &amp;&amp;&amp;f(n)=o(g(n))上界&amp;-----f(n)&lt; cg(n)\end{cases}$$<p>可以发现都是针对f(n)在讨论，很容易得出下题答案：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220628312.jpg"></p><h2 id="三、几个常用替换的式子"><a href="#三、几个常用替换的式子" class="headerlink" title="三、几个常用替换的式子"></a>三、几个常用替换的式子</h2><h3 id="1-Stirling公式："><a href="#1-Stirling公式：" class="headerlink" title="1.Stirling公式："></a>1.Stirling公式：</h3>$$n! \approx {(2 \pi n)}^{1/2}{(n/e)}^n$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605222019202.jpg"></p><h3 id="2-阶乘和二项式系数"><a href="#2-阶乘和二项式系数" class="headerlink" title="2.阶乘和二项式系数"></a>2.阶乘和二项式系数</h3>$$C_n^k = C_n^{n-k} \\ C_n^n = C_n^0 = 1 \\C_n^k = C_{n-1}^k +C_{n-1}^{k-1} $$<p>帕斯卡三角形可以辅助记二项式系数：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060522532381.jpg"></p>$$\sum_{j=0}^{n}{C_{n}^j}x^j= {(1+x)}^n $$<h3 id="3-和式"><a href="#3-和式" class="headerlink" title="3.和式"></a>3.和式</h3>$$\sum_{j=1}^{n}{a_{n-j}} = \sum_{j=0}^{n-1}{a_j} \\\sum_{j=0}^n{j \over 2^j} = \sum_{j=1}^n{j \over 2^j} = 2-{{(n+2)} \over {n^2}} = \Theta(1) \\\sum_{j=0}^njc^j = \sum_{j=1}^njc^j = \Theta(nc^n)$$<h3 id="4-定积分与和式转换"><a href="#4-定积分与和式转换" class="headerlink" title="4.定积分与和式转换"></a>4.定积分与和式转换</h3>$$\int_m^{n+1} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m-1}^nf(x)dx 递减函数 \\\int_{m-1}^{n} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m}^{n+1}f(x)dx  递增函数$$<p>可以采用画图的方法辅助记忆它的上下界：<br>以一个递增的函数为例，我们要求1（m）~6（n）他的面积，每一个小矩形$1*f(j)$如果我们积分每个点左边的矩形，那么总面积就是偏小的，积分右边矩形就会稍微偏大，这就找到了上下界，当函数平行于X轴时就会有等号。递减也是一个道理。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606094351964.jpg"><br>但是，如果是logn等函数会遇到定义域不存在的情况。我们应该从和式中把（在积分中）没有定义的点先拿出来，再去积分。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060610042349.jpg"></p><p>用代数方法证明就是放大缩小去求，用积分方法证明就是用上面那个公式。可以看到积分方法求时等式左边按公式应该为$\int_0^n jlogj !\ dj$定义域<br>不存在，所以对于右边：</p>$$\sum_{j=1}^njlogj=\sum_{j=2}^njlogj+1log1=\int_{2-1}^njlogj \!\ dj+1log1$$<h2 id="四、计算次数"><a href="#四、计算次数" class="headerlink" title="四、计算次数"></a>四、计算次数</h2><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法:COUNT输入:n=2k,k为正整数。输出: count的值 。count=0while n&gt;=1for j=1 to ncount=count+1n=n/2return countend COUNT<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>while要执行k+1次，$k=log_2n$.for循环在每次while的基础上执行n次所以(k+1)n即$nlog_2n$次计算</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法: MERGE输入:数组A[1..m]和它的三个索引p, q, r, 1&lt;=p&lt;=q&lt;r&lt;=m。两个子数组A[p..q]和A[q+1..r]各自按升序排列。输出:合并两个子数组A[p..q]和A[q+1..r]的升序数组A[p..r]for(s=p, t=q+1, k=p; S&lt;=q and t&lt;=r; k++)if A[s]&lt;=A[t] //两个指针从两个头开始排序B[k]=A[s]; //B[p..r]是个辅助数组S=S+1;elseB[k]=A[t];t=t+1;if s=q+1 B[k..r]=A[q+1..r] elseB[k..r]=A[s..q]A[p..r]=B[p..r]end MERGE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两段相邻数组分别有序，两个指针将两段变为有序的。2(r-p+1)先遍历一次排序，在从排好序的辅助数组移回来。</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">void insertion_ sort(Type *a, int n){// 代价 次数// ti: for的第i次while的循环次数for (int i=1; i&lt;n; i++){// c1   n(比较语句)key=a[i];// c2   n-1int j=i-1;// c3   n-1while( j&gt;=0 &amp;&amp; a[j]&gt;key ){  // c4   sum tia[j+1] = a[j];// c5   sum of (ti-1)j--;// c6sum of (ti-1)}a[j+1]=key; // c7   n-1}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>插入排序的思想是左手为空，右手的牌按序插入。这里t是个不确定的数，但是还是可以得出计算次数为：</p><p>$$c_1n+c_2(n-1)+c_3(n-1)+c_4\sum_{i=1}^{n-1}{t_i}+c_5\sum_{i=1}^{n-1}{(t_i-1)}+c_6\sum_{i=1}^{n-1}{(t_i-1)}+c_7(n-1)$$</p><p>最好情况就是已经排好序了，c5与c6都是0，每次for的while都只跑一次。<br>$$c_1n+c_2(n-1)+c_3(n-1)+c_4(n-1)+c_7(n-1) = O(n)$$<br>最坏情况就是倒序排的，n张牌每次都比上一次多查找一个。</p><p>$$\sum_{i=1}^{n-1}{(t_i-1)} = n(n-1)/2$$</p><p>复杂度$O(n^2)$</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">1. COUNT42.count &lt;-- 03.for i ←- 1 to Llogn」4.for j ←- i to i+55.for k ←- 1 to i^26.         count ←- count +17.end for8.  end for9.end for(a)第6步执行了多少次?(b)要表示算法的时间复杂性，用0和O哪个符号更合适?为什么?(c)算法的时间复杂性是什么?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>a）思路是把每次for循环乘起来。<br>$$\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}\sum_{k=1}^{i^2}{1}$$内层指的是从1到$i^2$个1相加$=\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}{i^2} = 6\sum_{i=1}^{\lfloor logn \rfloor}{i^2}$<br>利用平方和求和公式$n(n+1)(2n+1)/6$进一步化简$=\lfloor logn \rfloor(\lfloor logn \rfloor +1)(2\lfloor logn \rfloor+1)$<br>b） O 因为对于算出的确切的计算次数，这个用于表示算法时间复杂性的函数是它上界。<br>c）$O(log^3n)$</p><h2 id="五、解递归方程式"><a href="#五、解递归方程式" class="headerlink" title="五、解递归方程式"></a>五、解递归方程式</h2><h3 id="1-线性齐次递推式（二阶）"><a href="#1-线性齐次递推式（二阶）" class="headerlink" title="1.线性齐次递推式（二阶）"></a>1.线性齐次递推式（二阶）</h3><p>对于递推式:$$f(n)=a_1f(n-1)+a_2f(n-2)+…+a_kf(n-k)$$我们想要得到$f(n)$的确切解，它的解往往是$x^n$于是我们可以把这个递推式的等价于:$$x_n=a_1x^{n-1}+a_2x^{n-2}+…+a_kx^{n-k}$$将两边同时除以$x^{n-k}$并且移项可以得到与n无关还能解出x的式子$$x^k-a_1x^{k-1}-a_2x^{k-2}-…-a_k = 0$$这个就是常说的特征方程。</p><table><thead><tr><th align="center">步骤</th><th align="center">例1</th><th align="center">例2</th></tr></thead><tbody><tr><td align="center">序列</td><td align="center">1,4,16,64,256</td><td align="center">1,1,2,3,5,8(斐波拉契)</td></tr><tr><td align="center">递推关系</td><td align="center">f(n)=3f(n-1)+4f(n-2)</td><td align="center">f(n)=f(n-1)+f(n-2)</td></tr><tr><td align="center">特征方程</td><td align="center">$x^2-3x-4=0$</td><td align="center">$x^2-x-1=0$</td></tr><tr><td align="center">特征根</td><td align="center">$x_1=-1,x_2=4$</td><td align="center">$x_1= { {1+\sqrt5} \over 2},x_2={ {1-\sqrt5} \over 2}$</td></tr><tr><td align="center">通解</td><td align="center">$f(n) = c_1{(-1)}^n+c_24^n$</td><td align="center">$f(n)=c_1\left( { {1+\sqrt5} \over 2} \right)^n+c_2\left( { {1-\sqrt5} \over 2} \right)^n$</td></tr><tr><td align="center">带入序列中的点</td><td align="center">$c_1=0,c_2=1$</td><td align="center">$c_1={1\over {\sqrt5}},c_2=-{1\over {\sqrt5}}$</td></tr><tr><td align="center">最终解</td><td align="center">$f(n)=4^n$</td><td align="center">由于n无穷大$c_2\left( { {1-\sqrt5} \over 2} \right)^n$趋于0,$f(n)={1\over {\sqrt5}}\left( { {1+\sqrt5} \over 2} \right)^n$</td></tr></tbody></table><p>还有种特殊情况：$x_1=x_2=x$时$f(n)=c_1nx^n+c_2x^n$</p><h3 id="2-非齐次递推式"><a href="#2-非齐次递推式" class="headerlink" title="2.非齐次递推式"></a>2.非齐次递推式</h3><h4 id="2-1-f-n-f-n-1-g-n"><a href="#2-1-f-n-f-n-1-g-n" class="headerlink" title="2.1  f(n)=f(n-1)+g(n)"></a>2.1  f(n)=f(n-1)+g(n)</h4><p>对于这一类g(n)是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)+g(n) = \big(f(n-2)+g(n-1)\big)+g(n) = f(0)+ \sum_{j=1}^ng(j)$$<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606120205290.jpg"><br>这道题麻烦点在于怎么把前面的系数3搞没了。由1我们知道，这种递推式是$f(n)=x^n$变形令$f(n)=3^nq(n), f(0)=q(0)=3$于是乎原式变为：</p>$$3^nq(n)=3*3^{n-1}q(n-1)+2^n \\ q(n)=q(n-1)+{(2/3)}^n\\ q(n)=q(0)+\sum_{j=1}^n{{(2/3)}^n}\\ f(n)=3^n*\big(3+{(2/3)(1-{(2/3)}^n) \over {1-(2/3)}}\big) \\ f(n)=5*3^n+2^{n+1}$$<h4 id="2-2-f-n-f-n-1-g-n"><a href="#2-2-f-n-f-n-1-g-n" class="headerlink" title="2.2  f(n)=f(n-1)*g(n)"></a>2.2  f(n)=f(n-1)*g(n)</h4><p>对于这一类g(n)也是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)*g(n) = \big(f(n-2)*g(n-1)\big)+g(n) = f(0)\prod_{i=1}^ng(i)$$</p><h4 id="2-3-f-n-f-n-1-g-n-h-n"><a href="#2-3-f-n-f-n-1-g-n-h-n" class="headerlink" title="2.3  f(n)=f(n-1)*g(n)+h(n)"></a>2.3  f(n)=f(n-1)*g(n)+h(n)</h4><p>可以直接推，也可以带点技巧推。结果：$$=\prod_{i=1}^ng(i)\big( f(0)+\sum_{j=1}^n{h(j) \over{\prod_{i=1}^ng(i)} } \big)$$</p><h4 id="2-4-f-n-af-n-c-g-n"><a href="#2-4-f-n-af-n-c-g-n" class="headerlink" title="2.4 f(n)=af(n/c)+g(n)"></a>2.4 f(n)=af(n/c)+g(n)</h4>$$f(n)=\begin{cases} d&amp; \text{n=1}\\af({n\over c})+bn^x&amp; \text{n &gt;= 2} \end{cases}$$<p>其中d非负常量，g(n)非负函数，a，c正数。设$n=c^k$</p>$$f(n)=\begin{cases} bn^x*log_cn^x+dn^x&amp;{a=c^x}\\ \big(d+{{bc^x}\over{a-c^x}}\big)n^{log_ca}-\big({{bc^x}\over{a-c^x}}\big)n^x&amp; {a\neq c^x} \end{cases}$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606123724891.jpg"><br>这道题就没有为难菜鸡，直接把方向给了。这种做法在遇到f(n/2)的情况下叫做<strong>更换变元法</strong><br>$$f(2^k)=f(2^{k-1})+2^k=f(2^0)+\sum_{i=1}^k2^i=2^{k+1}-1 =2n-1\<br>g(2^k)=2g(2^{k-1})+1=\sum_{i=0}^k2^i=2^{k+1}-1=2n-1$$<br>也可以直接套公式:<br>对于f   $d=1,a=1,c=2,b=1$<br>对于g  $a=2,c=2,b=1,d=1$<br>这些公式太惨绝人寰了，其他还有一些和2.4一样复杂的，等遇到了再补充上去。</p><h2 id="六、p、np、np-hard、np-complete问题"><a href="#六、p、np、np-hard、np-complete问题" class="headerlink" title="六、p、np、np-hard、np-complete问题"></a>六、p、np、np-hard、np-complete问题</h2><p><a href="https://blog.csdn.net/birduncle/article/details/94646993">p、np、np-hard、np-complete问题</a>这篇文章讲的很清晰，附上一张从文章里拿的<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210704092654261.png"></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Advanced Mathematics </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
