<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>通过PTE统计页面访问次数</title>
      <link href="/2023/10/26/%E9%80%9A%E8%BF%87PTE%E7%BB%9F%E8%AE%A1%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AE%E6%AC%A1%E6%95%B0/"/>
      <url>/2023/10/26/%E9%80%9A%E8%BF%87PTE%E7%BB%9F%E8%AE%A1%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AE%E6%AC%A1%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>分析系统内存访问的方法之一：页面表条目(PTE)位跟踪。操作系统可以清空PTE中的访问(A)位和脏(D)位,硬件页表遍历器会设置这些位。通过定期重置这些位并检查A位的状态,操作系统可以判断页面是否在重置间隔内被访问过。</p><h2 id="A位被置位的时刻？"><a href="#A位被置位的时刻？" class="headerlink" title=" A位被置位的时刻？"></a><span class="label primary"> A位被置位的时刻？</span></h2><p>每当访问一个页面时，<strong>硬件</strong>会<strong>自动</strong>将A位设置为1。这意味着，当CPU访问一个页面时，硬件会检测到这个访问并将A位从0设置为1，表示页面已经被访问。</p><h2 id="A位置位被清除的时刻？"><a href="#A位置位被清除的时刻？" class="headerlink" title=" A位置位被清除的时刻？"></a><span class="label primary"> A位置位被清除的时刻？</span></h2><ul><li>操作系统可以定期检查A位的状态，通常是通过一种叫做”页面表条目位跟踪”（Page Table Entry (PTE) Tracking）的方法来实现。</li><li>将一个页面从物理内存中置换到磁盘上时，通常会清零该页面的A位。</li></ul><h2 id="会不会存在A位还没有置0的情况下，页面又被访问了？"><a href="#会不会存在A位还没有置0的情况下，页面又被访问了？" class="headerlink" title=" 会不会存在A位还没有置0的情况下，页面又被访问了？"></a><span class="label primary"> 会不会存在A位还没有置0的情况下，页面又被访问了？</span></h2><p>是的，如果在上一次访问后,PTE的A bit还没有被操作系统重置为0,然后页面又被访问了,那么硬件仍会将A bit置1。</p><h2 id="struct-page里有-count和-mapcount，这个和PTE的A位有什么关系吗？"><a href="#struct-page里有-count和-mapcount，这个和PTE的A位有什么关系吗？" class="headerlink" title="struct page里有_count和_mapcount，这个和PTE的A位有什么关系吗？"></a><span class="label primary">struct page里有_count和_mapcount，这个和PTE的A位有什么关系吗？</span></h2><p>_count字段表示这个物理页框被多少个虚拟页映射和共享。它反映了一个页框的引用数量。<br>_mapcount字段表示这个页框被多少个PTE映射。<br>也就是说:_count和_mapcount表示页面的映射关系,而PTE中的A位表示页面的访问状态。struct page和PTE中的信息反映了不同的内存管理需要。<br>两者也有联系,比如在回收内存页时,只有_mapcount为0时,表示没有PTE指向该页,才可以安全回收。</p><p>基于4.15内核通过PTE统计页面访问次数代码流程图：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6660cc59f6994938a01bd9ff25569339.png"></p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不同的总线</title>
      <link href="/2023/10/26/%E4%B8%8D%E5%90%8C%E7%9A%84%E6%80%BB%E7%BA%BF/"/>
      <url>/2023/10/26/%E4%B8%8D%E5%90%8C%E7%9A%84%E6%80%BB%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之后会加入CXL相关的信息。一些关于DDR-T更多的信息在论文Characterizing the Performance of Intel Optane Persistent Memory————A Close Look at its On-DIMM Buffering有被讨论。</p></blockquote><h2 id="DDR（双数据速率）"><a href="#DDR（双数据速率）" class="headerlink" title="DDR（双数据速率）"></a>DDR（双数据速率）</h2><p>DDR是一种用于系统内存（RAM）的接口和技术。它允许CPU（中央处理器）与系统内存之间进行高速数据传输。DDR标准有多个版本，例如DDR3、DDR4和DDR5，每个版本都有不同的数据传输速率和技术规格。DDR主要用于存储和读取计算机内存中的数据，以供CPU使用。CPU通过DDR接口来访问内存中的程序和数据。</p><h2 id="DDR-T"><a href="#DDR-T" class="headerlink" title="DDR-T"></a>DDR-T</h2><p>Intel Optane Persistent Memory使用DDR-T与CPU传输数据。特别设计用于大型数据中心和需要大量内存的高性能计算环境。DDR-T内存模块通常具有非常高的存储容量，通常以TB（Terabytes，千兆字节）为单位来衡量，因此得名为”Terabyte”。这使得DDR-T非常适合大规模数据分析、虚拟化、人工智能和其他内存密集型应用。</p><h2 id="PCIe（Peripheral-Component-Interconnect-Express）"><a href="#PCIe（Peripheral-Component-Interconnect-Express）" class="headerlink" title="PCIe（Peripheral Component Interconnect Express）"></a>PCIe（Peripheral Component Interconnect Express）</h2><p>PCIe是一种用于连接各种外部设备和扩展卡的高速总线标准。它通常用于连接图形卡、网络适配器、存储控制器、声卡等外部硬件设备。PCIe提供了高带宽和低延迟的数据传输通道，使外部设备能够与CPU和系统内存进行快速通信。PCIe通常不用于内存访问，而是用于连接外部设备，以扩展计算机的功能。</p><h2 id="UPI"><a href="#UPI" class="headerlink" title="UPI"></a>UPI</h2><p>跨NUMA节点间数据的传输。</p><h2 id="ISA"><a href="#ISA" class="headerlink" title="ISA"></a>ISA</h2><p>X86体系结构下ISA总线的直接内存存取控制器叫DMA</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bus </tag>
            
            <tag> DDR </tag>
            
            <tag> PCIe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>64位的用户空间和内核空间</title>
      <link href="/2023/10/26/64%E4%BD%8D%E7%9A%84%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/"/>
      <url>/2023/10/26/64%E4%BD%8D%E7%9A%84%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<p>文章根据<a href="https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&amp;mid=2247522087&amp;idx=2&amp;sn=fe8f4cd34d68e0a10658dee88bd337df&amp;chksm=f98dd38dcefa5a9ba43a9d1ac96852532f53278915a6f6b9f5187b8c1c885c1e5848ebabbc86#rd">公众号@bin的技术小屋</a>做了一定增删。</p><h2 id="为什么需要虚拟内存？"><a href="#为什么需要虚拟内存？" class="headerlink" title="为什么需要虚拟内存？"></a><span class="label primary">为什么需要虚拟内存？</span></h2><ol><li>内存隔离，使每个运行的进程都认为它拥有整个系统的内存。</li><li>延迟内存分配,只有当页面真正被访问时,才需要从磁盘加载到物理内存。</li><li>页式内存管理技术。</li><li>通过与磁盘空间的交换实现扩展内存容量。</li></ol><h2 id="虚拟地址怎么认？"><a href="#虚拟地址怎么认？" class="headerlink" title="虚拟地址怎么认？"></a><span class="label primary">虚拟地址怎么认？</span></h2><p>在32位机器上，指针的寻址范围为 $2^32$，所能表达的虚拟内存空间为4GB（$2^30$是从B到GB）。在目前的64位系统下只使用了48位来描述虚拟内存空间，寻址范围为 $2^48$，所能表达的虚拟内存空间为256TB。</p><ul><li>低128T表示用户态虚拟内存空间，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 。</li></ul><p>1左移47位得到的地址是 0x0000800000000000，然后减去一个 PAGE_SIZE（默认为 4K），就是0x00007FFFFFFFF000，共 128T。</p><ul><li>高128T表示内核态虚拟内存空间，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。</li></ul><p>0x[用户/内核态位][47位偏移量][保留位][12位页内偏移].用户空间的虚拟内存地址的高16位全部为0.内核空间的虚拟内存地址的高16位全部为1.</p><p>这样一来就在用户态虚拟内存空间与内核态虚拟内存空间之间形成了一段0x0000 7FFF FFFF F000 - 0xFFFF 8000 0000 0000 的地址空洞，我们把这个空洞叫做canonical address空洞。高16位不全为0也不全为1.</p><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/4956918c43e186d49df7b9802f080de8.png"></p><h2 id="64位Linux系统下，进程虚拟内存空间布局是怎样的？"><a href="#64位Linux系统下，进程虚拟内存空间布局是怎样的？" class="headerlink" title="64位Linux系统下，进程虚拟内存空间布局是怎样的？"></a><span class="label primary">64位Linux系统下，进程虚拟内存空间布局是怎样的？</span></h2><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/532e6cdf4899588f8b873b6435cba2d8.png"></p><ol><li>准备运行的程序编译成二进制文件存放在磁盘中，CPU会执行二进制文件中的机器码来驱动进程的运行。所以在进程运行之前，这些存放在二进制文件中的机器码需要被加载进内存中，而用于存放这些机器码的虚拟内存空间叫做<strong>代码段</strong>。</li><li>在代码段跟数据段的中间还有一段不可以读写的保护段，它的作用是防止程序在读写数据段的时候越界访问到代码段，这个保护段可以让越界访问行为直接崩溃，防止它继续往下运行。</li><li>程序代码中我们通常会定义大量的全局变量和静态变量，这些全局变量在程序编译之后也会存储在二进制文件中，在程序运行之前，这些全局变量也需要被加载进内存中供程序访问。<strong>指定了初始值的全局变量和静态变量在虚拟内存空间中的存储区域我们叫做数据段。没有指定初始值的全局变量和静态变量在虚拟内存空间中的存储区域叫做BSS段。</strong>因为内核知道这些数据是没有初值的，所以在二进制文件中只会记录BSS段的大小，在加载进内存时会生成一段0填充的内存空间。</li><li>程序在运行期间往往需要动态的申请内存，所以在虚拟内存空间中也需要一块区域来存放这些动态申请的内存，这块区域就叫做堆。堆空间中地址的增长方向是从低地址到高地址增长。内核中使用start_brk标识堆的起始位置，brk标识堆当前的结束位置。当堆申请新的内存空间时，只需要将brk指针增加对应的大小，回收地址时减少对应的大小即可。比如当我们通过malloc向内核申请很小的一块内存时（128K之内），就是通过改变brk位置实现的。</li><li>程序在运行过程中还需要依赖动态链接库，这些动态链接库以.so文件的形式存放在磁盘中，比如C程序中的glibc，里边对系统调用进行了封装。glibc库里提供的用于动态申请堆内存的malloc函数就是对系统调用sbrk和mmap的封装。这些动态链接库也有自己的对应的代码段，数据段，BSS段，也需要一起被加载进内存中。还有用于内存文件映射的系统调用mmap，会将文件与内存进行映射，那么映射的这块内存（虚拟内存）也需要在虚拟地址空间中有一块区域存储。这些都被放在<strong>文件映射与匿名映射区</strong>。</li><li>在程序运行的时候总该要调用各种函数吧，那么调用函数过程中使用到的局部变量和函数参数也需要一块内存区域来保存。这一块区域在虚拟内存空间中叫做<strong>栈</strong>。</li></ol><h2 id="虚拟内存是怎么被组织的？"><a href="#虚拟内存是怎么被组织的？" class="headerlink" title="虚拟内存是怎么被组织的？"></a><span class="label primary">虚拟内存是怎么被组织的？</span></h2><blockquote><p>可以通过 cat /proc/pid/maps 或者 pmap pid 来查看某个进程的实际虚拟内存布局。</p></blockquote><p>这涉及到具体的数据结构和红黑树，不是这篇blog重点，下面放一个概要图。</p><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/f28079c8b8fbf1853570302bee2a1929.png"></p><h2 id="ZONE类型与内核空间"><a href="#ZONE类型与内核空间" class="headerlink" title="ZONE类型与内核空间"></a>ZONE类型与内核空间</h2><ol><li>内核空间是所有进程共享的，不同进程进入内核态之后看到的虚拟内存空间全部是一样的。</li><li>内核态虚拟内存空间的前896M区域是直接映射到物理内存中的前896M区域中的，直接映射区中的映射关系是一比一映射。映射关系是固定的不会改变。<strong>直接映射区的前16M专门让内核用来为DMA分配内存，这块16M大小的内存区域我们称之为ZONE_DMA。</strong> X86体系结构下ISA总线的直接内存存取控制器叫DMA。16M到896M（不包含896M）这段区域，我们称之为 <strong>ZONE_NORMAL</strong>。</li><li>物理内存896M以上的区域被内核划分为<strong>ZONE_HIGHMEM</strong>区域，我们称之为高端内存。这块动态映射区内核是使用 vmalloc 进行内存分配,将不连续的物理内存映射到连续的虚拟内存上。<br><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/0bd4766b19d043bb4aebdd06bdf8e67c.png"></li></ol><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/e1f2e689c2754b2af540c6d0b6ab327f.png"></p><h2 id="与用户态内核态有什么联系？"><a href="#与用户态内核态有什么联系？" class="headerlink" title="与用户态内核态有什么联系？"></a>与用户态内核态有什么联系？</h2><p>内核空间用于存放操作系统内核的代码和数据，具有高特权级别，只能由内核态的代码访问。用户空间用于存放用户应用程序的代码和数据，具有较低的特权级别，只能由用户态的代码访问。这种分离允许操作系统保护其核心功能并确保系统的稳定性和安全性。用户应用程序需要通过系统调用来与内核进行通信和访问系统资源。</p><h2 id="namespace又是什么？"><a href="#namespace又是什么？" class="headerlink" title="namespace又是什么？"></a>namespace又是什么？</h2><p>Kernel space - 在某个特定namespace内,操作系统内核执行的内存区域。<br>User space - 在某个特定namespace内,用户模式程序执行的内存区域。</p><p>Namespace的类型有:<br>PID namespace - 进程ID的隔离Namespace<br>Network namespace - 网络设备和协议栈的隔离Namespace<br>Mount namespace - 文件系统挂载点的隔离Namespace<br>IPC namespace - 进程间通信的隔离Namespace<br>UTS namespace - 主机名和域名的隔离Namespace<br>User namespace - 用户和用户组的隔离Namespace</p><p>通过namespace技术,内核可以为不同的用户空间提供隔离的系统资源,增强安全性和容器化。所以在讨论用户空间和内核空间时,需要明确是在哪个namespace环境下。比如docker容器内的用户空间和内核空间就与宿主机隔离开来。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kernelspace </tag>
            
            <tag> userspace </tag>
            
            <tag> namespace </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux出现文件很少但磁盘容量已满的情况</title>
      <link href="/2023/10/26/Linux%E5%87%BA%E7%8E%B0%E6%96%87%E4%BB%B6%E5%BE%88%E5%B0%91%E4%BD%86%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F%E5%B7%B2%E6%BB%A1%E7%9A%84%E6%83%85%E5%86%B5/"/>
      <url>/2023/10/26/Linux%E5%87%BA%E7%8E%B0%E6%96%87%E4%BB%B6%E5%BE%88%E5%B0%91%E4%BD%86%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F%E5%B7%B2%E6%BB%A1%E7%9A%84%E6%83%85%E5%86%B5/</url>
      
        <content type="html"><![CDATA[<p>虚拟机磁盘显示99%的容量，但是我的文件只有2.5G，虚拟机一共100G，其他容量来自哪里？<br>使用ncdu磁盘分析工具<code>sudo ncdu /</code>，这会显示磁盘上的文件和目录，按大小排序。可以导航到占用大量空间的目录，并查看哪些文件或子目录占用了大量的磁盘空间。</p><p><code>/var/log/syslog</code>有64.5G，清空syslog文件<code>sudo truncate -s 0 /var/log/syslog</code></p><p>为避免将来再次出现类似问题，配置日志轮换（log rotation）定期删除旧的日志数据并限制日志文件的大小。syslog使用logrotate工具来进行日志轮换。编辑logrotate配置文件，<code>/etc/logrotate.conf</code>或<code>/etc/logrotate.d/</code>目录中。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">/var/log/syslog <span class="token punctuation">{</span>    size 100M       rotate <span class="token number">10</span>    compress    delaycompress    missingok    notifempty    create <span class="token number">644</span> root root<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>限制syslog文件的大小为100MB，保留最多10个旧日志文件，并启用压缩以节省空间。<br>很多日志都有一个.conf文件都能用这个方式限制大小。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncdu </tag>
            
            <tag> log rotation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transparent memory offloading in datacenters</title>
      <link href="/2023/10/15/Transparent-memory-offloading-in-datacenters/"/>
      <url>/2023/10/15/Transparent-memory-offloading-in-datacenters/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS’22四篇Best Paper之一 （但是这个公司和密西根大学之后合作的一篇内容相关性还挺高的）</li><li>TMO: Transparent Memory Offloading in Datacenters</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li> Johannes Weiner, Niket Agarwal, Dan Schatzberg, etc. Mate Inc.</li><li> Dimitrios Skarlatos, 卡内基梅隆大学CMU</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>机器学习等新兴应用对内存需求的大幅增长，加上DRAM设备扩展速度的放缓[19,25] 以及DRAM成本的大幅波动，使得DRAM作为唯一的内存容量解决方案变得昂贵得令人望而却步。</p><p>近年来，大量非DRAM更便宜的内存技术，例如NVMe SSD[8,11]和NVM[14,17,24,30,34,37]已成功部署在数据中心，或正在使用它们方式。此外，新兴的非DDR内存总线技术，例如CXL[9]提供类似内存的访问语义和接近DDR的性能。这些趋势的融合为内存分层带来了过去不可能的新机会[12,16,21,31–33,39,40]。</p><p>使用压缩内存可以大幅降低成本，但这仍然不够，我们需要NVMe SSD等替代内存技术来进一步降低成本。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>性能不变的情况下，节约内存。TMO根据设备的性能特征和应用程序对内存访问速度减慢的敏感度，自动调整要卸载到异构设备（例如压缩内存或SSD）的内存量。 </p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><blockquote><p>zswap是一项Linux内核的虚拟内存压缩功能，可为将要交换的页面提供压缩回写缓存。当内存页将要交换出去时，zswap不将其移动到交换设备，而是对其执行压缩，然后存储到系统RAM内动态分配的内存池中。回写到实际交换设备的动作则会延迟，甚至能完全避免，从而显著减少Linux系统用于交换的I/O；副作用则是压缩所需的额外CPU周期。</p></blockquote><p>唯一已知的大规模采用内核驱动交换用于延迟敏感的数据中心应用程序的是Google的zswap部署[18][43].在本文的其余部分中将其称为g-swap。作为先驱，g-swap极大地推进了现有技术水平，但仍然存在几个主要局限性:仅支持单个慢速内存层；某些应用程序的数据难以压缩，例如具有量化字节编码值的机器学习模型。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>TMO需要回答两个问题：要卸载多少内存以及要卸载哪些内存。<br>（1）页面会冷多久：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4d3acd6320e244119f41bdab3ffe1a62.png"></p><blockquote><p>service-level agreement SLA 网站服务可用性的一个保证<br>为了简化数据中心中应用程序的操作，需要使用大量内存来启用微服务并提供基础设施级功能。我们将软件包、分析、日志记录以及与数据中心应用程序部署相关的其他支持功能所需的内存定义为<strong>数据中心内存税</strong>。这部分占20%,最先被卸载，因为内存税的SLA比程序直接消耗的SLA少。</p></blockquote><p>（2）不同程序文件页和匿名页所占比比例不同<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/08366eac5f724234bd566fd6c704ff40.png"></p><p>（3）考虑到卸载到的SSD耐久性不同SSD设备的延迟等，设计架构<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/35bf4ada06834da894f7c606466dbe8d.png"></p><p>图6显示了TMO架构概述（左）以及内存和存储布局（右）。</p><ol><li>未修改的工作负载在容器1内执行，并通过系统调用和分页与内核内存管理子系统交互。 </li><li>Senpai是一个用户空间组件，负责控制内存卸载过程并决定应从每个工作负载中卸载多少内存。Senpai以压力信息的形式监控应用程序性能下降。</li><li>压力信息由内核中的压力失速信息（PSI）组件报告。</li><li>基于PSI信息Senpai通过写入cgroup控制文件来驱动卸载过程，从而触发内核的内存回收逻辑。回收逻辑决定要卸载哪些内存。</li><li>内存管理子系统向PSI模块暴露内存压力信息。</li><li>5触发对卸载后端和常规文件系统的读写操作。<br>TMO支持卸载到基于zswap的压缩内存池或通过交换存储设备。</li></ol><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/35bf4ada06834da894f7c606466dbe8d.png" alt="总体设计图"></p><h2 id="5-1-PSI有效捕获内存卸载对不同应用程序的影响"><a href="#5-1-PSI有效捕获内存卸载对不同应用程序的影响" class="headerlink" title="5.1 PSI有效捕获内存卸载对不同应用程序的影响"></a>5.1 PSI有效捕获内存卸载对不同应用程序的影响</h2><p>每个非闲置进程，PSI会进一步区分进程可运行的时间段和因资源不足而停滞的时间段。将计算潜力定义为以CPU数量为上限的非闲置进程数量。PSI=停滞数/计算潜力*100%. 对于容器和全系统域，PSI为每种资源引入了两个压力指标，分别称为 “some”和 “full”。some指标跟踪域内至少有一个进程在等待资源时停滞的时间百分比。full指标跟踪的是所有进程同时延迟的时间百分比。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0bdbab3ec366466d9c89cbbb2c0d1d0d.png"><br>虚线框是执行时间和停顿时间。执行时间标准化为100%并分为四个部分。some停顿蓝色箭头，full停顿绿色箭头。第一阶段A和B停止其中一个即可都有资源用some指标12.5%；第二阶段同时停止6.25%，full指标6.25%，some指标18.75%。“some”旨在捕获由于缺乏资源而导致各个进程增加的延迟，而“full”则表示容器或系统中完全非生产性的时间量。</p><div class="note danger">这个示例里执行时间归一化后为啥那个方块不是满的，就是25%没有拉满，难道是为了画图好看，听他解释应该是满的才对啊？黑色斜条纹块很weird</div> <p>PSI只出现在以下3个场景：<br>为了跟踪内存压力，PSI记录专门在内存不足时发生的事件所花费的时间。目前，这包括三种情况。第一种情况是当内存已满时进程触发回收页面并且进程尝试分配新页面。第二种情况是进程需要等待IO发生故障，即最近从文件缓存中逐出的页面出现重大故障。第三种情况是进程在从交换设备读取页面时发生阻塞。</p><div class="note primary">一种现有机制是驻留集大小（RSS），它反映了一个进程当前分配的物理内存量,包括该进程使用的代码、数据和堆栈等所占用的内存。RSS的主要限制是并不能捕捉到内存或缺乏内存对应用程序性能的影响。其他指标（例如升级率）代表每秒的swap-ins。提升率的一个缺点是它没有考虑卸载后端的性能特征。此外，由于卸载导致更多内存变得可用，它无法捕获应用程序性能的改进。<p>在PSI之前，操作员依赖于关联内核时间、应用程序吞吐量变化、回收活动事件计数器、文件重读和换入等间接指标来估计工作负载的资源健康状况。这需要对存储硬件设备特性和内核行为有直观的了解。例如，文件超前读取算法可能会在不同程度上使应用程序免受已记录的缓存重读的影响。另一方面，PSI可直接在进程级别测量合格停滞事件造成的生产力损失。它考虑了底层硬件的差异、内核内存管理算法的有效性，甚至工作负载的内部并发性（即部分停滞与完全停滞）。</p><p>所以这个指标反应资源总数变化的情况下，对每个进程性能的影响。仍然是对某个进程/程序，我按量分配DRAM，看他性能提高了，再给一点，<br>而且从热力图来看，热度的提升是有一定时间的。</p></div> <h2 id="5-2-Senpai"><a href="#5-2-Senpai" class="headerlink" title="5.2 Senpai"></a>5.2 Senpai</h2><p>每隔几秒，Senpai就会为每个cgroup计算一次要回收的内存量，如下所示：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bb712a3ad3724964a1443b7d3dcd8c05.png" alt="PSI确定内存压力，阈值根据实验动态调节，Senpai从LRU中执行卸载"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/59595c4f3cfc49699351c75a066e57ca.png"></p><p>𝑃𝑆𝐼𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑和 reclaim_ratio是可配置参数。 𝑐𝑢𝑟𝑟𝑒𝑛𝑡_𝑚𝑒𝑚 是cgroup的当前内存占用量。</p><p>动态工作负载在某些情况下可能会导致问题。例如，如果容器内存快速增长且其大小正在主动扩展，则它可能会被阻塞，直到 Senpai进一步提高其限制。为了解决这个问题，我们在内核中添加了一个无状态内存回收cgroup控制文件。通过这个控制钮，Senpai可以要求内核精确回收计算出的内存量，而不施加任何限制，从而避免了阻塞扩展工作负载的风险。（看不懂这个解决方案哩）</p><p>Senpai在导出内核之上的用户空间中以PSI和cgroup内存控制接口实现。与假设的内核内实现相比，这有几个优点。首先，用户空间可以完全访问浮点单元，因此可以更有效地执行计算。其次，用户空间组件的发布周期往往比内核快得多。计划在具有不同性能SLO阈值的工作负载中利用不同的Senpai配置。</p><h2 id="5-3-回收操作"><a href="#5-3-回收操作" class="headerlink" title="5.3 回收操作"></a>5.3 回收操作</h2><p>Senpai依靠内核从cgroups中回收冷内存页。Senpai让内核的回收算法选择要卸载的页面，而不是使用昂贵的全页表扫描来确定哪些内存页面是冷页面。该算法通过为文件和交换支持页面维护一对活动/非活动页面LRU列表，并首先回收较冷的页面来运行。这种机制经过生产测试，能以相对较低的CPU成本和较高的准确性识别较少使用的内存页面。在Senpai中使用这种机制大大简化了我们的实现过程。在生产中，Senpai驱动的回收只消耗所有CPU周期的0.05%，可以忽略不计。</p><p>但是有改进。由于很多文件只会被读一次，当文件缓存缺页时，换进来的Page会优先放在inactive list中。当再次读到这个页面时，按照soft fault的惯例，页框会从inactive 挪到active。Linux中使用radix tree来追踪页框，radix tree通过一种特殊的元素来追踪匿名内存被换到swap中的情况。作者扩充了这个特殊的元素，称之为shadow entry。当inactive list中的内容被换出时，有个计数器（workingset_time）会自增。每个页框被移除的时候，shadow entry中都会记录当前的workingset_time。也即当第一个被reclaim的页面对应的shadow entry记录0，第二个被reclaim的页面记录1，等等单调自增。</p><p>当某个被reclaim的页面发生PF被换回时，用当前的workingset_time减去shadow entry中记录的值可以得到页面被换出期间清理了多少页面，称之为refault distance。如果refault distance小于active list，则当前这次PF就被认为是refault。当发生refault时，内核就会开始倾向于回收匿名页。同时发生refault的页面会直接加入到active list并使active list 长度加一。当refault distance小于active list时，其实就说明这个页面使用的比较频繁，这时候我们就应该扩大一点active list把这个页面加到acitve list中，并且转向回收匿名内存。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>这篇文章主要是为了卸载冷数据，提出的指标很新颖也有效。而且这是关注内存和硬盘之间交互的，也挺有参考价值。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>迁移过程可以由应用程序本身、用户空间库[6,12,26,29]、内核或虚拟机管理程序驱动。</p><p>我们将内存卸载后端定义为保存卸载内存的慢速内存层。在我们当前的生产队列中，这由NVMe SSD和压缩内存池组成。未来我们预计这将包括NVM和CXL设备。</p><p>在跨卸载后端具有多样性的异构存储器系统中，给定的主要故障率（那些硬件计数器）可能在慢速存储设备上构成问题，而在较快存储设备上则无关紧要。一般来说，很难理解特定的内核事件是否是工作负载的功能问题。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Memory Access Similarity to Improve Irregular Application Performance for Distributed</title>
      <link href="/2023/10/15/Exploring-Memory-Access-Similarity-to-Improve-Irregular-Application-Performance-for-Distributed/"/>
      <url>/2023/10/15/Exploring-Memory-Access-Similarity-to-Improve-Irregular-Application-Performance-for-Distributed/</url>
      
        <content type="html"><![CDATA[<h2 id="0-论文信息"><a href="#0-论文信息" class="headerlink" title="0.论文信息"></a>0.论文信息</h2><div class="note primary"><ul><li>文章来自IEEE Transactions on Parallel and Distributed Systems，TPDS，2023</li><li>Exploring Memory Access Similarity to Improve Irregular Application Performance for Distributed</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Wenjie Liu， Xubin He，美国宾夕法尼亚州费城天普大学计算机与信息科学系</li><li>Qing Liu，美国新泽西州纽瓦克市新泽西理工学院电气与计算机工程系</li></ul><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1.Background"></a>1.Background</h1><p>stacked DRAM and off-chip DRAM的异构。<br>并行工作模式使得在高性能集群上产生跨节点的不规则内存访问行为（memory access behaviors，<strong>MAB</strong>）。<br>但是执行共享功能或者数据结构时，节点间会产生相似的MAB。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/59b533c0f549441a95aade87ee7d645f.png"><br>工作节点之间内存访问相似性的示例。每个工作节点执行共享二进制代码的不同部分, 创建MAB的各种组合。如图所示,两个节点之间的相似性是通过重叠的MAB以及每个MAB覆盖的页面数量来估计的,例如,节点K和节点S共享相同的MAB并产生高相似性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0ea19701091942909a12b353a43e0b5f.png"><br>最常用的6个MAB所覆盖的内存页的细分。可以得出三个观察结果。首先, MAB,例如MAB 0,可以被所有节点共享为执行相同的二进制代码。<br>第二，在节点3.4.6.7.8.11.12.13上观察到相同的MAB组合;因为这样的节点可以执行相同的任务。第三, <del>MAB 5仅在节点14.上显示</del> ，并且大量页面被MAB 5覆盖,这意味着相似性不仅存在于节点之间，而且存在于节点内的多个页面之间。</p><h1 id="2-解决的问题（Motivation）"><a href="#2-解决的问题（Motivation）" class="headerlink" title="2.解决的问题（Motivation）"></a>2.解决的问题（Motivation）</h1><p>为了利用堆叠DRAM实现的高性能，设计高效混合存储器系统的核心是两个挑战。</p><ul><li>第一个挑战是确定运行时的最佳数据位置。</li><li>第二个挑战是减少元数据造成的开销。下图比较了现有技术的堆叠DRAM命中率和元数据导致的相应性能下降。如图所示，最先进的方法不断提高堆叠DRAM命中率。同时,对于不规则的HPC应用程序,元数据导致的性能开销显著增加，这降低了堆叠DRAM带来的性能增益。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/7c1d020bbf2d4c08b35440e4307a2647.png"></li></ul><h1 id="3-其他学者解决这个问题的思路和缺陷"><a href="#3-其他学者解决这个问题的思路和缺陷" class="headerlink" title="3.其他学者解决这个问题的思路和缺陷"></a>3.其他学者解决这个问题的思路和缺陷</h1><p>现有的方法通过利用每个<strong>数据块的访问历史</strong>[7] [8] [9]或操作系统[10] [11]提供的提示来识别热数据，这些提示忽略了由集群的并行工作模式启用的共享MAB,并且可能在集群环境中表现不佳。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span> C. C. Chou, A. Jaleel, and M. K. Qureshi, “CAMEO: A two-level memory organization with capacity of main memory and flexibility of hardware-managed cache,” <span class="token keyword">in</span> Proc. IEEE/ACM 47th Annu. Int. Symp. Microarchitecture, <span class="token number">2014</span>, pp. <span class="token number">1</span>–12.<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span> J. B. Kotra, H. Zhang, A. R. Alameldeen, C. Wilkerson, and M. T. Kandemir, “Chameleon: A dynamically reconfigurable heterogeneous memory system,” <span class="token keyword">in</span> Proc. IEEE/ACM 47th Annu. Int. Symp. Microarchitecture, <span class="token number">2018</span>, Art. no. <span class="token number">533</span>.<span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span> E. Vasilakis, V. Papaefstathiou, P. Trancoso, and I. Sourdis, “Hybrid2: Combining caching and migration <span class="token keyword">in</span> hybrid memory systems,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2020</span>, pp. <span class="token number">649</span>–662.<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> A. Kokolis, D. Skarlatos, and J. Torrellas, “PageSeer: Using page walks to trigger page swaps <span class="token keyword">in</span> hybrid memory systems,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2019</span>, pp. <span class="token number">596</span>–608.<span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> A. Prodromou, M. Meswani, N. Jayasena, G. Loh, and D. M. Tullsen, “MemPod: A clustered architecture <span class="token keyword">for</span> efficient and scalable migration <span class="token keyword">in</span> flat address space multi-level memories,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2017</span>, pp. <span class="token number">433</span>–444.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>针对不规则应用的优化。现有的研究针对不规则应用, 要么将不规则性暴露给作业调度器进行自适应调度[31] ,[32] ,要么利用不规则性在异构集群中进行更好的资源分配[33] [34]。Nozal等人提出在使用OneAPI框架执行不规则应用程序时，平衡分配给不同硬件的负载[31]。 此外, Dai等人在运行时检测不均匀分布的不规则工作负载, 并将不堪重负的节点的工作负载迁移到分配较少工作负载的节点[32]。另一方面,利用不规则性来细化集群中的资源分配。杨等人观察了集群中运行的不规则应用程序导致的随机和不规则网络流量,并提出利用数据压缩技术来最大限度地减少网络带宽消耗[34]。此外, Shin等人分析了GPU上运行的不规则应用程序导致的性能下降,并提出加快地址转换过程，以最大限度地减少SIMD指令的数据准备时间[33]。</p><p>系统中的相似性。相似之处存在于许多方面各方面的计算系统, 以及先前的工作探讨了相似性以提高系统性能。Koller等人提出了一种基于观察到的存储和访问数据的高度相似的I/O内容的I/O重复数据消除方法[35]。同时,Xiao等人利用高级程序中丰富的自相似性来加速芯片之间的数据通信[36]。刘等人利用集群的并行工作模式,提出了提高掉队性能的方法。</p><p>混合存储器系统。现有工程确定热点通过监视每个数据块的存储器访问行为或依赖于操作系统发现的信息来获取数据。通过监测每个数据块的内存访问行为, 可以使用获得的访问行为来识别下面访问的缓存行，并且可以执行准确的数据迁移/缓存[7]、[8]、[9]。另一方面,Prodromou等人提出利用操作系统观察到的内存访问行为，并使用多数元素算法来预测热页[1]。此外,研究人员将计算能力集成到堆叠的DRAM中，这进一步减少了内存流量,并为内存密集型应用提供了更高的效率[37] ,[38]。</p><h1 id="4-围绕该问题作者如何构建解决思路（Design）"><a href="#4-围绕该问题作者如何构建解决思路（Design）" class="headerlink" title="4.围绕该问题作者如何构建解决思路（Design）"></a>4.围绕该问题作者如何构建解决思路（Design）</h1><h2 id="4-1-Similarity-Monitor"><a href="#4-1-Similarity-Monitor" class="headerlink" title="4.1 Similarity Monitor"></a>4.1 Similarity Monitor</h2><p>提出了一种量化方法来测量节点之间的内存访问相似度，<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/97d94f0e504f4a95936dca253e9c8429.png">将来自所有节点的MAB聚合为全局内存访问行为向量(<strong>GMABV</strong>) ,生成每节点内存访问行为矢量(<strong>PMABV</strong>) ，并计算PMABV之间的距离作为相似性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f907559fea214cc1b9b62e61ce11e4ef.png"><br>比较两种共享方案，全局共享无法为不规则应用提供高的共享精度，因为并非每个MAB都对其他节点有帮助。相反,选择性共享按需共享MAB,这会给部署的节点带来性能开销。由于MAB共享不准确,这两种方法都存在性能损失，GS和SS的性能损失分别高达13.1%和15.4%， 这表明在不规则应用的情况下需要一种有效的MAB共享方案。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Similarity-Based Sharing</span>Input: Receiving access behavior AccBhv from <span class="token function">node</span> k<span class="token keyword">if</span> AccBhv <span class="token keyword">in</span> GMABV <span class="token keyword">then</span><span class="token keyword">if</span> <span class="token function">node</span> k belongs to a Similarity Group <span class="token keyword">then</span>Forward AccBhv to nodes within the group with global sharing<span class="token keyword">else</span>Perform selective sharing with all Low Similarity nodesend <span class="token keyword">if</span>Update metadata<span class="token keyword">else</span>Update PMABV , GMABV and Similarity Matrixend <span class="token keyword">if</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-2-Access-Behavior-Buffer"><a href="#4-2-Access-Behavior-Buffer" class="headerlink" title="4.2 Access Behavior Buffer"></a>4.2 Access Behavior Buffer</h2><p>根据SMABS的设计，节点之间只共享当前工作集的MAB，这减轻了每个节点上MAB监控带来的开销。此外，由于每个节点处理的大量结构化数据和HPC应用程序中丰富的循环，跨节点的多个内存页共享相同的MAB。因此，即使是单个共享MAB也可以描述相应节点内多个页面的访问行为。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c8a23de4e70141a0a79a68b7c4769a7d.png"><br>如图访问行为缓冲区的一个条目。状态和覆盖比特都确定新观察到的MAB是否将被发送到相似性监视器以进行相似性更新。而起始地址和覆盖页是用来快速识别内存请求的滑动窗口部分。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e6b261c2909546e593d09f8c527e1daf.png"></p><h2 id="4-3-Swap-Executor"><a href="#4-3-Swap-Executor" class="headerlink" title="4.3 Swap Executor"></a>4.3 Swap Executor</h2><p>为了利用访问行为缓冲区中缓冲的M4B,交换执行器用于通过使用共享M4B作为“配方”来执行底层混合存储器系统的数据放置。</p><h2 id="4-4-SM-HMS-Overview"><a href="#4-4-SM-HMS-Overview" class="headerlink" title="4.4 SM-HMS Overview"></a>4.4 SM-HMS Overview</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d1f03539af6148d385ff12d71c165eaf.png"></p><h1 id="5-从结果看，作者如何有力证明他解决了问题"><a href="#5-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="5.从结果看，作者如何有力证明他解决了问题"></a>5.从结果看，作者如何有力证明他解决了问题</h1><p>在模拟器上运行的，首先是作业完成时间比较：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/02a690c97d864595a981b18d199b8106.png"><br>之后是IPC比较：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/963bb9358e614c849013917e811a2781.png"><br>还讨论了相应的参数敏感性、堆叠命中率（不过这都是针对片内DRAM的了）延迟和带宽的收益应该是来自于共享吧。之后容量占用之类的数据都很好，不放图片了。</p><p>我们对由多达256个节点组成的各种集群配置上的一组不规则应用程序的评估结果表明， SM-HMS在完成时间减少方面优于最先进的方法Cameo、Chameleon和Hyrbid2 ， 分别减少了58.6%、56.7%和31.3%， 平均减少了46.1%、41 .6%和19.3%。与理想的混合存储系统相比， SM-HMS的效率高达98.6%(平均91.9% )。此外,在常规应用.上的实验表明，所提出的<br>SM-HMS有利于更广泛的HPC应用。</p><h1 id="6-缺陷和改进思路"><a href="#6-缺陷和改进思路" class="headerlink" title="6.缺陷和改进思路"></a>6.缺陷和改进思路</h1><p>这些最开始设置的5类MAB分别是啥？到底是怎么将访问行为表示为向量的，<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a85a47828a004423ad5a501188b6a2b2.png"></p><h1 id="7-创新点"><a href="#7-创新点" class="headerlink" title="7.创新点"></a>7.创新点</h1><p>应该就是利用相似性去做，减少了一些重复操作？从根本上讲, SMHMS区别于存在通过根据量化的内存访问相似性在工作节点之间共享内存访问行为, 并将共享的内存访问行为作为“配方”来执行准确和知情的缓存替换,从而提高了堆叠DRAM的利用率,并相应地降低了元数据开销。</p><h1 id="8-该论文的相关资料"><a href="#8-该论文的相关资料" class="headerlink" title="8.该论文的相关资料"></a>8.该论文的相关资料</h1><p>srds，中文名作者一般都不会公布什么资料。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> 堆叠DRAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring the Design Space of Page Management for Multi-Tiered Memory Systems</title>
      <link href="/2023/10/13/Exploring-the-Design-Space-of-Page-Management-for-Multi-Tiered-Memory-Systems/"/>
      <url>/2023/10/13/Exploring-the-Design-Space-of-Page-Management-for-Multi-Tiered-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><div class="note primary"><ul><li>文章来自USENIX Annual Technical Conference, (ATC), 2021</li><li>Exploring the Design Space of Page Management for Multi-Tiered Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn, （韩国）亚洲大学</li></ul><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>AutoNUMA用于平衡Numa节点间的内存访问，基本思想是：定期统计各进程的内存访问情况， 并unmapping pages，然后触发NUMA hinting fault，在page fault中重新均衡内存访问，目的是使运行 进程的CPU尽量访问本地节点上的内存，提升性能。</p><h1 id="Abstract摘要"><a href="#Abstract摘要" class="headerlink" title="Abstract摘要"></a>Abstract摘要</h1><p>随着由各种类型的内存组成的分层内存系统的到来，如DRAM和SCM(storage class memory)，操作系统对内存管理的支持正变得越来越重要。然而，目前操作系统管理页面的方式是在假设所有的内存都具有基于DRAM的相同能力的情况下设计的。这种过度简化的做法导致了分层内存系统中非最佳的内存被使用。本研究深入地分析了在目前的Linux设计中页面管理方案，将NUMA扩展到支持同时配备DRAM和SCM的系统（英特尔的DCPMM）。在这样的多层内存系统中，我们发现影响性能的关键因素不仅是访问位置，而且还有内存的访问层。当考虑到这两个特性时，有几种替代页放置的方法。然而，目前的运行系统只优先考虑了访问位置。本文探讨了页面管理方案的设计空间， 称为AutoTiering，以有效使用多层内存系统。我们的评估结果表明，与现有的Linux内核相比，我们提出的技术可以通过释放多层内存层次结构的潜力，显著提高各种工作负载的性能。</p><h1 id="1-Introduction简介"><a href="#1-Introduction简介" class="headerlink" title="1 Introduction简介"></a>1 Introduction简介</h1><p>随着内存计算的出现，如数据分析、键值存储和图形处理，对高密度DRAM的需求在最近几年一直在稳步增长[27]。然而，由于扩大DRAM密度的挑战，一种新的内存类别已经受到关注，以弥补DRAM和SSD之间的性能差距。例如，英特尔最近发布了其基于3DXpoint技术的非易失性存储器，称为Optane DC Persistent Memory Module（DCPMM），它提供了比DRAM更高的密度，同时性能超过了基于闪存的SSD[23]。谷歌、甲骨文、微软和百度等云厂商已经在其云服务中采用了这种存储类内存（SCM）[4, 14, 17,25]。</p><p>由于现代服务器系统是用非统一内存访问（NUMA）架构构建的，未来的大内存系统将采用传统NUMA架构上的分层内存的形式，称为多层内存。图1展示了本研究中使用的一个真实世界的多层内存系统。每个计算芯片有两种类型的内存：DRAM（上层）和英特尔的DCPMM（下层）。我们将DRAM和DCPMM配置为完全暴露给软件的内存。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/080d36e6d55a472597e75b673d907ee2.png"></p><p>本文提出，最近在Linux[15]和分层内存研究[16,20,35]中的进展并没有导致多层内存系统中的最佳页面放置。随着新的内存类别成为主内存的一部分，影响性能的关键因素不仅是访问位置，而且是内存的访问层。然而，目前的页面放置方案是为纯DRAM的NUMA架构建立的，只考虑线程和内存之间的位置[2,8,12,13,21,38]。因此，目前的设计远远没有发挥出多层内存系统的潜在优势。例如，假设从低层（DCPMM）向高层（DRAM）内存升级页面时，本地DRAM变得满了。在这种情况下，目前的技术状况是将页面留在下层内存上，而不管远程DRAM（上层的）是否可用。对于纯DRAM的NUMA系统来说，这样的决定是合理的， 因为替代方案之间没有区别。然而，在多层内存系统中我们不能考虑与访问层相当的所有可能的选择。在放置页面时，应该先考虑内存的访问层，再考虑访问位置，因为访问层对性能有更大的影响。</p><p>这个限制促使我们重新审视商用操作系统的页面管理方案，并探索多层内存系统的页面管理的设计空间。在这项研究中，我们引入了一组新的页面管理方案。我们的第一个方案，称为AutoTiering-CPM，在找不到最佳内存节点（如本地DRAM）时，使用访问层和定位度量（locality metric）保守地寻找升级或迁移的替代方案。</p><p>虽然这种保守的方法可以通过考虑替代方案来实现更好的性能，但这样的设计并没有释放出软件管理的分层内存的全部潜力。为了有效地利用上层内存的有限容量我们设计了一个为多层内存系统量身定做的页面回收方案。我们的第二项技术是有机会的页面提升或迁移，称为AutoTiering-OPM，它可以明智地将页面从上层内存中剔除。为了有效地回收，我们通过估计页面的访问频率来预测上层内存中访问次数最少的页面作为受害者。当决定提升哪一页时，我们的OPM将该页与受害者进行比较，以确定哪一页的访问量相对较大。通过OPM，我们可以在减少对低层内存的访问的同时，实现上层内存的更好的有效性。</p><p>除非上层内存中有空闲空间，否则升级操作要等到降级操作完成后才进行。为了从关键路径上隐藏降级页的延迟，我们在上层内存中保留了一组空闲页，以便立即满足升级请求。当保留页的数量超过阈值时，我们的<code>kdemoted</code>会唤醒并在后台将访问量最小的页面回收到空闲页池中。<code>kdemoted</code>与传统的回收方式不同， 因为它只负责将页面降级到下层内存而不是存储。</p><p>在这项研究中， 我们在Linux内核v5.3的基础上实现了我们提出的方案。我们利用AutoNUMA设施，它定期扫描内存页面并标记为不可访问，以捕获非本地DRAM访问。一旦这些页面被重新访问，它就会产生一个缺页，称为NUMA暗示缺页。我们把NUMA缺页作为来自DCPMM节点的页面升级或来自远程DRAM节点的迁移的需求信号。我们用基于故障的设施建立每个页面的访问历史，并在降级页面时使用这些信息。</p><p>实验结果表明，我们的AutoTiering可以显著提高各种应用的性能。与基线Linux内核[15]相比，GraphMat和graph500的性能分别提高了2.3倍和6.9倍左右。大多数SPECAccel工作负载显示出2倍的速度提升。与英特尔最近的方法[36]相比，我们的性能改进显示出高达3.5倍的速度提升。</p><h1 id="2-Background-and-Motivation背景和动机"><a href="#2-Background-and-Motivation背景和动机" class="headerlink" title="2 Background and Motivation背景和动机"></a>2 Background and Motivation背景和动机</h1><h2 id="2-1-Large-Memory-Systems大型内存系统"><a href="#2-1-Large-Memory-Systems大型内存系统" class="headerlink" title="2.1 Large Memory Systems大型内存系统"></a>2.1 Large Memory Systems大型内存系统</h2><p>数据中心通常采用多芯片NUMA架构来提高具有高内核数和内存容量的商用服务器的性能。虽然这可以增加每台服务器的DIMM插槽的数量，但扩展DRAM密度仍然是一个重大障碍。它对经济有效地构建大型内存系统提出了挑战。同时，由于SCM提供了字节寻址和非易失性的特性，它在弥合DRAM和SSD之间的性能差距方面正获得越来越多的关注。英特尔最近发布了3DXPoint非易失性内存（DCPMM） ，可以不经修改安装在DIMM上[23]。许多云计算供应商，如谷歌、微软、甲骨文和百度已经在其云计算服务中采用了英特尔的DCPMM[4,14,17,25]。最近，</p><div class="note info">三星公司透露了一种基于CXL（Compute Express Link）的DRAM模块连接到系统中，形成分层的内存系统[1]。由于这种新型内存的速度不如DRAM，它们不能完全取代DRAM</div> 。相反，未来的计算机系统将提供一种带有DRAM和SCM的分层内存架构形式。<p></p><p>在这项研究中，我们利用DCPMM作为DRAM和SSD之间的一个新层级。英特尔DCPMM提供了两种类型的分层内存系统，可分为硬件辅助型和软件管理型。在硬件辅助模式下，DCPMM作为主内存暴露给软件而DRAM作为硬件管理的缓存，对软件来说是不可见的。内存控制器自动地将经常访问的数据放在DRAM缓存中，而其余的数据则保存在容量大但速度慢的DCPMM上。另一方面，在操作系统的支持下，DRAM和DCPMM都可以作为正常的内存暴露出来，对软件可见，将内存分为快慢两类[15]。我们把这称为软件管理的分层内存系统。在这种环境下，操作系统的支持可以有效地使用DRAM和DCPMM，因为完全由软件来控制。本文通过了解硬件的组织方式，重点讨论分层内存系统的系统软件方面。</p><h2 id="2-2-Performance-Characteristics性能特点"><a href="#2-2-Performance-Characteristics性能特点" class="headerlink" title="2.2  Performance Characteristics性能特点"></a>2.2  Performance Characteristics性能特点</h2><p>我们描述了一个与Linux操作系统一起运行的 、由软件管理的分层内存系统的明显性能特征。图1展示了本研究中使用的系统结构。该系统有两个CPU socket。对于每个CPU socket，都有一个DRAM节点和一个DCPMM节点整个物理地址空间是由DRAM和DCPMM节点组成的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4a382593ca9b41e8bd9675935b8b5456.png"></p><p>在多层内存系统中，影响性能的关键因素不仅是访问位置，而且还有内存的访问层数。图2a 显示了从MLC[18]测得的四个内存节点中每个节点的读访问延迟和带宽。访问本地DRAM的性能优于其他三个内存节点，这在传统的NUMA架构中是公认的。另一方面 ，我们观察到由于设备的特性，本地DCPMM（I-DCPMM）比远程DRAM（R-DRAM）要慢。这与本地内存总是比远程内存快的传统观点形成了鲜明的对比。请注意，我们在带宽测量中也观察到类似的模式。</p><p>同样，图2b显示了对四个CPU（Intel Xeon Gold 6242）socket和只使用DRAM的系统进行的同类型评估。在访问任何远程DRAM节点的延迟和带宽方面没有明显的差异。因此，在纯DRAM系统中，在远程DRAM节点上放置页面是一个相对简单的任务。</p><p>这些明显的特征促使我们去探索操作系统中页面管理的设计空间。操作系统需要有能力通过了解多层内存系统的性能特征来有效地、动态地（重新）定位内存。与纯DRAM系统不同，由于访问层的原因，不是所有的远程内存节点都可以被认为是平等的。</p><h2 id="2-3-OS-Support-of-Multi-Tiered-Memory操作系统对多层次内存的支持"><a href="#2-3-OS-Support-of-Multi-Tiered-Memory操作系统对多层次内存的支持" class="headerlink" title="2.3 OS Support of Multi-Tiered Memory操作系统对多层次内存的支持"></a>2.3 OS Support of Multi-Tiered Memory操作系统对多层次内存的支持</h2><p>本文所关注的多层内存层次结构与传统的两层内存不同。尽管如此，目前的Linux仍然依赖于现有的NUMA框架来支持多层内存系统。这种有限的操作系统支持使得页面放置在多层内存架构中成为次优的。尽管我们可以根据访问延迟重新定义NUMA距离表，但它并没有充分挖掘出多层内存系统的潜力。首先，Linux以二进制的方式将内存节点分类为本地或远程。当升级或迁移页面时，完全不考虑远程节点之间的几种选择。其次，Linux不支持从上层内存向下层内存迁移（或回收）页面。在这项研究中，我们通过考虑跨访问层以及访问位置的性能特征，重新审视了页面放置策略。</p><h1 id="3-Analysis-of-Page-Management-to-Multi-Tiered-Memory-Systems对多层次内存系统的页面管理分析"><a href="#3-Analysis-of-Page-Management-to-Multi-Tiered-Memory-Systems对多层次内存系统的页面管理分析" class="headerlink" title="3 Analysis of Page Management to Multi-Tiered Memory Systems对多层次内存系统的页面管理分析"></a>3 Analysis of Page Management to Multi-Tiered Memory Systems对多层次内存系统的页面管理分析</h1><p>在这一节中，我们研究了现有的Linux页面管理技术，这些技术是为纯DRAM的NUMA系统设计的。然后，我们发现对于分层内存系统缺乏足够的支持。尽管AutoNUMA[33]可以用于这种多层内存，但我们观察到它未能充分利用多层内存的优势。</p><h2 id="3-1-Initial-Page-Placement最初的页面放置"><a href="#3-1-Initial-Page-Placement最初的页面放置" class="headerlink" title="3.1 Initial Page Placement最初的页面放置"></a>3.1 Initial Page Placement最初的页面放置</h2><p>随着存储类内存（如Optane DCPMM）在主内存中的引入，传统的仅基于访问位置的页面放置对性能产生了负面影响， 因为性能的最关键因素不仅是位置，而且还有内存层。同时，目前操作系统中的页面放置方案已经在基于DRAM的NUMA架构中得到了很好的确立，只考虑线程和内存之间的访问位置[8,13,21]。在Linux中， 默认的页面分配策略试图尽可能地使用本地内存，以减少访问远程内存所带来的性能损失。只有在本地内存没有空闲空间的情况下，内存分配器才会在被称为fallback路径的远程内存节点上寻找空闲空间[9]。</p><p>因此，在多层内存系统中，默认的（本地优先）分配策略被认为是有害的。图1a中的数字介绍了当线程在CPU-0上运行时，仅考虑物理距离，默认回落路径中使用的顺序。如果本地DRAM节点没有足够的空闲空间，内存分配器就会检查回退路径，以确定分配请求应该被发送到哪个内存节点我们预计分配器应该要求远程DRAM节点获得一个空闲页，因为这个节点比本地DCPMM节点提供更好的性能。然而，令人惊讶的是，最先进的Linux内核中的fallback路径显示了本地DCPMM（低层）。它没有考虑到一个明显的特点，即在多层内存系统中，内存类型对性能比访问位置更敏感。</p><p>Problem: If the local DRAM is full, the fallback prioritizes allocating from the local DCPMM (lower-tier), even though the remote DRAM (upper-tier) performs better.</p><h2 id="3-2-Dynamic-Placement动态放置"><a href="#3-2-Dynamic-Placement动态放置" class="headerlink" title="3.2 Dynamic Placement动态放置"></a>3.2 Dynamic Placement动态放置</h2><p>尽管最初的页面放置在给定的内存空间中起着至关重要的作用，但这个决定可能不代表最佳性能， 因为它取决于运行时的内存访问流量。为了根据访问模式调整页面的位置，AutoNUMA 设施已被纳入Linux的一部分，自动将页面迁移到在运行时离运行的线程更近的内存节点[33]。操作系统检查访问位置以发现被访问的页面是放在本地内存还是远程内存上。如果是在远程内存上，该页就会被迁移到本地内存上，以避免次要请求的远程访问。这种方法提高了运行在纯DRAM的NUMA系统上的应用程序的性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/25e5b24a332942eda1d936b3f8790096.png"></p><p>然而，我们发现，目前的设计并没有利用多层内存层次的优势。图3显示了随着时间的推移，Graph500的内存节点对128GB数据集的内存使用率。第5节对实验设置进行了细节解释。首先，我们注意到上层内存没有得到有效利用，因为更频繁访问的页面（深红色）主要驻留在下层内存（节点2）。相比之下，访问频率较低的页面被放置在上层内存中。其主要原因是，当前的内存管理不允许在没有空闲空间的情况下将页面升级或迁移到上层内存。尽管这样的设计决定对于纯DRAM系统来说是合理的，但是对于多层内存系统，我们需要重新考虑这个假设。图4描述了三种情况，即AutoNUMA由于本地DRAM缺乏空闲空间而导致页面升级或迁移失败。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3eaa55b57d7d44b7925221a4b1cf7dcc.png"></p><p>即使不能将页面升级或迁移到满足访问层和访问位置的最佳内存节点上，在多层内存系统中也有有效的替代放置方法。例如，当从远程DCPMM到本地DRAM的页面升级失败时， 我们有两种可能的解决方法:将页面放在远程DRAM上或本地DCPMM上。</p><p>Problem: Pages in the lower-tier are not promoted whenthe upper-tier is fully utilized</p><p>第二，我们观察到在低层内存的内存节点上，页的分布是倾斜的。这是因为在目前的Linux操作系统中没有考虑到向CPU-less节点（DCPMM）的页面移动。因为传统的操作系统是在假设内存访问性能受CPU和内存节点之间的访问定位影响很大的情况下设计的，所以将页面移动到CPU-less节点的情况不会发生。只有当目标上层内存有空闲空间时，驻留在CPU-less节点（下层内存）的页面才能通过AutoNUMA进行升级。图1a显示了目前Linux内核支持的所有可能的页升级和迁移路径的箭头。即使上层内存已经满了，所以操作系统不能把页放在首选访问层上，我们也需要通过自由地允许页在任何CPU-less的节点上移动来保持下层内存的访问位置性。</p><p>Problem: Pages are never migrated to the CPU-less (lower-tier) nodes due to a NUMA policy that does not apply to multi-tiered memory systems.</p><h2 id="3-3-Page-Reclamation页回收"><a href="#3-3-Page-Reclamation页回收" class="headerlink" title="3.3 Page Reclamation页回收"></a>3.3 Page Reclamation页回收</h2><p>目前的页面回收也是为基于存储的交换设备支持的纯DRAM系统设计的，而不是分层的内存系统。传统上，当内存节点耗尽时，<code>kswapd</code>会将内存中不活动的页面直接回收到存储中，而不考虑内存层。将低层内存中的页面回收到存储设备上是合理的。然而，当下层内存有足够的空间时， 这对上层内存页来说并不是一个理想的解决方案。这个限制与第3.2节中解释的两个问题交织在一起。</p><p>Problem: Frequently accessed pages from the lower-tier cannot be promoted without demoting less frequently accessed pages from the upper-tier.</p><p>在Linux操作系统中，每个进程的虚拟地址空间的一部分可以被映射，即文件支持区或匿名区。在文件备份区域的页包含了内存中现有文件的内容这样以后对同一文件的I/O操作就可以用内存访问代替。另一方面，属于匿名区域的页面不代表任何文件内容。这用于在内存中保留任意数据（例如，malloc）。对于每个内存区域，Linux操作系统将内存页分为活跃和不活跃内核有非活动列表，包含可能不使用的页面，同时将最近访问的页面保留在活动列表中。 然而，目前的页面分类过于保守，无法从活动和非活动列表中，精确区分哪些页面是经常被访问的，哪些是不经常被访问的。</p><p>Problem: Binary page classification (either active or inactive) is too coarse-grained to be used for tiering.</p><p>最后，当前的页面回收是在后台小心执行的，以隐藏从关键路径访问外存设备的花销。在分层内存系统中，将页面降级到低层内存的成本比将页面换出到存储设备的成本更低。我们需要将降级到低层内存与传统的回收到存储设备的做法脱钩。</p><h1 id="4-Automatic-Multi-Tiered-Memory自动多层内存"><a href="#4-Automatic-Multi-Tiered-Memory自动多层内存" class="headerlink" title="4 Automatic Multi-Tiered Memory自动多层内存"></a>4 Automatic Multi-Tiered Memory自动多层内存</h1><p>本节探讨了页面管理对分层内存系统的设计空间。我们的页面管理的目标是是充分利用多层内存的优势，以提高证明大内存应用程序的性能。为了保持我们的设计简单我们的设计以AutoNUMA设施为基础。表1总结了每个设计空间的支持机制。在下面的小节中，我们解释每个方案的设计和实现。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4fedc098d9c04b1b8a7b139292ecea75.png"></p><h2 id="4-1-Exploiting-Multi-Tiered-Memory多层次内存开发"><a href="#4-1-Exploiting-Multi-Tiered-Memory多层次内存开发" class="headerlink" title="4.1 Exploiting Multi-Tiered Memory多层次内存开发"></a>4.1 Exploiting Multi-Tiered Memory多层次内存开发</h2><p>利用多层次的内存如图4所示，我们将NUMA故障作为DCPMM节点或远程DRAM节点的页面升级或迁移的需求信号。请注意，目前的AutoNUMA只在上层（本地DRAM）内存有空闲空间的情况下才处理升级和迁移请求。否则，该请求将被丢弃，而故障页将重新出现在原始内存中。当本地DRAM被完全占用时，我们提出的设计允许页面被提升或迁移到多层内存层次结构中的下一个最佳内存节点。这种方法可以利用多层内存层次结构的优势， 提供比现有Linux内核更高的性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5642f9ffa89d45269910ea0e188cfbbc.png"></p><p>图5描述了当本地DRAM满的时候，我们如何反应。在多层内存系统中，有三个需求来源，即页面迁移或升级到本地DRAM。多层的层次结构为设计内存布局提供了新的机会。首先，（5a）当故障页驻留在本地DCPMM中时①，我们将该页升级远程DRAM中，作为第二最佳位置②。由于远程DRAM提供了比本地DCPMM更低的延迟和更高的带宽，我们可以提高那些最初在低层内存中分配内存的应用程序的性能。第二， （5b）如果出错的页面位于在远程DCPMM①，我们有两个选择， 即开发利用多层内存层次结构的优势。我们试图将该页提升到远程DRAM②。如果如果远程DRAM也没有空闲空间，我们就尝试将该页迁移到本地DCPMM③。与现有的Linux内核不同，我们修改后的内核支持将页面迁移到CPU-less的节点（本地DCPMM）。我们称这种AutoTiering 为保守的升级和迁移（CPM）。最后，（5c） 发生故障的页面在远程DRAM中①。这意味着该页已经在排第二的最佳位置了。现有的AutoNUMA无法完成上层内存节点之间的页面迁移操作。因此，它导致了次优的性能。在这种情况下，我们考虑采用页面交换的方式来满足对内存亲和性的需求②。之前的研究提出了分层内存的页面交换机制[35]。我们重新利用这个机制来解决同层内存节点之间的迁移故障。在内部，对于每个内存节点，我们跟踪哪些页面不能被迁移。然后我们利用这些信息来确定可以通过交换操作解决的迁移需求。我们将此称为AutoTiering CPM with Exchange（CPMX）。</p><p>由于这种设计不需要对在现有的Linux操作系统的基础上，它很容易集成到AutoNUMA设施之上。我们预计，我们的构架式设计可以成为这种软件管理的分层内存系统的实用解决方案。</p><h2 id="4-2-Opportunistic-Promotion-and-Migration机会性的升级和迁移"><a href="#4-2-Opportunistic-Promotion-and-Migration机会性的升级和迁移" class="headerlink" title="4.2  Opportunistic Promotion and Migration机会性的升级和迁移"></a>4.2  Opportunistic Promotion and Migration机会性的升级和迁移</h2><p>由于我们设计了一个保守的方法来寻找最佳的替代方案，这仅限于提取软件管理的分层内存的全部性能优势。在我们的保守设计中，经常使用的页面可以驻留在低层（DCPMM）内存中，而上层（DRAM）内存则存放不经常访问的数据。为了缓解这种不理想的内存位置，我们探索了一种渐进式的策略，即从上层内存中机会主义地降级一个页面，以创造自由空间。这是保守设计和渐进性设计之间的主要区别。通过降级页面，页面升级的请求可以成功。对于页面降级的效率，我们需要有能力选择一个在短时间内极不可能被重用的页面。否则，错误的选择会对性能产生负面影响。我们在下面的小节（4.2.1） 中解释我们如何选择降级的页面。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1234890e7c7c4e7a8e08f26aa480b00b.png"></p><p>图6描述了我们的渐进式方法是如何进行页面降级的。当NUMA页面故障发生时①， 我们从上层（本地DRAM）内存中找到访问次数最少的页面，并比较访问次数最少的页面和故障页面的访问频率。如果所选页面的访问频率相对低于故障页面，我们就把所选页面降级，把故障页面放在更高层次的内存节点上。否则，我们会阻止页面升级或迁移请求，以保持上层内存有更频繁的访问页面。</p><p>为了从本地DCPMM（6a）或远程DCPMM（6b）中提升一个页面， 我们将访问次数最少的页面降级到低层内存②a或者②b。页面降级目的地取决于该页之前被访问的地方，以保持其位置性。之后，③我们可以最终将该页提升到本地DRAM节点。此外，图6c显示了如何从远程DRAM进行页面迁移的请求。我们把这种称为AutoTiering的机会性推广和迁移（OPM） 。</p><p>我们进一步优化我们的渐进式设计 ，将降级和升级（或迁移）融合到一个交换操作中，称为AutoTiering OPM with Exchange（OPMX）。当降级的目的地与晋升或迁移的来源相同时，我们利用交换操作，而不是单独的晋升和迁移。例如（6a），如果我们需要将一个选定的页面降级到本地DCPMM②a， 并将该页面提升到本地DRAM③，两个单独的操作被融合。该交换操作消除了不必要的页面分配和释放操作。</p><p>万一我们找不到要从上层内存中降级的页面，我们会尝试将该页面升级到下一个最佳位置——远程DRAM——就像通常的情况一样。由于我们是以机会主义的方式进行页面升级和迁移，我们可以减少过度的页面升级和降级导致的相关性能开销——页表操作和TLB shootdown。</p><h3 id="4-2-1-Predicting-the-Least-Accessed-Page预测访问量最少的页面"><a href="#4-2-1-Predicting-the-Least-Accessed-Page预测访问量最少的页面" class="headerlink" title="4.2.1 Predicting the Least Accessed Page预测访问量最少的页面"></a>4.2.1 Predicting the Least Accessed Page预测访问量最少的页面</h3><p>为了使渐进式设计有效，我们的目标是从上层内存中找到访问最少的页面。正如第3.3节所解释的，Linux操作系统将内存分为文件支持的和匿名的页面，作为LRU列表。当页面升级或迁移由于上层内存缺乏可用空间而失败时，我们会优先调查文件支持区域的页面，如果我们无法从文件支持区域找到访问量最小的页面，则会转移到匿名区域。</p><p>①文件支持的页面。我们研究是否可以通过降低属于文件支持区域的页面的等级来腾出空间。由于文件支持的页面被维护在两个LRU列表中，活动和非活动， 我们将非活动列表中最老的页面视为访问量最小的页面。每当文件支持的页面被重新访问时（例如，<code>sys_read</code>或<code>sys_write</code>），操作系统会将该页面标记为被访问，并将其移至活动列表。由于操作系统可以跟踪文件支持的页面的访问情况，我们通过查看非活动列表来估计访问最少的页面。如果不是，我们就转到活动列表。注意，我们保留在内核参数中配置的页面缓存的部分（例如，<code>vfs_cache_pressure</code>）。如果我们不能在文件支持区域找到可重用的空间，那么我们就在匿名区域寻找一个页面作为后备路径。</p><p>②匿名页面。另一方面，我们保留每个页面的访问（故障）信息，以明智地选择匿名区域中访问最少的页面。为了尽量减少监控开销，我们利用AutoNUMA使用的页面扫描工具，跟踪页面在给定的时间窗口内是否被访问。然后，我们用N比特向量为每个页面建立访问历史。这意味着我们要保持到最后N次的访问历史。我们将N设置为8。基于访问历史，我们将页面分为N个级别（最少访问的页面列表），其中N是被设置的比特数，如图7所示。一旦需要进行页面降级，我们在LAP[0]列表中找到其中一个页面，因为这些页面至少有N次没有被访问了。如果LAP[0]列表是空的，我们尝试从LAP[1]列表找一个，等等。这之后，我们就能选择在高层最少被访问的页面，并且实施页面降级去底层内存。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c4a1f0a968e442a9880a2d9da44d6095.png"></p><h2 id="4-3-Hiding-Latency-of-Page-Demotion隐藏页面降级的延时"><a href="#4-3-Hiding-Latency-of-Page-Demotion隐藏页面降级的延时" class="headerlink" title="4.3  Hiding Latency of Page Demotion隐藏页面降级的延时"></a>4.3  Hiding Latency of Page Demotion隐藏页面降级的延时</h2><p>为了执行页面升级，我们应该将一个页面从目标节点上降级。在完成页面降级之前，由于缺乏空间，我们不能继续进行升级请求。故障处理时间是关键路径中两个操作的总和：页面降级和升级。为了从关键路径中移除页面降级，我们探索了一种软件优化技术。</p><p>我们保持一个由几个保留页组成的页面池。我们把4KB和2MB的保留页分别确定为16和4。保留页允许我们在不需要降级过程的情况下立即为升级请求提供服务，即使上层内存已满。这种方法比页面交换方案[35]更具成本效益，因为它将页面降级的延迟隐藏在关键路径中。页面降级到下层内存比页面晋升到上层内存需要更长的时间，因为用于下层外存提供了更好的读取性能而不是写性能。为了在后台有效地降级页面，我们维护了一个新的内核线程，称为<code>kdemoted</code>，在一个批次中降级最少接受的页面。一旦保留页的数量低于某个阈值，我们就唤醒内核线程，开始降级过程。通过敏感性研究，该阈值被设定为4。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4f2ba49324264133bedc6c4d123f4641.png"></p><p>图8描述了简单的优化如何隐藏了页面降级的延迟。对于每一个晋升请求①黑，即使上层内存已满②黑 ，该页也可以被晋升。在完成升级后，NUMA故障处理程序被返回，没有降级过程，而未来对该页面的访问③黑將在上层内存发生。同时<code>kdemoted</code>将降级访问量最小的页面作为一个批次①回收到内存池②的需求。我们将此称为OPM-BD（背景降级）。</p><h1 id="5-Evaluation评估"><a href="#5-Evaluation评估" class="headerlink" title="5 Evaluation评估"></a>5 Evaluation评估</h1><h2 id="5-1-Experimental-Setup实验设置"><a href="#5-1-Experimental-Setup实验设置" class="headerlink" title="5.1 Experimental Setup实验设置"></a>5.1 Experimental Setup实验设置</h2><p>为了评估我们提出的方案，我们使用了一台配备了两个英特尔至强黄金5218处理器的NUMA服务器，并为每个CPU socket配置了一个16GBDDR4-2666 DIMM和一个128GB英特尔Optane DC持久内存（DCPMM） 的多层内存层次结构。服务器系统总共有32GB DRAM和256GB DCPMM作为主内存。<strong>为了尽量减少测量变化，我们关闭了硬件功能，包括超线程、 DVFS、Turbo-Boost和预取</strong>。我们使用Linux内核5.3和Ubuntu18.04服务器作为我们的基线，并在内核之上实现我们提出的方案。我们的代码可在<a href="https://github.com/csl-ajou/AutoTiering%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%90%E8%A1%8C%E6%9D%A5%E8%87%AAgraph500%E3%80%81SpecACCEL%EF%BC%88OpenMP%EF%BC%89%E3%80%81">https://github.com/csl-ajou/AutoTiering。我们运行来自graph500、SpecACCEL（OpenMP）、</a> Graph-Mat[32]和Liblinear[22]的基准，这些基准在最近的大内存系统中使用[2,35]。我们为所有的基准测试配置了它们，在两个socket上使用所有的32个内核和超过64GB的内存，以充分强调多层内存系统。由于页面大小会以各种方式影响性能我们评估了大页面（2MB）和基本页面（4KB）的性能。</p><h2 id="5-2-Experimental-Results实验结果"><a href="#5-2-Experimental-Results实验结果" class="headerlink" title="5.2 Experimental Results实验结果"></a>5.2 Experimental Results实验结果</h2><p><strong>使用我们的保守方法(CPM)的性能</strong>。图9显示了比原版Linux内核（第一条）的加速结果。请注意，在默认的Linux内核中，AutoNUMA已经启用。图9中的第二条显示了我们保守的升级和迁移（CPM）的速度，第三条显示了应用保守的交换（CPMX）时的性能变化。对于我们评估的大多数工作负载，我们可以看到我们的CPM有明显的性能改善。在503.postencil，553.pclvrleaf和560.pilbdc中，与基线相比，速度提高了2倍以上。另外，559.pmniGhost显示了1.6倍的性能提升。我们的保守设计（CPM）可以将页面从低层（DCPMM）内存节点升级到远程DRAM节点，尽管由于远程DRAM比本地DCPMM更快，所以局部性没有得到保留。我们还可以在两个DCPMM节点之间迁移页面，以便在访问低层内存时有更好的访问局部性。因此，我们可以更有效地利用多层内存系统，提高性能。对于graph500和GraphMat，我们看一 下，其速度分别为17%和19%左右。GraphMat和Liblinear将大型数据集从文件中读取到其内存数据结构中。文件支持的页面占据了DRAM节点的很大一部分，而关键的数据结构位于DCPMM节点中。之后，它执行Pagerank算法进行分析。不幸的是，我们无法观察到调用kswapd来释放不活动的文件备份页。因此，我们的CPM和CPMX没有机会有效利用DRAM节点。</p><p>另一方面，我们观察到555.pseismic的性能完全没有提高。这表明，在基线中，内存放置在上层和下层内存之间是很平衡的。因此，我们无法利用升级和迁移的机会。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d3a9355909c84c6c906d8538b4fc4f30.png"></p><p><strong>使用我们的渐进式方法(OPM)的性能</strong>。我们在第4.2节中描述了我们的渐进式设计，它从上层（本地DRAM）节点中找到访问量最小的页面，并将其降级到下层内存中。如图9所示，OPMX在大多数工作负载中都比CPMX有更好的性能。特别是graph500，GraphMat，Liblinear和559.pmniGhost显示出比CPMX明显的速度提升。503.postencil，553.pclvrleaf和560.pilbdc的性能略有下降，大约下降了7%到8%。560.pilbdc未能摊销页面升级和降级的开销，而553.pclvrleaf显示页面交换的机会减少了。</p><p>我们分析了graph500的运行行为，以了解性能的提高。在执行核心图算法之前，它产生了大量的中间数据，占据了DRAM空间。因此，基线和我们的CPMX在运行BFS（广度优先搜索）时花了大部分时间访问低层内存。有趣的是，它在地址空间的一小部分上表现出了内存访问位置性，尽管整个地址空间是巨大的。有了OPMX，我们可以将访问频率较低的数据降级。到低层内存，并将高层内存空间用于更频繁访问的数据。因此，我们可以大幅度地提高性能。</p><p>对于GraphMat来说，我们看到显著的性能改善来自于文件支持的页面的降级。通过默认，文件支持的页面最初被放在非活动列表中。在该页面被重新访问后，它将从非活动列表移至活动列表。通过将访问量最小的页面从文件支持的页面中降级，我们可以提高上层内存的利用率。</p><p>通过OPM（BD），我们可以进一步提高大多数工作负载的性能。与OPMX相比，graph500和GraphMat的改进是相当大的。555.pseismic和559.pmniGhost显示出明显的改善，其他SPEC工作负载也恢复了交换版本的退化性能。同时，Liblinear显示出轻微的性能下降。</p><p><strong>内存使用的分布</strong>。我们分析了随着时间的推移，多层次的内存是如何被利用的。图10比较了基线和我们对选定工作负载的渐进式设计的内存使用情况，这从我们的CPM中受益。对于三个SPEC工作负载，低层内存与CPM的平衡性更好。这是因为我们允许页面被迁移到CPU-less的节点上。尽管CPM无法以满足所需的访问层，它可以保留低层内存的访问局部性。对于GraphMat来说，与SPEC工作负载相比，性能改善相对较小，555.pseismic也没有改善。原因是基线已经在较低级别的DIMM上保持内存平衡。在下一段中，我们看一下我们基于OPM的方案对这两个工作负载的有效性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bf26e3ef51dd4cc3b632f01088b2e185.png"></p><p><strong>LAP分类的有效性</strong>。为了评估我们的LAP方案的有效性，我们将所有的页面分解到每个LAP级别，并随着时间的推移建立一个直方图。图11显示了选定的三个工作负载，与CPM相比，OPMX显示了明显的改善。对于graph500，GraphMat和559.pmniGhost，相对频繁访问的页面（深红色）对应于第7级或第8级被放置在DRAM节点上，而DCPMM节点服务于相对较少访问的页面。这个结果表明，我们的OPMX对于工作集适合于上层内存的应用是有效的。</p><p>为了实现我们的LAP方案，每个页面都需要额外的空间来保存访问历史（8B）、两个列表指针（16B）、页面帧号（8B）和最后故障的CPU编号（1B）。与基线相比，由于8字节的对齐方式，我们需要每页32字节的元数据，它导致系统内存32GB的288MB DRAM与256GB DCPMM。它稍微减少了0.91%的有效DRAM内存空间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/41fc75455ce841e8a5d9b6049aba02c1.png"></p><p><strong>使用后台降级的延迟隐藏</strong>。我们测量了晋升和交换延迟在页面上的分布我们用ftrace以及内核和用户时间来测量页面故障路径中的推广和交换延迟分布。图12显示了为OPM（BD）的页面推广和OPMX的页面交换服务的延迟CDFOPM（BD）和OPMX的页面交换的延迟CDF。实线是分布，包括延迟隐藏方案（OPM（BD）），而虚线是OPMX方案。这两种方案的故障延迟都不同。我们观察到，OPM（BD）在所有工作负载中都显示出比交换版本更好的延迟分布，因为降级是在关键路径之外。特别是graph500和555.pseismic对后台降级有利。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/177121b8fad946a08ce966a4d8c602c3.png"></p><p>另一方面，Liblinear的最终表现在图9中显示，Liblinear的最终性能没有得到改善。即使延迟减少了，内核的执行时间也没有明显改变。时间并没有显著改变。这是由于有可能引起应用程序线程和后台内核线程之间的内存访问争夺。我们计划在我们未来的工作中用严格的kdemoted调度来解决不希望出现的后台降级情况。</p><p><strong>多程序工作负载的性能</strong>。我们评估了我们提出的方案在每个套接字和跨套接字两种情况下对多程序工作负载的工作情况。图13显示了两个应用程序在同一台服务器上运行时，CPM和OPM（BD）的速度提升。我们用工作负载的组合模拟了四种多程序的场景（mix-1到4）。当基于套接字（每个套接字）隔离应用程序时，CPM没有提供任何预期的性能改进，因为它缺乏利用内存的多层次结构的机会。另一方面，我们可以观察到OPM（BD）在所有情况下都有明显的性能改进，除了553.pclvrleaf在mix-2中，因为更多经常访问的页面可以被放在上层内存上。当允许应用程序跨套接字运行时，它们可能无法有效地利用上层内存节点，因为应用程序的线程之间的内存使用并不是均匀分布的。在跨套接字设置中，我们观察到CPM可以通过利用多层内存层次结构而发挥作用。此外，与CPM相比，OPM（BD）可以进一步提高性能。与CPM相比，在所有情况下都能进一步提高性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3bfb0281431041f1b389b5061fd5f563.png"></p><p><strong>工作集的敏感性</strong>。图14显示了工作集大小从32GB到160GB变化时的性能。160GB的选定工作负载的性能。我们观察到，graph500和559.pmni,Ghost的性能被CPM和OPM（BD）显著提高。对于503.postencil和然而，对于503.postencil和553.pclvrleaf，OPM（BD）和CPM显示出类似的随着工作集大小的增加，OPM（BD）和CPM表现出类似的性能改进。正如在LAP分类段落中所解释的，在这些基准中，大部分的<br>在那些对OPM（BD）不太有利的基准中，大部分页面都是均匀访问的。对OPM（BD）有利。请注意，与原版Linux（Baseline）相比，我们的方法的提速是显著的，而且仍然有效。由于其他工作负载，如GraphMat和Liblinear。有固定的问题输入大小，我们无法评估工作集的敏感性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b3f99c46e51c414c81c4860c205caa19.png"></p><p><strong>与先前研究的性能比较</strong>。图15展示了与英特尔最近提出的名为Tieringv0.6[36]的性能比较结果。请注意，Tieringv0.6是基于Linux内核5.9版本的，但没有并入主线。我们表明，我们的OPM（BD）在大多数工作负载中的性能超过了Tiering v0.6。对于匿名内存区域，Tieringv0.6通过使用AutoNUMA框架，将页面升级到上层定时器内存。通过监控设施，他们调查该页是否在最近两次连续扫描中被访问。如果是这样，他们认为该页是热的，并升级它。另一方面，我们的OPM（BD）保持了过去N（8）次的访问历史。这个决定比看前两次访问更准确此外，Tieringv0.6也继承了传统AutoNUMA的限制。一旦本地DRAM满了，它就不允许将页面推广到远程DRAM或本地DCPMM上。相反，<code>kswapd</code>会被触发，将页面回收到较低层的内存。相比之下，我们的LAP方案通过有选择地执行升级和降级，使上层内存得到更好的利用。对于graph500，我们观察到Tieringv0.6比OPM（BD）表现得更好。我们的方案在执行前构建图形的时间上表现出色，但在遍历图形时，与Tieringv0.6相比采用OPM（BD）的graph500访问了更多的低层内存中的页面。在Liblinear中，它表明Tieringv0.6更积极地将文件支持的页面降级，导致上层内存的更好利用。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5bfcb18fbfea4bcfb1cc5b9fc8a65689.png"></p><p><strong>大页的性能</strong>。为了尽量减少大内存应用的TLB缺失的开销，现代计算机系统提供了大页面选项。图16显示了当我们利用大页面（2MB）而不是基本页面大小（4KB）时的速度提升结果。对于GraphMat和553.pclvrleaf，在OPM （BD）方案中可以观察到大页面的性能改进。另一方面，大多数的工作负载并没有出现明显的差异。559.pmniGhost和560.pilbdc的性能有所下降。当分析具有大页面的SPEC工作负载的LAP直方图时，与基础页面相比，我们的方案可以快速将页面分离到LAP级别。然而，由于在移动大页面时，页面降级的开销会增加，所以会出现性能下降的情况。为了减少开销，我们利用了复制页面的多线程（MT）版本的优势[35]。559.pmniGhost只显示了改进的性能，但其他工作负载变得更差。正如前面一段提到的（延迟隐藏），我们将在未来的工作中进一步研究如何扩展我们的方案以减轻性能开销。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0193ff1662d8412b825f712b723451d3.png"></p><h1 id="6-相关工作"><a href="#6-相关工作" class="headerlink" title="6 相关工作"></a>6 相关工作</h1><p>在整个硬件和软件方面，为有效使用分层（或异质）内存系统做出了巨大努力。我们将我们的方案与之前的方法进行比较。首先，以前的研究大多集中在设计两层内存系统的分页机制和策略[3, 10, 11, 19, 20, 24, 26, 29, 35]与之前的工作不同，本研究将问题空间扩展到分层内存在传统的NUMA架构上增加了一个多层次的内存系统。由于在多层内存系统中， 有几种替代性的页面放置方式本文在放置、升级和删除页面时利用了这些机会。第二，AutoTiering 并没有用先前工作中使用的预定义阈值将页面区分为热和冷[3,19,20]。在这项研究中，晋升和降级的决定是根据整个内存层的相对访问频率和重复性做出的。为了估计页面访问活动，我们依靠AutoNUMA设施。最后，我们使用一个真实世界的基础设施来评估我们提出的基于英特尔的OptanePersistentMemory（DCPMM）的想法，它最近引起了关注。之前的工作模拟或仿真了基于DRAM的两级内存系统[3,11,1.9,20,35]。尽管现实世界中的存储类内存（SCM）在读写性能上显示出不对称性，但在用DRAM模拟分层内存<br>时，它并没有被正确建模。</p><p>下面，我们将介绍操作系统界以前在软件方面的努力为了了解Linux中内存系统的异质性，ACPI6.2规范引入了异质性内存属性表（HMAT），为用户提供各种内存类型的性能信息[39]。从Linux内核5.0-rc1开始，持久性内存（这里指英特尔的DCPMM）可以被用作易失性主内存，尽管它比DRAM慢[15]。它可以提供丰富的主内存空间， 但支持分层内存层次的策略和机制还处于起步阶段。最近，一个新的用于硬件管理的DRAM缓存的内存分配器被称为shuffle page allocator，在Linux内核中被引入[34]。 然而，它不足以提取分层内存的全部性能，因为它没有考虑内存节点之间的距离和NUMA类型。另外，已经有一些努力来有效地支持快内存和慢内存之间的页面迁移，但是这些仍然依赖于活动和非活动列表管理[5,16,30]，直到现在还没有被合并到Linux内核的主线中。在Windows操作系统中，他们通过Mi ComputeNumaCost测量系统初始化时各种页面操作的成本， 并建立-个类似于Linux的NUMAdis-tance的表格，但这反映的是访问成本[37]。 由于Windows操作系统的信息有限，我们无法找到它们对多层内存的作用。</p><p>架构界的研究人员介绍了有效利用两层内存系统的硬件技术，同时将估计访问频率的性能开销降到最低[7,28,29,31]。Choe等人提出了硬件辅助的多层内存系统的内存分配方案，其中DRAM节点对软件是不可见的[6]。除此以外，没有一项工作考虑到多层内存系统的情况。通过架构的支持，跟踪和迁移页面等开销可以大大减少，但是考虑到放置的替代方案，做出晋升和降级决定的灵活性是有限的。</p><h1 id="7-Conclusion-总结"><a href="#7-Conclusion-总结" class="headerlink" title="7 Conclusion 总结"></a>7 Conclusion 总结</h1><p>这项工作探索了一套名为AutoTiering的新的页面管理方案，它从多层内存系统中获益。我们发现，Linux操作系统关注的是NUMA所带来的访问局部性，而不是内存层。然而，在多层内存系统中，访问内存的成本并不只与局部性成正比。我们通过考虑两个因素，即访问层和局部性，全面解决了利用多层内存系统的不同方面。我们用一个真实世界的分层内存系统建立了一个概念验证。我们的评估显示，在各种基准测试中，与现有的Linux内核5.3版本和英特尔的Tieringv0.6之前的方法相比，性能有了明显的改善。</p><p>未来的分层存储系统预计会更加多样化和异质化。为了使我们的方法更加普遍，我们可以维护一个描述可能的替代安置的表格。在操作系统中初始化内存时，我们可以测量各内存节点的实际性能。有了这样一个新的表格， AutoTiering 可以调整页面需要晋升、降级或迁移的地方，正如所解释的那样，没有一个静态的决定。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> NUMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Micron’s Perspective on Impact of CXL on DRAM Bit Growth Rate</title>
      <link href="/2023/10/13/Micron%E2%80%99s-Perspective-on-Impact-of-CXL-on-DRAM-Bit-Growth-Rate/"/>
      <url>/2023/10/13/Micron%E2%80%99s-Perspective-on-Impact-of-CXL-on-DRAM-Bit-Growth-Rate/</url>
      
        <content type="html"><![CDATA[<p>CXL（Compute Express Link™）是一种高速互连、行业标准接口，用于处理器、加速器、内存、存储和其他IO设备之间的通信。CXL通过允许异构和分布式计算架构的可组合性、可扩展性和灵活性来提高效率。CXL的主要优势是计算节点的内存扩展，填补了需要高带宽、容量和低延迟的数据密集型应用程序的空白。</p><p>在本文中，我们将阐述美光科技的观点：内存市场提供强劲的增长前景，Compute Express Link™ （CXL） 将为DRAM bit需求增长和可寻址市场 （TAM,total addressable market）增长带来净积极影响。</p><p>我们将首先讨论当今IT系统中的两个挑战，然后讨论CXL如何解决这些问题。最后，我们将解释我们认为CXL将对内存市场产生的影响。</p><h2 id="内存墙问题"><a href="#内存墙问题" class="headerlink" title="内存墙问题"></a>内存墙问题</h2><p>现代并行计算机架构很容易出现系统级瓶颈，从而限制应用程序处理的性能。历史上，这种现象被称为“内存墙”，微处理器性能的提高速度远远超过DRAM内存速度的提高速度。在过去的十年中，CPU核心数量的增长速度导致CPU和内存性能之间的差距越来越大（图 1），从而阻碍了复杂的计算挑战。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/82284a0bb0484aba83155e934c0f2760.png"></p><p>添加处理器核心只是解决许多应用计算挑战的一部分。在大多数情况下，拥有足够的内存带宽来为这些处理器内核提供数据至关重要。CPU供应商试图通过增加更多内存通道并提高新一代CPU中这些通道的数据速率来逐步改进，从而缓解扩展差距问题。新一代DRAM技术为内存数据速率的发展提供了暂时的缓解。</p><p>表1显示了过去十年中CPU核心数量和DDR DRAM数据速率随着2011年、2017年、2021年和2023年更多内存通道的增加而增加的进展。然而，即使有了理论内存数据速率和更多内存通道，内存带宽要跟上CPU核心数量的增长并随着时间的推移保持每个核心4GB/s是一项挑战。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c7bb70cbebb94edd8bb4cc227a450588.png"></p><p>以CPU核心数量衡量的平台处理能力与可用内存容量扩展之间的关系也同样受到挑战。如图2中的历史趋势数据所示，处理器核心数量增长相当迅速，而每个核心的系统内存容量增长却稳步下降。将内存控制器集成到CPU中通常会导致处理器与内存容量比率更加直接和受限。可以通过为每个通道添加更多DIMM来增加容量。然而，由于通道负载增加，为每个通道添加更多DIMM通常需要降低内存时钟速度，从而减少内存带宽，从而加剧前面讨论的内存墙问题。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/85106b4286dd43de958c0f23847650cd.png"></p><h2 id="IT资源效率最大化问题"><a href="#IT资源效率最大化问题" class="headerlink" title="IT资源效率最大化问题"></a>IT资源效率最大化问题</h2><p>应用程序和服务被分解为微服务，使得可以随着工作负载需求的起伏而优化可用资源。IT效率的主要限制因素之一是没有一种基础设施资源的组合适合所有工作负载。工作负载对计算能力、内存、存储、延迟和IO带宽有动态需求。随着算法的性质和复杂性发生变化，工作负载和服务经过优化，可以通过已安装且不变的公共和私有云硬件基础设施来交付。</p><p>IT工作负载历来都是针对高峰需求进行配置的。架构师和服务规划人员预测在一段时间内提供给定服务级别所需的最大资源需求，然后确保提供适当的峰值级别（以及一些额外的缓冲区）计算、内存、存储和网络资源给定服务器或服务器机架上的工作负载，包括满足峰值水平需求的功率。然而，这通常意味着资源的严重过度配置，因为工作负载需求很少在峰值水平运行。在最近的大部分时间里，该行业的整体数据中心资源利用率都非常低（低于50%，并且通常远低于2）。</p><p>随着时间的推移，虚拟化和云基础设施提供了重要的功能，通过增强的自动化、工作负载迁移和放置以及其他技术，帮助节省甚至回收因过度配置和利用不足而损失的资源。许多人认为这将大大减少数据中心服务器基础设施的TAM。这产生了完全相反的效果，产生了对更高密度CPU计算平台的需求，因为提高效率可以节省其他方面的成本，如电源和运行管理。因此，强化了杰文斯悖论“资源效率的提高将导致资源消耗的增加而不是减少”。</p><p>人们对提高灵活性和效率的渴望从未如此强烈，业界一直在讨论如何实现可组合的“未来数据中心”。下一代数据中心对资源的使用进行更细粒度的控制，包括重新思考如何不仅在数据中心级别共享资源，而且还跨机架共享资源甚至在服务器内。</p><h2 id="CXL架构数据中心的演变"><a href="#CXL架构数据中心的演变" class="headerlink" title="CXL架构数据中心的演变"></a>CXL架构数据中心的演变</h2><p>CXL已成为一种经济高效、灵活且可扩展的架构解决方案，将塑造未来的数据中心。 CXL将改变服务器和光纤交换机的传统机架和堆栈架构在数据中心的部署方式。拥有由CPU、内存、网络和存储组件组成的专用固定资源的专用服务器将让位于更灵活和可扩展的架构。如果机架中的服务器与网络、存储和计算的固定资源互连，就能通过软件管理基础设施动态组合，以满足人工智能和深度学习等现代和新兴工作负载的需求。</p><p>业界一直关注可通过具有内存访问功能的CXL设备（如CXL连接内存设备）释放的潜力。内存附加节点提供高容量内存扩展，可用于密集型服务器工作负载，并具有增加的内存带宽、低延迟和内存一致性，以实现异构计算/处理，并实现内存基础设施的分层。内存分层的引入方式与过去几十年存储分层的引入方式大致相同，最终将包括直连内存扩展、内存池和内存共享。</p><p>数据中心将更加以内存为中心，能够动态组合具有高Terabyte字节（TB）以上内存池的服务器，从而使更多应用程序能够在内存中运行。存储级内存成为新的主要活动数据存储层，NAND和磁盘驱动器用于在多个主机之间共享热数据和非活动数据。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/65fa682280034b73a0741413825ee9e2.png"></p><p>最终，数据中心将升级到所有服务器元素完全分解（包括计算、内存、网络和存储）的状态。大规模部署的容器和微服务将推动动态配置所需的底层资源，以实现具有<strong>平衡计算和内存比率</strong>且无性能损失的优化解决方案。借助CXL，随着可组合性管理软件的出现，按需配置中使用的服务和底层硬件的部署将显得无缝且快速，从而在异构环境中的即服务模型中创造更高的效率。</p><h2 id="CXL-如何解决内存墙问题"><a href="#CXL-如何解决内存墙问题" class="headerlink" title="CXL 如何解决内存墙问题"></a>CXL 如何解决内存墙问题</h2><p>用于内存设备凝聚和一致性的CXL协议属性将通过支持将内存扩展到服务器DIMM插槽之外来解决“内存墙”问题。CXL内存扩展是一种双管齐下的方法，通过增加带宽来克服“内存墙”问题，并为支持CXL的服务器增加数据密集型工作负载的容量。</p><p>对于典型的工作负载，保持每个CPU核心的带宽以获得理想的效率非常重要。随着核心数量迅速增加，带宽会出现不足（参见表 1）。直接连接CXL内存扩展允许服务器平台进行扩展并缩小额外带宽的差距以保持平衡。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/aacf6242525244268488b272ffbfa75f.png"></p><p>另一个需要考虑的因素是，随着核心数量的增加，每个核心的容量会减少。应用程序的工作负载需求不断增长，需要快速分析收集的数据并将结果用于有用的业务洞察。这些高价值工作负载（即<strong>机器学习、NLP、计算机视觉、推荐系统、内存数据库</strong>等）可以通过每个系统中更高级别的内存来经济地运行。CXL内存模块可以直接插入服务器，为处理器提供超出直接连接内存通道的更多带宽和容量，并且<strong>延迟时间与双插槽服务器中处理器之间的NUMA链路相当</strong>。</p><h2 id="CXL如何应对I​​T效率和可持续发展挑战"><a href="#CXL如何应对I​​T效率和可持续发展挑战" class="headerlink" title="CXL如何应对I​​T效率和可持续发展挑战"></a>CXL如何应对I​​T效率和可持续发展挑战</h2><p>跨应用垂直领域的各种工作负载对计算操作、内存容量、带宽和延迟高度敏感。在云、企业或边缘数据中心的传统机架服务器上运行的应用程序必须满足服务级别协议（SLA）。一种常见的方法是将这些类型的应用程序工作负载分布到多个系统上。构建IT基础设施并不总是遵循简单的经验法则来实现计算和设备资源之间的系统平衡。平衡这些资源取决于工作负载，这些工作负载可以是计算限制、内存限制或IO限制。</p><p>基于CXL的系统的初始部署提供了性能和容量的扩展选项，以匹配基于工作负载需求的计算资源的扩展。内存、存储、网络和随着外形和连接的标准化，加速器成为可互换的模块，并且可以根据工作负载需求来组合服务器。这种方法允许服务器制造商（包括云提供商）减少他们需要开发和维护的服务器SKU的数量，以满足其客户群的无数应用程序。它还可以帮助IT管理员正确调整具有足够资源的服务器的大小，以减少单个工作负载必须分布的服务器数量，从而提高效率和性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a8ca37be3e8a473e81e8b4f6dffe46a3.png"></p><p>随着时间的推移，CXL架构的价值将扩展到机架，从而实现可组合性。可组合性是指在支持一个或多个工作负载的一台或多台服务器中更灵活地配置内存与计算资源比率的能力。资源平衡可以通过内存扩展、内存池或内存共享来实现。在机架上，横向扩展方法允许根据应用程序的要求动态分配资源池（计算、网络、内存、存储和IO）并进行无缝集成。当实例通过使用机架内本机设备级发现的组合管理软件上线时，计算、内存、网络和存储被分配给应用程序或微服务。在高峰需求期间，可以为应用程序动态分配额外的资源以满足SLA。当应用程序工作负载需求缓和时，可以释放额外的资源并将其重新分配给其他服务。资源共享或池提供了更高的利用率，而无需过度配置系统，这也意味着更高的性能、降低的软件堆栈复杂性和更低的总体系统成本。</p><p>当然，将多少共享（因此可组合）资源聚合到任何给定工作负载总是存在限制，因为客户需要考虑安全和容错因素以及工作负载效率和利用率问题。尽管内存池创建了满足峰值水平所需的资源，但85%的组织需要99.99%的正常运行时间才能满足SLA（service level agreements服务级协议），这必须在机架内的内存池中考虑到，即使在池化CXL连接内存时，也会导致一定程度的超额订阅被采纳。此外，虽然内存池可以缓解近期内存过度配置的问题，但必须仔细考虑内存池扩展故障，以避免整个机架的服务器故障，从而驱动冗余以避免停机。一种受到青睐的方法是创建资源区或Pod，在有效使用共享资源与最大限度地减少服务中断的影响之间取得平衡，并提供适当的安全性和合规性功能。</p><p>数据中心最大的举措之一是推动net-zero排放。效率是数据中心可持续发展方程中的一个关键变量。就像服务器虚拟化一样，扩大设备共享和资源池通常会减少数据中心的过度配置，但规模更大。将专用设备资源转换为共享池资源并将其进行动态分配可降低计算节点的功耗。不仅减少了每个计算节点的功耗，还改善了气流和热量，以实现机架内更高效的冷却，减少对HVAC系统的需求，从而进一步降低数据中心的功耗。</p><h2 id="CXL对DRAM-bit需求增长的影响"><a href="#CXL对DRAM-bit需求增长的影响" class="headerlink" title="CXL对DRAM bit需求增长的影响"></a>CXL对DRAM bit需求增长的影响</h2><p>现在让我们讨论一下CXL将如何影响DRAM bit需求增长。</p><p>CXL支持的池化和CXL支持的内存带宽扩展对比特需求增长的净影响将是积极的。总而言之，我们预计CXL将在中短期内帮助维持数据中心比特20%的高增长。</p><p>近期CXL内存市场取决于支持CXL的服务器平台向广泛行业推出的速度。由于CXL内存是一个新兴市场，CXL上的内存增长将非常快，但直到2026年才会对整个DRAM市场产生巨大影响。Yole Intelligence市场研究小组预测，到2028年，CXL上的DRAM bit需求将增长到接近100艾比特。Yole Intelligence 预测，到2028年，CXL位将占服务器DRAM位总数的31%。</p><p>卡内基梅隆大学/微软最近发表的一篇论文讨论了池化如何影响CXL TCO节省。该论文提出了一种基于CXL的池解决方案，通过将给定超大规模工作负载集的内存需求减少9-10%，可以节省4-5%的TCO。数据中心DRAM位增长复合年增长率（CAGR）仍保持在20%的范围内，其中包括CXL的影响。即使添加了内存池，这对整个数据中心DRAM位增长的影响也很小。通过粗略数学计算，将9-10%的减少量乘以20-30%的预期复合年增长率即可实现这一效果。理论上最坏情况的计算表明，由于合并，损失会减少2-3个百分点。当然，这种理论场景是不可行的，因为池化伴随着延迟权衡和软件优化要求，并且<strong>池化并不适用于所有工作负载</strong>。其次，池的适用性和扩展受到一定程度冗余的容错需求以及跨多个托管服务器级联内存池故障的风险的限制。最后，逐步采用CXL会减弱任何影响。支持CXL的池将无法解决当前非CXL数据中心安装基础的问题。</p><h2 id="CXL-对行业收入的影响-TAM-和美光的财务模型"><a href="#CXL-对行业收入的影响-TAM-和美光的财务模型" class="headerlink" title="CXL 对行业收入的影响 TAM 和美光的财务模型"></a>CXL 对行业收入的影响 TAM 和美光的财务模型</h2><p>内存的收入TAM增长取决于位数和价格，而价格取决于供需平衡。 CXL是一种互连解决方案，其技术采用本身并不会增加市场供应。CXL本身不应成为行业供需的破坏性因素，定价预计将促进TAM的增长。在某些配置中，连接到CXL接口的内存与标准内存插槽相比更具成本效益，使服务器系统的构建和部署规模超出预算目标。CXL的第一个用例围绕单主机配置的内存扩展。内存扩展可恢复内存限制工作负载的计算和内存之间的平衡，否则这些工作负载将分布在多个服务器之间，并将内存从这些服务器整合到CXL扩展插槽。能够支持CXL 1.1+ 的新型服务器将于2023年上市，但主要用作CXL新兴内存解决方案的概念验证。真正的部署将于2024年底开始，届时支持CXL 2.0的服务器将提供更多内存扩展选项，并标志着服务器中平均DRAM内容量开始增加。我们预计这将是CXL接口收入增长的开始，并预计到2025年该市场将达到20亿美元。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e31421c95c7f41d4893436c06e38d3f6.png"></p><p>资源扩展是CXL演进的第一步，然后再转向完全可组合性和内存池，我们目前预计这将在2026年开始增长。2026年，许多新服务器将支持CXL 3.0，服务器市场预计将增长到2100万个单位，为分解提供必要的支持。影响内存池采用率的因素包括CXL交换机以及可以处理分层内存池和跨多个主机分配以最大限度减少延迟的软件。超大规模企业将在短期内成为内存池扩展的早期采用者。它们很可能在单主机内存扩展和机架内内存池之间均匀分配增长。我们以及行业分析师Yole Intelligence预计，到2030年，CXL附加内存市场将超过200亿美元，数据中心内存市场预计将达到1000亿美元，其中大部分增长将在2025年之后。</p><p>我们对CXL影响的看法和预期已纳入我们的长期模型和跨周期财务模型中，因此对CXL技术采用的预期不应改变投资者对我们财务业绩的预期。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>CXL提供了平衡“内存墙”问题所需的架构，并为通过内存扩展实现经济的内存解决方案提供了新的向量。此外，CXL灵活且可扩展的架构可提高计算和内存资源的利用率和运行效率，以便根据工作负载需求纵向扩展或横向扩展资源。 CXL 附加内存为分层内存存储的新领域提供了巨大的增长机会，并实现了独立于CPU内核的内存扩展。 CXL将有助于维持比没有它时更高的DRAM bit增长率。换句话说，我们预计CXL不会导致DRAM bit增长加速，但它对DRAM增长具有净积极作用。</p><p>美光对 CXL 技术的承诺使客户和供应商能够推动内存创新解决方案的生态系统。要了解有关美光如何实现下一代数据中心创新的更多信息，请访问 micron.com/solutions/server。</p>]]></content>
      
      
      <categories>
          
          <category> White Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CXL </tag>
            
            <tag> Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Characterizing the Performance of Intel Optane Persistent Memory</title>
      <link href="/2023/10/13/Characterizing-the-Performance-of-Intel-Optane-Persistent-Memory/"/>
      <url>/2023/10/13/Characterizing-the-Performance-of-Intel-Optane-Persistent-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自Proceedings of the Seventeenth European Conference on Computer Systems, (EuroSys), 2022</li><li>Characterizing the Performance of Intel Optane Persistent Memory————A Close Look at its On-DIMM Buffering</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Lingfeng Xiang, Xingsheng Zhao, Jia Rao, Song Jiang, Hong Jiang, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>最近的研究[8,13,23,26,30,32,35,36]发现Optane DCPMM不应简单地被视为速度较慢的持久DRAM。与DRAM相比，Optane DCPMM表现出复杂的行为，并且性能会根据访问大小、访问类型和模式而发生巨大变化。</p><p>连接Optane DCPMM和集成内存控制器（iMC）的新DDR-T协议支持异步存储，以隐藏长写入延迟；而DRAM使用的DDR4协议对于加载和存储是同步的。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>目标是了解DIMM缓冲如何影响应用程序感知的性能。我们使用微基准评估了现有的两代Optane DCPMM，并得出了以下以前未报道过的发现：</p><ul><li>在单独的DIMM读缓冲区和写缓冲区中对读和写的管理方式不同。读取缓冲区提供更高的并发性和有效的DIMM预取，从而实现高读取带宽和卓越的顺序性能，但无助于隐藏媒体访问延迟。写入缓冲区提供的并发性有限，但却是支持DDR-T协议中异步写入的管道中的关键阶段。</li><li>除了写入合并之外，写入缓冲区还提供低于读取的延迟和一致的写入延迟，无论工作集大小、写入类型、访问模式或持久性模型如何。</li><li>缓存行访问粒度和3D-Xpoint媒体访问粒度之间的不匹配会对 CPU缓存预取的有效性产生负面影响，并导致持久内存带宽的浪费。</li><li>由于异步DDR-T协议，缓存行刷新或普通写入在到达iMC中的写入挂起队列时返回，以隐藏较长的介质写入延迟。栅栏指令对读写操作进行排序以实现崩溃一致性，仅保证刷新全局可见，但不一定完成。因此，在栅栏指令返回后读取最近刷新的缓存行可能会经历几乎一个数量级的延迟，因为读取需要等待刷新完成。</li></ul><div class="note info">但是实际在使用的时候，延迟的差距带来的性能影响并不是很严重，主要还是带宽影响了TPS系统吞吐（每秒处理的数量），虽然延迟和带宽是有瓜葛不能完全分开的。</div> <h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><p>这里直接纪录一些个人比较感兴趣的这篇论文的观察结论：</p><p>测试台允许通过BIOS配置单独启用/禁用英特尔可扩展处理器中的三个CPU缓存预取器。我们首先禁用所有三个CPU预取器，以研究是否存在独立于CPU预取的DIMM预取机制。图6（a）和（e）显示，在G1和G2 Optane DCPMM中，Optane DCPMM和iMC的读取比率均接近1，这表明没有观察到明显的DIMM上预取活动。相反，当分别使能各个CPU预取器时，DCPMM的读取比率与iMC的读取比率不同。根据WSS，每个图中都有三个区域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/303687a65e5d422588e1d496884118e5.png"></p><ul><li><p>WSS（working set sizes）小于读缓冲区16KB。 WSS完全适合读缓冲区，并且CPU预取器预取到读缓冲区的所有数据都会导致后续访问中的缓冲区命中。因此，除了程序要求的数据之外，不从介质加载任何附加数据。</p></li><li><p>WSS大于读缓冲区但小于L3缓存。工作集不再适合读缓冲区，但仍适合最后一级缓存LLC（G1服务器上为27.5MB，G2服务器上为 36MB）。虽然iMC的读取比率保持为 1，因为所有CPU预取数据都会导致LLC命中，但Optane DCPMM的读取比率显着增加。由于WSS大于读取缓冲区大小，因此预取数据在命中缓冲区之前就会被逐出，从而导致介质的浪费和重复加载。</p></li><li><p>WSS大于L3缓存。由于大型WSS会调用频繁的CPU预取，导致LLC未命中，并导致Optane DCPMM和iMC中的读取比率增长。一个值得注意的观察结果是DCPMM读取率明显高于iMC。对于256B的访问块（即4个高速缓存行），当DCPMM从介质加载整个XPLine时，iMC最多会在访问块的边界误预取一个额外的高速缓存行。</p></li></ul><p>Optane DIMM中的预取活动是由CPU预取决定的，DCPMM中的误预取损失比DRAM中的预取损失特别高。 CPU预取中的高速缓存行粒度与媒体访问粒度之间的不匹配需要在预取错误预测时从媒体加载4个高速缓存行或一个XPLine。对于针对XPLine大小和对齐数据访问进行优化的工作负载，CPU预取可能会占DCPMM带宽的一半。建议程序员仔细权衡在此类工作负载中预取的好处和成本。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Optane NVDIMMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An empirical guide to the behavior and use of scalable persistent memory</title>
      <link href="/2023/10/12/An-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/"/>
      <url>/2023/10/12/An-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自18th USENIX Conference on File and Storage Technologies, (FAST), 2020</li><li>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jian Yang, Juno Kim, Morteza Hoseinzadeh, Steven Swanson. 加州大学圣迭戈分校UCSD</li><li>Joseph Izraelevitz. 科罗拉多大学博尔德分校</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>Optane DIMM这种新型非易失性DIMM支持字节粒度访问，访问时间与DRAM相当，同时还提供断电后仍可保存的数据存储。在过去的十年中，研究人员撰写了大量论文，提出了新的编程模型、文件系统、库和应用程序，旨在利用NVDIMM承诺提供的性能和灵活性。这些论文得出结论并做出设计决策，但没有详细了解真正的NVDIMM的行为方式或行业如何将它们集成到计算机架构中。现在我们可以为这些系统的程序员提供详细的性能数据和具体指导，重新评估现有技术的性能，并为真正的Optane DIMM重新优化持久内存软件。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>从微观和宏观层面探讨了英特尔Optane DIMM的性能特性。特别注意其性能相对于传统DRAM或过去用于模拟NVM的其他方法的特殊性。</p><p>我们发现Optane DIMM的实际行为比“较慢、持久 DRAM”标签所暗示的更为复杂和细致。与DRAM性能相比，Optane DIMM性能更依赖于访问大小、访问类型（读与写）、模式和并发程度。此外，Optane DIMM的持久性与英特尔最新处理器提供的架构支持相结合，为软件设计人员带来了更广泛的设计选择</p><p>根据这些观察结果，我们推荐了一组最佳实践，以最大限度地提高设备的性能。随着我们加深理解，我们随后探索并重新优化持久内存应用级软件中现有技术的性能。</p><h2 id="4-围绕该问题作者如何构建解决思路"><a href="#4-围绕该问题作者如何构建解决思路" class="headerlink" title="4. 围绕该问题作者如何构建解决思路"></a>4. 围绕该问题作者如何构建解决思路</h2><h3 id="4-1-Optane-DIMM架构详细信息"><a href="#4-1-Optane-DIMM架构详细信息" class="headerlink" title="4.1 Optane DIMM架构详细信息"></a>4.1 Optane DIMM架构详细信息</h3><blockquote><p>当PM做内存：直接访问（Direct Access，DAX） 机制是一种支持用户态软件直接访问存储于持久内存（Persistent Memory，PM） 的文件的机制，用户态软件无需先将文件数据拷贝到页高速缓存（Page Cache）</p></blockquote><p> 与传统DRAM DIMM一样，Optane DIMM位于内存总线上，并连接到处理器的集成内存控制器（iMC）。英特尔的Cascade Lake处理器是第一个（也是唯一一个）支持 Optane DIMM的CPU。在此平台上，每个处理器包含一个或两个处理器芯片，其中包含单独的NUMA节点。每个处理器芯片有两个iMC，每个iMC支持三个通道。因此，处理器芯片总共可以在其两个iMC上支持六个Optane DIMM。</p><p> 为了确保持久性，iMC位于异步DRAM刷新（ADR）域内Intel的ADR功能可确保到达ADR域的CPU stores能够在电源故障中幸存下来（即，将在保持时间内刷新到NVDIMM，&lt;100 µs）。 iMC为每个 Optane DIMM维护读取和写入挂起队列（RPQ和WPQ）（图1(b)），并且ADR域包括WPQ。一旦数据到达WPQ，ADR将确保iMC在电源故障时将更新刷新到3D-XPoint介质。ADR域不包括处理器缓存，因此存储仅在到达WPQ后才会持久。</p><p> <strong>iMC使用高速缓存线（64字节）粒度的DDR-T接口与Optane DIMM进行通信，该接口与DDR4共享机械和电气接口，但使用不同的协议来实现异步命令和数据计时。</strong></p><p> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/adac8e025a634145a7ffef40338f51f9.png"></p><p> 对NVDIMM（图1(b)）的内存访问首先到达DIMM控制器（本文中称为XPController）该控制器协调对Optane介质的访问。与SSD类似，Optane DIMM执行内部地址转换以进行磨损均衡和坏块管理，并为此转换维护一个地址间接表（AIT）。地址转换之后，就发生对存储介质的实际访问。由于3D-XPoint物理介质访问粒度为256字节（本文中称为 XPLine），XPController将较小的请求转换为较大的256 字节访问，导致写入放大，因为小型存储变成了读-修改-写操作。 XPController有一个小的写合并缓冲区（本文称为XPBuffer），用于合并相邻的写操作。</p><p> Optane内存可以（可选）跨通道和DIMMs交错使用，如下图：<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d0857fcda1134f34a2ec95d566c57664.png"></p><h3 id="4-2-Optane-DIMM与DRAM的不同之处"><a href="#4-2-Optane-DIMM与DRAM的不同之处" class="headerlink" title="4.2 Optane DIMM与DRAM的不同之处"></a>4.2 Optane DIMM与DRAM的不同之处</h3><p> 这些测量反映了软件看到的加载和存储延迟，而不是这些底层内存设备的加载和存储延迟：<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/451bbc5df2304383a25a93ef1a2fe1b4.png" alt="和硬盘间的读写IO延迟"></p><blockquote><p>尾延迟是指在读取或写入数据时，一些操作所需的时间明显超过了大多数操作的平均时间。这种情况可能导致一些请求的响应时间明显延长，从而影响系统的整体性能。</p></blockquote><blockquote><p>99.9百分位延迟：这是在一组操作中，有99.9%的操作所经历的延迟时间的阈值。换句话说，只有0.1%的操作会在这个延迟时间之上。通常，这个度量标志着绝大多数操作的延迟情况，但允许一小部分极端情况的出现。</p></blockquote><blockquote><p>99.99百分位延迟：这是更严格的延迟度量，表示在一组操作中，有99.99%的操作所经历的延迟时间的阈值。只有0.01%的操作会在这个延迟时间之上。这个度量用于更强调延迟的可靠性，确保绝大多数操作都在非常短的时间内完成。</p></blockquote><blockquote><p>最大延迟：这是一组操作中的最长延迟时间，表示最慢的操作需要多长时间才能完成。最大延迟是最极端的情况，通常用于评估系统的最坏情况行为。在一些实时应用和服务中，尤其是需要低延迟的情境下，最大延迟非常重要，因为即使只有极少数操作的延迟超出预期，也可能对系统的性能和用户体验产生重大影响。</p></blockquote><p> 尾部延迟一项显示顺序写入一小块内存区域（热点）的尾部延迟的实验。Optane内存具有罕见的“异常值”，其中少量写入需要长达50µs才能完成（比通常的延迟增加了100倍）</p><p>  <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/521c5119e9214b84ad2290110aa03cc0.png"></p><p>  DRAM带宽不仅高于Optane，而且随线程数可预测（且单调）扩展，直至DRAM带宽饱和，而这在很大程度上与访问大小无关。首先，对于单个DIMM，最大读取带宽是最大写带宽的2.9倍，而DRAM只有1.3倍的差距。除了交错读取之外，Optane性能随着线程数的增加而呈现非单调性。对于非交错（即单DIMM）情况，性能在1到4个线程之间达到峰值，然后逐渐下降。</p><p>   <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/74c2552a0c1c429ca5d497c61f411c81.png" alt="带宽与线程数 该实验显示了本地DRAM、非交错式和交错式 Optane内存上线程数增加（从左到右）时的最大带宽。所有线程都使用256B访问大小。（注意垂直刻度的差异）"></p><p>   <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b2704fb5d70b40a6bfe34bf494accd34.png" alt="访问大小的带宽 实验显示（从左到右）本地DRAM、交错和非交错Optane内存上不同访问大小的最大带宽。图表标题包括每个实验中使用的线程数（读/写（ntstore）/写（clwb））"></p><h3 id="4-3-Optane-DIMM与其他仿真技术的不同之处"><a href="#4-3-Optane-DIMM与其他仿真技术的不同之处" class="headerlink" title="4.3 Optane DIMM与其他仿真技术的不同之处"></a>4.3 Optane DIMM与其他仿真技术的不同之处</h3><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3773889806b14aa9bba6d6952daa2a06.png"></p><p> 这些图中的数据表明，没有一种模拟机制能够捕获 Optane 行为的细节——所有方法都与真实的 Optane 内存有很大的偏差。他们无法捕捉傲腾内存对顺序访问和读/写不对称性的偏好，并对设备延迟和带宽给出非常不准确的猜测。</p><h3 id="4-4-Optane-DIMM最佳实践"><a href="#4-4-Optane-DIMM最佳实践" class="headerlink" title="4.4 Optane DIMM最佳实践"></a>4.4 Optane DIMM最佳实践</h3><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9c439f09a06947798a73788207f85a40.png" alt="单个 DIMM 上的 EWR 和吞吐量之间的关系 每个点代表不同访问大小、线程数和功率预算配置的实验。注意指标之间的相关性"></p><p> 图8绘制了我们系统性地扫描Optane性能的所有测量结果中单个DIMM的EWR和有效设备带宽（dram和cpu之间的！）之间的强相关性。基于这种关系，我们得出结论，努力最大化EWR（Effective Write Ratio）是最大化带宽的好方法。</p><p> 而且由于最小粒度是256B，但是iMC发出64B的话，就会在缓冲区被合并为256B，所以下图探究了每次写入更新多少大小合适。<span class="label primary">避免小型的stores超过16KB也不合适。</span><br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b2618ae65fc840ee9e2bfca3cca67d6b.png"></p><p> 绘制iMC争用图。在固定数量6个线程的情况下，<span class="label primary">随着每个线程访问的DIMM数量的增加，带宽会下降</span>。为了获得最大带宽，线程应固定到DIMM。图15说明了当多个核心针对单个DIMM时，iMC中有限的队列容量如何影响性能。该图显示了使用固定数量的线程（24个用于读取，6个用于 ntstore）向6个交错 Optane DIMM 读取/写入数据的实验。我们让每个线程随机访问N个 DIMM（跨线程均匀分布）。随着N的增加，针对每个DIMM的写入器数量会增加，但每个DIMM的带宽会下降。可能的罪魁祸首是XP Buffer的容量有限，但EWR仍然非常接近1，因此性能问题肯定出在iMC上。</p><p> <span class="label primary">避免对远程 NUMA 节点进行混合或多线程访问</span>PM性能下降率与远程DRAM与本地 DRAM的性能下降率类似。然而，当线程数量增加或读/写混合工作负载时，Optane内存的带宽会急剧下降。根据我们的系统扫描结果，相同工作负载下，本地和远程Optane内存之间的带宽差距可能超过30倍，而本地和远程DRAM之间的差距最大仅为3.3 倍。<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/125f5e3edb7949c0811862d4532ac6cd.png"></p><h2 id="5-如何扩展到未来几代-NVM"><a href="#5-如何扩展到未来几代-NVM" class="headerlink" title="5.如何扩展到未来几代 NVM"></a>5.如何扩展到未来几代 NVM</h2><p> 增加或减少 256 B 内部写入大小可能会很昂贵。人们普遍认为Optane是相变存储器，由于功率限制，较小的内部页面尺寸长期以来一直是相变存储器的标志[2]。较小的内部页面大小不太可能，因为由此产生的存储器密度较低。不同的底层存储单元技术（例如自旋扭矩 MRAM）将带来更彻底的改变。事实上，电池供电的 DRAM 是一种众所周知且广泛部署的持久内存技术（尽管可扩展性或成本效益不高）。对于它，我们的大多数指南都是不必要的，尽管由于缓存一致性模型的限制，非临时存储对于大型传输仍然更有效。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Optane NVDIMMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Summary of hybrid main memory systems</title>
      <link href="/2023/10/09/A-Summary-of-hybrid-main-memory-systems/"/>
      <url>/2023/10/09/A-Summary-of-hybrid-main-memory-systems/</url>
      
        <content type="html"><![CDATA[<p>很显然，目前看到的两篇都有考虑到了PM停产CXL比较热的事情，所以他们第二层其实说的比较抽象。最后实验的时候大框架没变，就是直接说用PM做。PM还有挺多和DRAM差距大的地方。<br>—— 转向CXL<br>—— 改策略里面的参数<br>—— 归纳这些都有的特点<br>—— 或者人家做NUMA<br>—— 或者把重心放在观察workload上，然后让内核收集分析那些数据，只是最后实验的时候需要</p><p>分层存储系统。英特尔傲腾内存模式[2]在硬件中实现了操作系统透明的分层内存系统，软件无法为各种应用程序自定义分层策略。 [38,42,51]是硬件辅助的分层存储系统，在仿真中进行评估。 [7,24,26,29,31,37,56,58]在操作系统级别对应用程序透明地管理分层内存系统，类似于我们的系统。 </p><p>Nimble [56] 专注于优化大页的迁移。<br>HeteroOS [29] 为客户操作系统提供异构内存，并协调客户操作系统和 VMM 进行页面放置。<br>恒温器 [7] 使用页面采样将页面分类为热页面或冷页面。 [24,26,31,37]扩展了页面回收以降级冷页面来代替交换，而我们的系统应用特定于每个应用程序类的降级年龄策略。<br>[24,26,31,37,58]还扩展了 Linux NUMA 平衡页面迁移，以在轻微提示页面错误时提升热页面。这种基于页错误的技术可以比 A 位扫描和内存访问采样更快地检测到对第 2 层页面的访问，但代价是增加页错误和 TLB 失效，特别是对于这些技术并不总是发生的重复页错误。在第一次访问时推广二级页面。<br>AutoTiering [31] 使用 Optane 作为 tier2 内存，而 [7、29、37、56、58] 使用模拟的 tier2 内存进行评估，这不能准确反映较慢内存层的性能影响。这些系统都没有通过仓库规模数据中心的生产工作负载进行评估。有在应用程序或库级别管理的分层内存系统，例如 X-Mem [20]、Unimem [53]、AIFM [44] 和 pVM [30]，它们依赖于自定义内存 API 和软件修改。</p><p>其中，HeMem [43]与我们的系统类似，使用Optane作为tier2内存，并使用PEBS来跟踪Optane上的热点页面以进行推广。这些方法中的许多技术，例如配置文件 [20, 51, 53]、自定义分配和预取 [10, 36]，可以用来优化我们系统中的应用程序性能，类似于第 8.3 节中的分配提示。基于交换的远内存。使用交换来扩展内存是众所周知的。<br>交换目标可以是内存压缩 [32, 52]、本地磁盘 [52] 或通过 RDMA [8]、Infiniband [23] 或其他互连的远程设备。</p><p>用于页面回收和交换的冷页面识别技术[17,41,59]也适用于内存分层。一些控制交换主动性的反馈技术，例如 PSI [52]，也可以扩展到适用于直接访问的分层内存系统。交换设备和慢速层内存之间的访问延迟的数量级差异也需要不同的页面放置策略。分解的记忆。最近许多针对分解内存的软件运行时工作 [11, 45] 都是为网络上的 RDMA 设计的，其延迟比直接连接或通过 CXL.mem 连接的分层内存高一个数量级。</p><p> DirectCXL [22] 和 Pond [34] 通过 CXL.mem 扩展具有机架规模分解内存的机器中的内存。这样的硬件系统可以配置为用于容量和带宽扩展的非分层存储系统[22]，也可以配置为可以应用我们的分层软件的分层存储系统。此外，Pond 单独研究了 158 个应用程序，而我们在动态生产设置中使用超过 100K 个应用程序来评估我们的系统。<br>（来自仓库级规模那一篇）</p><h2 id="1-异构内存设备"><a href="#1-异构内存设备" class="headerlink" title="1. 异构内存设备"></a>1. 异构内存设备</h2><p>随着字节可寻址的非易失性设备的出现以及商业化，目前面临的内存价格高昂，以及一些关键的应用如图计算、高性能计算、内存数据库等需要大内存容量，这些问题可以得以缓解。但是这具体还是取决于异构内存的架构设计。</p><p>现在做的话需要尽量独立于Optane的特性去做，那么确实有必要收集一下这些的共性。</p><p>下面收集了目前用于做异构内存的设备以及他们的特点：<br>缩写 名称 访问方式<br>RDMA 通过内存总线和处理器直接相连 DDR4<br>PCM 变相存储器 通过内存总线和处理器直接相连<br>ReRAM 电阻式存储器 通过内存总线和处理器直接相连<br>Optane PM(3D-XPoint)  通过内存总线和处理器直接相连 DDR-T</p><p>DCPMM的特性和指导原则是否适用于其他PMEM技术?DCPMM的一些特性只与它的内部结构和存储技术有关。因此，近期DCPMM研究得出的指南并不广泛适用于其他类型的PMEM。遵循这些指导方针可能会得到一个高度专门化的系统，该系统可以很好地与DCPMM一起工作，但与其他PMEM不兼容。为了防止这种过度专业化，我们认为系统设计者应该正确地理解这些特性的根本原因，以及它们如何适用于未来不同类别的PMEM。</p><p>不对称的</p><h2 id="2-分层架构设计"><a href="#2-分层架构设计" class="headerlink" title="2. 分层架构设计"></a>2. 分层架构设计</h2><h3 id="2-1基于LRU的异构内存系统设计"><a href="#2-1基于LRU的异构内存系统设计" class="headerlink" title="2.1基于LRU的异构内存系统设计"></a>2.1基于LRU的异构内存系统设计</h3><p>CLOCK-DWF 维护 DRAM 和 NVM 的两种时钟算法。在这之前还有CLOCK-DNF？ 然后……虽然都遵循了内核最初的设计，但是他们各有创新性的思考，侧重点不同比如读写次数、耗能、吞吐量这些都有显著提升。大家都遵循这种数据结构很大程度是因为不需要复杂的热度监控，而且大部分的工作持续证明这样简单的架构可以获得很大的性能提升，也是为数不多被Linux社区接受的（并入分支）。</p><h2 id="3-缓存架构设计"><a href="#3-缓存架构设计" class="headerlink" title="3. 缓存架构设计"></a>3. 缓存架构设计</h2><h2 id="4-NUMA架构下的异构内存"><a href="#4-NUMA架构下的异构内存" class="headerlink" title="4. NUMA架构下的异构内存"></a>4. NUMA架构下的异构内存</h2><p>随着cpu处理器的增加，内存总线的争用变得剧烈，NUMA架构受到了大家的青睐。但是NUMA架构的实现是基于一定的硬件和软件策略的。</p><h2 id="5-针对特定程序的设计"><a href="#5-针对特定程序的设计" class="headerlink" title="5. 针对特定程序的设计"></a>5. 针对特定程序的设计</h2><h3 id="5-1-图"><a href="#5-1-图" class="headerlink" title="5.1 图"></a>5.1 图</h3><p> 21B也有一篇，华科也有一篇，不一样</p><h3 id="5-2-并行并发"><a href="#5-2-并行并发" class="headerlink" title="5.2 并行并发"></a>5.2 并行并发</h3><h3 id="5-3-数据仓库"><a href="#5-3-数据仓库" class="headerlink" title="5.3 数据仓库"></a>5.3 数据仓库</h3><h3 id="5-4-大规模的统计"><a href="#5-4-大规模的统计" class="headerlink" title="5.4 大规模的统计"></a>5.4 大规模的统计</h3><p> 追溯到比较早的异构内存的研究，还没有可字节寻址的非易失存储设备的时候，异构内存通常在研究片上内存和主存之间的数据放置等。虽然他不是今天这个方向的主角（更多的异构内存是PCM、PM等硬件来做的），但是仍然有许多可以值得借鉴的工作。</p><p>片上和片外内存主要问题是————允许所有应用程序共享LLC（末级缓存）和DRAM组，从而在许多情况下导致严重的<strong>争用</strong>。这通常会导致程序间扰动、资源抖动、内存/缓存利用率不佳，从而导致性能下降。</p><p>大部分策略使用页面着色的方法指定DRAM可以对应放置的缓存，只是具体对应放置策略各有不同。</p><p>几种解决方案尝试通过将主内存（DRAM 组）[10,16,17,29] 或缓存 [15,24,30,31,32] 水平分区为独占片来隔离具有不同内存资源需求的应用程序。比如[10]通过扩展操作系统的物理帧分配算法让映射到同一DRAM Bank的物理帧可以专门分配给单个线程。</p><p>这些方法避免了对内存占用较小的程序的干扰，但可能会通过有效减少容量来影响较大工作负载的性能。</p><p>以[1]为例，商品生产并行机中使用的大多数现有内存和缓存管理机制都采用通用地址交错或调度/分区方法，这些方法忽视了当今异构环境中不同的内存利用率特征和不同的资源需求。这通常会导致程序间扰动、资源抖动、内存/缓存利用率不佳，从而导致性能下降</p><p>结合工作负载特点，并且将常用的2种着色位分为3类：bank-only、仅高速缓存位（C位）和重叠位（O位在图 1 中同时索引bank和高速缓存）。</p><p>23年关于片上和片外内存的研究主要关于堆叠式DRAM（stacked DRAM and off-chip DRAM）</p><h2 id="6-页面粒度与TLB"><a href="#6-页面粒度与TLB" class="headerlink" title="6. 页面粒度与TLB"></a>6. 页面粒度与TLB</h2><h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2><h3 id="7-1-异构内存架构"><a href="#7-1-异构内存架构" class="headerlink" title="7.1 异构内存架构"></a>7.1 异构内存架构</h3><h3 id="7-2-异构内存分配"><a href="#7-2-异构内存分配" class="headerlink" title="7.2 异构内存分配"></a>7.2 异构内存分配</h3><p> API之类的？</p><h3 id="7-3-页面相关属性监控"><a href="#7-3-页面相关属性监控" class="headerlink" title="7.3 页面相关属性监控"></a>7.3 页面相关属性监控</h3><p> 读、写、热度</p><h3 id="7-4-异构内存数据迁移"><a href="#7-4-异构内存数据迁移" class="headerlink" title="7.4 异构内存数据迁移"></a>7.4 异构内存数据迁移</h3><h3 id="7-5-数据回收-卸载"><a href="#7-5-数据回收-卸载" class="headerlink" title="7.5 数据回收(卸载)"></a>7.5 数据回收(卸载)</h3><p> mot之类的，watermake的修改</p><h2 id="9-CXL"><a href="#9-CXL" class="headerlink" title="9. CXL"></a>9. CXL</h2><h2 id="10-引用"><a href="#10-引用" class="headerlink" title="10. 引用"></a>10. 引用</h2><p>:tear: “很不规范的引用”<br>[1] Going Vertical in Memory Management: Handling Multiplicity by Multi-policy,ISCA,2014</p><p>（其实还有CXL）<br>每个标签结束后必须空一行:</p><div class="note info">这里是 info 标签样式</div> <div class="note info no-icon">这里是不带符号的 info 标签样式</div> <div class="note primary">这里是 primary 标签样式</div> <div class="note primary no-icon">这里是不带符号的 primary 标签样式</div> <div class="note warning">这里是 warning 标签样式</div> <div class="note warning no-icon">这里是不带符号的 warning 标签样式</div> <div class="note danger">这里是 danger 标签样式</div> <div class="note danger no-icon">这里是不带符号的 danger 标签样式</div><p>然后是行内标签，比加粗更能显示重点，Fulid移植的。<br><span class="label primary">Label primary</span></p><p><span class="label default">Label default</span></p><p><span class="label info">Label info</span></p><p><span class="label success">Label success</span></p><p><span class="label warning">Label warning</span></p><p><span class="label danger">Label danger</span></p><p><img src="https://images.weserv.nl/?url="></p>在这之间写LaTex或者其他造成的符号转义冲突之类的报错<p>会不会这种策略是跨整个子系统的，既有线程进程调度，又有迁移热度这些，就是把以前的策略都调一调？</p><p>现代主存储器系统利用空间局部性来提供高带宽，同时最小化存储器设备功耗和成本。利用空间局部性主要通过四种方式提高带宽和效率：（1）DRAM地址可以分为行地址和列地址，分别发送到存储设备以节省地址引脚； (2)访问粒度可以很大，以分摊控制和冗余开销； (3) 启用页面模式，其中，整个内存行都保存在行缓冲区中，因此具有相同行地址的后续请求只需发送列地址； (4)因为不访问主数据阵列，所以这样的行缓冲器命中请求消耗显着更少的能量并且具有更低的延迟。（这好像都是硬件设置啊啊啊啊啊啊啊啊~）</p><p>许多应用程序呈现具有高空间局部性的存储器访问模式，并受益于页面模式访问所实现的效率和性能。然而，随着芯片上内核数量的增加，内存访问流的空间局部性降低，因为独立线程的访问流可能在内存控制器处交错[2, 23]。根据应用程序组合，交错流的空间局部性可能会进一步下降，因为某些应用程序比其他应用程序更具侵入性。例如，SPEC CPU2006基准测试mcf是内存密集型的，但空间局部性非常差，因此严重干扰其他应用程序的内存访问。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Operating System Level Data Migration Scheme</title>
      <link href="/2023/10/05/An-Operating-System-Level-Data-Migration-Scheme/"/>
      <url>/2023/10/05/An-Operating-System-Level-Data-Migration-Scheme/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">- 文章来自Design, Automation &amp; Test in Europe (DATE), 2016- An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Reza Salkhordeh and Hossein Asadi, Data Storage Systems &amp; Networks (DSN) Lab, 伊朗谢里夫理工计算工程系</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>本文提出了一种在主存储器中同时采用DRAM和NVM的混合存储器架构中的数据迁移方案。所提出方案的主要目的是减少DRAM和NVM存储器之间无益数据迁移的数量，以提高性能和功效。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>与Clock-DWF不同的是，每次写入命中都会导致将页面移动到DRAM主存储器，在所提出的方案中，NVM LRU中的每次命中都将被与LRU算法类似地处理，但有一个区别。如果某个页面停留在LRU顶部页面的时间超过阈值访问次数，则该页面将被视为热页面，并将被移至DRAM。由于在两个存储器之间移动数据页的成本很高，因此使用此阈值将防止在以前的研究（例如Clock-DWF）中很可能发生的无益迁移。</p><p> CLOCK-DWF 使用两种时钟算法，每个内存模块使用一种时钟算法。当发生页面错误时，如果导致页面错误的请求是写入，则该页面将被移动到DRAM，否则它将被移动到NVM。时钟算法的修改使 CLOCK-DWF 能够找到活跃的并且写入为主的数据页并将它们移动到 DRAM 内存。如果针对驻留在 NVM 存储器中的数据页的写入请求到达，则该数据页将被移动到 DRAM。在两个存储器之间迁移页面需要对两个存储器进行多次访问。但CLOCK-DWF中没有考虑这种影响，这将导致其模型不准确</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>使用两个最近最少使用（LRU）队列（一个用于DRAM，一个用于NVM），并优化NVM的LRU队列，以防止无益的迁移到DRAM。LRU队列中的优化是最小的，因此所提出的方案将具有与未修改的LRU几乎相同的命中率。</p><h3 id="1）建立性能能量模型"><a href="#1）建立性能能量模型" class="headerlink" title="1）建立性能能量模型"></a>1）建立性能能量模型</h3><p>18年华科的对象级迁移的性能能量建模就和这个差不多。但是这里的性能能量建模只适用于说明之前方案的缺陷，和后面方案设计没太大关系。</p><p>性能模型取决于DRAM和NVM的延迟、驱逐的粒度以及存储器之间迁移的延迟。为了衡量性能，我们使用平均内存访问时间(AMAT)。迁移的开销将在对内存的所有访问之间按比例分配。公式1显示了AMAT 的公式。表I中提供了参数的描述。在该等式中，前两项计算DRAM或NVM中所有命中访问的 AMAT。第三项考虑页面错误。由于将数据页从磁盘传输到内存将通过DMA 完成，因此将数据块写入内存的延迟将与从磁盘读取下一个数据块的延迟重叠。因此，操作系统只看到磁盘延迟，在这个术语中我们只考虑磁盘延迟。最后两项计算两个存储器之间的迁移成本。当发生迁移时，将从一个内存读取数据页并将其写入另一个内存。由于数据页的粒度比对内存的实际访问要大得多（通常为4到16B），因此我们使用PageFactor，它是一个系数，可将数据页的移动转换为要被访问的内存数量。</p><div class="note info no-icon">PageFactor这个转换多余了吧，按页迁移也没法只移动这几B的，属于附带消耗。这位伙计定义的表格里的能耗什么的怎么收集到的，还有那些概率</div> <p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b5b2cad055864692a88d2b9ecbb5485d.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/add1e59ff2e8473f8454f2bf31f34f89.png"></p><p>无论到达存储器的请求数量如何，都会消耗静态功耗，而每个发送到存储器的请求都会消耗动态功耗。我们的电源模型考虑了两个内存之间的迁移以及将页面从磁盘移动到任一内存模块以及服务请求的静态和动态电源。每次访问内存时都会计算动态功耗。这将导致功耗模型独立于应用程序运行时和内存大小。因此，我们引入每个请求的平均功率(APPR)作为测量功率的指标，如公式2所示。与性能模型类似，前两项计算对存储器的所有命中访问的功率。第三项和第四项考虑将数据页从磁盘移动到内存模块的写入功率。最后两项考虑了两个存储器之间迁移的功率效应。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/78719c197cd24a28942ca867cf81a3a1.png"></p><p>由于静态功耗与请求无关，因此我们引入了一个名为 AvgStaticPower 的新参数，该参数按比例分配给定时间间隔内到达内存的所有请求之间的静态功耗。对所有请求按比例分配静态功耗的原因是，从操作系统的角度来看，主内存会消耗功率（包括静态和动态）来服务请求，并且这两个功耗来源都应被视为服务成本的请求。对于特定工作负载，AvgStaticPower根据公式3计算。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1103cf68e870402d91ee88f71097d61c.png"></p><h3 id="2）迁移方案"><a href="#2）迁移方案" class="headerlink" title="2）迁移方案"></a>2）迁移方案</h3><p>为了找到在迁移时能够改善功耗和性能的数据页（相对于迁移成本），所提出的方案存储了一些关于数据页的附加信息，例如NVM LRU队列中的读和写计数器。请注意，此附加信息不会干扰LRU，并且不需要了解此内务信息。对于NVM队列中的每个数据页，将存储两个计数器，用于计数从数据页进入队列时起对相应数据页的读写访问的次数。</p><p>图 3 显示了所提出的由两个 LRU 队列组成的数据迁移方案的架构。虚线表示所提出的技术执行的操作，实线表示传统的 LRU 管理算法。暗数据页访问更频繁，被视为热数据页。与 CLOCK DWF 将读取请求发出的页面错误放置在 NVM 上相反，所提出的方案将所有页面从磁盘移动到 DRAM 区域。这是因为移动到 NVM 或 DRAM 将导致 NVM 中的页面写入，因为 DRAM 始终已满，并且将数据页面移动到 DRAM 会向 NVM 发出驱逐。因此，就 NVM 写入而言，迁移到 NVM 或 DRAM 的成本是相同的。与旧数据页相比，新访问的数据页具有更高的访问概率，并且将此新页移动到 DRAM 将导致 DRAM 命中率而不是 NVM 命中率的增加。这将有助于提高性能和电源效率，因为 DRAM 在动态功耗和延迟方面具有优越性。存储内务信息的开销并不大，对于 4KB 数据页来说约为 0.04%。然而，在 NVM 中保留所有页面的计数器有一些缺点。首先，它需要一个排序方案，以便识别冷但会在很长一段时间内访问一次的数据页。这些数据页将在 NVM 中驻留足够长的时间以具有较高的计数器值，因此将被移动到 DRAM，在那里它们无法与热数据页竞争，并且将返回到 NVM，这使得它们迁移到 DRAM 没有任何好处。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/26073db180a64a9c888e27b6f08665c5.png"></p><p>算法 1 显示了在收到请求的情况下所提出方案的流程。由于 DRAM 包含最热的数据页，因此所提出的方案首先搜索 DRAM，如果没有找到，则转到 NVM。在 DRAM 中查找数据页将导致正常的 LRU 内务处理。否则，NVM 中的额外内务信息将根据请求类型进行更新。读取和写入计数器将分别存储在 NVM 中的 readperc 和 writeperc 顶部数据页。因此，在命中的情况下，从顶部数据页掉落的数据页的读和写计数器将被清除。第 10 行到第 22 行初始化相应数据页的计数器。如果 NVM 中数据页的计数器值超过读取阈值或写入阈值（取决于请求类型），则会将其迁移到 DRAM。将新数据页插入内存和逐出策略与 LRU 相比没有变化，因此，为了简洁起见，算法中省略了这些细节。读取阈值和写入阈值的值决定了我们计划如何积极地阻止有用概率较低的迁移。它与 DRAM 和 NVM 之间的迁移成本密切相关，而迁移成本又与所采用的 NVM 的性能和功耗特性有关。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c39c07a13b894e9ba202c0ea626bdc5b.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>发现了先前研究的缺点，并提出了一种新颖的混合内存数据迁移方案。在LRU上加了一些小改动。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>在过去几年提供的各种NVM中，相变存储器(PCM)、自旋转移扭矩(STT-RAM) 和电阻RAM(PRAM)被认为是主存储器中最有前途的NVM[2] 。</p><p>本文中我们使用COTSon全系统模拟器[13]。</p><p>CLOCK-DWF 维护 DRAM 和 NVM 的两种时钟算法。 NVM中的时钟算法与传统的时钟算法有一点不同。如果对 NVM 中的数据页进行写访问，则相应的数据页将被移动到 DRAM。因此，NVM 不会响应任何写访问。该方法的主要目的是减少 NVM 中的写入次数。虽然这会阻止任何写入到达 NVM，但 NVM 中数据页的每次写入访问都会导致两个存储器之间的数据页迁移。然而，DRAM 的时钟算法有所不同，它尝试将写主导的数据页保留在 DRAM 内存中，并逐出主要读主导的数据页。这是因为与 NVM中的写入请求相比，只读页面具有更好的性能与功耗权衡。发生页面错误时，如果请求是读，则相应的数据页将被移动到NVM，如果是写，则数据页将被移动到DRAM。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Characterizing and Optimizing Hybrid DRAMPM Main Memory System with Application Awareness</title>
      <link href="/2023/10/04/Characterizing-and-Optimizing-Hybrid-DRAMPM-Main-Memory-System-with-Application-Awareness/"/>
      <url>/2023/10/04/Characterizing-and-Optimizing-Hybrid-DRAMPM-Main-Memory-System-with-Application-Awareness/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自Design, Automation &amp; Test in Europe(DATE), 2022</li><li>Characterizing and Optimizing Hybrid DRAM-PM Main Memory System with Application Awareness</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Yongfeng Wang, Yinjin Fu, Yubo Liu, Zhiguang Chen, Nong Xiao中山大学计算机科学与工程学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>PM大容量之后，<span class="label primary">多个应用程序工作负载并发执行的数据放置、内存并发和工作负载调度方面存在关键的管理挑战</span>作者总结在混合内存系统场景下讨论的3个问题：数据放置、优化线程分配、并发程序执行顺序如表1</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c2fd2e2f7b514331a1be9c6f6fe7fcb0.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>提出了一系列应用感知的操作策略，包括：应用感知的数据放置、自适应线程分配和避免应用间干扰，以提高混合存储器上多个应用的并发性能。</p><p><span class="label primary">1)如何将应用工作负载分配到合适的内存设备上，以提高并发应用的整体系统性能?</span><br>2)如何通过为每个应用程序分配最佳的线程数来保证不同工作负载的并发执行的公平性?<br>3)什么样的应用工作负载调度方案可以避免或减轻两个或多个同时运行的应用之间的干扰，以实现更高的性能?</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>但是这些方案没有去具体考虑并发应用的个别需求(应用感知)。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="1）将应用感知解释为带宽敏感或者延迟敏感。"><a href="#1）将应用感知解释为带宽敏感或者延迟敏感。" class="headerlink" title="1）将应用感知解释为带宽敏感或者延迟敏感。"></a>1）将应用感知解释为带宽敏感或者延迟敏感。</h3><p>根据系统资源需求的不同，我们可以将计算服务器的应用工作负载分为CPU绑定和内存绑定。为了将我们的工作重点放在内存资源管理上，绑定内存的应用被进一步分为对带宽敏感的应用和对延时敏感的应用。我们的分类使我们能够清楚地区分工作负载类别，因为每个工作负载类别都形成了自己独特的集群。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cddfdfc49de643f0b30cf713269f15c8.png"></p><p>图1显示了所有工作负载的延迟和内存带宽需求的敏感性。当一个应用程序在x轴和y轴上的数值都很低时，它<br>就是一个受CPU约束的应用程序。对性能要求高的数据库工作负载对延迟最敏感，而对带宽的敏感度低。而大数据<br>分析的工作负载，如OLAP、Lu cb和facesim,对带宽和延迟的敏感性处于中间水平。HPCG、Graph500和一些数据密<br>集型工作负载对带宽的敏感性最高，对延迟的敏感性最低绑定CPU的工作负载，如HPL、swaptions和freqmine, 对<br>内存延迟或带宽的敏感性不高。</p><div class="note info no-icon">不过，这个图是怎么得到的？</div> <p>下图表示应用感知优化后达到的效果：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b4675b4852e44b7cba5a23362663c62f.png"><br>大致看起来是想说，第三节数据放置达到缩短运行时间的效果、第四节线程分配、第五节接口冲突。</p><h3 id="2）数据放置策略"><a href="#2）数据放置策略" class="headerlink" title="2）数据放置策略"></a>2）数据放置策略</h3><p>在混合内存中，两种内存介质上的应用性能是完全不同的，而且这种影响的大小在很大程度上取决于应用的特性为了评估这一点，我们从PARSEC[16]、splash2x[17]和NPB[18]选择了几个测试。</p><div class="note info no-icon">这里选择都是并发的程序，而且两种介质可以影响性能的差别就是延迟和带宽（还有其他什么吗？），程序又有不同的延迟带宽敏感。在纯PM上运行，这只说了物理机配置，所以这是怎么跑起来的？</div> <p>图3显示了在PM上运行的8线程应用程序工作负载与DRAM上运行的应用程序工作负载的性能下降情况以及完成时间的比率。每个应用程序将分别在PM和DRAM上运行10次，我们记录它们的平均完成时间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8b9d7567cae24be79b422c66b5289d60.png"></p><p><span class="label primary">持久性存储器的带宽比DRAM低，这使得应用程序在PM上运行的时间更长。以ocean_cp为例，它在计算的不同阶段流经其分割的许多不同的网格，随着问题大小的增加，会产生大量的容量和冲突失误[17]。当工作集超过高速缓存的<br>大小时，内存带宽就会成为这个应用的瓶颈。在这个实验中，ocean_cp在DRAM上的读和写分别消耗了40.9GB/s和<br>15.3GB/s的带宽，而在PM上由于其带宽较低，只消耗了2.45GB/s和0.8GB/s。这个17倍的带宽差距可以解释ocean_cp在PM上的减速。PM的有限带宽也是对带宽敏感的应用减速的主要原因，如canneal、radix、NPB/FT、NPB/MG和<br>NPB/SP。此外，其他应用工作负载在PM上的运行时间不到DRAM上的1.5倍，如freqmine和swaption。 它们都是与CPU绑定的应用，可以充分利用内存的局部性来减少高速缓存的miss。因此，内存不是这类应用的瓶颈。因此,在不同的内存介质上运行的应用程序对这些工作负载的执行时间有着难以察觉的影响。</span></p><div class="note info no-icon">这里的意思是带宽敏感和数据放置关系更紧密，延迟敏感和之后要说的会比较相关。后面的分配策略完全也不考虑热页面迁移了，直接不是带宽敏感丢给PM。硬件性能计数器很多啊，带宽敏感是用啥表示？而且你在DRAM不够时怎么在分配之前知道这是不是带宽敏感的，那就是用户手动指定嘛，运行时硬件计数肯定不能提前知道；另一方面正在运行时的内存分配要按照不同workload做标记咯，要不你现在要分配的page你也不知道是哪个程序申请的，他自己是不是带宽敏感</div> <p>当应用程序工作负载启动时，如果有足够的可用空间，它首先尝试将其分配到DRAM上。否则，我们必须判断应用程序是否对带宽敏感。对于带宽敏感的应用程序，它可以将CPU-Bound应用程序的一些占用的DRAM页面迁移到PM，然后为新应用程序分配这些DRAM页面。如果DRAM中的所有应用程序工作负载都是带宽敏感的，我们只需将新应用程序分配给PM即可。此外，我们还将对带宽不敏感的应用程序分配给性能下降较低的PM。为了使我们的应用程序感知策略切实可行，应用程序类型可以由用户定义或根据硬件性能计数器的信息自动分类。</p><p>下面这个实验表示通过这种方法程序放置对了，而且节约了总的执行时间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/23f0d575dd7547d4902c544be540cd3a.png"></p><h3 id="3）线程分配（内存并发）"><a href="#3）线程分配（内存并发）" class="headerlink" title="3）线程分配（内存并发）"></a>3）线程分配（内存并发）</h3><p>在不同介质上，线程可扩展性不同，在PM上运行的对带宽敏感的应用程序的性能不能随着线程数的增加而同步提高。因此，有必要为运行在不同内存介质上的应用程序找到一个合适的并行参数。</p><div class="note info no-icon">这个怎么还能放先验知识，而且the remain thread resources can be allocated to other applications这个资源是可以被量化的吗？</div> <p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f9dbccacad674d3b8eb4008a6a6113cc.png"><br>在第1组中，左侧结果显示每个应用程序传统均匀分布的8个线程的持续时间。根据先验知识，我们发现PM上的最佳线程NPB/FT数量为4。</p><h3 id="4）带宽争用"><a href="#4）带宽争用" class="headerlink" title="4）带宽争用"></a>4）带宽争用</h3><p>当两个或多个应用程序同时访问混合内存时，会出现带宽争用，这将导致严重的干扰和并发性能下降。而且混合内存中的这种干扰比纯DRAM内存系统中的干扰更为复杂。为了证明这一点，我们首先尝试遍历所有内存访问模式。我们的测试包括4种类型的内存访问模式（1）SR：顺序读取，（2）RR：随机读取，（3）SW：顺序写入，（4）RW：随机写入1、2、4、8和10DRAM和PM上运行的线程，将形成40种内存访问模式。然后，我们运行带宽基准测试并获得所有内存访问模式的带宽，没有任何干扰。最后，分别运行40种内存访问模式构建40种干扰，对每种内存访问模式重新进行基准测试，得到每种干扰下的带宽缩减率。结果如图7所示，概括为以下两个方面：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a4f8361713864c139a2c9e25be08ccb5.png" alt="X轴代表干涉类型，Y轴代表基准类型。颜色越深意味着与没有干扰时相比，带宽减少得越多"></p><p><span class="label primary">访问DRAM对PM的带宽影响很小，而PM的读或写会显着减少DRAM的带宽。</span> PM上10线程顺序读取的带宽为13GB/s，相对于10线程DRAM读写的干扰，最多可以降低15%（11GB/s）。相比之下，PM上有10个干扰线程进行顺序读取，DRAM上顺序读取的带宽可减少80%（94GB/s至18GB/s）。而且，对于DRAM的带宽来说，PM读写造成的干扰比DRAM上的干扰更为显着。</p><p>其次，线程数量和内存访问模式对并发应用的干扰影响很大。随着线程数量的增加，干扰会变得更加严重。在具有1、2、4、8或10个线程的PM上随机写入将使具有10个线程的DRAM上的顺序写入的带宽减少33.8%、68.2%、89.9%、96.7%、97.6%（68GB/s 至 1.7GB/s）。对于内存访问模式来说，随机写入会对DRAM的写入性能产生最严重的干扰。考虑到PM上的密集写入会减少带宽95%以上在 DRAM 上运行的带宽敏感应用程序仍然会受到严重影响。<span class="label primary">PM上的密集访问将显着减少DRAM的带宽。对PM的随机写入和顺序读取分别对DRAM的写入和读取带宽有更严重的干扰</span></p><p>解决方式就是通过上面的分析动态调整这些应用程序工作负载的执行顺序,也是通过先验知识去查找。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b406960826234fc8a2610dc1b7a2befd.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>这个在上一节已经说了。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>也是在5部分吐槽了。解决方案很多靠先验经验或者用户直接指定，而且细节也不咋提。虽然这个是针对并发程序设计的，也可以和其他混合内存系统相比较一下啊。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>从带宽敏感考虑了内存分配，而且性能提升还不错。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>GPU内存系统中，应用感知的内存调度方案[13], [14]可以通过减少地址转换和数据请求之间的干扰来提高<br>公平性和整体系统性能。</p><p>应用感知的存储器通道划分算法[15]可以为不同的应用分配首选的存储器通道，以减少应用间的存储器干扰。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Going Vertical in Memory Management</title>
      <link href="/2023/09/15/Going-Vertical-in-Memory-Management/"/>
      <url>/2023/09/15/Going-Vertical-in-Memory-Management/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自International Symposium on Computer Architecture，ISCA，2014</li><li>Going Vertical in Memory Management: Handling Multiplicity by Multi-policy</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Lei Liu，Zehan Cui，Yungang Bao，Mingyu Chen，Chengyong Wu，中国科学院计算机研究所计算机体系结构国家重点实验室</li><li>Yong Li匹兹堡大学ECE系</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>来自不同领域的许多新兴应用程序通常表现出异构内存特征。<span class="label primary">当在并行平台上组合运行时，这些应用程序会呈现出令人畏惧的各种工作负载行为</span>，这对任何<span class="label primary">内存分配策略的有效性</span>提出了挑战。先前的基于分区或随机内存分配方案通常仅管理内存层次结构的一级，并且通常针对特定工作负载。</p><p><strong>共享内存资源</strong>的有效管理对于应用程序性能和系统吞吐量非常重要。然而，商业化并行机中使用的大多数现有内存和缓存管理机制都采用通用地址交错或调度/分区方法，这些方法忽视了当今异构环境中不同的内存利用率特征和不同的资源需求。这通常会导致程序间扰动、资源颠簸、内存/缓存利用率低下，从而导致性能下降。</p><p>当时架构允许所有应用程序共享LLC（末级缓存）和DRAM组，从而在许多情况下导致严重的争用。一般用页面着色解决，存在两种基于页面着色的分区技术，即高速缓存分区和DRAM存储体分区。如图 1 所示，可以通过使用操作系统物理页地址中表示LLC集索引（LLC颜色位）的位作为颜色位来实现缓存分区。当为应用程序分配页面时，操作系统可以为物理页面分配特定的颜色，以便应用程序只能访问指定颜色的缓存集。</p><div class="note info no-icon">说人话：运行的特定某个程序只能使用颜色A的DRAM分区，或者只能缓存到颜色A'的LLC缓存分区中</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/510d5e5c024448c78ccbc62c9e047875.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>处理各种动态变化的内存和缓存分配需求。需要设计一种能够通过区分内存特性来选择合适的分配策略的内存管理系统。</p><p>（为了实现这一目标，简单地集成最佳性能的机制是不切实际的，因为几乎所有最先进的方案都需要对内存控制器/分配器或缓存层次结构进行昂贵的更改，更不用说检测和预测应用程序需求和冲突方面的挑战了。）</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>最近的几种解决方案尝试通过将主内存（DRAM 组）[10,16,17,29] 或缓存 [15,24,30,31,32] 水平分区为独占片来隔离具有不同内存资源需求的应用程序。这些方法避免了对内存占用较小的程序的干扰，但可能会通过有效减少容量来影响较大工作负载的性能。操作系统级别的分区和其他内存分配优化更加灵活，并且在许多案例方面表现良好。</p><p>先前的研究工作[12,29]表明，LLC和DRAM争用会显着降低整体系统性能，并且已经提出了许多解决方案来缓解争用问题。最有效的优化之一是基于页面着色的软件分区，它允许操作系统内核利用底层架构信息，例如 LLC 和 DRAM 的物理地址映射。通过页面着色，可以通过修改内核伙伴系统来缓解争用问题 [4,10,15,17,21,22,24,26]，同时避免对内存控制器或缓存层次结构进行昂贵的硬件更改。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>为了处理多样化且动态变化的内存和缓存分配需求，我们通过垂直分区增强现有的“水平”缓存/DRAM 存储体分区，并探索由此产生的多策略空间。着色位分为三类：bank-only、仅高速缓存位（C位）和重叠位（O位在图 1 中同时索引bank和高速缓存）。特别是，O位启用垂直分区（VP），通过内存层次结构垂直地对高速缓存和bank组进行分区。结合水平和垂直分区形成了以前未研究过的分区策略空间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d82182ffc00c4216b76bef9a55d7bd4a.png" alt="从操作系统和典型多核机器上三类颜色位的角度来看的地址映射"></p><p>水平内存和缓存分区的好处是否可以累积（即，我们应该进行垂直分区吗？）所以作者测了测如下表所示的几种搭配<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/26bbd7193c5f467c9dbd3405d6959b7c.png"></p><p>把214个workload根据性能提升的原因做了可视化<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/93ea1c4e302d44109902c0078a399e3f.png"></p><p><strong>从上述定量研究中可以得出一个明显的结论：内存分配策略的有效性取决于特定的应用程序特性，特别是缓存需求。实际上，工作负载可能包含多个同时运行的应用程序，这些应用程序具有不同特征的任意组合，这使得确定适当的内存分配任务具有挑战性。</strong>然后总结了一下其他因素影响不大，缓存分区性能表现出的性能差异更大。为了验证缓存利用率特征对缓存分区策略的潜在影响，我们收集了当缓存配额从8/8（使用整个缓存）减少到1/8时各种应用程序的性能下降情况。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d6f56fdc5eca4311851b27a93bc66713.png"></p><p>每个应用程序都会执行八次，每次都会通过基于页面着色的缓存分区分配不同数量的 LLC。根据结果​​，我们将应用程序的缓存行为分为四类：Core Cache Fitting（CCF）、LLC High （LLCH）、LLC Middle（LLCM） 和LLC Thrashing（LLCT）。图 4 报告了SPEC2006基准测试套件中各种基准测试的分类 [1]。CCF应用程序（表示为绿色曲线），例如hmmer和namd，在使用较少的LLC资源时不会显着降低性能，因为它们的工作集大小足够小，可以容纳L1和L2每核专用缓存。LLCT应用程序（黑色曲线），例如libquantum，也对缓存配额不敏感，但这是由于缓存抖动行为而不是较小的工作集大小。LLCH应用程序（红色曲线）（例如mcf）由于其资源匮乏的特性，因缓存配额减少而遭受最严重的性能下降。与LLCH相比，LLCM（蓝色曲线）应用程序使用更少的缓存资源，因此速度减慢没有LLCH应用程序那么多。例如，gcc和bzip2是LLCM，因为当缓存从8/8减少到4/8时，它们不会遭受明显的降级。然而，当缓存配额降至3/8以下时，性能会急剧下降。</p><p>但是要动态分类：做图4时的静态分析发现热页面的数量在很多情况下可以反映应用程序的LLC需求。图5显示了多个基准测试的热门页数量和缓存需求之间的相关性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e10451bd578c4c58bbc3ceb565996949.png"></p><p>研究了这些策略针对<strong>2000</strong>多个工作负载的性能，并通过<strong>数据挖掘方法将结果与应用程序特征相关联</strong>。基于这种相关性，我们得出了几种实用的内存分配规则，并将其集成到统一的多策略框架中，以指导动态和多样化的多编程/线程工作负载的资源分区和合并。（生成了一套实用的分区和聚合规则以及一棵策略决策树，帮助HVR自动选择策略、动态资源分区和聚合。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c95f6f8820ec410285d3dadd9678cb5c.png"></p><p>（置信度和支持度是数据挖掘中的术语。在我们的工作中，支持被定义为规则中包含特定类型应用程序的工作负载的比例；置信度表明该规则的准确性。）</p><div class="note info no-icon">其实就是先采样热页，制定一些阈值，得到分类算法。然后将workload名称和分类绑定成一个键值对，再将分类和着色位的性能（就是最初的散点图）建立相关规则（通过观察得到的，比如：“包含 LLCT 和其他应用程序（LLCH、LLCM、CCF）的工作负载应使用 C-VP 或 A-VP（37.1% 支持，94.4% 置信度）”）。！！总是一开始使用bank-only然后迁移！！</div><p>在 Linux 内核 2.6.32 中将我们的方法实现为重构的页面索引系统加上一系列内核模块。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>大量实验表明，在实践中，我们的框架可以选择适当的内存分配策略，并始终优于未修改的Linux内核，与现有技术相比，性能提升高达11%。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>运行时采样得到分类，再和着色位绑定太晚了吧。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Chip on </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dancing in the Dark Profiling for Tiered Memory</title>
      <link href="/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/"/>
      <url>/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Parallel and Distributed Processing Symposium, (IPDPS), 2021</li><li>Dancing in the Dark: Profiling for Tiered Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jinyoung Choi, 加州大学河滨分校（UC Riverside）</li><li>Sergey Blagodurov, 美国超威半导体公司（AMD）</li><li>Hung-Wei Tseng, UC Riverside</li></ul><h2 id="2-Introduction-amp-Background"><a href="#2-Introduction-amp-Background" class="headerlink" title="2. Introduction &amp; Background"></a>2. Introduction &amp; Background</h2><ul><li>究竟如何向软件公开每种内存类型是一个悬而未决的问题</li><li>多种内存技术协同-&gt;更多内存、更少延迟、更大带宽</li><li>tiered memory architectures （TMAs）</li><li>缓存会给更多设计给硬件，平面寻址会有更多软件策略</li></ul><p><strong>软件监控方法，通过TLB条目和内存页的现状：</strong></p><ol><li>页面访问不可见只有缺页时才知道，识别被多次访问的代码段很难。</li><li>硬件监控的多样性，是由供应商提供，没有标准。</li><li>创建一个新的分析工具，组合硬件监控器的信息，抽象出一些二级指标，再通过一些策略，做出页面热度排名之类的。</li></ol><p><strong>与硬件缓存相比，软件控制的分层内存4个优点：</strong></p><ol><li>首先，分层内存允许直接从数据所在的层进行就地内存访问。使用硬件缓存时，请求的内存块将从第2层可寻址内存引入第1层缓存，从而导致流量增加。而且缺页导致的迁移开销大。</li><li>其次，在TMA中，内存页面可以在任一层中找到；缓存会在内存中创建重复的、可能不一致的页面副本，并且需要机制来保持数据一致。RDMA做缓存带宽效率会比较低[2][10]</li><li>第三，分层内存解决方案允许缓存策略微调（通过工作负载混合、服务级别协议等）以适应高层策略决策并消除过度迁移。（不懂）</li><li>第四，由于与根深蒂固的NUMA架构相似，分层内存可以利用NUMA和异构内存管理（HMM）[5]系统基础设施。 Linux社区中关于如何将NVM暴露给操作系统一直存在争论。当前的提案围绕将NVM配置为无CPU的NUMA节点以及使用AutoNUMA或其他现有的管理NUMA方法[11]、[12]管理TMA平衡。这里再次强调的是内存如何分配和移动，而不是如何分析热度（例如，AutoNUMA 中的定期取消映射和页错误处理如何产生开销 [13]）。我们的工作重点是比较各种监控方法，以最小的开销获得最大的热度可见性，这对 NUMA 和分层内存都有好处。</li></ol><p><strong>内存分析方法：</strong></p><ol><li><span class="label info">页表项PTE位跟踪。PTE包括已访问A位和脏D位。操作系统可以清除这些PTE位，并且硬件页表遍历器PTW将设置它们。</span>PTW在TLB未命中时设置A位。A位不区分在分析间隔期间访问一次的页面和多次访问的页面。更频繁的A位检查可以提高分析的信息量，但也会增加开销。</li><li><span class="label info">TBP能够从加载或存储指令收集地址跟踪</span>。在AMD系统上，基于指令的采样IBS[18]、[19]使CPU指令能够在穿过管道时被标记，允许在指令执行时收集数据，并在指令退出时引发中断。</li><li>轻量级分析（LWP） [20] 是AMD64系列15h AMD处理器的硬件扩展，与IBS的不同之处在于LWP在生成中断之前收集大量数据。</li><li>英特尔的处理器基于事件的采样（PEBS）是一种基于跟踪的功能，类似于IBS/LWP，其中处理器在指定的内存区域中记录标记的样本。PEBS样本可以根据许多事件（例如缓存未命中）来选择，每个PEBS记录包含<span class="label info">时间戳、线性地址和物理地址</span>等[21]</li><li>硬件性能计数器HWPC是大多数现代CPU和GPU上可用的特殊硬件寄存器，作为性能监控单元PMU的一部分。通过事件复用，perf和pfmon等软件工具可以监控比物理寄存器更多的事件。HWPC是粗粒度的（所有进程页面的一个指标），并且不能用于获取内存访问跟踪 [25]</li><li>BadgerTrap[6]拦截TLB Miss，并且对TLB做相应修改，用采样页预估总体未命中次数。</li></ol><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>这篇文章评估了很多内存监视器，提出一种统一的方法，为分层内存提供详细、低开销的可见性。</p><p>(1) 提出了一种低开销、高精度的分析机制，可以缓解TMA中的性能问题。<br>(2) 本文产生的见解可以指导TMA的高效内存管理策略。<br>(3) 本文使用所提出的分析机制来实现和评估内存管理策略，以在不进行任何硬件修改的情况下实现加速。<br>(4) 介绍了一种分析工具作为可升级的解决方案，以提高分层内存系统的性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b3e4837a025a42dbaec15d17e0663308.png"></p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>figure1系统的架构图，说明了每个部分的功能，其实就是将以上提到的组合了一下，取长补短。TMP 使用多种互补的监测方法，最大限度地提高信息量并最大限度地减少开销（表 I）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d1e3a9c929af46c796a7d2a5061b421c.png"><br>实施方案，非常细节可以对着代码看。</p><p>TMP在分层内存中的使用还是识别页面冷热，并且排序，然后确定哪些页面是可以迁移的，迁移时虚拟地址不变，页面物理地移动（这一段介绍也很细节，可以对着代码看）作者figure6的分析表明，是想要去提高第一层的命中率的。</p><div class="note danger no-icon"> 迁移会涉及TLB shootdown这个开销大？</div><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>选用了云工作负载和HPC工作负载，但是这位怎么只给了表格没有引用emmm，而且云工作负载看起来像自己取的名字。</p><h2 id="7-缺陷和wuwu改进思路"><a href="#7-缺陷和wuwu改进思路" class="headerlink" title="7. 缺陷和wuwu改进思路"></a>7. 缺陷和wuwu改进思路</h2><p>但是整个组合会变得复杂，到底在收集信息时内核负担会不会变大反而效率更低。文章说：保持工作负载开销低于应用程序开销的 5%。</p><p>有的对硬件的采样是不允许的，那么这个可能有的地方收集不了数据。</p><p>有的采样率还是得手动调。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>数据密集型服务器：内存中键值存储、数据库、整合在各个云服务器上的虚拟机以及需要大量内存的高性能计算HPC应用程序。</p><p>对延迟的优化，可以通过命中更多的热页面，常用的优化内存延迟的方式。</p><p>内存技术及其相关字节可寻址接口（例如 NVDIMM P、CXL、3D XPoint DIMM、CCIX和Gen-Z）具有不同的延迟、带宽、功耗、持久性和每GB成本特征，成功的分层内存架构必须依赖系统来最小化与内存内容访问和每个内存请求的地址转换相关的延迟。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> 内存监控工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Center Ethernet and RDMA Issues at Hyperscale</title>
      <link href="/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/"/>
      <url>/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE Computer Volume: 56, Issue: 7, July 2023</li><li>Data Center Ethernet and Remote Direct Memory Access: Issues at Hyperscale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Torsten Hoefler , ETH Zürich</li><li>Duncan Roweth, Keith Underwood, and Robert Alverson, Hewlett Packard Enterprise</li><li>Mark Griswold, Vahid Tabatabaee, Mohan Kalkunte, and Surendra Anubolu, Broadcom</li><li>Siyuan Shen, ETH Zürich</li><li>Moray McLaren, Google</li><li>Abdul Kabbani and Steve Scott, Microsoft</li></ul><p>使服务器应用程序可以直接操作远程服务器的内存,不需要经过操作系统和CPU。</p><p>Fabric，就是支持RDMA的局域网(LAN)。</p><p>怎样才能对内存进行传输，注册。 因为RDMA硬件对用来做数据传输的内存是有特殊要求的。</p><p>在数据传输过程中，应用程序不能修改数据所在的内存。<br>操作系统不能对数据所在的内存进行page out操作————物理地址和虚拟地址的映射必须是固定不变的。<br>注意无论是DMA或者RDMA都要求物理地址连续，这是由DMA引擎所决定的。</p><p>几十年来，以太网一直在有线局域网领域占据主导地位，范围从私人住宅的部署到最大的数据中心。数据中心经历了巨大的增长在过去的十年中，联网机器的数量超过了当今最大的超级计算机的规模。尽管仍存在一些差异，但此类超大规模数据中心和超级计算机的网络要求非常相似。[1]然而，超级计算机传统上使用专用互连进行连接，而数据中心则建立在以太网上。由于类似需求和规模经济，随着每一代新技术的产生，两者的结合不断紧密。我们认为现在是重新思考融合互连的基本假设和架构的最佳时机。</p><h3 id="数据中心以太网的新环境"><a href="#数据中心以太网的新环境" class="headerlink" title="数据中心以太网的新环境"></a>数据中心以太网的新环境</h3><p>多种技术趋势正在加速高性能互连的融合。首先，不断增长的网络性能要求推动了可支持TB级带宽的更高效的主机堆栈的发展，每秒数亿美元的交易以及新兴数据密集型应用程序（例如人工智能（AI））所需的单位微秒延迟。[2]这些极端要求迫使所有协议和硬件都尽可能高效，以至于尽可能有效排除许多传统上驱动数据中心网络的TCP/ IP状堆栈远程直接内存访问（RDMA）是大约三十年前用于高性能计算（HPC）工作负载的远程内存访问（RDMA），后来又将其扩展到InfiniBand(IB) verbs RDMA的目标存储。RDMA可以通过网络启用CPU释放的硬件加速DMA。在过去的10年中，它事实上已成为低开销和高速网络的标准。</p><blockquote><p>InfiniBand verbs 是InfiniBand架构中定义的一套编程接口(verbs),用于应用程序访问InfiniBand网络资源并利用RDMA(远程直接内存访问)技术进行通信。</p></blockquote><blockquote><p>在数据中心网络中排除传统TCP/IP栈的主要原因有:</p></blockquote><ol><li>TCP/IP栈开销大。TCP/IP栈在主机内核中运行,每次网络IO都需要经过完整的网络协议处理,包括缓冲拷贝、上下文切换、校验计算等,增加延迟和CPU使用。</li><li>不必要的内存复制。数据在应用、内核、NIC间多次缓冲拷贝,影响吞吐量和延迟。</li><li>CPU利用率高。网络栈处理占用大量CPU资源,对计算密集型应用影响很大。</li><li>通信堆栈长。TCP/IP通信堆栈包含过多层,每个层在主机内核和NIC都要处理,增大延迟。</li><li>难以实现kernel bypass。 TCP/IP难以实现应用直接控制网络硬件,无法实现kernel bypass架构。</li><li>可扩展性差。TCP难以实现应用级的可扩展性和负载均衡。 </li><li>缺乏数据中心网络特性。TCP/IP缺乏RDMA、RoCE、sr-iov、overlay网络等数据中心特性。）所以数据中心采用RDMA、userspace network stack、overlay network等技术,可以获得更高性能、弹性和效率）。</li></ol><p>如今，几乎所有超级计算机架构以及领先的数据中心提供商都在生产中使用 RDMA。几十年前对负载平衡、拥塞控制和错误处理的简单假设并不适用于当今带宽提高100倍、消息速率提高10倍以上的网络。此外，简单的RDMA网络接口卡（NIC）通常会通过附加功能进行增强。由此产生的“智能网卡”通常会减轻大量负载。服务并实施专门的网络协议。现代网络交换机还具有改进的功能，从先进的网内遥测和网内计算功能到网内负载平衡和拥塞控制。3我们认为，当前现有的标准和部署的基础设施存在根本差距必须在不久的将来解决这个问题，以支持高效的高性能网络。</p><blockquote><p>遥测数据(Telemetry Data)是指从网络设备中采集、汇报的与网络运行状态相关的数据。它通过实时反映网络的运行情况来帮助网络管理和运维。常见的网络遥测数据包括:</p></blockquote><ul><li>基础信息:设备型号、配置、软件版本等静态信息。</li><li>流量数据:接口流量速率、总流量、流量方向等。</li><li>QoS数据:接口队列长度、延迟、丢包、拥塞情况等。 </li><li>路由信息:路由表、下一跳等动态路由信息。</li><li>会话信息:活动的会话连接数、来源/目的地等。</li><li>资源利用率:CPU使用率、内存使用率、链接利用率等。</li><li>环境数据:设备温度、风扇速度等环境信息。</li><li>事件和警报:设备故障、链路中断等事件信息。<br>遥测数据可以通过SNMP、NETCONF等管理协议获取,也可以通过流式遥测技术像gRPC Streaming、 Kafka等机制订阅获取。收集到的遥测数据可以用于网络状态分析、事件检测、容量规划、流量工程等。</li></ul><blockquote><p>网内遥测(In-band Telemetry, INT)是一种网络遥测技术,它可以在网络数据包中携带遥测数据,并随着数据包传递通过网络。网内遥测的主要特征和优势包括:</p></blockquote><ul><li>遥测数据直接嵌入数据包中,不需要额外的控制消息,更加高效。</li><li>可以提供数据包在网络中的实时状态,如延迟、丢包等信息。</li><li>可以细粒度地反映网络状态,每一个数据包都是探针。</li><li>可以快速发现网络热点,进行负载均衡和故障定位。</li><li>无需专用监控网络,不会占用额外带宽。</li><li>可以配合软件定义网络(SDN)实现可编程的遥测控制。<blockquote><p>网内遥测通常需要数据平面支持,在交换机或网卡中实现遥测头插入和解析。通过网内遥测技术,可以极大地增强网络的可观测性, 一些主流的网内遥测技术包括: </p></blockquote></li><li>INT: IETF正在推进的INT标准。</li><li>IOAM: In-situ OAM,由Cisco推出。</li><li>P4INT: 基于P4语言的可编程INT实现。</li><li>iOAM: IOAM的衍生协议。</li></ul><h3 id="RoCE：融合还是胶带"><a href="#RoCE：融合还是胶带" class="headerlink" title="RoCE：融合还是胶带"></a>RoCE：融合还是胶带</h3><p>RoCE（RDMA over Converged Ethernet）是可以在Ethernet网络上运行RDMA的网络协议。其主要特点如下:</p><ul><li>RoCE在Ethernet上实现了RDMA功能,使RDMA不再只局限在专用的InfiniBand网络上。</li><li>在IP/Ethernet网络基础设施上,通过对数据平面进行改进来实现RDMA。</li><li>支持两种传输方式:RoCEv1使用UDP封装;RoCEv2使用一种特殊的以太网帧格式。</li><li>RoCEv1依赖数据中心级别的Lossless Ethernet技术来实现可靠传输。RoCEv2新增了自身的流控机制。</li><li>RoCE可以获得与InfiniBand接近的低延迟和高吞吐性能。</li><li>与iWARP相比,RoCE更加依赖硬件卸载,实现CPU利用率更低。<br>RoCE的优势在于兼容现有的以太网网络,使RDMA应用更易于部署,不再需要专门的IB交换机网络。RoCE已得到广泛支持,是数据中心采用RDMA的主流选择之一。</li></ul><p>传统上，当交换机缓冲区已满时，以太网会丢弃数据包，并依赖于端到端重传。为了支持RoCE，CE引入优先流控制（PFC）来实现链路级无损操作。PFC重新利用以太网中存在的以太网暂停帧来支持具有不同链路传输速率的网络。PFC增强暂停帧以停止（或限制）特定优先级上的流量，以避免数据包丢失。不幸的是，这套复杂的协议会干扰网络中的不同层，并降低当今一些最重要工作负载的效率。</p><p>RoCE的语义、负载平衡和拥塞控制机制继承自InfiniBand。这意味着所有消息都应该按顺序出现在目的地，就好像它们是通过静态路由传输一样，本质上不允许许多数据包级负载平衡机制。对于人工智能训练工作负载（长期存在的流），多路径机制可以大大缩短作业完成时间。此外，RoCE v2使用基于IP显式拥塞通知（ECN）的简单拥塞控制机制。当检测到拥塞时，兼容 ECN 的交换机会对数据包进行标记，接收方会将该信息转发回发送方，从而在单个参数的引导下降低其注入率。无拥塞期后，使用第二个配置参数再次自动提高速率。</p><h3 id="下一代高性能网络"><a href="#下一代高性能网络" class="headerlink" title="下一代高性能网络"></a>下一代高性能网络</h3><p>对于某些工作负载，消息延迟（有时是消息速率）起着核心作用。其中一些属于 OBS 类别，但其他一些具有复杂的数据相关消息链，形成应用程序中的关键性能路径。这些通常是强大的扩展工作负载，解决问题的时间很重要，并且必须容忍低效的执行。具有严格期限的大规模模拟（例如天气预报和石油勘探）属于这一类，但一些事务处理和搜索/推理工作负载也属于这一类。在这里，通常具有严格的（个位数微秒）延迟要求。</p><p>除了流量类型之外，部署环境也在发生变化。新出现的机密计算理念要求所有流量在线路上进行加密。理想情况下，流量在安全飞地中进行端到端加密和解密，并且没有网络设备（NIC 或交换机）值得信任。此外，相关的新兴多租户场景需要管理来自单个主机的数万个连接。这些通常由智能 NIC 提供支持，通过管理资源（例如带宽和安全性）虽然有速率限制和过滤。此外，需要更先进的负载平衡和路由的新的经济高效、小直径和专用拓扑成为极端带宽部署的必要条件。2,8 这些要求的许多组合对下一代高性能网络。</p><h3 id="RoCE存在的问题"><a href="#RoCE存在的问题" class="headerlink" title="RoCE存在的问题"></a>RoCE存在的问题</h3><p> 关于论文中提出RoCE需要改进的8个方面的问题,我总结如下:</p><ol><li> congestion control (拥塞控制)。RoCEv1没有拥塞控制机制,需要依赖DCQCN。RoCEv2虽有拥塞控制但需要进一步完善。</li><li>physical layer (物理层) 。RoCE对PHY层时钟同步和链路断开检测还需改进。</li><li>path MTU (路径MTU)。RoCE需要更好处理不同MTU路径的情况。</li><li> flow steering (流导向)。需要更好的QoS和流量工程能力来导向不同优先级的RoCE流量。</li><li> resilience (弹性) 。如何改善RoCE的故障恢复能力需要进一步研究。</li><li> labeling (标签)。RoCE当前还不支持MPLS等标签交换技术。</li><li> standards (标准)。需要更多针对RoCE在数据中心使用的标准化工作。</li><li> debugging (调试) 。RoCE网络故障定位和性能诊断工具需要加强。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards an Adaptable Systems Architecture for Memory Tiering at WarehouseScale</title>
      <link href="/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/"/>
      <url>/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS, 2023</li><li>Towards an Adaptable Systems Architecture for Memory Tiering at Warehouse-Scale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>all Google. Padmapriya Duraisamy, Wei Xu, Scott Hare etc.</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>DRAM主导大规模计算环境中的基础设施支出，如果不进行架构转变，这种趋势可能会恶化（给不起钱了）。使用PM在高度多租户的仓库规模计算环境中提出了许多挑战。其<span class="label primary">应用程序的多样性</span>和规模激发了一般情况下应用程序透明的解决方案，可适应特定的工作负载需求。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><div class="note info">将25%的DRAM换为PM，性能损失不到5%。该设计点在性能下降的限制下最大限度地降低成本。</div> <ul><li>定义内存分层的机器级优化指标，用于自适应平衡复杂的高层次全机群应用性能和利用率目标（第2节）。</li><li>引入了用于大规模实时复杂系统评估的健壮A/B测试方法（第4节）。</li><li>首次对生产仓库规模环境中的可直接访问分层内存系统进行全面分析，该系统成功服务于不同的应用程序类别（第5节和第7节）。</li><li>评估一系列策略并证明硬件辅助事件分析满足性能要求的有效性（第6节），确定主动降级和快速检测升级的重要性。（还是免不了做这个工作，hh）</li><li>揭示了一旦布局得到很好的优化，地址转换开销、干扰效应和页面大小问题如何成为关键挑战，需要新的malloc级技术来减少“访问碎片”，特别是对于大页面。</li><li>利用实时A/B测试机制，以事实为依据开发自适应分层感知集群调度，在最大限度提高利用率的同时减少性能影响tails，并利用应用指导的分层感知巨页管理，减少访问碎片，提高第二层利用率（第8节）。</li></ul><p>我们的经验凸显了大规模管理内存层的复杂性，其中工作负载行为可能每天都会发生变化，或者稳定数周后突然发生变化。我们相信，这种系统设计和用于捕获大规模实时多样化工作负载的复杂交互影响的方法将开辟新的、日益重要的研究途径。</p><h2 id="4-围绕该问题作者如何构建解决思路"><a href="#4-围绕该问题作者如何构建解决思路" class="headerlink" title="4. 围绕该问题作者如何构建解决思路"></a>4. 围绕该问题作者如何构建解决思路</h2><blockquote><p>SLI是请求得到正常响应的百分比。Service Level Object 服务水平目标，是围绕SLI构建的目标。通常是一个百分比，并与一个时间范围挂钩。比如，月度、季度、年度等。99%（2个9的正常运行时间）：意味着在过去30天中有1%，或者说7.2小时的停机时间。</p></blockquote><p>与内存分层相关的应用服务可分为两类——高重要性延迟敏感型（HILS）和其他（非HILS）。HILS包括对响应时间有严格要求的面向用户的应用、处于其他HILS应用关键路径上的缓存应用以及数据处理任务中的生产层（Production Tier，见[48]）。非HILS包括面向吞吐量的应用、批处理、ML训练管道和其他SLO[48]要求较弱的应用。这些类通常位于同一台机器上。Borg调度器[50]会根据观察到的性能主动管理集群中的作业，内存分层也会产生性能影响。</p><h3 id="4-1-指标"><a href="#4-1-指标" class="headerlink" title="4.1 指标"></a>4.1 指标</h3><p>为了更好地分析分层堆栈，我们定义了两个直接连接到分层架构本身的代理指标：<br>• 次要层驻留率(STRR)是<strong>驻留在第2层的已分配内存的比例</strong>。它提供了有关层使用情况的标准化视角。<br>• 次要层访问率(STAR)是针对<strong>驻留在第2层的页面的应用程序的所有内存访问的比例</strong>。较低的STAR意味着较低的性能影响。<br>未充分利用Tier1 DRAM并没有任何好处。 STAR增加反映了性能下降。使用tier2越多，风险就越大，但通过保持STAR较低可以降低风险。目标：最小化STAR，最大化STRR。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/15f7395f6de14157835ea50861ebf19a.png" alt="STAR的累积分布函数CDF显示，在超过99%的实例中socket级别Tier2访问率保持在1%以下。我们实现了将性能下降总体限制在 5%以下的运营目标，特别是当STAR低于0.5%时"></p><h3 id="4-2-架构"><a href="#4-2-架构" class="headerlink" title="4.2 架构"></a>4.2 架构</h3><p>TMTS以页面粒度动态管理分层内存放置。其系统架构包括四层，如图2所示。底层对硬件进行抽象，呈现跨两种或多种类型存储设备分段的物理地址空间。从顶部开始的第二层将页面管理策略与候选检测和页面迁移机制分开，这些机制由较低层内核组件执行。针对工作负载的特定方面进行优化，用户空间策略层提供了这种灵活性和速度。顶层集群层由Borg调度程序组成，它与节点代理 Borglet一起工作，管理连续的多机器作业请求流，观察负载和性能指标，在每台机器上并将任务分派到各个服务器[48]。在TMTS 中​​，它采用分层感知调度策略来实现更好的工作放置，如第8.2节所述。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/046b396e4d8b45c7926231cf250b74da.png"></p><h3 id="4-3-冷页驱逐与降级策略"><a href="#4-3-冷页驱逐与降级策略" class="headerlink" title="4.3 冷页驱逐与降级策略"></a>4.3 冷页驱逐与降级策略</h3><p>当一个页面在之前几秒内没有被访问过时，我们就将其归类为阈值为t的冷页面[32]。降级策略选择根据应用程序类别、每个应用程序或随时间变化确定𝑡的值及其粒度。<br><span class="label primary">浅浅提了一下他们的工作负载98%都是匿名页，所以策略也有所偏好</span></p><h3 id="4-4-热页升级策略"><a href="#4-4-热页升级策略" class="headerlink" title="4.4 热页升级策略"></a>4.4 热页升级策略</h3><p>对最后一级缓存LLC未命中事件进行采样来分析最近访问的第 2 层内存中的地址。由于第2层是可缓存的，因此只需考虑LLC未命中事件。对所有LLC未命中事件进行采样是不切实际且没有用的，绝大多数未达到第一级的流量。不幸的是，硬件不支持对内存存储精确事件过滤。We configure sampling to collect 1% of memory loads from tier2 and promote all the pages identified by this sampling. 我们在第6.2节中检查了这种实用但不完美的检测器的有效性。</p><div class="note warning no-icon">就是指把第二层内存分为100份，迁移其中的一整份去第一层内存，但是这个采样怎么采？是相邻的还是指定步长，选中1%是要排序还是其他的怎么着？</div> <p>为了检测采样可能遗漏的热点页面，我们还执行主动、定期的基于扫描的升级。我们将页面热龄定义为最近访问页面的扫描周期数。我们扩展了内核中的页面位扫描器来跟踪页面热年龄，这使得它能够区分活跃页面和轻度访问页面。为了提高效率，我们不在A位扫描中进行TLB失效。尽管这种优化可能会牺牲一些页面年龄的准确性，但我们并没有看到它在实践中影响降级和升级的有效性。</p><div class="note info no-icon">页面年龄这个20年发在TC的APM做过，但是搞两个不会运行时有啥冲突吗？</div> <h3 id="4-5-页面迁移"><a href="#4-5-页面迁移" class="headerlink" title="4.5 页面迁移"></a>4.5 页面迁移</h3><p>ufard后台线程使用Linux中的标准perf接口（例如 perf_event_open()）来设置采样和处理访问事件。它还在内核中安装了一个小型BPF[3]程序，以优化从内核内页面A位扫描器到每个NUMA节点BPF环形缓冲区的tier2热页面年龄及其页面地址的收集。</p><h3 id="4-6-硬件限制导致的策略约束"><a href="#4-6-硬件限制导致的策略约束" class="headerlink" title="4.6 硬件限制导致的策略约束"></a>4.6 硬件限制导致的策略约束</h3><p>我们的部署使用英特尔傲腾持久内存的变体作为第2层。此类第2层DIMM的带宽受到高度限制，支持的内存带宽仅为英特尔®至强®可扩展处理器上典型DDR4通道的内存带宽的1/10。</p><p>在当前的硬件实现中，第2层DIMM带宽饱和也会影响常规DRAM延迟，因为第2层DIMM与DRAM DIMM共享内存通道。这限制了升级和降级的积极性。表1列出了我们的配置中不同访问模式下测得的Tier2 DIMM带宽。测得的空闲读访问延迟约为325ns，详情请参见[28,55]。根据冷页配置文件（图3）、可用DIMM容量和硬件带宽限制，我们的目标是将25%的系统内存容量用于第2层。这些硬件限制导致我们当前的部署受到以下策略决策限制：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8add2e9fbff5488da1074b431149e3de.png"></p><p>没有直接分配到第2层：此约束避免了第2层中新分配的页面直接就很热。系统仅将任务内存分配到第1层，并依靠降级来填充第2层。仅分配到tier1可能会增加OOM情况。QoS在限制对连接到远程socket的第2层DIMM的访问方面效果较差，因为反馈信号必须在插槽之间传输。所以只将任务的内存降级到任务运行的socket上的第2层DIMM。</p><p><span class="label info">这些选择并不是TMTS的基础，而是底层硬件实现的产物。未来的硬件实现可能不会受到这些限制，从而放宽上面讨论的限制。当前限制的另一个好处是展示系统设计的弹性。</span></p><h3 id="4-7-TLB-Misses-and-Huge-Pages"><a href="#4-7-TLB-Misses-and-Huge-Pages" class="headerlink" title="4.7 TLB Misses and Huge Pages"></a>4.7 TLB Misses and Huge Pages</h3><p>大页面（例如2MB）可以减轻4KB页面带来的虚拟到物理地址转换延迟和TLB覆盖问题。但这些大小给基于页面迁移的分层内存系统带来了新的挑战。由于地址映射的更改，迁移会导致TLB失效。此外，大页面中的小热区域会导致整个页面显得很热。</p><p>在当前的实现中，对象最初分配在tier1中。如果TMTS将大页识别为降级候选，则该页首先被拆分为4KB页面，然后降级。这允许未来的访问提升单独的4KB页面，并减少迁移成本和不必要的tier1占用。并非原始大页面的所有降级4KB页面都可以升级，从而阻止它们成功地重新组合成大页面。由于系统的DRAM容量较低，我们看到DRAM压力增加，这会导致tier1碎片化。</p><p>我们进行了两项实验，这两项实验都以最小的副作用提高了TLB 命中率。<br>首先，我们尝试完整迁移大页面，而不是在降级时将其分解为4KB页面。这使TLB未命中平均减少了4.7%，并将平均性能提高了0.5%。升级带宽按预期增加，但增幅&lt;1%，并且带宽拥塞事件没有增加。平均STRR降低 &lt;1%。<br>其次，我们提高了内存压缩的积极性，目标是提高tier1中4KB页面的重组率。我们发现大页覆盖率提高了25%，平均性能提高了0.5%。额外的CPU成本微不足道，&lt; 0.1%。</p><div class="note warning no-icon">但是，兄弟你说拆就拆吗？页表怎么转换，页表转了TLB的缓存还要一致性。两种页面混合你要怎么做地址转换？wow，amazing.而且所以最后要使用哪种策略呢？</div> <h2 id="5-评估"><a href="#5-评估" class="headerlink" title="5. 评估"></a>5. 评估</h2><p>在Optane PM上做的。这里只记录感兴趣的。<br>作者们首先说加入PM后的情况和只使用DRAM差不多；然后说明了冷页面控制的很好：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/37b3a657a7fa48d6a39d0c8b3f687177.png"><br>图5（a）是STRR，（b）是将冷内存覆盖率定义为存储在tier2中的2分钟冷页的比例。一个重要的参考点是地址空间中可用冷页的相对数量，在图3（a）的实验中观察到在28%到42%之间变化。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bbc62466bd56426ba61a32a89ba0a1d2.png"></p><p><span class="label warning">但是有一点，PM不可能刚刚好就给25%，肯定会多给吧？你又不放在远端，被定义的冷的超出25%，那dram降级有装不下，只能OOM？</span></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/761b677057cc4885830e9522fee1f0fa.png" alt="升级、降级和应用程序访问tier2的带宽消耗"></p><h2 id="6-应对仓库级规模问题的适应性政策"><a href="#6-应对仓库级规模问题的适应性政策" class="headerlink" title="6. 应对仓库级规模问题的适应性政策"></a>6. 应对仓库级规模问题的适应性政策</h2><p>虽然部署的TMTS满足大多数应用程序的性能目标，但一些异常值会出现显着的性能下降，如图6c所示。上述案例研究展示了我们计算环境中典型的冷内存访问模式的多样化组合。为了在这样的环境中扩展分层，我们必须实现跨节点、集群和应用程序层的垂直集成，并制定策略，共同实现更高的冷内存识别，同时将性能下降降至最低。本节讨论支持扩展TMTS的三种自适应策略。</p><p>Lagar-Cavilla等人[32]描述了一种动态的、按应用程序冷龄阈值策略，旨在维持目标升级率。<br>然后以前调度的优化Borglet在集群间、机器间选择策略的完善，考虑每个机器的第一层使用率之类的。</p><p><span class="label info">我们相信，使用软件提示来帮助调整页面内的内存分配和对象放置的类似方法将变得越来越重要。例如，除了开发人员将应用程序内的分配注释为可能的热或冷之外，编译器还可以使用自动化技术（例如配置文件引导优化）来提供类似的提示。</span></p><h2 id="7-积累"><a href="#7-积累" class="headerlink" title="7. 积累"></a>7. 积累</h2><p>冷页面驱逐还挺不一样，不直接分配PM也有理由也挺新奇；作者在设计和描述时感觉也尽可能在表示我知道CXL，我知道硬件会变，但是我们这个策略仍然可以参考。</p><p>图6是不同应用程序IPC的情况，说明不同应用程序差异，用这个方式分类应用程序第一次见。</p><p>使用TensorFlow框架[6]构建的面向吞吐量的机器学习训练管道，具有高内存带宽使用率和不可预测的访问模式。用这个workload也是头一次。机器学习应用程序是一个面向吞吐量的训练管道，在训练阶段频繁更新内存中的ML模型。该应用程序存储的内存数据可以是密集的，也可以是稀疏的，具体取决于更新当前正在训练的ML模型所需的数据量。这些ML模型更新需要大量内存带宽，并且它们的内存访问模式是不可预测的，因为应用程序经过优化，可以有效地将训练数据从磁盘读取到内存中，而不是针对内存中访问的局部性进行优化。训练工作线程的单个实例的任何减慢都可能减慢应用程序的整个训练阶段，从而导致CPU和加速器周期的浪费。</p><p>奇怪，他不是直接降级去PM吗，为什么要在分配时隔离冷热的虚拟地址空间。“为了避免并置冷热对象，我们扩展了new运算符以接受一个提示参数，该参数指示预期分配的对象的访问频率。C++内存分配器的开源TCMalloc实现[4]使用此参数来分隔虚拟地址空间中的“冷”和“热”分配。由于不经常访问的对象聚集在页面上，因此可以建议内核不要使用透明大页面映射冷区域。这些策略导致频繁访问和不经常访问的对象的分离，并为每个对象提供不同的策略。”</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> Warehouse-Scale Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Merchandiser Data Placement on Heterogeneous Memory for TaskParallel HPC Applications with LoadBalance Awareness</title>
      <link href="/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/"/>
      <url>/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自PPoPP，2022</li><li>Merchandiser: Data Placement on Heterogeneous Memory for Task-Parallel HPC Applications with Load-Balance Awareness </li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Zhen Xie，University of California, Merced Argonne National Laboratory</li><li>Jie Liu，University of California, Merced</li><li>Jiajia Li，North Carolina State University</li><li>Dong Li，University of California, Merced</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ea9867b9f36043af9a699eb42ec35696.png"></p><p>图1.a给出了一个基于MPI的任务并行应用的例子。在DMRG中，一个哈密顿矩阵首先被分割成多个块，每个块被分配给一个MPI 进程(1-3行) 。然后每个MPI进程运行一个计算循环，作为输入(第5-7行)循环的一次迭代被认为是1个任务实例。因此，MPI进程中的任务是重复执行的。在每个迭代结束时，有一个全局的MPI进程之间的同步。</p><p>图1.b给出了1个基于OpenMP的任务并行应用的例子，一个主循环运行了许多SpGEMM (C=A*B) 。在主循环的每一次迭代中，A首先被分割成若干个bins，部分 A*B 得到部分C</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2d6f15b49475435ba52e45722d3e969f.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>Input-Aware Memory Access Quantification<br>既然是从任务的角度来解决问题，那么如何确定什么样的数据放入快速内存中？<br>连串的、一定步长的，需要考虑左右两边数据的、随机的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519a7d4c0e2149bf9fd8ddbdd870afcc.png"></p><p>阿尔法的值，根据步长和数据类型来确定，由作者提前枚举好了的。综上给了两种方法来判断输入数据的将来被访问量，一种是用户直接通过API指定，如果没有指定的情况就是使用公式计算，依据具体计算方式设置阿尔法。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b690c63eb3a3414991abdf491177a583.png"></p><p>迁移的决策模型应该是，作者的想法就是怎么把数据放置在不同PM的敏感性展现出来，区分出数据在不同异构放置的差别。<br>时间上的预测完全参考13年的一篇论文<br>梯度提升回归：每个学习算法准确率都不高。但是它们集成起来可以获得很好的准确率。这些学习算法依次应用。也就是说每个学习算法都是在前一个学习算法的错误中学习.</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/89e360308c2b400b8d7318f8e4b2efc6.png"></p><p><strong>性能模型的设计</strong><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/03757318787c480091970cfc3d2290a2.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transparent Page Placement for CXL Enabled Tiered Memory</title>
      <link href="/2023/08/19/Transparent-Page-Placement-for-CXL-Enabled-Tiered-Memory/"/>
      <url>/2023/08/19/Transparent-Page-Placement-for-CXL-Enabled-Tiered-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS，2023，CCFA</li><li>TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hasan Al Maruf，Mosharaf Chowdhury – University of Michigan密歇根大学</li><li>Hao Wang，Niket Agarwal，Pallab Bhattacharya – NVIDIA</li><li>Abhishek Dhanotia，Johannes Weiner，Chris Petersen，Shobhit Kanaujia，Prakash Chauhan – Meta Inc.脸书</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><ul><li>CXL [7] 是一种基于 PCI Express（PCIe）接口的开放式、业界支持的互连。<br>它支持主机处理器和设备（例如加速器、内存缓冲区、智能 I/O 设备等）之间的高速、低延迟通信。</li><li>CXL 在同一物理地址空间中提供字节可寻址内存，并允许使用标准内存分配 API 进行透明内存分配。</li><li>CXL-Memory 访问延迟也与 NUMA 访问延迟类似。</li></ul><p>这篇文章将直接附加到 CPU 的内存称为本地内存，将 CXL 附加内存称为CXL-内存。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5aac1cd90eef40299199fc6d08e7b1f0.png"></p><blockquote><p><strong>补充：</strong>现在CXL同时支持多种内存的应用案例都是DRAM和HBM（是一种高带宽内存 High Bandwidth Memory）。HBM更适用于对带宽要求极高的场景,如高端GPU、AI加速卡等。HBM单片容量较小(4-16GB),但带宽巨大(超过500GB/s)。</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>现实实践中面对的问题：<br><strong>数据中心应用程序的内存需求增加</strong>在META公司中每一代硬件内存消耗的能源和总花费占比都在不停增长。</p><p><strong>同类服务器设计中的扩展挑战</strong>内存控制器仅支持单代内存技术，这限制了不同技术的混合搭配，具有不同的每 GB 成本和带宽与延迟配置。大内存容量都是2的几次方。限制了细粒度；每一代服务器的带宽和容量都有限制。图4</p><p><strong>数据中心应用程序的轻量级表征</strong>现有的工具会导致较高的 CPU 开销（每个核心超过 15%），并且通常会减慢应用程序的速度。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/149d075453ee4bfb9e5e3e7214a92947.png"></p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="5-1采样数据工具"><a href="#5-1采样数据工具" class="headerlink" title="5.1采样数据工具"></a>5.1采样数据工具</h3><p>Chameleon 的主要用例是了解应用程序的内存访问行为，即应用程序内存的哪一部分保持热-温-冷状态、页面在特定温度层上存活多长时间、访问它们的频率等等。长期内存。</p><p>收集器做两类采样，放入hash表，定时唤醒。<br>收集器唤醒工作线程以处理当前哈希表中的数据，并移动到另一个哈希表以存储下一个间隔的采样数据<br>工作器处理哈希表里的数据，统计每个页面的位图大小是64位，生成报告后休眠。</p><blockquote><p>load是从内存读取数据到处理器的寄存器中的指令,而store则是将寄存器中的数据写入内存的指令。两者都是与内存打交道的指令,但方向不同,load是内存到寄存器,而store是寄存器到内存。</p></blockquote><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5c874a20c38e4191b939a5ef0ca59536.png"></p><p>我们使用 Chameleon 来分析我们生产中运行的跨不同服务域的各种大型内存绑定应用程序，并进行以下观察。 </p><ol><li>访问的内存的很大一部分在几分钟内保持冷状态。我们可以将其卸载到慢速层内存，而不会对性能产生重大影响。 </li><li>大部分匿名内存（为程序的堆栈、堆和/或 mmap 调用创建）往往更热，而大部分文件支持的内存往往相对更冷。</li><li>页面访问模式在有意义的持续时间（分钟到小时）内保持相对稳定。这足以观察应用程序行为并在内核空间中做出页面放置决策。 </li><li>工作负载对不同页面类型（文件和匿名页面）具有不同程度的敏感度，并且随着时间的推移而变化。</li><li>冷页重新访问时间因工作负载而异。分层内存系统上的页面放置应该意识到这一点，并主动将热页移动到较低的内存节点，以避免高内存访问延迟。于Web而言，几乎80%的页面会在 10 分钟内被重新访问。</li></ol><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ec7f0938b1844787b423dbe944b29744.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/927aa0606137423b80bc79092b10fdca.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f7c7b158461d442ea37c3eb4d5b9d7f9.png"></p><h3 id="5-2架构设计"><a href="#5-2架构设计" class="headerlink" title="5.2架构设计"></a>5.2架构设计</h3><p>TPP 将“较热”页面放置在本地内存中，并将“较冷”页面移动到 CXL 内存中。<br>TPP 的设计空间可分为四个主要区域：</p><ul><li>(a) 轻量级降级到 CXL-Memory，</li><li>(b) 解耦分配和回收路径，</li><li>(c) 热页升级到本地节点，</li><li>(d) 页面类型感知内存分配。</li></ul><p> 一般来所NUMA系统的回收顺序是：首先回收本地节点上不活跃的页面；然后回收远程节点上不活跃的页面；如果还不够,才考虑回收本地节点上最近最少使用的页面。所以在图的上面①会将本地回收候选页面迁移到XCL节点的降级列表中，除非本地节点的容量小于工作集大小的热部分，否则在回收期间热页面迁移到 CXL 节点的机会非常低。如果降级期间的迁移失败（例如，由于 CXL 节点上的内存不足），我们将回退到该失败页面的默认回收机制。</p><p> Linux为节点内的每个内存区域维护三个watermakes（最小、低、高）。如果节点的空闲页面总数低于low_watermark，Linux会认为该节点面临内存压力并启动该节点的页面回收。在我们的例子中，TPP将它们降级为CXL节点。对本地节点的新分配将停止，直到回收器释放足够的内存以满足high_watermark。<span class="label info">由于分配率较高，回收可能无法跟上</span>，本地内存分配频繁停止，<span class="label info">更多页面最终出现在CXL节点中</span>，最终降低应用程序性能。所以现在将回收和分配化为不同的门槛②，回收和驱逐会提早发生，而禁止页面分配会在内存饱和度更高一步的时候。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d51196b572b849d48550379551aa0801.png"></p><p>NUMA系统中CPU访问另一节点页面时,生成缺页异常提醒内核进行页面迁移。不常访问的页面产生的升级流量很容易填满本地节点的空闲空间，并为CXL节点产生更高的降级流量。于是通过页面在LRU列表中的位置来检查页面的年龄。如果故障页面处于非活动LRU状态，我们不会立即考虑该页面进行升级，因为它可能是不经常访问的页面。仅当在活动 LRU 中找到故障页面（图13中的①）时，我们才将其视为升级候选页面。这大大减少了升级流量。</p><p>然而，操作系统使用LRU列表进行回收。如果内存节点没有压力并且回收没有启动，则非活动LRU列表中的页面不会自动移动到活动LRU列表。由于CXL节点可能并不总是处于压力之下，因此经常可以在非活动LRU列表中找到缺页的页面，又会没法向上迁移。为了解决这个问题，每当我们在非活动LRU列表中发现缺页异常时，我们都会将该页面标记为已访问，并立即将其移至活动LRU列表（图13中的②）。如果在下一次NUMA提示缺页异常，则它将处于活动LRU中，并提升到本地节点（图13中的③）。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>因为工作负载就这些特性，那就直接对症下药，效果都挺明显的。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>但是换了workload就没这个效果了。按照这种方式，每家都得自己去设计啰。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>是根据工作负载来设计架构的，关注的问题点比较小，也就容易做到很细致出色吧。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>一个奇怪的现象，虽然大家可能观察值不同，但是最后系统设计都比较像，好像这两部分可以分裂一样。<br>那个Chameleon工具，说是开源但是并没有，之后可以再去看看。<br>TMO: Transparent memory offloading in datacenters. In ASPLOS, 2022.可以看看。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</title>
      <link href="/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/"/>
      <url>/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><p><strong>文章来自23年7月TechRxiv</strong><br>Performance, Energy and NVM Lifetime-Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</p></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Manolis Katsaragakis,微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院,鲁汶大学(KU Leuven)比利时</li><li>Christos Baloukas, Lazaros Papadopoulos, 微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院</li><li>Francky Catthoor,鲁汶大学微电子研究中心</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>为了有效利用 DRAM/NVM 异构内存系统，多年来已经提出了几种数据放置算法 [15]、[16]。尽管数据放置算法通常很复杂，并且能够有效地利用正在执行的应用程序的复杂内存层次结构，但它们的结果通常受到以下事实的限制：<span class="label info">原始应用程序通常是为 DRAM 而不是为异构内存设计的</span></p><p>作者通过一些实验展示了数据组织改进的应用程序级方法对异构 DRAM/NVM 系统上数据放置算法结果的影响（作为整个论文的动机）。其中有两个新的概念（这两个概念也对后续设计有很大影响，但是这两个概念并不常见）：</p><ul><li><span class="label danger">动态数据类型细化方法 Dynamic Data Type Refinement methodology（DDTR）</span>（D. A. Alonso, S. Mamagkakis, C. Poucet etc. Dynamic memory management for embedded systems. Springer, 2015）</li><li><span class="label danger">帕累托最优 Pareto Optimal</span></li></ul><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>异构内存中可以从数据组织的角度改进数据放置，所提出的方法旨在满足三个主要目标：<br>(i) 展示通过应用程序级优化实现的数据放置算法的改进结果。<br>(ii) 基于多个目标（性能、能耗、对 NVM 寿命的影响）评估布局解决方案。<br>(iii) 提供可用性、可扩展性和可扩展性特征（例如，即使对于相对较大的设计空间，也有合理的探索时间，并支持各种应用领域、数据放置算法和真实或模拟的 NVM 技术）。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><ul><li>几种数据放置算法 [15]、[16]</li><li>应用级数据优化方法[18]、[19]</li><li>库支持 [13]、[17] </li><li>根据布局粒度级别进行分类：数据结构、内存对象或内存页 [23] [15] [24] [25][23] 的作者提出了一种在线配置文件引导的数据分层解决方案，涉及异构内存系统的页面粒度放置，旨在提高 HPC 应用程序的性能。其他指示性方法包括静态代码检测工具，用于自动执行内存对象放置以实现性能和能源优化[15]。在数据结构放置粒度上，[24]中提出了缓存和非缓存 NVM 的写感知数据结构放置。 [25] 中提出了基于人工智能的页面粒度数据放置。</li><li>异构 DRAM/NVM 系统上的应用程序域特定布局：最近还研究了应用程序域特定布局方法，目标是与领域无关的布局算法相比获得更好的结果。这些领域包括与数据库相关的工作负载[26]、大数据应用程序[27]、[28]、基于图的[29]、[30]、[31]和基于深度神经网络的应用程序[32]、[33]。</li></ul><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/85597fd358cb4e0882741b4808f25738.png"></p><p>图2显示了所提出的方法的概述。该方法的输入是目标应用程序的源代码。它由三个步骤组成：</p><ol><li><p><strong>数据组织的优化</strong>，此步骤的主要目标是有效选择应用程序的数据结构实现，以最大限度地减少数据访问次数和内存占用。实现方式是源代码的原始数据结构被库的数据结构替换。（仅限于基于列表的数据结构实现的 C++ 标准模板库 (STL) 兼容变体。）<span class="label default">这一步生成一组帕累托最优解</span>,每一个都对应于正在优化的应用程序的数据结构实现的不同组合。</p></li><li><p><strong>内存对象分析</strong>，基于内存跟踪和分析工具，<span class="label default">分析上一步提供的应用程序的帕累托最优版本</span>。通过平台感知采样和分析 (2b)，我们收集每个对象的内存跟踪，包括：加载和存储操作、对象大小和 LLC 未命中。还可以根据所选数据放置算法的输入要求来监视其他指标。</p></li><li><p><strong>异构内存系统上的放置和评估</strong>，模型接收（i）每个内存页、对象或数据结构的分析信息和（ii）内存规格作为输入，例如读/写延迟、读/写能耗和内存容量（3b）。每个应用程序版本都根据所选的数据放置算法部署在异构内存系统上，并执行。为了监控执行时间、能耗和NVM写入次数，需要实时监控（3e）。应用程序版本在真实或模拟的异构存储器系统上放置和执行。开发人员可以在性能、能耗和 NVM 写入访问次数之间进行权衡，并选择满足设计约束的帕累托最优解决方案。</p></li></ol><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>所提出的方法根据以下标准进行评估：<br>● DDTR 等高级数据组织优化方法对数据放置结果的影响，包括性能、能耗和对 NVM 寿命的影响。<br>● 该方法可以在多大程度上有效地集成具有不同优化目标的各种数据放置算法以及各种内存技术（模拟的或真实的硬件）。<br>● 当应用于具有相对大量数据结构的应用程序时，该方法在探索时间方面的可扩展性。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ul><li>相当于需要人工干预去做这个优化。</li><li>而且优化过程依赖帕累托（来自财经领域）最优和DDTR（15年一篇论文），这是以前没有过的应该，那么到底有多可靠呢？审稿方对这些态度咋样呢？</li><li>评估方面，没有和其他的工作比较，也看不出什么优势emmm。</li></ul><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>没有任何相关工作研究应用级数据优化和异构内存系统上的数据放置之间的相互作用，以提高性能、能耗和对 NVM 寿命结果的影响。提出了一种内存管理方法，该方法结合了动态数据结构细化步骤以及异构内存系统上的放置算法。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><ul><li>最开始对程序数据组织优化应该是离线完成的，<a href="https://github.com/mkatsa/DDTR-DRAM-NVM">可能可以作为参考</a>。</li><li>可以考虑的workload：从 Shark ML 库 [42] 和 Chrono 物理引擎 [43] 中选择了五个代表性应用程序。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> Rxiv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>P2Cache An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</title>
      <link href="/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/"/>
      <url>/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li><strong>文章来自HotStorage 23</strong> </li><li>P2Cache:An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Dusol Lee, Inhyuk Choi, Chanyoung Lee, Jihong Kim, 首尔大学Seoul National University</li><li>Sungjin Lee, 大邱庆北科学技术院DGIST</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p><span class="label info">大多数操作系统在主机DRAM内存中使用页面缓存，以利用I/O访问的局部性。</span>但页面缓存的高速缓存管理策略可能与应用程序的特定I/O特征不太匹配。如果应用<span class="label info">程序随机访问I/O地址空间，则页面缓存使用的标准读取策略可能无效。</span></p><p>现有缓存策略的局限性——评估三种常见技术：</p><h3 id="2-1操作系统级缓存"><a href="#2-1操作系统级缓存" class="headerlink" title="2.1操作系统级缓存"></a>2.1操作系统级缓存</h3><p>采用 LRU 替换策略与预读算法相结合，如果应用程序具有中等局部性，通常可以实现较高的命中率 [11]。现有的数据密集型应用程序通过<span class="label info">高度定制的算法处理大量数据</span>，从而导致复杂的 I/O 模式。不幸的是，由于其通用设计，操作系统级页面缓存通常<span class="label info">无法捕获各个应用程序的独特行为</span>，从而即使可以实现更高的命中率，也无法提供次优的性能。</p><p>下图是Lumos [2]，执行图形处理算法 - Pagerank [15]的IO模式和性能趋势（它使用专门的数据结构和优化技术来优化图形处理引擎，因此生成的 I/O 访问模式非常复杂。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2cc466ecd6cb409bb417d9e68a446c3b.png" alt="数据集大小超过系统内存后，应用程序开始遭受缓存抖动，导致性能严重下降（参见图 1(b) 中的基线）"></p><ul><li>性能低下是由于操作系统级页面缓存的内存管理无效，该缓存使用 LRU 和预读策略，而不考虑输入工作负载模式。操作系统级页面缓存优先驱逐最近最少引用的页面，<span class="label danger">但这些页面实际上很快就会再次被引用，特别是在循环 I/O 模式下。</span></li><li>可以观察到，使用具有小数据固定的 MRU 策略（而不是 LRU）会带来更高的性能，如图 1 中的 MRU+PIN 所示，性能提高了 25%。然而，很难改变内部的情况。适应输入工作负载的内核缓存替换策略。</li></ul><h3 id="2-2基于提示的操作系统级缓存"><a href="#2-2基于提示的操作系统级缓存" class="headerlink" title="2.2基于提示的操作系统级缓存"></a>2.2基于提示的操作系统级缓存</h3><p>作为一种替代方案，一些应用程序（例如 GridGraph [1] 或 SQLite [7]）尝试通过 fadvise [16] 和 madvise [17] 向内核提供应用程序级提示来更好地管理缓存数据。在保留内核级缓存管理相同优点（强大的数据保护和高效的数据共享）的同时，它能够<span class="label info">通过在应用程序代码中嵌入重要的缓存管理决策</span>（例如，WILLNEED、SEQUENTIAL、DONTNEED）来实现更高的缓存命中率。</p><p>然而，它也有缺点。首先，它需要在修改现有应用程序代码方面付出巨大的努力。其次，仅通过注入提示很难精细控制内核级页面缓存。我们在代码中精心添加了提示信息，以MRU方式管理内核缓存。修改后的版本表现出更高的命中率，但性能仍然比使用 MRU 慢得多（参见图 1 中的 FADV）。这是因为缓存抖动。 </p><h3 id="2-3用户级缓存"><a href="#2-3用户级缓存" class="headerlink" title="2.3用户级缓存"></a>2.3用户级缓存</h3><p>为了减轻I/O特征（应用程序）和（内核级页缓存）策略之间的不匹配问题，数据密集型应用程序经常在应用程序级别[3-7]上实现自己的页面缓存。</p><ul><li>受内核缓存策略干扰，驱逐了有用的页面，违反了应用程序的意图。</li><li>无法利用内核的保护和共享功能。<br>下图使用 Simrank [19] 进行了实验，这是一个具有自己的用户级缓存的图形应用程序。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2439afe15dd740a6917ea14962b24687.png"><br>如图 2（a）所示，Simrank 的 I/O 参考模式大多是随机的，因为它将流行数据缓存在用户级缓存中。由于这种特定于应用程序的管理，对于相同的数据集，Graph-Walker 表现出比 Lumos 更高的性能（见图 2(b)）。然而，当内存不足时，GraphWalker 的性能下降幅度比 Lumos 更高。</li></ul><h2 id="3-解决的问题"><a href="#3-解决的问题" class="headerlink" title="3. 解决的问题"></a>3. 解决的问题</h2><p>内存密集型程序的I/O更加具有特性，内核级页面缓存由于无法考虑特定于应用程序的 I/O 模式而无法提供高性能。应用程序级提示可以缓解该问题，但与最佳效果相比，效果有限。虽然用户级自定义缓存可以高效工作，但它无法利用内核的基础设施，并且会因内核干预而导致性能下降。于是做了一个<strong>允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE</strong>。 </p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>如果直接实现了用户级页面缓存（例如，如Jaydio [8]或RockSDB的Direct-io [9]），则可能无法对某些内核功能进行介入，例如用于确保数据保护和数据一致性的功能。更重要的是，如果SSD或主机存储系统发生重大变化，则需要重新实现用户级缓存。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1392a78bce7340b886bd89b864b1e283.png"><br>图 3 显示了 P2Cache 的操作概览。要为应用程序创建自定义页面缓存，可能需要应用程序的特定于应用程序的数据来开发新的缓存策略。如果需要，数据会被移动到内核的受保护内存(1)。为了避免其他应用程序对数据进行未经授权的访问，为每个自定义页面缓存分配了一个密码(passwd)。使用P2C API函数以及应用程序的内核数据，为P2Cache的每个探测点实现一个eBPF程序 (2)。将eBPF程序加载到各自的探测点(3)后，只要内核的执行流到达这些点，就会执行eBPF程序，应用程序的自定义页面缓存就会生效。在执行eBPF程序之前，扩展的eBPF VM会验证程序是否有权访问应用程序拥有的内核数据以及特定于应用程序的数据。这是通过将eBPF程序的passwdprobe与从相应应用程序传递来的passwd进行比较来实现的。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，使用我们的P2Cache实施的Cusmom Page缓存可在数据密集型图应用中提高32％的性能，内存容量低于数据集容量时也是相比其他方案性能下降更缓慢。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>做了一个允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE，别人的都是应用层级的。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><blockquote><p>数据密集型工作负载：Lumos [2] and GraphWalker [4]。目前使用的工作负载，容量如果内存不能容下，则会杀死进程，这里使用这两个工作过负载还可以使得Memory Size / Dataset Size成比例。Lumos 维护多个文件并同时扫描它们，将混合 I/O 模式发送到磁盘。 Lumos 还使用多个元数据文件并重复读取它们，从而产生高度本地化的 I/O 模式。</p></blockquote><blockquote><p>2.2节的实现手段可以模仿。eBPF程序也能去影响内核。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> O </tag>
            
            <tag> Cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Coalescing for Multi-Grained Page Migration</title>
      <link href="/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/"/>
      <url>/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>Coalescing for Multi-Grained Page Migration</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>华中科技大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>观察结果1，一些热页在虚拟地址和物理地址上都有连续性。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>Tamp使用DRAM作为数据缓冲器来缓存NVM中多种尺寸的热页(所谓的多粒度页)。相应地使用分裂的超级页TLB和多粒度TLB来分别加速NVM和DRAM的地址转换。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>每一个idea都有对应的问题要解决，最后嵌套太多了，所以投的刊物可能会不好。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> O </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ACM Transactions on Architecture and Code Optimization, (TACO), 2019</li><li>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>XIAOYUAN WANG, HAIKUN LIU, XIAOFEI LIAO, JI CHEN, HAI JIN, YU ZHANG, and LONG ZHENG, 华中科技大学</li><li>BINGSHENG HE, 新加坡国立大学</li><li>SONG JIANG, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>在大内存系统中，<strong>超级页一直被用来减轻地址转换开销</strong>。 然而，在由DRAM和NVM组成的混合存储系统中，<strong>超页面往往会阻碍轻量级页面迁移，而轻量级页面迁移对性能和能量效率至关重要</strong>。 </p><blockquote><p>Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and NVM.</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><ol><li>如果大多数内存引用<strong>分布在超级页的一个小区域中</strong>，那么以超级页粒度（例如2MB）进行的页迁移会导致DRAM容量和带宽的巨大浪费，从而导致无法承受的性能开销。 成本可能比超级页面迁移的好处还要大。 这给超级页面的使用带来了一个困境，因为<strong>轻量级页面迁移可能会超过扩展TLB覆盖的好处</strong>。 </li></ol><blockquote><p>However, page migration at the superpage granularity (e.g., 2MB) can incur unbearable performance overhead due to a vast waste of DRAM capacity and bandwidth if most memory references are distributed in a small region ofthe superpage (see Section 2.2). The cost may be even larger than the benefit of superpage migration. This presents a dilemma for the use of superpages,<br>since the lightweight page migration can outweigh the benefits of extended TLB coverage.</p></blockquote><p><a href="https://www.jianshu.com/p/bea989a85a31">累积分布函数图怎么看</a><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d3b808800554ed3b2d0a153353c4e26.png" alt="2MB超级页的累积分布函数与给定区间(108个周期)内一个超级页中被触及的4KB小页的数量"><br>从图中可以看到，很大一部分工作负载有80%以上的概率：2MB的页面中被访问的4kb页面只有12.5%。<br>还有一张表格来说明问题：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/abbbcb642622478cb35fd087610a7f69.png" alt="4kb热页访问统计"><br>使用的工作负载：</p><ul><li><a href="https://www.spec.org/cpu2006">SPEC CPU2006</a></li><li><a href="http://parsec.cs.princeton.edu/index.htm">Parsec</a></li><li><a href="http://www.cs.cmu.edu/%E2%88%BCpbbs/">Problem Based Benchmarks Suit（PBBS）</a></li><li><a href="http://whitedb.org/">WhiteDB</a></li><li><a href="https://redis.io/">Redis</a></li><li><a href="http://graph500.org/">Graph500</a></li><li><a href="http://www.netlib.org/benchmark/">Linpack</a></li><li><a href="https://www.nas.nasa.gov/publications/npb.html">NPB-CG</a></li><li><a href="http://icl.cs.utk.edu/hpcc/">HPC Challenge Benchmark GUPS</a></li></ul><ol start="2"><li><strong>轻量级热页的标识</strong>：为了支持轻量级页迁移，大量工作提倡通过内存控制器监视内存访问。 然而，当<strong>主存容量变大时</strong>，以每页粒度（即4KB）使用<strong>访问计数器会导致高得令人望而却步的存储开销</strong>。</li></ol><blockquote><p>Identification oflightweight hot pages: to support lightweight page migration, a large body of work advocates monitoring memory accesses through the memory controller [55, 63]. However, using access counters at per-page granularity (i.e., 4KB) leads to prohibitively high storage overhead when the capacity of main memory becomes large.</p></blockquote><ol start="3"><li>轻量级页面迁移对TLB覆盖率的影响：页面迁移通常会<strong>分割超级页面</strong>，从而<strong>破坏物理地址的连续性</strong>。 </li></ol><blockquote><p>Impact oflightweight page migration on TLB coverage: page migrations often fragment superpages and thus break the physical address continuity.</p></blockquote><ol start="4"><li>热页寻址效率：由于热页占应用程序内存引用的主要部分，因此必须进一步<strong>减少DRAM中那些热页的地址转换开销</strong>。 </li></ol><blockquote><p>Efficiency of hot pages addressing:ashot pages<br>contribute to a major portion of applications’ memory references, it is essential to further reduce<br>the overhead of address translation for those hot pages in the DRAM.</p></blockquote><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的工作主张分割超级页面以实现轻量级内存管理，如页面迁移和共享，同时牺牲地址转换的性能[37,58]当<strong>超级页面中的热小页面迁移到DRAM时，保持改进的TLB覆盖率</strong>仍然是一个挑战。</p><blockquote><p>Previous work has advocated splintering superpages to enable lightweight memory management such as page migration and sharing, while sacrificing the performance of address translation [37,58]. It is still a challenge to retain the improved TLB coverage when the hot small pages within superpages are migrated to the DRAM.</p></blockquote><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>针对上述问题，提出了一种新的内存管理机制Rainbow, Rainbow在Superpage粒度上管理NVM，并使用DRAM在每个Superpage内缓存频繁访问（热）的小页面。相应地，Rainbow利用拆分TLB[2,7,30,52]的可用硬件特性来支持不同的页面大小，其中一个TLB用于寻址超级页面，另一个TLB用于寻址小页面。 Rainbow将SuperPage中的热小页迁移到DRAM中，而不会损害SuperPage TLB的完整性。 因此，Rainbow实际上将DRAM架构为NVM的缓存。</p><ul><li>为了减少细粒度页面访问计数的存储开销，分两个阶段进行计数。在<strong>给定的时间间隔内</strong>，Rainbow<strong>首先计算Superpage粒度下的NVM内存访问</strong>，然后选择<strong>前N个热门Superpage作为目标</strong>。 在第二阶段，我们<strong>只监视那些小页面</strong>(4KB)粒度的热点超页面，以识别热点小页面。 这种基于历史的策略避免了监视大量冷超页中的子块（4KB页），从而显著降低了热页识别的开销。 </li><li>我们采用<strong>拆分TLB</strong>来加速DRAM和NVM引用的地址转换性能。当一些小页迁移到DRAM时，为了保持SuperPages TLB的完整性，我们在内存控制器中使用位图来识别迁移的热页，而<strong>不会分裂SuperPages</strong>。</li><li>我们提出了一种<strong>物理地址重映射机制来访问DRAM中迁移的热页</strong>，而不必为寻址DRAM页而遭受昂贵的页表遍历。为了实现这一目标，我们将迁移的热点页面的目的地址存储在其原始住所（超级页面）中。 一旦热页对应的TLB未命中，DRAM页寻址应求助于对超级页的间接访问。这种设计在逻辑上利用了SuperPage TLBS作为4KB页面TLB的下一级缓存。 因为Superpage TLB命中率通常很高，所以Rainbow可以显著加快DRAM页面寻址的速度。 </li></ul><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，与现有的内存迁移策略相比，在没有Superpage支持的情况下，Rainbow可以将应用程序的TLB丢失率降低99.9%，并将应用程序的性能（以OFIPC为标准）平均提高2.9×(45.3%)。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA, 2021</li><li>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Mazen Alwadi1, Vamsee Reddy Kommareddy1, Clayton Hughes2, Simon David Hammond2, Amro Awad3<br>University of Central Florida1, Sandia National Laboratories2, North Carolina State University3</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>它们存在着高写延迟和有限的写持久性。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。对于大容量的NVM快速和持久的缓存能力是有限的。利用DRAM作为NVM的一个快速持久的缓存，受到能源支持的限制。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新DRAM缓存内容到NVM</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>实现NVM的快速持久的DRAM缓存，我们利用选择性的NVM镜像对缓存在DRAM中的持久页面进行了新的内存控制器。</p><p>在DRAM中缓存时将持久区域的更新镜像到NVM。 Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>写次数不变，对于写耐久性的破坏</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>虽然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作。仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变</p><blockquote><p>crash-consistent applications 崩溃一致性程序;crash宕机，或主机、程序停止工作等情况。</p></blockquote><p><code>这篇翻译的不好，还有很多问题，会持续修改</code></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>非易失性存储器（NVM）具有传统外存系统（storage systems）持久性和传统内存系统（memory systems）字节可寻址的特点。然而，它们存在着<strong>高写延迟和有限的写持久性</strong>。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。然而，这可能会使缓存页面的持久性失效，从而导致在性能和可靠性方面的权衡问题。在本文中，我们提出了Stealth-Persist，这是一个新的架构支持功能，允许需要持久性的应用程序在DRAM中运行，同时保持NVM提供的持久性功能。Stealth-Persist创造了一个持久性内存的假象，供应用程序使用，同时利用DRAM进行性能优化。我们的实验结果表明，Stealth-Persist将持久性应用的性能提高了42.02%。</p><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p><strong>新兴的非易失性存储器（NVMs）正逐渐成熟，达到了接近大规模生产阶段和广泛采用的水平[11],[15],[16],[59],[65]<strong>。 例如，最近，英特尔发布了OptaneDC产品，它是一种使用3DXPoint技术的内存模块[21]。这些NVM-based内存模块以超低的空闲功率运行，但仍然具有非常高的密度。例如，每一个Optane DC模块可以有512GB的容量[3]。因此，在需要高内存容量但可能受到电源限制的服务器中，它们是一个非常引人注目的补充。虽然DRAM模块必须频繁地进行刷新操作，它占了很大比例的电力消耗[38]，但NVM并不这样做。传统上，</strong>NVM在断电或关机后仍能保留数据，这对崩溃一致性应用程序非常有用</strong>[9],[17]，并能承载快速访问文件[52]。然而，新兴的NVM的读写延迟比DRAM的延迟要低很多。例如，在英特尔的OptaneDC上，读取延迟为300ns，而DRAM的读取延迟接近70ns[46]4.3倍慢的读取访问。虽然NVM的<strong>写延迟可以通过外部缓冲或利用电池支持的内部写缓冲器来隐藏</strong>(例如， 英特尔的Optane DC)写入延迟队列)，NVM的设备写入延迟可以比DRAM的延迟慢十倍[13],[33]。</p><p>新兴NVM持久特征的未来对于许多<strong>数据恢复和崩溃一致性至关重要的应用</strong>是有吸引力的[40]。此外，新兴的NVM允许直接访问和更新持久性文件，而不会产生昂贵的缺页[52]。例如，数据库的在NVM上可以有几百个千兆字节，并且可以直接在应用程序中进行读写操作，类似于DRAM[52],[59]。此外，英特尔的持久性内存开发工具包( PMDK)允许开发应用程序，<strong>利用NVM的持久性对关键数据结构进行持久更新[9]。理想情况下，在崩溃之后，持久性应用程序应该能够通过从NVM读取数据结构来恢复</strong>[14]。然而，NVM的大的读/写延迟会大大减慢对持久性数据的访问。在其他方面，每个应用程序需要在使用低(相对于DRAM)的NVM和使用DRAM之间做出选择，前者可以实现崩溃一致性 ，后者则会失去内存子系统中的持久性能力。随着新兴的NVMs的不断采用，以及持久性编程库( 如PMDK)的日益普及，我们预计越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。</p><p>新兴的NVM可以作为存储设备( 例如，在SSD内)如英特尔的Optane硬盘[1] ，或作为部分或系统的内存层次。为了将NVM集成到内存层次中，有很多标准和选项[2],[3],[6]。 最值得注意的是，英特尔的类似DIMM的NVM模块( 称为OptaneDC[3])可以作为主存储器或作为主存储器的一部分与其他存储器选项(如DRAM和HBM)一起集成。在作为主内存的一部分使用时，它可以作为一个单独的物理内存添加区，扩大DRAM的物理地址范围，<strong>或者将DRAM作为OptaneDC的物理范围的硬件管理缓存使用[3]。前者被称为App Direct 模式，这类似于将不同的内存区域暴露于非统一存储器体系结构（NUMA）中的系统</strong>。我们把后者称为内存模式。<strong>内存模式放弃了持久性特性</strong>，因为当应用程序从内部缓存刷新其更新时，可以在易失性DRAM中更新内存块。然而，<strong>由于DRAM缓存大量的NVM页面，可以大大改善访问延迟</strong>，特别是对于频繁使用的页面。另一方面，<strong>AD模式确保了映射到NVM地址范围的页面的持久性，但由于它依赖于容量有限的内部处理器缓存(而不是外部DRAM)，因此会产生巨大的成本</strong>。   因此，目前Optane内存模式的集成操作，忽略了持久性应用对持久和高性能的需求。</p><p>JEDEC还为含有NVM的内存模块提供了几种标准。特别的，JEDEC为含有NVM的DIMM (称为NVDIMM)定义了三种不同的标准，即NVDIMM-N、NVDIMM-P和NVDIMM-F[5], [6]。这三种不同的选择提供了不同的暴露容量、持久性保证和管理复杂性的折中。特别的，NVDIMM-N只将DRAM暴露给软件，并在崩溃期间利用超级电容器为DIMM供电，提供将DRAM数据复制到NVM的能力( 目前是 flash-based基于闪存的)。因此，NVDIMM-N具有类似于DRAM的<strong>暴露延迟</strong>，但将内存容量限制在DRAM大小。NVDIMM-F將NVM (目前是基于闪存)暴露给软件，并作为一个块设备直接访问。平均而言，NVDIMM-P对不同的NVM技术有更广泛的定义，并允许内部DRAM缓存与NVDIMM-P的几个持久性选项，如<strong>深冲命令</strong>，因为NVDIMM-P没有能量支持。NVDIMM-P的一个主要优是它利用了<strong>一个交易协议，允许它使用非确定性的时间</strong>，与NVDIMM-N和NVDIMM-F相比，它依赖于确定性的时间。显然，在这三种选择中，NVDIMM-P是最适合新兴的NVM (不是为闪存定制的)和高容量系统。此外，由于DIMM上有高效的能源支持，内部DRAM可以被认为是NVDIMM模块内的NVM的一个高速缓存。NVDIMM-P没有能量支持（energy-backing），除了DRAM缓存在NVDIMM-P中而不是像内存模式下的独立模块，其他与内存模式下的Intel’s Optane DC类似。</p><p>虽然NVDIMM-P作为一个概念很有趣，但由于以下原因，对于大容量的NVM快速和持久的缓存能力是有限的。首先，如果需要几十或几百GB的DRAM来有效地缓存大型NVM，那么就需要笨重和潜在的昂贵的电池来提供能量支持。此外，由于能源支持只支持内部DRAM，客户被限制在相同的供应商和NVDIMM-P中可用的DRAM缓存的特定大小容量。另外，具有独立的NVM和DRAM模块的杠杆老化内存模式解决了NVDIMM-P的灵活性限制，但需要昂贵和不环保的( 和笨重的)电池支持[43]。换句话说，利用DRAM作为NVM的一个快速持久的缓存，受到能源支持(或独立能源)能力的限制，然而这种能力需要在需要大型DRAM模块时得到更多的提升。因此，我们在<strong>本文中的目标是允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新（flush）DRAM缓存内容到NVM</strong>。因此,<strong>我们能够在有NVM的系统中集成首选的DRAM模块，同时也能在DRAM中实现持久性数据的缓存，而不牺牲持久性或需要额外的电池支持能力。</strong></p><p>为了实现NVM的快速持久的DRAM缓存，我们利用<strong>选择性的NVM镜像</strong>对缓存在DRAM中的持久页面进行了新的内存控制器。<strong>我们的方法支持两种内存模式和AD模式</strong>，并能确保对缓存在DRAM中的持久页面的更新的持久性。此外，我们的内存控制器通过放松对DRAM缓存页更新的镜像， <strong>将写入NVM的次数降到最低</strong>，如果他们在NVM中的源页是在NVM的逻辑非持久性部分(即，用于支持不需要持久化的页面)。类似于当前处理器中的内存模式支持我们的内存控制器透明地在NVM和DRAM之间迁移页面。然而，我们通过从NVM中的原始地址推断出其语义来确保DRAM缓存页的持久性。如果页面被缓存在DRAM中，我们的方案只会产生额外的写入，此外，NVM的写入也会发生。然而，未来的读取将从DRAM中进行，这使得快速和持久的NVM页面的缓存成为可能。此外，通过允许持久性页面位于DRAM中，我们的方案利用额外的银行级别的并行性来访问持久性页面，而不是强迫所有的访问到NVM。我们的方案在精神上类似于内部处理器缓存中通常使用的write-through方案，但是由于写的性质和DRAM如何暴露给系统(内存模式或应用直接模式)，涉及到新的优化和设计考虑。虽<strong>然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作</strong>。</p><p>为了评估我们的设计，我们使用了Whisper基准[41]中的持久性应用程序。为了研究我们方法的可靠性，我们还开发了6个内存密集型基准，类似于Janus[37]以前的工作。 一个开放源码的架构模拟器，结构模拟工具包(SST) [50]用来被模仿我们的方法。平均来说，与只使用NVM持久性应用相比，我们观察到<strong>42.02%性能的提高和88.28%NVM的读取减少</strong>。请注意，对持久性数据只使用NVM是唯一允许在没有任何备用电池的情况下进行数据持久化的选择，因此被作为我们的基线使用。<br>综上所述，我们的工作的贡献如下。</p><ul><li>我们提出了Stealth-Persist，这是一种新的硬件支持， 通过在DRAM中实现对热持久页的缓存来提高持久性应用的性能，同时确保数据的持久性，不需要外部电源支持且软件透明。</li><li>我们讨论了将Stealth-Persist与混合DRAM-NVM主存储器系统的实现垂直和水平的相结合。</li><li>我们讨论了Stealth-Persist的设计方案，在方案性能和镜像页面的数量之间进行了权衡。</li><li>我们广泛地分析了Stealth-Persist在不同的区域大小、不同的替换策略和不同的镜像阈值下的开销。</li></ul><h1 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h1><p>在本节中，我们将介绍与我们的工作相关的主题，以帮助读者了解我们的工作，然后是工作动机。</p><h2 id="A-新兴的非易失性存储器"><a href="#A-新兴的非易失性存储器" class="headerlink" title="A.新兴的非易失性存储器"></a>A.新兴的非易失性存储器</h2><p>新兴的NVM，如3DXPoint和英特尔的OptaneDC，具有更高的密度、字节寻址能力、更低的每比特成本、比DRAM更低的空闲功耗和非易失性，但具有<strong>更高的访问延迟和有限的写入耐久性</strong>[15],[32],[33],[35]。例如，NVM-based DIMM可以被用来存储文件和内存页，它可以使用常规的加载/存储操作来访问。为了实现这种类型的访问，最近的操作系统(OS)开始支持通过DAX文件系统[52]将内存配置为持久性或非持久性。在DAX文件系统中，一个文件可以直接被内存映射和使用常规的加载/存储操作访问，而不需要将其内容复制到页面缓存中[15]。然而，<strong>NVM的访问延迟比DRAM的访问延迟慢3-4倍</strong>。因此，研究者主张建立同时具有NVM和DRAM部分的内存系统[48]，[62]。</p><h2 id="B-混合主存储器-HMM"><a href="#B-混合主存储器-HMM" class="headerlink" title="B.混合主存储器(HMM)"></a>B.混合主存储器(HMM)</h2><p>混合主存储器( HMM)系统由于其密度和超低的空闲功率，预计将有很大的NVM部分，而由于其快速的读/写操作DRAM部分將很小。HMM可以以两种不同的方式部署，水平或垂直。在垂直方向上，NVM作为一个新的内存层被连接起来，DRAM被用来缓存NVM的数据[3],[6]。这种方案允许更快地访问大型内存池，并且需要特殊的硬件将数据从NVM迁移到DRAM，例如，在英特尔的OptaneDC内存模式下，英特尔的Xeon可扩展处理器的内存控制器对缓存线进行缓存。然而，因为DRAM的波动性，这样的方案并不能提供持久性。在第二种方法中，HMM系统的水平实现将NVM和DRAM暴露在物理地址空间中，就像在NVDTMM-P和OptaneDC的AD模式中那样，如果需要的话依靠OS来处理数据访问和页面迁移[27],[47],[48],[58]。在这两种情况下， 需要有混合内存管理方案，管理不同的持久性和性能要求。</p><p>文献中提出了基于内存层次的不同混合内存管理方案。诸如HetroOs[27]、RTHMS[44]和Nimble[58]提出了软件解决方案，以检测哪些页面需要迁移到最快的内存(例如，DRAM)。当DRAM和NVM都被内存映射并暴露给操作系统时，这些方案适用于<strong>混合内存系统的水平实施</strong>。另一方面，混合内存系统的垂直实施将DRAM作为一个缓存。因此，DRAM没有暴露给操作系统，其中缓存页使用专用硬件处理，通常是内存控制器的扩展，如英特尔的OptaneDC内存模式[3]。像Ramos等人[48]提出的方案，根据每个页面被访问的频率，使用多队列(MQ)结构对页面进行排名，然后使用页面的排名来决定哪些页面需要迁移到DRAM中，哪些页面需要保留在NVM中。然而，跟踪所有的页面并检查MQ结构来提升和降低页面的等级会产生很高的开销，因此在每个时间段只检查队列的头部。在讨论了混合内存系统的管理方案之后，我们讨论了一些用于HMM中的页面缓存的方案。</p><h2 id="C-页面缓存策略"><a href="#C-页面缓存策略" class="headerlink" title="C.页面缓存策略"></a>C.页面缓存策略</h2><p>页面缓存策略是用来决定哪些页面应该被缓存在DRAM中，如果你想缓存NVM的页面。在这一节中，我们将讨论两个策略，这些策略将在我们的设计中使用。</p><p><strong>首次触摸策略</strong>。该策略在第一次访问时缓存页面，并根据LRU算法选择一个页面进行驱逐。<br><strong>多队列(MQ)<strong>。多队列(MQ)。MQ最初被设计用来对磁盘块进行排序，后来被Ramos等人[48]用于混合内存系统的页面放置。MQ的工作方式如下。MQ定义了M个块描述符的LRU队列。队列的编号从0到M-1， 队列M-1的块是访问量最大的块。每个描述符都包含区块的编号、一个引用计数器和一个逻辑到期时间。在第一次访问一个区块时，它的描述符被放在队列0的尾部，其过期时间被更新为<code>CurrentTime+LifeTime</code>。这</strong>两个时间都是以访问次数来衡量</strong>的，<code>LifeTime</code>代表在区块过期前对不同区块的连续访问次数。每次区块被访问时，其过期时间被重置为<code>CurrentTime +LifeTime</code>，其引用计数器被递增，其描述符被推到其当前队列的尾部。在队列i中的块的描述符被访问了一定数量后，它被提升到队列i+1，在队列M-1中达到饱和。另一方面，最近没有被访问的区块会被降级。在每次访问时，所有队列头部的描述符都被检查是否过期。如果描述符过期了，它就被放在下面队列的尾部，其生命时间被重置，其降级标志也被设置[48]。如果一个描述符连续收到两次降级，该描述符将被从MQ结构删除。为了减少升级/迁移的开销，这些操作只在每个周期结束时进行。</p><p>由于已经证明MQ在选择要替换的页面方面优于其他算法[48], [64], 它与我们的目标一致。因为它有助于检测出性能关键的页。因此，在我们的实验中，我们使用了Ramos等人提出的MQ设计[48]。在讨论了缓存策略之后，我们现在提到了目前可用的工业混合内存系统的实现。</p><h2 id="D-目前的工业HMM系统"><a href="#D-目前的工业HMM系统" class="headerlink" title="D. 目前的工业HMM系统"></a>D. 目前的工业HMM系统</h2><p>目前，市场上有不同类型的HMM系统。例如，JEDEC定义了三种不同的被称为NVDIMM的HMM标准。NVDIMM类型有不同的特点、持久性和性能特征。此外，英特尔最近透露了关于Optane DC的内存模式和AD模式的细节。<br><strong>NVDIMM-N</strong>包含一个DRAM部分，一个NVM部分，以及一个超级电容。系统在正常执行时使用DRAM，而NVM只在崩溃时使用超级电容供电用于复制DRAM数据。[6]<br><strong>NVDIMM-F</strong>模块是连接到DDR总线上的NVM，它的访问延迟相对高于DRAM。因此，可以在系统中安装一个DRAM，并用于缓存NVDIMM-F的数据，代价是数据的持久性|6]。<br><strong>NVDIMM-P</strong>仍然是一个DIMM的建议，它有内存映射的DRAM和NVM，其中软件根据数据的大小和持久性要求将数据放在NVM或NVDIMM中。<br><strong>Optane DC内存模式</strong>是英特尔持久性内存的一种操作模式。它类似于NVDIMM-P。</p><h2 id="E-持久性内存编程模式"><a href="#E-持久性内存编程模式" class="headerlink" title="E. 持久性内存编程模式"></a>E. 持久性内存编程模式</h2><p>由于NVM的持久性特点，访问一个NVM内存对象就像访问一个存储文件。因此。应用程序需要一种方法来重新连接到先前分配的存储器对象。因此，持久性内存区域需要名称和访问控制来进行访问。存储网络工业协会（SNIA）建议操作系统提供命名、权限和内存映射的标准文件语义。因此，一些操作系统增加了对文件系统的直接访问（DAX）支持[4]。DAX允许应用程序直接使用持久性内存而不使用系统的页缓存。图1显示了持久性内存感知文件系统的工作原理[52]。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d916d5cd134a4909a11c4901a00fe76b.png"></p><p>使用持久性内存（PM）对象需要程序员考虑多个问题以确保数据的持久性和一致性。其中一个问题是原子性；什么样的支持由硬件提供，而什么是留给软件处理的[52]。英特尔的硬件确保了8字节写入的原子性，因此如果一个对象大于8字节，那么软件就有责任确保更新对象的原子性[52]。此外，<strong>确保数据的持久性需要将数据一直推送到持久性领域</strong>，因为大部分的数据更新是在易失性处理器的缓存中完成的。持久域从内存控制器中的一个小的缓冲区，即<strong>待处理队列（WPQ）开始。WPQ由异步DRAM (ADR)刷新功能支持。ADR提供的电源可以确保在断电的情况下将WPQ的内容刷新到NVM上</strong>[15], [55], [59], [65]。图2显示了具有持久性存储器的系统中的持久性域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/444a25e88ec046da99d0953429fe4d21.png"></p><p><strong>为了将数据一直刷新到持久化区域。确保原子性和排序，需要遵循一组特定的指令</strong>。清单1显示了一个代码例子，它取自来自SNIA的NVM编程模型V1.2[8]。该代码显示了持久化对象<code>a</code>和<code>a_end</code>。为了确保这些持久化对象的更新的一致性、原子性和顺序性，每次更新这些持久化对象之一时都会调用<code>msync</code>操作。请注意，第7行的更新没有使用<code>msync</code>操作，因为它不是在更新一个持久化对象。<code>msync</code>操作是用来<strong>强制更新一个内存范围到持久化领域的</strong>。此外，它创建了一个屏障，以保证在继续进行之前执行先前的存储，<code>fsync</code>操作做的是对文件具有相同的功能[52]。</p><h2 id="F-动机"><a href="#F-动机" class="headerlink" title="F.动机"></a>F.动机</h2><p><strong>在主存储器中拥有一个持久性的部分可以使应用具有不同的持久性要求。然而，为了确保数据的持久性，应用程序的持久性数据应该被放置在内存的NVM部分，由于NVM的访问延迟较慢，这阻碍了这些应用程序的性能</strong>。另一方面，将应用程序的数据放在DRAM上，会导致更好的性能，但不能满足这些应用程序的数据持久性要求。为了确保应用程序的数据持久性，持久性应用程序应该遵循第二节E中提到的编程模型。如前所述，<strong>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域</strong>。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。虽然持久性应用的数据大小不太可能适合易失性缓存，但在大得多的DRAM中缓存这种持久性数据可以为持久性对象提供明显的读取速度提升。同时，期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择( 如供应商)， <strong>是现有解决方案的主要缺点</strong>。因此，仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b9c998558da04df3b8e44015a419ded8.png"></p><p>表一是现有技术之间的比较。从表一中，我们可以观察到，在支持高技术方面的差距。性能持久性内存，以及大容量持久性内存，因此<strong>Stealth-Persist旨在弥补这一差距</strong>。图3显示了在OptaneDC AD模式(所有持久性数据都在NVM中)上运行的持久性应用的性能开销，与在不提供数据持久性的DRAM系统上运行的性能开销相比。从图3中，我们可以看到，在OptaneDC的AD模式上运行的应用平均会产生2.04倍的速度下降。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/003104b458bd4b08824b99b3b496a649.png"></p><h1 id="三、设计"><a href="#三、设计" class="headerlink" title="三、设计"></a>三、设计</h1><p>在本节中，我们根据可能的设计方案及其权衡来讨论Stealth-Persist的设计。首先，我们开始讨论设计要求，以及潜在的设计方案。</p><h2 id="A-设计要求"><a href="#A-设计要求" class="headerlink" title="A.设计要求"></a>A.设计要求</h2><p>我们的设计应该满足必要的要求，以允许广泛采用和高性能，同时保留持久性对象的语义。总的来说，这些要求如下。</p><ul><li>灵活性:我们的设计应允许将任何DRAM模块，无论其容量如何，都集成到配备有NVM的系统中，而不需要任何特殊的电池备份或特定的DIMM修改。</li><li>持久性:任何被认为是持久的内存页或对象(即，可从崩溃中恢复)应该是可恢复的，而不需要任何额外的电池备份支持，无论该页位于何处(NVM或DRAM)。</li><li>高性能:对持久性页面和对象的访问应该和对DRAM的访问一样快。</li><li>透明度:利用持久性内存进行崩溃恢复的应用程序不应该需要明确地管理当前驻留在DRAM中的对象的缓存和持久化。</li></ul><p>为了把这些要求放在持久性应用的背景下，我们可以想象一个访问几十GB持久性对象的持久性应用。 理想情况下，除了NVM模块外，系统还应该能够集成DRAM模块。系统的所有者应该可以灵活地选择什么样的容量和供应商来选择这样的DRAM和NVM模块，这就提供了灵活性。然而，对持久性对象的更新应该是持久的并且在崩溃时持久存在，无论它们存在于何处(DRAM或NVM)。虽然通过持久性模型和框架，即<code>clflushes</code>和内存栅栏使易失性缓存中的对象的更新变得持久，但如果持久性对象被缓存在片外DRAM中，目前还没有支持保证它们的持久<br>性，这给我们带来了持久性要求。最后，<strong>应用程序最好能将其持久性对象缓存在DRAM中</strong>，以尽量减少获取不适合在易失性处理器缓存中的持久性对象的成本。因为<strong>这些对象通常只有几兆字节的易失性处理器缓存</strong>。获取片外持久性对象的延迟比慢速NVM的延迟短（300ns的读取延迟与70ns的DRAM相比），这一要求使我们看到了设计要求的第三个要素，即高性能。因此,持久性应用应该能够将不适合内部易失性缓存的持久性对象缓存在快速的片外DRAM中，同时保留其持久性能力。最后，所有用于缓存和持久化持久性对象页面的操作都应该透明地发生在软件上，而不需要将这些细节暴露给应用程序，这就给我们带来了最后一个要求，即透明度。</p><h2 id="B-设计选项"><a href="#B-设计选项" class="headerlink" title="B.设计选项"></a>B.设计选项</h2><p>现在我们将讨论有可能满足我们要求的设计方案。</p><p>一种选择是支持新的指令，<strong>在缓存线被刷新之前不提交——不仅从易失性缓存，而且从片外DRAM到NVM</strong>。这种设计方案可以通过在内存控制器的支持下向指令集架构(ISA)引入新的指令来实现，或者通过修改当前指令的实现，<strong>使其从内部易失性高速缓存( 例如，clflush)以及从DRAM到NVM中刷新缓存线。假设DRAM是通过内存控制器作为NVM数据的硬件管理缓存来运行的，这样的指令就需要内存控制器首先检查要保存的缓存线是否在DRAM中，读取它，然后将它刷新到NVM中</strong>。这种方法的主要问题是。(1)它需要改变ISA、持久性编程库和处理器内核来支持这种新的指令。此外，(2)持久化数据的延迟将大大增加，特别是当刷新的块在DRAM中被标记为脏块时。请注意，即使DRAM缓存的是页而不是缓存行，它仍然需要类似的支持，但需要新的指令在页的粒度上操作，而不是<code>clflush</code>。</p><p>另一个选择是利用小型固定尺寸的备份能力(例如，超电容)， 为刷新DRAM的特定部分提供动力。例如，无论模块的总大小如何，都要有足够的电力来刷新8GB的DRAM。内存控制器或系统的软件可以潜在地迁移或在地址空间的这个子区域放置持久的页面，标记为持久的。当电源故障发生时，内存控制器( 或外部系统电路)有足够的电力来刷新DRAM的那一部分。虽然这样的解决方案在精神上类似于NVDIMM-N，它提供了选择任何DRAM模块和容量的灵活性。然而，具有持久性支持的部分的大小被限制在系统的备份能力范围内。另一方面，这样的解决方案需要外部系统的支持，并将DRAM的持久性部分的大小限制在电源备份能力上。同样，这样的备份能力通常成本很高， 需要高面积(笨重)， 而且可能对环境不友好。</p><p>虽然第一种方案提供了高性能、持久性和灵活性，但它缺乏透明度。同时，第二种方案具有部分灵活性(需要系统支持和可能的ISA变化)， 部分的高性能(只有一小部分DRAM可以作为持久性内存使用)， 透明度和持久性。因此，我们的设计应该提供完全的透明度、高性能、持久性和灵活性，而不需要任何额外的系统支持或超出现代系统所提供的备份能力。</p><h2 id="C-Stealth-Persist"><a href="#C-Stealth-Persist" class="headerlink" title="C.Stealth-Persist"></a>C.Stealth-Persist</h2><p>在满足上述设计要求的同时，我们的设计还应该与混合内存系统的不同集成方式兼容。特别是垂直内存模式(如Optane DC的内存模式)和水平内存模式(如OptaneDC的AD模式)在深入探讨不同集成模式下Stealth-Persist支持的细节之前，我们将讨论Stealth-Persist如何满足设计要求。</p><p>为了满足灵活性的要求，Stealth-Persist的实现是为了支持在DRAM中缓存时将持久区域的更新镜像到NVM。因此，它不需要系统的任何支持，并且可以在任何DRAM大小下工作。通过镜像缓存在DRAM中的持久性页面的更新，持久性的要求得到了满足。为了使我们的解决方案对软件透明， Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存。图4描述了Stealth-Persist中的读写操作，在高层次上。</p><p>如图4所示，内存控制器集线器MCH)处理对持久性页面的写入的镜像，如果在而直接从DRAM上提供读取请求。通过这样做，Stealth-Persist确保了对NVM的写入的持久性，同时允许对这种持久性对象进行快速的读取操作。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/69796fb223084662b46606e733932280.png"></p><p>虽然在高层次上，这种设计<strong>看起来类似于内部处理器缓存中通常使用的写通方案</strong>，但在考虑到混合内存系统的背景时，会出现许多挑战和潜在的分歧。第一个挑战是<strong>如何决定一个页面是否应该被镜像</strong>。第二个挑战是<strong>如何快速识别一个页面是否被缓存在DRAM中，它被缓存在哪里</strong>?以及<strong>在运行期间如何保证两个副本都是一致的</strong>。第三，<strong>由于不是所有NVM中的页面都需要被持久化，对存储在NVM中的页面的更新需要有选择地进行镜像</strong>。最后，Stealth-Persist需要进行调整，以适应无数种整合混合内存系统的方式。本节的以下部分将讨论这些挑战以及我们如何克服它们。</p><p>1)页面镜像。无论使用何种HMM管理方案，水平(例如应用程序直接模式)或垂直(内存模式)， Stealth-Persist都需要将部分(或整个) DRAM作为持久性页面的镜像区域。<strong>在垂直内存设置中， 整个DRAM将被用作NVM的缓存，因此，任何缓存在DRAM中的页面也可能被镜像到NVM中</strong>。同时，<strong>在水平设置中，由于DRAM和NVM的物理范围明确地暴露在系统中，我们让内存控制器保留一部分DRAM，仅作为镜像区域使用。DRAM的其余部分将直接暴露给系统</strong>，就像在AD模式中一样。位于NVM中的任何持久性页面都可以被缓存在DRAM的镜像区域中， 而不考虑设置，也就是这种区域的大小。<strong>在每个以NVM地址为目标的内存访问中，我们需要透明地检查该页是否当前驻留在DRAM中。读取和写入操作都需要这种检查;如果访问的页面被缓存在DRAM中，那么读取操作可以直接从DRAM中进行，而写入操作则需要更新NVM中的副本，以保证镜像页面副本之间的一致性并确保持久性。当一个页面不在DRAM中时我们需要从NVM中读取它(或写人它)。由于镜像区域可以被认为是NVM中持久性页面的缓冲区/缓存，我们需要为DRAM中的所述缓存/缓冲区定义插入和驱逐策略</strong>。</p><p>为了简单起见，我们使用了一个类似于垂直内存管理方案中使用的页面插入策略。通过这样做，如果使用内存模式，除了在DRAM中缓存持久性页面时向NVM进行额外的写入外，不需要对管理策略进行任何改变。同时，对于AD模式，在DRAM中定义的镜像区域将被管理，类似于内存模式中的DRAM缓存，此外还有对NVM的镜像写入。考虑到这一点，我们<strong>使用了两个简单的策略来放置DRAM缓冲区中的页面</strong>: ( 1)首次触摸策略(FTP)和(2) 多队列(MQ)策略，正如之前的工作中所提出的[48]。</p><p>2)DRAM镜像区域查询。为了确保Stealth-Persist可以快速<strong>检查一个页面是否在DRAM中(镜面区域的地址）， Stealth-Persist使用一个硬件管理表来跟踪镜面区域的页面</strong>。镜像的映射表包含了镜像的缓存页地址的转换，如图5所示。映射表的每个条目都包含一个组ID,它是用NVM中的镜像页地址与镜像区域的页数的模数函数来计算的。此外，每个条目包含六对转换，将36位NVM的页面地址映射到36位镜像DRAM的页面地址。此外，我们为每个翻译使用3位(共18位)作为替换策略的LRU位，这使得翻译总共有450位，其余512位用于组ID (32位) 和填充。因此，一个页面可以通过时钟替换策略或条目内的LRU驱逐从镜像区域中移除。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8130fea0f2ba4f3bbadb1dff9316d3c4.png"></p><p>请注意，镜像的映射的存储要求是在镜像区域中， 每6页的映射表是64字节。因此，我们<strong>在内存控制器中使用一个小的缓存来缓存镜像的映射表项，同时在DRAM中保持该表</strong>。每当收到对持久化区域的内存请求时，就会计算被请求页的组ID，并检查镜像的映射表缓存是否有被请求的组ID,这可能导致三个不同的情况。(1）该条目被缓存，页面被缓存，请求从相关的DRAM页面提供。(2)条目缓存，页面没缓存，条目没有镜像，请求要去NVM。(3)条目没有被缓存，必须检查DRAM中的映射表以获得条目和它的镜像页。由于映射表缓存缺失可能会导致从DRAM中提供请求，并进行两次访问，或者在检查DRAM后从NVM中提供请求，所以我们将请求发送到DRAM中，如果条目在表中，NVM再从DRAM中提供请求，如果不在表中，则从NVM中提供。</p><p>3)镜像页的连贯性更新。在Stealth-Persist中，镜像区域页和NVM页之间应该保持一致性 。由于持久性页面被认为是可恢复的，对持久性页面的写入应该是持久的。因此，对镜像区域的写入应该被推送到两个内存。Stealth-Persist将对镜像区域页面的写请求推送到DRAM的易失性写缓冲区和NVM的持久性WPQ。请注意，<strong>一个写请求只有在它被放入WPQ后才会退役，这保证了写的持久性。另一方面，属于非持久性区域的镜像页不需要数据一致性 ，也不需要可恢复性，这就是为什么Stealth-Persist实现了选择性镜像</strong>。Stealth-Persist对一致性没有任何影响。如果DRAM和NVM模块在同一个插座（socket）上，这是英特尔的DCPMM所支持的配置，NVM和DRAM拷贝之间的一致性由MC通过镜像来管理，而与内部处理器缓存的一致性是在传统系统中处理。然而，如果我们偏离了目前的标准，即NVM和DRAM在同一个插座上，即各自在不同的插座上，那么我们可以指定靠近NVM的内存控制器作为主控，因此它将负责处理镜像、重映射等，并相应地将镜像表缓存中的任何请求转发给拥有DRAM模块的插座的内存控制器。</p><p>4)选择性镜像。Stealth-persist实现了选择性镜像技术，以减少对NVM的写入次数,这可以通过将<strong>指向非持久性区域的写入只提交给其DRAM镜像版本来实现</strong>。Stealth-Persist在垂直HMM实现中实现了选择性镜像，就像在Optane DC的内存模式中一样，在水平HMM实现中实现了OptaneDC的AD模式。在这两种情况下，StealthPersist需要持久性内存区域的地址范围，这可以在系统启动时由内核传递给Stealth-Persist，例如，Linux 命令<code>memmap= 2G!8G</code>可以用来保留一个从地址8G开始的2GB持久性区域。请注意，将非持久化区域中的页的写操作只转发到其镜像版本，会违反这些页的一致性。 然而，由于这些页面是在非持久性区域中，而且这些应用预计是不可恢复的，所以写入可以只提交给镜像页面，而如果页面被驱逐，整个页面将被写回NVM。</p><h2 id="D-概述"><a href="#D-概述" class="headerlink" title="D.概述"></a>D.概述</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ec2628a98966478c8c30d234e670ce39.png"></p><p>图6显示了整个Stealth-Persist设计。对于每个最后一级缓存(LLC)的缺失，首先内存控制器检查该请求是否是对持久性区域的请求①如果是对持久性区域的请求，镜像表缓存被查询到NVM页面的当前状态②。如在三C2节中讨论过，镜像表缓存通过查看已经缓存的镜像表条目，或者从存储在DRAM中的镜像表中获取条目，并使用LRU策略替换组ID和相应的映射表条目，来验证NVM的页面镜像状态③如果页面被镜像，读取请求被转发到DRAM，而写入请求被转发到DRAM，以更新镜像区域，以及NVM，以保持数据④。在读的情况下，持久化内存访问被转发到多队列或FTP单元CT⑤这单元决定一个页面是否应该被镜像，如果是，则镜像表缓存被触发，使用LRU策略⑥替换其中一个映射。</p><h2 id="E-Stealth-Persist-NVM库的比较"><a href="#E-Stealth-Persist-NVM库的比较" class="headerlink" title="E. Stealth-Persist NVM库的比较"></a>E. Stealth-Persist NVM库的比较</h2><p>一些研究提出使用NVM库来解决NVM作为主存储器时的原子性、崩溃一致性和性能问题。NVM库的重点是将写操作移出关键路径以提高性能，但没有减少读延迟。相比之下，Stealth-Persist通过减少基本的内存读取操作的延迟来提高性能，这在NVM库中仍然是需要的。一些方案专注于容错(如Pangolin[61])、性能和强一致性(如NOVA[57]) 、减少编程工作量和性能(如Pronto[39]) 。虽然这些方案通过将写入开销移出应用程序的关键路径，或者通过在DRAM中缓冲一些更新来提高系统的性能，但是如果需要持久性，对NVM的写入是不可避免的。与此相反，Stealth-persist在写到NVM时，如果它们是指向NVM中的一个持久性区域，就会传播到NVM，并将写到非持久性区域的写入缓冲在DRAM的缓存页中。此外，Stealth-Persist在与所提出的NVM库不同的层中运行，这使得Stealth-Persist与这些方案正交。事实上，Stealth-Persist可以与上述方案同时使用，以进一步提高性能。</p><p>在一个不同的方向，Hagmann[24]提出了一个方案维护个日志以恢复磁盘中的文件系统。Petal[34]通过创建虚拟磁盘，使客户能够访问分布式磁盘，从而改善系统的性能，提高吞吐量。为了提供可恢复性，Petal使用写前记录。Condit等人[23]提出了一个方案，使用影子分页使持久性存储器的崩溃一致性， 在这个方案中，写是原地提交或使用局部的写时拷贝。Linux文件系统的BTRFS[49]使用B树数据结构，并使用写时拷贝作为更新方案。Rosenblum和Ousterhot[51]提出了一个日志结构的文件系统，该系统以顺序的方式执行所有写入磁盘的操作，并保持索引信息以加快数据检索。Seltzer等人。[53]提出了一个日志结构的文件系统，它具有更好的写入性能，更少的恢复时间，并能实现嵌入式事务和版本管理。提出这样的方案是为了确保原子性、可恢复性，并提高性能在磁盘中。然而，在确保持久性的同时，这些方案并没有提<br>高读取操作的性能。因此，它们与Stealth- Persist是正交的。</p><h1 id="四、方法论"><a href="#四、方法论" class="headerlink" title="四、方法论"></a>四、方法论</h1><p>我们在结构模拟工具包( SST)模拟器中对Stealth-Perit进行建模[50]。SST是一个基于周期级事件的仿真器， 对不同的硬件组件进行模块化设计。SST在工业界和学术界被广泛使用[28],[30],[31]。我们实现了一个混合的内存控制器组件，以处理DRAM和NVM。表二显示了模拟系统的配置。模拟的系统包含4个失序的内核，每个内核在每个周期执行2条指令。核心的频率是2GHz。三个级别的缓存，L1，L2和L3(包括)被模拟为32KB，256KB和1MB。DRAM的容量为1GB，NVM的容量为4GB。（请注意，选择DRAM和NVM的大小是由于仿真速度的限制，然而，最重要的参数是镜像区域的大小( 32MB)和应用程序的平均足迹( 256MB)。， 由于持久性应用程序的所有数据将驻留在NVM中，并且可以在镜像区域中持久性地缓存，因此我们关注应用程序的足迹与镜像区域的比例(8:1的比例) ，我们在本文后面将对此进行修改。） NVM的读写延迟为150ns和500ns[14]。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f75fb2353b384d6cb0f88987635402e5.png"></p><h2 id="A-负载工作"><a href="#A-负载工作" class="headerlink" title="A.负载工作"></a>A.负载工作</h2><p>为了评估我们提出的方案，我们运行了11个持久性应用程序。如表三所示，其中六个基准是内部开发的，所有这些都是为了强调内存的使用，并在以前的工作中使用过[37]。每个应用程序的功能描述如下</p><ol><li>ARSWP:该基准从数据库中随机选择两个密钥并进行交换。 </li><li>RANDWR:选择随机钥匙，用所选钥匙的数据库条目更新一个随机值。</li><li>SEQWR:这与RANDWR类似，但钥匙是按顺序选择的，从数据库的第一个元素开始。 </li><li>AVL:数据库被映射到AVL树上，并在映射的数据库中搜索一个随机生成的密钥。如果没有找到密钥，则触发插入操作。 </li><li>BTREE:该基准将数据库映射为B树，与AVL类似，搜索一个随机密钥， 如果没有找到，则用假数据插入密钥。 </li><li>RBTREE:与AVL和BTREE基准类似，RBTREE基准将数据库映射为RB树， 并搜索一个随机密钥。</li></ol><p> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d02f12cdf7f04e548613eeaf042b87fb.png"><br>我们还运行了由威斯康星大学麦迪逊分校与惠普实验室合作开发的WHISPER基准测试套件[41]中的五项基准(表三中前面有W：)。TPCC基准测量的是基于复杂数据库和在其上执行的各种数据库事务的在线交易处理系统(OLTP)的性能。雅虎云服务基准( YCSB)是一个用于评估数据库管理系统的编程套件。W:TPCC和W:YCSB基准是Whisper基准套件的变种，它是以N-Store[12]为模型的，N-Store是一个持久性内存的远程数据库管理系统。W:CTREE和W:HASHMAP基准是使用NVML[56]库开发的，该库对持久性存储器区域执行插入、删除和获取操作。W:ECHO是一个用于持久性内存区域的可扩展键值存储。对W: CTREE和W:HASHMAP基准的地图获取功能进行了评估。</p><p>所有这些基准的密钥大小为512比特，数据库大小为1GB。在评估这些基准之前，首先用随机密钥填充数据库。这些基准的每千条指令的失误率( MPKI)见表III。每个基准对500M指令进行了评估。</p><h2 id="B-DRAM-Mirror-Configuration镜像配置"><a href="#B-DRAM-Mirror-Configuration镜像配置" class="headerlink" title="B. DRAM Mirror Configuration镜像配置"></a>B. DRAM Mirror Configuration镜像配置</h2><p>为了镜像虚拟机的数据，我们使用了32MB的DRAM。然而，正如第五章E节中所讨论的那样， 我们把镜像区域的大小从2MB到1GB不等(整 个DRAM被用作镜像区域)。最小化是以页为单位进行的。在MQ机制中，只有当页面达到MQ级别4时才会被镜像，也就是说，当一个页面被读取16次时。纪元的时间间隔被设置为10000次读取操作。尽管我们用上述配置评估了StealthPersist方法，但我们通过改变DRAM镜像、大小和阈值水平来进行敏感性分析。由内存控制器维护的镜像表缓存的大小为128组，每组有 6个映射。镜像表缓存的查找延迟为1ns。</p><h1 id="五、评估"><a href="#五、评估" class="headerlink" title="五、评估"></a>五、评估</h1><p>在本节中， 我们讨论了Stealth-Persist与直接使用NVM进行持久化的系统的对比结果。我们进一步展示了通过改变影响性能的不同参数而进行的敏感性分析。</p><h2 id="A-Stealth-Persist对性能的影响"><a href="#A-Stealth-Persist对性能的影响" class="headerlink" title="A.Stealth-Persist对性能的影响"></a>A.Stealth-Persist对性能的影响</h2><p>图7显示了使用Stealth-Persist方法的性能改进。基准方案是OptaneDC AD模式方案，其中所有的持久性内存请求都只存储到持久性内存(NVM)。这是实现此类系统的持久性应用的数据持久性的典型方式。平均而言，使用Stealth-PersistMQ和FTP方法，性能提高了30.9%和42.02%。应用程序的性能改善是镜像区域命中率的一个函数，这一点在第V-B节中讨论。Stealth-Persist FTP的改进高于Stealth-Persist MQ方法，因为每一个被读取的页面都在DRAM中被镜像这导致了大量的页面从NVM复制到DRAM中。平均来说，我们观察到隐身传输FTP比隐身传输MQ方法多镜像了542.96倍的页面，这大大增加了内存总线的流量和能源使用。对于像SEQWR和W:ECHO这样的顺序内存访问基准，Stealth-Persist FTP的改进是巨大的–2.34倍和2.5倍。分别为2.2倍。由于这些基准是按顺序访问内存的，所以这些基准的空间定位性很高。因此，当一个页面被读取时它立即被镜像到Stealth-persist FTP中，并被访问为连续的内存访问。另一方面，Stealth-Persist MQ方法，首先，页面应该达到一个阈值才能被镜像。对于AVL和RBTREE工作负载来说，隐身主义MQ方法优于隐身主义FTP，因为隐身主，义FTP非常频繁地替换镜像区域的页面，这导致了从镜像区域驱逐热页面。另一方面，Stealth-persist MQ方法倾向于将热页保留在镜像区域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/fd4aaaa8276a419c8b119a5ad7190607.png"><br>对于ARSWP工作负载，Stealth-Persist方案的性能与Optane DC应用二维模式相比几乎没有变化，从图3来看，与使用DRAM作为主内存的系统相比，它受到了很大的影响它的速度要慢4.39倍。然而，ARSWP应用程序的内存访问非常稀疏，因此页面的重用距离很高，这导致在Stealth-persist FTP方法中，在这些页面被重用之前就被驱逐了。此外，ARSWP应用程序的页面没有达到Stealth-Persist MQ方法的镜像限制。因此，性能下降了3%，在由于检查镜像区域而只有0.02%的命中率，MQ方法。另一方面，由于有3%的命中率，Stealth-persist FTP的性能在ARSWP基准中提高了1.6%。然而，如第V-E1节所示，当镜像区域的大小增加时，ARSWP的性能得到改善。</p><h2 id="B-DRAM镜像命中率"><a href="#B-DRAM镜像命中率" class="headerlink" title="B.DRAM镜像命中率"></a>B.DRAM镜像命中率</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/55e20b926dcb4b9ababf12230c83b888.png"><br>图8显示了由DRAM镜像区域提供服务的读取百分比。我们注意到，有顺序内存访问的应用显示出最好的性能改进FTP在这些应用中显示出非常高的命中率。另一方面，在Stealth-Persist MQ方法中，具有随机跨度访问的应用和具有热页的应用显示出最高的命中率。如图8所示，在Stealth- Persist FTP方法中，镜像页平均占整个内存读取的57.81%。对于Stealth-Persist MQ，与Stealth-Persist FTP相比，它以合理的页面镜像数量服务于平均24.78%的整体读取。如图8所示，具有最高命中率的内存约束应用程序显示出最高的性能改进。在Stealth-Perist FTP中，WHISPER基准的镜像命中率，如CTREE和HASHMAP很高，但性能的提高没有SEQWR和ECHO基准那么多。这是因为CTREE和HASHMAP应用不像EPOCH和SEQWR那样内存密集，这与CTREE和HASHMAP的MPKI有关，如表三所示CTREE的MPKI为1.75，HASHMAP的MPKI为0.84。</p><h2 id="C-Stealth-Persist对NVM读取的影响"><a href="#C-Stealth-Persist对NVM读取的影响" class="headerlink" title="C.Stealth-Persist对NVM读取的影响"></a>C.Stealth-Persist对NVM读取的影响</h2><p>在这一节中，我们展示了使用Stealth-Persist方法减少发送到NVM的读取次数。当镜像区域的命中率很高， 大部分的读取都由镜像区域提供<br>这就减少了发送到NVM的读取数量。图9显示，与Optane DC AD模式( 100%)相比,使用Stealth-Persist FTP和MQ方法，NVM的读取次数平均减少了 88.28%和73.28%。对于SEQWR和W:ECHO基准来说，使用Stealth-Perist FTP显示出最高的性能改进，NVM读数分别明显减少了98.42%和98.02%。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0491934cb8024507bcb52dedff4c7835.png"></p><p>如图10所示，Stealth-Persist万案对写入NVM的次数没有任何影响。然而,Stealth-Persist也会将镜像页的写入发送到DRAM中。因此，Stealth-Persist不影响NVM的写入持久性，也不增加能耗，这可能是由于增加NVM的写入量造成的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cf110ec94f8d4ebeace793e8ad7a01e1.png"></p><h2 id="E-敏感度分析"><a href="#E-敏感度分析" class="headerlink" title="E.敏感度分析"></a>E.敏感度分析</h2><p>尽管Stealth-Persist FTP和MQ与基线( OptaneDC AD模式)相比，平均提高了42.02%和30.9%的性能,但仍有改进的余地，因为镜像区域的命中率平均为57.81%和24.78%。错过的原因有很多，但主要是受Stealth-Persist设计中的镜像区域大小和镜像阈值的影响然而增加镜像区域的大小将增加硬件的复杂性( 镜像表的大小),而减少镜像阈值可能会导致提前替换所需的页面，这可能会导致会降低整体性能。为了充分分析镜像区域大小和镜像阈值的影响，我们在本节中改变了镜像区域大小和镜像阈值。此外，我们还展示了快速和慢速NVM.上的性能改进。所<br>有工作负载的平均值显示在敏感性结果中。</p><p>1)镜像区域对性能的影响。在DRAM中可以被镜像的持久性页面的数量取决于为镜像保留的DRAM内存的百分比。为了避免大量的内存开销， Stealth-Persist只保留了32MB的DRAM，也就是模拟系统中DRAM的3.125%，用于镜像持久性内存页。然而，正如前面所讨论的， 在使用Stealth-Persist时，可以镜像的页面越多，系统性能的上限就越大。因此，我们将镜像区域的大小从2MB改为1GB，以评估Stealth-Persist的性能改进。请注意，当镜像区域的大小为1GB时整个DRAM都被保留用于缓存镜像页面。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/43bef6e28284442c84f9a398b2558af4.png"></p><p>图11显示，增加镜像区域的大小可以提高FTP和MQ的性能。当镜像区域大小从2MB增加到1GB时，Stealth-Perist FTP的性能改善从1.28倍增加到1.83倍，Stealth-Perist MQ从1.14倍增加到1.38倍。由于MQ是一种基于确认的方法，只有当NVM页面被访问的次数超过阈值(4)时才会被镜像，所以在使用Stealth-Persist MQ的64MB镜像区域大小后，改进已经饱和。因此，即使镜像区域的大小增加了， 要镜像的页面数量也受到阈值的限制，因此性能的提高是饱和的。当镜像区域大小为64MB时，使用Stealth-Perist FTP的性能提升为1.48倍，使用Stealth-Perist MQ为1.35倍。另外，正如所断言的那样，ARSWP基准在32MB的镜像大小下没有显示出性能的改善， 在镜像区域大小为64MB、128MB、 256MB、512MB和1GB时，Stealth-Persist FTP分别实现了1.06倍、 1.22倍、 1.75倍、 2.65倍和3.22倍的改善。然而，使用Stealth-Persist MQ，我们观察到没有任何改善，因为ARSWP应用程序的页面没有达到镜像阈值。</p><p>2)镜像阈值水平对性能的影响。在图12中，我们显示了改变镜像的结果。阈值队列水平。当阈值水平降低时，使用Stealth-Perist<br>MQ方法的性能改善会增加。我们观察到，当阈值水平设置为1时，性能提高了1.46倍;当阈值水平为4时，性能提高了1.3倍。当阈值水平降低时，Stealth-Persist表现得很积极，因为有更多的页面被识别为镜像的候选。也就是说，当阈值水平为1时，如果应用程序至少读了2次页面，该页面就被确定为镜像候选。但是，当阈值水平为4时，只有当一个页面被读取至少16次时才会被镜像。因此，通过降低阈值水平实现的性能改善是以增加镜像的页面数量为代价的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6e837cd8fd5146e4bf6af4d90366c21e.png"></p><p>另一方面，由于两个原因，增加阈值水平会损害性能的提高。1) 一个页面在达到阈值水平后被镜像，随着队列水平的增加，应用程序必须更频繁地访问该页面才能被确定为镜像候选。一般来说，这些页面的比例很小，而且它们经常被缓存在处理器中。2)在达到阈值水平后，页面的热度会消失。例如，如果阈值水平被设置为6，一个页面必须被访问至少64次才能被镜像。然而，在访问该页64次后，应用程序可能不再需要访问该页，从而否定了镜像的影响。</p><ol start="3"><li>NVM读/写延迟对性能的影响。尽管NVM的读取延迟与DRAM的读取延迟相当， 但它仍然比DRAM的读取延迟慢。与DRAM相比，NVM的写延时明显受到影响。在将页面从NVM镜像到DRAM的过程中，NVM的读/写延迟至失重要。因此，我们研究了Stealth-Persist对慢速和快速NVM的读/写延迟的影响。我们改变了NVM的读和写延迟，如图13所示。图13将NVM分为4种类型–中等:读写延迟为150ns和500ns，慢:读写延迟为300ns和700ns，非常慢:读写延迟为500ns和900ns，超慢:读写延迟为750和1000ns。随着NVM的读/写延迟的增加，使用Stealth-Persist的性能改善也会增加。对于超慢的NVM，Stealth-Persist在使用FTP和MQ时，性能分别提高了1 .87倍和1.54倍。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/32440afd47dd4fa4a7d9a7960e24f83c.png"></li></ol><h1 id="六、相关工作"><a href="#六、相关工作" class="headerlink" title="六、相关工作"></a>六、相关工作</h1><p>混合内存。最近，很多工作都在探索如何提高混合存储器系统的性能。例如，Ramos等 人[48]提出了一个在混合内存系统中放置<br>页面的方案。该方案使用多队列对页面进行排序，并且只将性能关键的页面迁移到DRAM中。然而，该方案并没有确保数据的持久性，只是专注于将性能矣键的页面放在DRAM中。HetroOs[27]的作者 提出了一个应用程序透明的万案，利用操作系统提供的应用程序的内存使用信息来决定在异构内存系统中放置数据。然而，HetroOs的动机纯粹是为了系统性能，并没有提供持久性的保证。因此，有持久性要求的应用仍将不得不承受高的NVM延迟。Nimble[58]的作者提出了一个方案，重用操作系统的页面跟踪结构，在存储器之间进行页面分层。此外，Nimble提供了一些优化，如透明的巨大页面迁移和多线程页面迁移，与本地Linux系统相比，这导致了40%的性能提升。然而，Nimble改善了内存之间的页面迁移，但并没有确保数据的持久性。Agarwal等 人[10]为混合内存系统中的GPU提出了一个页面放置方案。然而，所提出的方案是根据应用带宽要求在存储器之间迁移页面，没有考虑数据的持久性。Yoon等人[60]设计了一个策略，使DRAM能够在NVM内存中<br>缓存具有高频率的行缓冲区失误的页面。CAMEO[20]、PoM[54]、Mempod[45]和BATMAN[21]讨论 了可能的放松措施，以最大化整体内存带宽。所提出的技术依赖于编译器支持或Linux内核来检测感兴趣的页面。Lim等人[36]和Kommareddy等人[30]探讨了在分解内存系统中把远程页面迁移到本地内存的问题。</p><p>NVM数据的持久性。确保NVM驻留数据的持久性、性能和崩溃的一-致性最近一直是 人们矣注的焦点。例如，Janus[37]通过将后端内存操作分解为较小的超操作， 然后将超操作重叠起来，改善了持久性应用的写入延迟。除了前面提到的NVM库，英特尔的PMDK[9]、REWIND[18]、 NV-Heaps[22]和LSNVMM[25]为程序员提供了基于软件的高级接口，以确保数据的持久性并提供崩溃一致性支持 。基于硬件的</p><h1 id="七、结论"><a href="#七、结论" class="headerlink" title="七、结论"></a>七、结论</h1><p>要提高Hyblid内存系统中的持久性应用的性能，需要在DRAM中缓存NVM常驻数据。然而，將持久性应用的数据缓存在DRAM中会使这些缓存页的持久性失效。确保DRAM缓存页的持久性可以通过DRAM的电源备份来实现。然而，使用电池给DRAM供电是昂贵的，不可靠的，与传统的系统不兼容，而且不环保。因此，我们提出了Stealth-Pert，一种新型的内存控制器，允许在DRAM中 缓存NVM的识别页面，同时保证页面的持久性。通过从DRAM为NVM请求提供服务，Stealth-Perist利用了银行级别的并行性，减少了内存的消耗并带来了额外的性能提升。Steallh-Persit使 用StealthPersiFfP，在混合内存系统中， 每项应用的系统性能平均提高42.02%。然而，Stealth-PersiFfP需要将大量的页面从虚拟机复制到DRAM中。通过Steallb-PersitMQ方法，我们在合理的页面min-or下，性能提高了30.09%。Tealthisl通过商场硬件管理表、U1e内存控制器中的商场缓存以及利用WPQ来实现这一改进。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA 2022属于CCF-A</li><li>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Haikun Liu, Renshan Liu, Xiaofei Liao, Hai Jin, and Yu Zhang 华中科技大学大数据技术与系统Lab、服务计算技术与系统Lab、集群与网格计算Lab</li><li>Bingsheng He 新加坡国立大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>充分利用NVM和DRAM混合系统的优势，主要目标是将应用程序数据正确放置在混合存储器上。</p><p>观察发现1：每个页面中的热数据只占应用程序总内存足迹的一小部分，但却导致了应用程序总内存引用的很大比例。X轴显示了应用程序所有内存页中频繁访问的数据流量的百分比(按访问频率降序排列)，Y轴显示了应用程序的总内存引用的累积分布。对于soplex来说，近35%的热数据占了总内存引用的98%。这意味着迁移soplex中的部分热数据(变量和对象)比迁移整个页面更有好处。然而，对于一些应用程序，如dict，内存访问均匀地分布在应用程序的地址空间中，因此从数据迁移中获益较少。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4168f8514f9242c3981f5314d9399b78.png"></p><p>观察发现2：很大一部分的应用程序的对象都比页小得多。这些应用程序的对象大小的分布函数。我们可以发现对于dict、isort、gcc和maxMatch来说，95%以上的对象都小于50字节。对于soplex，小对象的比例也超过85%。由于大多数应用层面的内存引用是对象和变量。程序员可以充分利用应用语义来优化对象粒度的数据放置/迁移。相反，一个页面可以包含许多对象，这些对象可能有不同的访问行为，合成的页面级访问统计可能变得复杂和不可预测。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/91c7474aaf8b47afb3816ffc63ed1b4c.png"></p><p>观察发现3：很大一部分应用程序的对象显示出较长的寿命。虽然以前的研究表明，许多程序的对象往往有一个相对较短的寿命[15]，[22], [23], 我们发现，在某些情况下，仍然有很大一部分长寿命的对象。图3显示了一些应用程序的对象寿命的累积分布函数。我们发现在gcc、isort和soplex中几乎所有的对象都有超过1000秒的寿命。特别是，gcc中所有对象的寿命都等于应用程序的总执行时间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9618ef5f5ec84e2c83d4519155af6982.png"></p><p>观察发现4：一部分应用对象在不同的执行阶段表现出高度突变的内存访问频率。 尽管许多对象在其整个生命周期中表现出相对一致的访问行为（即所谓的不可改变的对象）， 但仍有很大一部分对象在不同的执行阶段表现出高度突变的访问频率（即所谓的易变对象），如表1所示。这意味着仍有很大的空间可以通过运行时的对象迁移来优化数据放置。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ce50f76d94ed4edc96bb02efd8325a12.png"></p><p>总之，观察结果1和2清楚地表明，数据访问监控和内存分配/迁移应该是在对象上，而不是在页面上。观察结果3和4表明，在混合内存系统中需要对对象迁移进行仔细的设计。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>提出混合内存系统编程接口，<strong>对象粒度迁移，减少迁移开销</strong>。减少系统耗能，同时提高应用性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的研究主要集中在页面迁移方案上，以实现更高的性能和能源效率。但是，这些方案都依赖于在线页面访问监控成本高。并且还会由于多核时维护缓存/TLB一致性和dram带宽争用产生更多页粒度迁移的开销。</p><p>依赖于页面访问的recency and frequency来决定数据在DRAM或NVM上的位置。1)一些用硬件辅助页迁移的方案由于目前硬件不支持页面访问技术，需要对硬件架构进行大量修改[5][6][11]。2)用操作系统监控内存访问只能引用1个访问位不足以表达页面冷热度[12][Thermostat13]。3)页面迁移通常<span class="label primary">需要一段时间来检测热页</span>。4)<span class="label primary">预测的页面访问模式可能与未来的访问行为不一致</span>，导致不必要的页面迁移。5)大页（huge page）已被越来越多地用于大数据应用和虚拟化平台[13][14]，由于对DRAM容量和带宽的低效利用，粗粒度的页面迁移甚至会降低系统性能。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/257cf52418304fce863269f59094a936.png"></p><p>1）首先静态分析对象被引用的次数。使用LLVM从源代码中生成中间代表（IR） 。为了计算对象级的内存引用，我们利用LLVM的PASS来遍历所有的加载/存储/分配/调用/映射指令，并在IR中插入探针。运行由修改后的IR生成的可执行文件，并通过LLVM PASS自动收集所有对象的访问信息到一个跟踪文件中。内存跟踪包含5个项目，包括”内存访问类型、虚拟地址、对象名称、对象类型和访问时间”。通过跟踪，我们很容易计算出对象的总内存引用和寿命。</p><p>2）<span class="label primary">然而，只有一部分指令导致了对主内存的实际数据流量。原因是片上高速缓存可以过滤大量的内存访问热数据。</span> 于是设计了缓存模拟器，将LLVM生成的跟踪文件作为输入，并过滤程序中所有在模拟缓存中被击中的虚拟地址。剩余的虚拟地址反映了高速缓存的缺失或高速缓存的驱逐操作，并导致对主内存的实际内存访问。图5显示了不同应用程序对高速缓存和主内存的所有数据访问的分布。我们发现，片上高速缓存平均能够过滤68%的总内存引用。对于gcc，甚至87%的数据访问都被过滤掉了。因此，当考虑到缓存过滤的影响时，应用程序的内存访问模式可能会非常不同。一些非常热的数据可能总是在高速缓存中被命中，而只导致对主内存的极少量的数据访问。因此，没有必要将这些数据从NVM迁移到DRAM上。通过我们的缓存过滤器，我们可以在NVM上获得真实的内存访问统计，并避免在运行时进行不必要的对象迁移。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ff01b08a245141909a3c4b67a028325b.png"></p><div class="note info no-icon">但问题是片上缓存也不是缓存的对象啊？</div> <p>3）建模决定数据效用（就是一个该不该放DRAM的权重吧）。提出了一个效用函数（延迟和能耗）来计算在给定的时间段 $T_i$内将对象放在DRAM或NVM上的好处<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3c7f6307e86e457ca5419d132398b9a5.png"></p><p>4）程序执行期间的热对象迁移工作。因为之前说了有易变对象和不易变对象（指热度）。将对象的内存访问模式分为不同的阶段，设定阈值E，在每个时隙去计算当前这个对象效用值，并且判断是不是不可变对象。对于不可变的对象，如果其平均效用大于E，这些对象将被放置在DRAM上。对于突变的对象，其第一阶段的平均效用决定了该对象应该被放置的位置，而对象的迁移应该在一个阶段发生变化时进行。在图6中，对象A、B和D首先应该被分配到DRAM上。对象C一开始应该被分配在NVM上，然后应该被迁移到DRAM上。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c40ea2c19e95465ea086d09c0534c9d7.png"></p><p>5）初始对象分配的实现。根据效用值的阈值、当前DRAM容量这些情况来决定分配的层级的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8b3058d3eb274687b9c3d8c01ea24884.png"></p><p>扩展了Glibc库，为混合内存系统提供DRAM分配和NVM分配API。因此我们修改了Linux内核，在虚拟地址空间（VMA）中将NVM页与DRAM页区分开来。因此，我们在逻辑上将主内存分为两个区域。我们在VMA中标记NVM页，并为NVM分配提供一个新的分支NVM mmap。</p><p>6）运行时对象迁移的实现。采用静态代码工具，在应用程序的源代码中添加对象迁移指令。在运行时，应用程序本身执行对象迁移，没有任何操作系统的干预。我们观察到，<span class="label primary">大量密集的内存访问主要归因于循环状态[25]</span>。我们用循环作为断点，将源代码分成几个代码片段。首先，我们用LLVM跟踪对象的访问次数以及循环开始和结束时的时间戳。对于每个对象，我们在循环的执行过程中计算其效用值。循环开始和结束时效用值的突然变化意味着内存访问模式发生了变化，该对象应该在循环之前被迁移到另一种内存。我们用时间戳来定位这个特定的循环状态。最后，我们使用LLVM在循环的开头插入对象迁移语句。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1c528b5e931d43f2a379af33b9eb4d8a.png"></p><p>图7显示了一个混合内存分配和对象迁移的代码工具化的例子。我们根据LLVM中的抽象语法树（AST），以静态单一赋值（SSA）的形式遍历所有对象，并用DyMalloc改变源代码中的所有malloc。我们使用shared_ptr创建对象，这是一个智能指针，通过指针保留对象的共享所有关系。这允许几个shared_ptr对象共享同一个对象的内存。shared_ ptr使用一个控制块来记录所有指向目标对象的指针和总引用的数量。例如，在图8中，指向对象3的两个指针被记录在控制块中。只有当引用计数器变为零时，被管理对象才会被销毁。对于可改变的对象，我们使用MigrateToX API将对象复制到另一种内存中，然后所有存储的指向被管理对象的指针现在应该指向新的内存地址，如图8所示。对象迁移指令被插入到一个适当的地方（很可能是在循环语句之前）， 在那里对象的访问模式的变化。通过这种方式，我们在迁移后改变对象的虚拟地址，并在运行时更新对对象的所有引用。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d25170131ab4c1b87f596593e283a87.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>“有效的静态内存分配：在于2pp执行时间和性能差不多的情况下，与只使用DRAM的系统相比，””OAM w/o migration “”能够平均减少51%的内存能量消耗。这些结果表明，我们最初的OAM数据放置策略对提高混合内存系统的性能和能源效率是有效的。</p><p>在线内存迁移的有效性：与静态内存分配方案相比，对象迁移可以进一步提高应用性能，平均提高11%。没有迁移的OAM平均可以实现51%的EDP减少，而有迁移的OAM平均可以进一步减少10%的EDP</p><p>与一些页面迁移算法相比较：与CLOCK-DWF和2PP相比，OAM可以显著减少迁移流量，平均分别为42%和22%。开销也比他们都小。</p><p>适应不同数据和规模：执行时间都比不使用该方案要节省时间。</p><p>对不同NVM性能的敏感性：测试了目前的产品，对于Optance是由读密集引起的迁移，因为写延迟是差不多的。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>只为用C++编写的应用程序提供对象级迁移接口。<br>动态迁移都集中在循环。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>提供了一个离线剖析工具来详细描述应用程序的内存访问模式，并提出了一个<strong>性能/能源模型</strong>来指导应用程序对象的初始内存分配和动态迁移<strong>从能源消耗上解决了读写不均衡</strong>，而不需要任何硬件修改和操作系统干预的在线内存监控。</p><p>一个静态代码工具，用于自动转换应用程序源代码中的对象级内存分配和迁移，而不会给应用程序的程序员带来负担。</p><p>运行时监控和以往的想法都不同，也因此监控开销更小。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> 细粒度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/05/01/hello-world/"/>
      <url>/2023/05/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-Create-a-new-post"><a href="#1-Create-a-new-post" class="headerlink" title="1.Create a new post"></a>1.Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行命令会在/source/_posts下创建新文章，之后需要使用MarkDown语法编写该文章。  </p><p><code>---</code>包括起来的内容称之为<code>Front-matter</code>有很多配置选项可以添加。<br>更多的简单语法可以参考<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程</a><br>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="2-清除旧数据"><a href="#2-清除旧数据" class="headerlink" title="2.清除旧数据"></a>2.清除旧数据</h3><p>文章写好之后，首先清除掉旧的数据</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo clean <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会清除掉之前生成的网页，即站点根目录下的public文件夹</p><h3 id="3-Generate-static-files"><a href="#3-Generate-static-files" class="headerlink" title="3.Generate static files"></a>3.Generate static files</h3><p>然后使用如下命令生成新的页面：More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate 或者简写 hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会将source文件夹下所有的md文件进行渲染，生成HTML页面，存放在public文件夹下</p><h3 id="4-Run-server"><a href="#4-Run-server" class="headerlink" title="4.Run server"></a>4.Run server</h3><p>在本地开启服务器，预览一下文章是否满意</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server  <span class="token string">'hexo s'</span> <span class="token keyword">for</span> short<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="5-Deploy-to-remote-sites"><a href="#5-Deploy-to-remote-sites" class="headerlink" title="5.Deploy to remote sites"></a>5.Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy <span class="token string">'hexo d'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="Blog-Template"><a href="#Blog-Template" class="headerlink" title="Blog Template"></a>Blog Template</h2><p>更高阶更详尽的Hexo Markdown教程参考<a href="https://blog.17lai.site/posts/cf0f47fd/#%E5%B8%B8%E7%94%A8%E6%A0%87%E8%AE%B0">夜法之书的博客</a><br>一些可以用到的LeTax数学公式编辑方式<a href="http://t.csdn.cn/VivVj">超详细 LaTex数学公式</a> || <a href="http://t.csdn.cn/iPVFt">LaTeX数学公式-详细教程</a></p><p>图床就是用csdn了 <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 还有好多文章慢慢搬运过来，不急。<br>但是需要在每个csdn的图片的链接上加上<a href="https://images.weserv.nl/?url=%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5LRU%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%89%8D%E7%BC%80%E3%80%82">https://images.weserv.nl/?url=真正的图片链接LRU，这个前缀。</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span>https://images.weserv.nl/?url<span class="token operator">=</span><span class="token punctuation">)</span><span class="token punctuation">{</span>% raw %<span class="token punctuation">}</span>在这之间写LaTex或者其他造成的符号转义冲突之类的报错<span class="token punctuation">{</span>% endraw %<span class="token punctuation">}</span>toc: <span class="token boolean">true</span>mathjax: <span class="token boolean">true</span>hide: <span class="token boolean">false</span>categories: Paper<span class="token comment"># password: 4dea5c7cb70f50322ec9d734aa4aa078be9227c05251e18991c596f387552370</span>tags:  - A - Hybrid Memory Systemsimg: https://images.weserv.nl/?url<span class="token operator">=</span>summary: 。---<span class="token comment">## 1. 论文信息</span><span class="token operator">&lt;</span>div <span class="token assign-left variable">class</span><span class="token operator">=</span><span class="token string">"note primary"</span><span class="token operator">&gt;</span>- 文章来自IEEE International Symposium on High-Performance Computer Architecture, <span class="token punctuation">(</span>HPCA<span class="token punctuation">)</span>, <span class="token number">2022</span>- 名字<span class="token operator">&lt;</span>/div<span class="token operator">&gt;</span> <span class="token comment">### 所有作者及单位</span> - A, 佛罗里达国际大学<span class="token punctuation">(</span>FIU<span class="token punctuation">)</span><span class="token comment">## 2. Background</span><span class="token comment">## 3. 解决了什么问题</span><span class="token comment">## 4. 其他学者解决这个问题的思路和缺陷</span><span class="token comment">## 5. 围绕该问题作者如何构建解决思路</span><span class="token comment">## 6. 从结果看，作者如何有力证明他解决了问题</span><span class="token comment">## 7. 缺陷和改进思路</span><span class="token comment">## 8. 创新点</span><span class="token comment">## 9. 积累</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个标签结束后必须空一行:</p><div class="note info">这里是 info 标签样式</div> <div class="note info no-icon">这里是不带符号的 info 标签样式</div> <div class="note primary">这里是 primary 标签样式</div> <div class="note primary no-icon">这里是不带符号的 primary 标签样式</div> <div class="note warning">这里是 warning 标签样式</div> <div class="note warning no-icon">这里是不带符号的 warning 标签样式</div> <div class="note danger">这里是 danger 标签样式</div> <div class="note danger no-icon">这里是不带符号的 danger 标签样式</div><p>然后是行内标签，比加粗更能显示重点，Fulid移植的。<br><span class="label primary">Label primary</span></p><p><span class="label default">Label default</span></p><p><span class="label info">Label info</span></p><p><span class="label success">Label success</span></p><p><span class="label warning">Label warning</span></p><p><span class="label danger">Label danger</span></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>this is a test for my first blog</title>
      <link href="/2023/04/30/this-is-a-test-for-my-first-blog/"/>
      <url>/2023/04/30/this-is-a-test-for-my-first-blog/</url>
      
        <content type="html"><![CDATA[<p><strong>詹青云2018年华语辩论世界杯决赛结辩</strong></p><p>大家好，我们今天和对方有三个根本的分歧。一是成功路径不同。我方承认，聚焦没有问题。如果一个年轻人在年轻的时代完全知道一生要什么，一生走下去从不后悔。没问题，挺幸福的。可是现实是，这个决定对于大多数人来说不应该在青年时代做。这个时候你的大脑没有发育完全、你的人生还在不停地变动、你的智识还有限，而这个世界在飞快的变化。很有可能你想要聚焦的东西有一天是你不适应、不喜欢或者被时代淘汰的东西。</p><p>这时候您方跳到了第二点告诉我说没关系，我只要坚定自己的内心我就没有问题了。这就是我们双方第二点分歧：幸福观的不同。您方的幸福观是一种妥协的幸福观，而我放的幸福观是进取的幸福观。您方的意思是不管我人生发挥得怎样，社会如何对待我，不用在乎！我妥协、我看开、我豁达，就可以幸福。对方辩友，那些历史上真正收获了豁达心态的人，杨慎“是非成败转头空”，王维“行到水穷处，坐看云起时”的时候，他们是在什么时候收获这种豁达，是在遍历人生的沧桑，经历了繁华，经过了奋斗，见识了人世中更深刻的道理，他可以领悟到繁华。就算我退一步，俗一点讲，我多读一点书，多看一点世界，对这个世界的理解和思考方式丰富一点，这种做加法的方法您才能收获真的豁达。</p><p>最后我们双方最根本的分歧是对这个时代理解不同。您告诉我说这个世界纷繁复杂，已经把太多选择推到年轻人的面前，所以我选择加就是在随大流。不是。我们仔细想一想，这个时代给我们多的选择不过是您方说的商品、营销课、成功学。可是人生加减法上，那些人生重大关头的选择是什么，这个社会真的在逼我们做加法吗？不是。我到了这个年纪就应该结婚生子，成家立业。在人生重大关头的选择上，这个社会是要求青年人割舍那些不切实际的幻想，割去那些错误的观念，回归一套社会范式，一套人生范式，是要求你做减法的。</p><p>这个时候真正追随自己内心是应该不顾这套范式的束缚，冲破束缚去追寻自己心中所爱，活出一个真正多元的世界。更重要的是，我们今天不是在替一个年轻人的幸福说话，是一代青年人。青年人拓宽人生边界的可能是在拓宽这个社会价值判断的可能。</p><p>既然这个世界号称它是多元而包容的，我们就应该去试，去让他实现这个诺言。</p><p>既然这个社会多元而包容，既然这个世界告诉我们“人不轻狂枉少年”，就没有人应该天然地觉得“轻狂”是一个贬义词。对方辩友一直在劝我们：人生选到自己最幸福的东西才是快乐的。对方辩友，各位，我们都是年轻人。在我们人生的这个阶段，有什么东西是唯一珍贵的？什么叫“欲买桂花同载酒，终不似，少年游”，什么叫“旧游无处不堪寻。无寻处，惟有少年心”。那个唯一带不走的东西，不就是青春本身吗？这一份机会你不珍惜，这一份可能你不珍惜，您跟我谈的是什么？是那一份安顿了的幸福，是那一份成熟了的幸福。可是这不是年轻人的幸福。因为年轻人不是在替你一个人谋幸福，不是你一个人看开了就可以。他是要为这个世界拓宽边界，是让所有的人都有机会把道路越走越宽，是</p><p>趁着年轻，我偏要勉强。</p><p>谢谢大家。</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perspective </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts, Practices, and Future Direction</title>
      <link href="/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/"/>
      <url>/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>虽然这篇的有些引用也是十多年前的数据，但是作为学习一个阶段的总结，和大佬对比一下在知识结构上的完整度，还有什么是不清楚的。还是挺有用的。</p></blockquote><div class="note primary"><ul><li>文章来自Journal of Computer Science and Technology, (JCST), 2021</li><li>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts,Practices, and Future Direction</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hai-Kun Liu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Di Chen, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Hai Jin, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Xiao-Fei Liao, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Binsheng He, 新加坡国立大学</li><li>Kan Hu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Yu Zhang, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li></ul><hr><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>非易失性主存储器（NVMM）最近已成为未来存储系统的一种有前途的技术。通常，NVMM具有许多理想的属性，例如高密度、字节寻址、非易失性、低成本和能效，但代价是高写入延迟、高写入功耗和有限的写入耐用性（写寿命短）。NVMM已经成为动态随机存取存储器（DRAM）的强有力的替代品，并将从根本上改变内存系统的格局。它们在系统架构设计、操作系统内存管理以及混合内存系统的编程模型方面带来了许多研究机会和挑战。在本文中，我们首先回顾了新兴NVMM技术的概况，然后对NVMM技术的最新研究进展进行了综述。我们根据不同的维度（如内存架构、数据持久性、性能改进、节能和磨损均衡）对这些研究进行分类。其次，为了展示构建NVMM系统的最佳实践，我们从架构、系统和应用的维度介绍了我们最近的混合存储系统设计工作。最后，我们对NVMM的未来研究方向提出了展望，并对设计挑战和机遇进行了阐述。</p><p><strong>关键词</strong>：非易失性存储器、持久性存储器、混合存储器系统、存储器层次结构<br>non-volatile memory, persistent memory, hybrid memory systems, memory hierarchy</p><hr><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在大数据时代，内存计算越来越受到数据密集型应用的青睐。内存子系统对现代计算系统的功能和性能具有越来越大的影响。使用DRAM (动态随机存取存储器)的传统大内存系统[1 ,2]在功率和密度方面面临严峻的可扩展性挑战[3]。尽管DRAM规模从2013年的28nm持续到2016年的10+ nm[4，5] ，扩展已经放缓，变得越来越困难。此外,最近的研究[6-10]表明，基于DRAM的主存储器约占物理服务器总能耗的30%-40%。新兴的非易失性主存储器（NVMM）技术，如相变存储器（PCM）、自旋转移扭矩存储器（STT-RAM）和3D XPoint[11]通常提供比DRAM更高的内存密度、更低的每的比特成本和待机功耗。NVMM技术的出现有可能弥合慢速持久存储（即磁盘和SSD）与DRAM之间的差距，并将从根本上改变存储系统的格局。</p><p>表1显示了闪存SSD、DRAM、PCM、STT-RAM、ReRAM和Intel Optane DC持久内存模块（DCPMM）的不同内存特点，包括读/写延迟、写耐久性和待机功耗[7，12，13]。尽管NVMM在密度和能耗方面具有各种优势，但其写入延迟比DRAM高约6倍-30倍， 写入功耗比DRAM高约5倍- 10倍。此外，NVMM的写入耐久性非常有限(约 $10^8$ 倍) ，而DRAM能够承受约 $10^{16}$ 次的写入操作。这些缺点使得很难直接替代DRAM。使用NVMM的一种更实用的方法是混合内存架构，由DRAM和NVMM组成[15,16]。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>为了充分发挥两者在混合内存系统中的优势，在性能提升、节能降耗、磨损均衡、数据持久性等方面存在许多开放的研究问题。为了解决这些问题，已经有许多关于内存层次结构的设计[15-18]、内存管理[19-21]和内存分配方案[22-24]的研究。这些研究成果导致了混合内存架构、操作系统和编程模型的创新。尽管学术界和工业界已经做出了大量工作来将新兴NVMM集成到存储器层次结构中，但仍有许多挑战需要解决。</p><p>另一方面，先前对NVMM技术的研究大多基于模拟/仿真NVMM器件，与真正非易失性（双列直插式内存模块Dual In-line Memory Modules）DIMM相比，NVMM设备承诺的性能可能存在各种偏差。最近宣布推出的Intel Optane DCPMM终于将NVMM DIMM商业化。真正的Intel Optane DCPMM与之前的研究预期承诺的功能相比，表现明显不同[18，20，26，28]。例如，如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3GB/s，而DRAM的读写带宽之间的差距要小得多（1.3x）。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。由于Optane DCPMM DIMM的这些关键特性，以前关于持久性内存系统的研究应该重新审视和优化，以适应真正的NVMM DIMM。</p><p><strong>贡献</strong>。本文首先回顾关于混合内存架构、操作系统级混合内存管理和混合内存编程模型的最新研究现状。表2显示了NVMM技术的最新研究分类。我们根据不同维度对这些研究进行分类，包括内存架构、持久内存(PM)管理、性能改进、节能、磨损均衡、编程模型和应用程序。我们还讨论了它们的相似性和差异，以突出设计挑战和机遇。其次，为了展示构建NVMM系统的最佳实践，我们<strong>从架构、系统和应用的维度展示了我们在混合存储系统设计方面的努力</strong>。最后，我们提出了在实际应用场景中使用NVMM的未来研究方向，并对研究领域的设计挑战和机遇进行了一些说明。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9239e045ee11414cb228f4b752b13f86.png" alt="将相关工作分类"></p><p>尽管有其他关于NVMM的研究，但鉴于NVMM的快速发展，这篇综述从一个独特的角度对NVMM进行了回顾。在[97]中, 作者介绍了PCM技术的最新研究。以解决有限的写入耐久性、潜在的长延迟、高能量写入、功耗等问题以及一些对内存隐私的担忧。在[98]中，作者对PCM设备及其架构和软件进行了全面的调查和回顾。其他一些有趣的调查侧重于在架构上将四种NVM技术(PCM、MRAM、FeRAM和ReRAM )集成到现有存储层次结构[99]中，或将NVM用于存储和主存储器系统的软件优化[100]。我们的调查与那些调查有三个不同之处。首先，先前的研究[97 ,98]从计算机架构的角度关注PCM设计。相比之下，我们的论文主要从存储器层次、系统软件和应用的维度来研究使用混合存储器的系统。其次，我们的论文包含了更多新发表的期刊、会议论文的评论。特别是，我们对新发布的Intel Optane DCPMM设备进行了更多研究。第三，我们介绍了最近关于存储器系统的近期经验，以阐明未来混合存储器系统的挑战和机遇。</p><p>本文的其余部分组织如下。第2节描述了现有的由DRAM和NVMM组成的混合内存架构。第3节介绍了NVMM中数据持久性保证的挑战和当前解决方案。4节介绍了混合存储器系统中性能优化和节能的最新研究。第5节介绍NVMM写耐久性的研究。第6节介绍了研究NVMM技术所做的努力和实践。在第7节中，我们讨论了NVMM的未来研究方向。我们在第8节结束本文。</p><h1 id="2-Hybrid-Memory-Architectures混合内存架构"><a href="#2-Hybrid-Memory-Architectures混合内存架构" class="headerlink" title="2. Hybrid Memory Architectures混合内存架构"></a>2. Hybrid Memory Architectures混合内存架构</h1><p>已经有很多关于混合存储器架构的研究。通常，主要有两种混合存储器架构，即水平和分层[18]，如图1所示。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e565e90afda244f6a8dcfbe3081068c1.png" alt="常见的架构，最近华科的工作也有混合的架构出现"></p><h2 id="2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构"><a href="#2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构" class="headerlink" title="2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构"></a>2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构</h2><p>许多DRAM-NVMM混合存储器系统[14 ,15 ,31]通过OS在平面(单个)存储器地址空间中管理DRAM和NVMM，并将它们两者用作主存储器[31 ,32]。为了提高数据访问性能，这些混合内存系统需要通过将频繁访问的热NVMM页面迁移到DRAM来克服NVMM的缺点，如图1（a）所示。需要开发内存访问监控机制来指导页面迁移。</p><p><strong>内存访问监控</strong>。Zhang和Li[31]使用多队列算法对页面的热度进行分类，并将热页面和冷页面分别放置在DRAM和NVMM中。Park等人[32]也主张用于管理DRAM和NVMM的水平混合存储架构。此外，他们还提出了三种优化策略来降低混合存储系统的能耗。他们以非常细的DRAM行粒度监视内存数据，并定期检查每个DRAM行中的访问计数器。根据计数器将数据写回NVMM，以减少DRAM刷新的能耗。在再次访问数据之前，数据不会从NVMM缓存到DRAM。脏数据尽可能长时间地保存在DRAM中，以减少DRAM和NVMM之间的数据交换开销以及NVMM的昂贵写操作。</p><div class="note info no-icon"> 听起来监控成本比较高 </div><p><strong>页面迁移</strong>。针对不同的优化目标，已经提出了许多页面迁移算法。Soyoon等人[33]认为，在识别热页时，NVMM写入的频率比数据访问的最近性更重要，并提出了一种称为CLOCK with Dirty bits and Write frequency（CLOCK-DWF）的页替换算法。对于每个NVMM写入操作, CLOCK-DWF需要首先将相应的页面提取到DRAM，然后在DRAM中执行写入。这种方法可能会导致许多不必要的页面迁移，从而给NVMM带来更多的能耗和写回操作。Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面，并使用两个最近最少使用的(LRU)队列分别选择DRAM和NVM中的要被驱逐的页面。Yoon[17]等人基于行缓冲区局部性进行了页面迁移，其中行缓冲区命中率低的页面被迁移到DRAM，而行缓冲区点击率高的页面仍保留在NVMM中。Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性。Khouzani[35]等人考虑了程序的内存布局和内存级并行性，以迁移混合内存系统中的页面。</p><p><strong>架构限制</strong>。在水平混合存储器架构中管理NVMM和DRAM有几个挑战。</p><p>首先,<span class="label primary">页面级内存监控成本高昂</span>。一方面，由于当今的商品x86系统不支持页面粒度的内存访问监控，因此硬件支持的页面迁移方案需要大量的硬件修改来监控内存访问统计[14,15,33]。另一方面, OS层的内存访问监控通常会导致显著的性能开销。许多操作系统在页面表条目(PTE)中为每个页面维护一个“已访问”位，以标识该页面是否被访问。然而，该位不能真实地反映页面访问的最近性和频率。因此，一些基于软件的方法将禁用Translation Lookaside Buffer（TLB）[102]来跟踪每个内存引用。这种页面访问监控机制通常会导致显著的性能开销，甚至抵消混合内存系统中页面迁移的好处。</p><p>第二，页面迁移成本也很高。<span class="label primary">一次页迁移可能导致多次页读/写操作（代价高昂）</span>。页面可能只包含一小部分热数据，因此由于内存带宽和DRAM容量的浪费，页面粒度的迁移成本相对较高。</p><p>第三，热页面检测机制可能需要很长时间来预热页面，从而降低页面迁移的收益。此外，<span class="label primary">对于某些不规则的内存访问模式，热页面预测可能不准确，从而导致不必要的页面迁移</span>。</p><h2 id="2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构"><a href="#2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构" class="headerlink" title="2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构"></a>2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构</h2><p>许多研究建议通过分层的缓存-内存架构来组织DRAM和NVMM[16,38,39]。他们使用DRAM作为NVMM的缓存，如图1（b）所示。DRAM缓存对操作系统和应用程序是不可见的，完全由硬件管理。</p><p>Qureshi等人[16]提出了一种由大尺寸PCM和小尺寸DRAM组成的分层混合存储系统。DRAM缓存包含最近访问的数据，以减少最昂贵的NVMM访问，而大容量NVMM存储大部分所需数据，以避免在应用程序执行期间进行昂贵的I/O操作。类似地，Mladenov[38]设计了一个具有小容量DRAM缓存和大容量NVMM的混合存储系统，并基于应用程序数据的空间局部性对其进行管理。DRAM作为按需缓存进行管理，并通过LRU算法进行替换。Loh和Hill[39]以缓存行的粒度管理DRAM，以提高DRAM缓存的效率，并使用组连接方式将NVMM数据映射到DRAM缓存。他们将元数据tag和数据放在同一个存储行中，以便可以快速访问缓存命中的数据，并减少标记查询的性能开销。</p><div class="note info no-icon"> 缓存是得把这个DRAM100%用起来的，水平就不好说了 </div><p>在这种内存结构中，由于DRAM被组织为N路集合关联缓存，因此需要额外的硬件来管理DRAM缓存。例如，需要SRAM存储器来存储DRAM高速缓存中数据块的元数据tag，并且需要硬件查找电路来查找DRAM高速缓冲存储器中所请求的数据。因此，为了访问DRAM缓存中的数据，需要两个内存引用，一个用于访问元数据，另一个用于实际数据。为了加速元数据访问，Qureshi[16]等人使用了高速SRAM来存储元数据。Meza等人[40]通过将元数据放在同一DRAM行中的数据块旁边，降低了标记存储的硬件成本。他们还建议使用片上元数据缓冲区将频繁访问的元数据缓存在小型SRAM中。</p><p><strong>架构限制</strong>。尽管分层混合存储器架构通常比单独访问NVMM中的数据的场景提供更好的性能，但在运行具有较差局部性的工作负载时，它可能会导致性能显著下降[103]。原因是大多数硬件管理的分层DRAM-NVMM系统为了简化而利用基于按需的数据预取策略，因此DRAM缓存位于存储器分层的关键数据路径中。如果数据块未命中DRAM缓存，则无论页面热度如何，都必须将其从NVMM提取到DRAM。这种缓存填充策略可能会导致DRAM和NVMM之间频繁的数据交换（类似于缓存抖动问题）。另一方面，硬件管理的缓存架构不能充分利用DRAM容量。由于DRAM缓存被设置为关联的，因此每个NVMM数据块被映射到一个固定的集合。当集合已满时,它必须在将新的NVMM数据块提取到DRAM之前驱逐数据块，即使其他缓存集合为空。</p><h2 id="2-3-Intel-Optane-DCPMM的体系结构"><a href="#2-3-Intel-Optane-DCPMM的体系结构" class="headerlink" title="2.3 Intel Optane DCPMM的体系结构"></a>2.3 Intel Optane DCPMM的体系结构</h2><p>最近发布的Intel Optane DCPMM与DRAM结合使用时支持水平和分层混合内存结构。OptaneDCPMM DIMM目前有两种操作模式:内存模式和应用程序直接模式[25]。这些模式中的每一种对于特定的用例都有其优点。</p><p><strong>内存模式</strong>。在这种模式下，DCPMM充当<strong>大容量的主存储器</strong>。操作系统将DCPMM识别为传统DRAM，<strong>并禁用DCPMM的持久性功能</strong>。如果将传统DRAM与DCPMM结合使用，它将隐藏在操作系统中，并充当DCPMM的缓存层。因此，DCPMM和DRAM实际上被组织在分层混合存储器架构中。内存模式的主要优点是提供在内存总线通道上提供优越内存容量。这种模式强烈强调在不修改上层系统的情况下围绕内存空间构建大容量存储环境以及应用程序。推荐的用例是扩展主内存容量，以实现更好的基础设施扩展，例如用于大数据应用程序的并行计算平台（MapReduce、图形计算）。</p><div class="note info no-icon"> 这种用例应该是指DRAM很少很少，PM特别多的，数据仓库那种级别的缓存吧？通常一般服务器的比例拿去做缓存真的很浪费容量咦 </div><p><strong>AD模式</strong>。在这种模式下，DCPMM为操作系统和应用程序提供了所有持久性特性。操作系统将DRAM和DCPMM分别作为主存储器和持久存储向应用程序公开。与DCPMM混合的传统DRAM仍然充当应用程序的标准DRAM，而DCPMM也被分配到存储器总线以实现更快的存储器访问。DCPMM用作两种namespace之一:直接访问存取（DAX）和块存储。前者的namespace是字节可寻址的持久存储，应用程序通过特殊的apis直接访问。因此，DCPMM和DRAM在这种模式下被逻辑地组织在一个水平混合内存架构中。后一个命名空间将DCPMM 作为区块存储设备提供给应用程序，类似于SSD，但是可以通过更快的内存总线访问。应用程序直接模式强调减少延迟和提高带宽的优势，比NVMe快2.7倍。推荐的用例适用于大型内存数据库，这些数据库需要满足数据持久性的要求。</p><p>还有一种结合了内存模式和应用程序直接模式的混合内存模式。DCPMM的一部分容量用于内存模式操作,DCPMM剩余的容量用于应用程序直接模式操作。这种混合内存模式为管理不同应用场景的混合内存系统提供了一种更灵活的方法。</p><div class="note info no-icon"> 这里没有说通过热插拔的dvdax模式，变成易失内存 </div><h2 id="2-4-Summary总结"><a href="#2-4-Summary总结" class="headerlink" title="2.4  Summary总结"></a>2.4  Summary总结</h2><p>上述两种混合存储器架构对于不同的场景有各自的优缺点。<span class="label primary">通常，分层架构更适合具有良好数据局部性的应用程序，而平面可寻址架构更适用于延迟不敏感或占空间较大的应用程序。</span>关于哪种架构比另一种架构更好，目前尚无定论。实际上，Intel Optane DCPMM支持分层和平面可寻址混合内存架构。当前DCPMM的一个限制是，在重新配置DCPMM模式后，系统需要重新启动。如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。</p><h1 id="3-Persistent-Memory-Management-持久内存管理"><a href="#3-Persistent-Memory-Management-持久内存管理" class="headerlink" title="3 Persistent Memory Management 持久内存管理"></a>3 Persistent Memory Management 持久内存管理</h1><p>数据持久性是NVMM的一个重要设计考虑因素。下面，我们首先介绍持久内存（PM）管理所面临的技术挑战，然后介绍有关持久内存管理的最新研究，包括持久内存的使用、持久内存访问模式、容错机制和持久对象。</p><h1 id="3-1-Technical-Challenges-技术挑战"><a href="#3-1-Technical-Challenges-技术挑战" class="headerlink" title="3.1 Technical Challenges 技术挑战"></a>3.1 Technical Challenges 技术挑战</h1><p>在混合内存系统中，NVMM可以在运行应用程序时充当主内存，并在应用程序完成时充当持久存储。NVMM的字节寻址能力和非易失性特性消除了内存和外部存储的区别。然而，当NVMM中的数据需要持久化时，就需要对NVMM中的数据进行重新组织和重新定位。</p><p>图2显示了持久内存（PM）的管理操作。NVMM区域是物理PM设备。NVMM区域可以像DRAM一样用作工作内存，也可以像磁盘一样用作持久存储。当程序完成时，工作内存中的数据应该刷新到持久存储中。此外，为了保证高可靠性，检查点机制被广泛利用来从电源故障或系统崩溃中恢复系统。Gao等人[104]开发了一种利用NVMM在混合内存系统中进行实时检查点的新颖方法。</p><div class="note info no-icon">Checkpointing（检查点）用于在运行过程中定期保存系统的状态和数据快照。这些检查点允许系统在发生故障或需要恢复时，从之前的状态继续执行，而不是从头开始重新计算。</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ba021f22f99b414582492752b9d4377a.png" alt="没看懂这个图"></p><p>有效管理PM面临多项挑战。首先，持久存储以文件系统的形式被广泛管理。作为字节可寻址NVMM提供比传统块设备更好的随机访问性能，基于PM的文件系统的性能瓶颈已经从硬件转移到系统软件堆栈。缩短软件堆栈中的数据路径至关重要。其次，由于许多CPU使用回写式缓存来实现写操作的高性能，因此末级缓存（LLC）可能会改变写回PM的数据顺序。如果出现断电或者系统崩溃的情况，可能会导致数据不一致的问题。因此，为了保证PM中数据的一致性，需要写操作的顺序和写原子性模型来保证PM中数据的一致性。第三，与基于PM的文件系统相比，持久对象和数据结构对于PM编程更有前景，因为它们消除了文件系统中的复杂数据结构，包括i节点、元数据和数据。然而，这些持久对象和数据结构仍然面临着保证数据一致性的挑战。接下来，我们将回顾试图解决这些挑战的研究<br>（这后面就跳过了，大部分在讨论文件系统，不是我目前考虑的，下次一定）</p><h1 id="4-Performance-Improvement-and-Energy-Saving性能改进和节能"><a href="#4-Performance-Improvement-and-Energy-Saving性能改进和节能" class="headerlink" title="4 Performance Improvement and Energy Saving性能改进和节能"></a>4 Performance Improvement and Energy Saving性能改进和节能</h1><p>由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。这些研究可分为三类:减少NVMM写入次数、减少NVM写入本身的能耗以及通过页面迁移减少DRAM的能耗。</p><h2 id="4-1-NVMM-Write-Reduction-NVMM写入减少"><a href="#4-1-NVMM-Write-Reduction-NVMM写入减少" class="headerlink" title="4.1 NVMM Write Reduction NVMM写入减少"></a>4.1 NVMM Write Reduction NVMM写入减少</h2><p>为了减少NVMM写入,分层结构显然更合适,因为DRAM缓存减少了大量NVMM写入。为此,开发了两种主要技术,即页面迁移和绕过NVMM写入。</p><p><strong>页面迁移</strong>。页面迁移[14 ,15 ,33 ,58 ,59]策略主要基于写入次数和每个页面的最近访问频率来选择要迁移的页面。它们的主要区别在于触发页面迁移的条件。</p><p>PDRAM[15]根据写入次数将PCM页迁移到DRAM。在PDRAM中,存储器控制器维护一个表以记录每个PCM页的访问计数。如果写入PCM页的次数超过给定阈值,则触发页故障，然后将该页从PCM页迁移到DRAM。</p><p>CLOCK-DWF[33]将页面的写入历史集成到CLOCK算法中。当发生页面错误时，虚拟页面将从磁盘提取到PCM。否则, 该页将在DRAM中分配,因为该页可能是写密集型页。</p><p>RaPP[14]根据页面的等级在DRAM和PCM之间迁移页面。在RaPP中, 页面按访问频率和最近度排序。排名靠前的页面从PCM迁移到DRAM。因此，频繁写入的页面被放置在DRAM中,而很少写入的页面则被放置在PCM中。此外, RaPP还将任务关键页面放置在DRAM中，以提高应用程序性能。通过监视LLC中每个页面的写回操作的数量,存储器控制器能够跟踪每个页面的访问频率和最近性。RaPP根据多队列（MQ）算法对页面进行排序[118]。传统MQ定义了多个最近最少使用（LRU）队列。每个LRU队列是一个页面描述符队列, 其中包括参考计数器和逻辑过期时间。当第1次访问页面时,页面将移动到队列0的尾部。如果页面的引用计数达到 $2^{i+1}$ ,则提示页面排队i+1。一旦PCM页面被移动到队列5,它就被迁移到DRAM。</p><p><strong>缓冲NVMM写入</strong>。在混合内存系统中, 缓存能够减少对NVMM的大量写入。适当的缓存替换策略不仅可以提高应用程序性能, 还可以降低NVMM的能耗。先前的研究[7,18]发现, 缓存中的许多块在被从缓存中逐出之前不会被再次使用。这些块称为死块，并消耗宝贵的缓存容量。DASCA[7]提出了一种死块预测方法,以减少STT-RAM缓存的能耗。驱逐这些死块将减少对STT-RAM缓存的写入,并且不会影响缓存命中率。WADE[62]进一步利用了NVMM读取和NVMM写入之间的能耗不对称性。由于NVMM写入操作比NVMM读取操作消耗更多的能量,因此频繁写入的块应保留在缓存中。WADE将缓存中的块分为两类:频繁回写的块和非频繁回写块。非频繁写回的块被替换,以提供更多机会将其他数据块保留在缓存中。</p><h2 id="4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低"><a href="#4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低" class="headerlink" title="4.2 NVMM Energy Consumption Reduction NVMM能耗降低"></a>4.2 NVMM Energy Consumption Reduction NVMM能耗降低</h2><p>由于NVMM写入显示的能耗是NVMM读取的能耗的几倍，因此在降低NVMM写入的能耗方面已经做出了许多努力。这些方法可以分为两类:差分写入（仅写入脏位而不是整行）和在单个写入期间并行多个写入。</p><p>如果要写入的位数超过缓存行中总位数的一半，则Flip-N-Write[64]尝试通过翻转位来减少PCM写入能耗。在一次写入期间, 如果行中超过一半的位被写入 ,则每个位被翻转,因此位翻转不超过总位的50%。同时,设置标记位以识别行中的位是否被翻转。当读取行时,标记位用于确定行中的位是否应该翻转。与Flip-N-Write类似, Andrew等人[73]提倡细粒度写入。它只监视脏位而不是一行中的所有位。一个叫做PCM的新术语引入功率令牌以指示单次写入期间的电源。假设为每个芯片分配Plimit Watts功率, 并且每个位写入需要Pbit Watts , Plimitpbit可以同时写入位。在芯片内,可以同时写入bank。在单个写入期间,如果多个写入请求位于不同的存储库中,并且总功耗不超过Plimit ,则可以同时执行这些写入。因此，细粒度写入不仅减少了NVMM写入，而且通过实现更高的存储体并行性来提高系统性能。</p><p>一些研究[65 ,66]通过分离SET和RESET操作来提高NVMM的能量效率。由于NVMM写入1比写入0消耗更多的能量和时间，如果以正确的方式执行这些写入,则可以减少写入延迟和能耗。三阶段写入[65]将写入操作分为比较阶段、写入零阶段和写入一阶段。在比较阶段,利用Flip-N-Write机制来减少写入次数。零位和一位分别在写零级和写一级中被分别写入。因为在大多数工作负载中,零写操作占了大部分写操作，所以Tetris write[66]进一步考虑了SET和RESET操作的不对称性, 并行调度代价高昂的写操作。在功率约束下, 写零操作被插入到写操作的剩余间隔中。</p><p>CompEx[67]提出了一种压缩扩展编码机制,以减少MLC/TLC NVMM的能耗。为了提高MLC/TLC单元的寿命,首先压缩数据以减少数据冗余。然后将扩展码应用于压缩数据并写入物理NVMM单元。对于具有8个状态的TLC单元，状态0、1、6和7称为终端能量状态，而状态2、3、4.和5称为中心能量状态。中心能量状态消耗更多的时间和能量，因为它们需要更多的编程和验证迭代。CompEx 利用扩展代码仅使用NVMM单元的终端能量状态。由于在编程MLC/TCL单元时，终端能量状态需要比中心能量状态更少的能量和时间，所以这一想法起效。混合片上缓存也被提出以减少CPU的功耗。RHC[68]构建了一个混合缓存，其中SRAM和NVMM中的每种方式都可以独立地打开或关闭。如果一行很长时间没有被访问，该行将被关闭，而其标签仍处于打开状态，以跟踪该行的访问。当对标签的访问超过阈值时，该行将通电。为了更好地利用高性能SRAM和低动态功耗NVMM，RHC对SRAM和NVMM采用不同的阈值。</p><h2 id="4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低"><a href="#4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低" class="headerlink" title="4.3 DRAM Energy Consumption Reduction DRAM能耗降低"></a>4.3 DRAM Energy Consumption Reduction DRAM能耗降低</h2><p>在只有DRAM的存储系统中, 静态能耗可以占存储系统总能耗的一半以上[69-71]。在混合存储器系统中,页面迁移技术被广泛用于减轻DRAM的能耗。非活动页面可以从DRAM迁移到NVMM，以便空闲的DRAM组可以断电。当页面稍后变为活动时, 它将再次迁移到DRAM。然而, 如果页面迁移没有正确执行,DRAM列组可能会频繁断电并重新激活。额外的能耗可能会抵消页面迁移带来的好处。</p><p>为了减少混合存储器系统的能耗，RAMZzz[8]揭示了高能耗的两个主要根源。一个是活动页面的稀疏分布,另一个是页面迁移可能不有效,因为DRAM的多能量状态之间的传输会引入额外的能量消耗。为了解决前一个问题, RAMZzz使用多个队列将具有类似活动的页面收集到同一个DRAM列中，从而避免频繁的能量状态转移。多个队列具有L个LRU队列来记录页面描述符。页面描述符包含一段时间内页面的ID和访问（读和写）计数。为了减少数据迁移的能量开销，将具有类似内存访问行为的页面重新组合在一起。这样，需要将页面分配给新的bank。RAMZzz在banks间并行迁移这些页面。</p><p>Refree[72]通过避免DRAM刷新，进一步降低了混合存储器系统中的DRAM能耗。当DRAM行需要刷新时,这意味着该行很长时间没有被访问。行中的数据已过时,不久以后不太可能再次访问。Refree将这些行逐出PCM，而不是在DRAM中刷新它们。在Refree中,所有行都会定期监视。此周期的间隔等于DRAM行自上次刷新以来的保留时间的一半。因此，行分为活动行和非活动行。激活行在访问。非活动行被逐出PCM，从而消除DRAM刷新。</p><h1 id="5-Write-Endurance-Improvement-写入耐久性改进"><a href="#5-Write-Endurance-Improvement-写入耐久性改进" class="headerlink" title="5. Write Endurance Improvement 写入耐久性改进"></a>5. Write Endurance Improvement 写入耐久性改进</h1><p>在混合存储器系统中,主要有两种策略来克服NVMM的有限写入耐久性。一个是减少NVMM写入，另一个是磨损均衡,它在所有NVMM单元之间均匀分布写入流量。</p><h2 id="5-1-Write-Reduction写入减少"><a href="#5-1-Write-Reduction写入减少" class="headerlink" title="5.1 Write Reduction写入减少"></a>5.1 Write Reduction写入减少</h2><p>已经提出了许多用于改善NVM寿命的写减少策略，包括数据迁移[8、14、15]、 缓存或缓冲[16]和内部NVM写减少[64、73、74]。</p><p>提出了一种延迟写入机制[16],以减少对PCM的写入。在分层混合存储器系统中,DRAM缓冲器用于隐藏高延迟PCM访问。当发生页面错误时, 数据将直接从磁盘提取到DRAM缓存中。在从DRAM高速缓存中逐出页面之前,页面不会写入PCM。行级写入还可以减轻NVMM上的写入操作, 从而减少NVMM的磨损[16]。对于内存密集型工作负载,写操作可能集中在几行中。通过跟踪DRAM中的缓存行，只有脏行被写回PCM,而不是页面的所有行。提出了内存压缩机制[67 ,75] ,以提高MLC/TLC NVMM的寿命。在写入NVMM单元之前, 首先压缩数据。因此,只有一小部分NVMM单元被写入。然而, 耐久性的提高是以性能适度下降为代价的。如果NVMM单元以较低的功耗写入, 则该单元可以以较高的写入延迟为代价维持更多的写入。具体地, 当写入NVMM单元的速度下降N倍时,单元的耐久性可以提高N到N3倍。Mellow Write[76]探索了这一功能，以提高NVMM的寿命。为了减轻性能下降, Mellow Write只采用只有一次写入操作的缓慢的存储体写入。</p><h2 id="5-2-Wear-Leveling磨损均衡"><a href="#5-2-Wear-Leveling磨损均衡" class="headerlink" title="5.2 Wear-Leveling磨损均衡"></a>5.2 Wear-Leveling磨损均衡</h2><p>与减少写入的方法不同, 磨损均衡在所有NVMM页面之间均匀分布写入。尽管写入总数没有减少,磨损均衡技术可以防止某些页面被高强度写入而快速磨损。</p><p>对于NVMM ,我们可以记录每行的写入计数,以指导磨损均衡策略。但是,不能忽略外部存储开销。Start Gap[77]提出了一种细粒度磨损均衡方案。PCM页的行以旋转方式存储。在0和15之间随机生成旋转值,以指示移位的位置。对于具有16行的PCM页面,旋转值的范围可以从0到15。当旋转值为0时,页面存储在其原始地址中。如果旋转值为1 ,则第0行存储在第1行的物理地址中,并且每一行的地址都被旋转值移位。</p><p>在PDRAM[15]中,磨损均衡由写入计数阈值触发。当页面的写入计数超过给定阈值时,将触发页面交换中断以将页面迁移到DRAM。交换的PCM页面被添加到列表中，这些页面将再次重新定位。</p><p>Zombie[78]为实现weal-leveling(这是不是多写了一个l)提供了另一个方向,并进一步延长了PCM的整体寿命。与在PCM单元之间均匀分配写入的Start-Gap之外, Zombie利用禁用页面中的空闲块为工作内存提供更多的纠错资源。当PCM单元磨损时,它变得不可用。由于从软件的角度来看,内存占用的空间是按照页来组织的,因此包含故障单元的整个页面将被禁用。但是，如果提供了一些备用单元格来替换出现故障的单元,则可以再次使用该页面。这些备用单元称为纠错资源。当所有备用单元耗尽时, 最终放弃包含失败单元的页面。通常，当页面被禁用时,大约有99%的位可用。Zombie利用禁用页面中的大量好比特作为备用纠错资源,其中好比特被组织在细粒度块中。通过将工作页面与纠错资源配对, Zombie可以延长NVMM的使用寿命。</p><p>DRM[79]在虚拟地址空间和物理NVMM地址空间之间添加了中间映射层。在中间地址空间中，一个页面可能映射到PCM中的一个好页面或两个有故障的兼容PCM页面。兼容页面意味着一对具有错误字节的页面,但这些错误字节都不位于两个页面的同一位置。因此，两个兼容的页面可以被组合成一个新的好页面通过这种方式, DRM将PCM寿命显著提高了40倍。</p><h1 id="6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践"><a href="#6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践" class="headerlink" title="6 Practices of Hybrid Memory System Designs混合存储系统设计实践"></a>6 Practices of Hybrid Memory System Designs混合存储系统设计实践</h1><p>在本节中，我们从内存架构、OS支持的混合内存管理和NVMM支持的应用程序的角度介绍了我们最近在NVMM系统设计和优化方面的努力和实践，如图5所示。在下文中，我们将简要介绍我们的实践。</p><h2 id="6-1-Memory-Architectural-Designs内存架构设计"><a href="#6-1-Memory-Architectural-Designs内存架构设计" class="headerlink" title="6.1 Memory Architectural Designs内存架构设计"></a>6.1 Memory Architectural Designs内存架构设计</h2><p>在本小节中，我们将介绍我们对混合内存模拟和仿真、硬件/软件协同混合内存架构、细粒度NVM压缩和磨损均衡以及混合内存感知片上缓存管理的研究。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/37d5195a01334fcb9a70359bfca5ba53.png"></p><h3 id="6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真"><a href="#6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真" class="headerlink" title="6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真"></a>6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真</h3><p>混合存储器体系结构仿真是研究混合存储器系统的先决条件。我们将zsim[27]与NVMain[20]集成, 以构建全系统架构模拟器。Zsim是用于x86-64多核架构的快速处理器模拟器。它能够对多核、片上缓存层次结构、缓存-致性协议(如MESI)、片上互连拓扑网络和物理内存接口进行建模。<strong>Zsim</strong>使用Intel Pin工具包收集进程的内存跟踪,然后回放内存跟踪以表征内存访问行为。<strong>NVMain</strong>是用于NVMM的架构级主存储器模拟器。它能够模拟不同的内存配置文件, 如读/写延迟、带宽、功耗等。它还支持子阵列级内存并行性和不同的内存地址编码方案。此外，NVMain还可以对混合存储器（如DRAM和存储器层次结构中的不同NVMM）进行建模。由于操作系统级内存管理不是由zsim模拟的,因此我们通过添加Translation Lookaside Buffer（TLB）和内存管理模块（如伙伴内存分配器和页表）来扩展zsim , 以支持全系统模拟。实施细节参考我们的开源软件。我们的工作为研究界提供了一个快速、完整的体系结构仿真框架。它可以帮助研究人员了解不同的NVMM特性,设计混合存储系统,并以简单高效的方式评估各种系统设计对应用程序性能的影响。</p><h3 id="6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器"><a href="#6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器" class="headerlink" title="6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器"></a>6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器</h3><p>当前基于仿真的NVMM技术研究方法太慢,或者无法运行复杂的工作负载,例如并行和分布式应用程序。我们提出HME[28]，一种轻量级NVMM使用非统一内存访问（NUMA）架构的性能仿真器。HME利用商品Intel CPU中可用的硬件性能计数器来模拟较慢NVMM的性能特性。为了模拟NVMM的访问延迟,HME定期向远程NUMA节点上的DRAM访问注入软件生成的延迟。为了模拟NVMM带宽,HME利用DRAM热控制接口在短时间内限制对DRAM通道的内存请求量。不同于另一个NVMM仿真器Quartz[29] ,它不模拟NVMM的写入延迟,HME识别写直通和写回缓存逐出操作, 以分别模拟它们的延迟。通过这种方式,与Quartz相比, HME能够显著减少NVMM访问延迟的平均仿真误差[29]。在真正的NVMM设备Intel Optane DCPMM问世之前, 这项工作可以帮助研究人员和程序员评估NVMM性能特性对应用程序的影响, 并指导混合内存系统的系统设计和优化。</p><h3 id="6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存"><a href="#6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存" class="headerlink" title="6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存"></a>6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存</h3><p>基于我们的混合存储器模拟器， 我们提出了一种称为HSCC[18]的硬件/软件协同混合存储器架构。在HSCC中,DRAM和NVMM在物理上组织在单个存储器地址空间中，并且都用作主存储器。然而,DRAM在逻辑上可以用作NVMM的缓存,也可以由OS管理。图6显示了HSCC的系统架构。我们扩展了页表和TLB，以维护NVMM到DRAM的物理地址映射, 从而以缓存/内存层次结构的形式管理DRAMNVMM。通过这种方式, HSCC能够像虚拟到NVMM地址转换一样高效地执行NVMM到DRAM地址转换。此外, 我们在每个TLB条目和页表条目中.添加一个访问计数器，以监视内存引用。与以前在内存控制器或操作系统中监视内存访问的方法不同,我们的设计可以精确地跟踪所有数据访问, 而无需额外的存储(SRAM)和性能开销。我们通过动态阈值调整策略识别频繁访问的(热)页面，以适应不同的应用程序,然后将NVMM中的热页面迁移到DRAM缓存，以获得更高的性能和能效。此外,我们开发了一种基于实用程序的DRAM缓存填充方案,以平衡DRAM缓存的效率和DRAM利用率。由于软件管理的DRAM顶面能够映射到任何NVMM页面,因此DRAM实际.上用作完全关联的缓存。这种方法可以显著提高DRAM缓存的利用率,并且还提供了根据应用程序的动态内存访问行为重新配置混合内存架构的机会。由于CPU可以绕过DRAM缓存直接访问NVMM中的冷数据，因此DRAM既可以用作平面可寻址混合存储器架构中的主存储器,也可以用作分层混合存储器架构。因此,与最先进的工作相比,HSCC可以将系统性能显著提高9.6倍，能耗降低34.3%[16]。我们的工作为实现可重构混合存储器系统提供了第一个架构解决方案,该系统可以在水平和分层存储器架构之间动态改变DRAMNVMM管理。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c8216069dd1e4b42b6526c54ad69b35c.png"></p><p>我们进一步在HSCC上提出了以下技术,以提高缓存性能并改进磨损均衡机制。</p><p>由于NVMM块的缓存未命中惩罚是DRAM块的几倍,因此在平面可寻址混合存储器体系结构中，缓存命中率不是唯一需要改进的性能指标。为了最好地利用昂贵的LLC，我们提出了一种新的度量，即平均存储器访问时间（AMAT）,以评估混合存储器系统的总体性能。我们考虑了DRAM块和NVMM块的非对称缓存未命中惩罚，并提出了一种LLC未命中惩罚感知替换算法,称为MALRU[36,37] ,以改进混合存储器系统中的AMAT。MALRU动态地将LLC划分为保留区域和正常替换区域。MALRU优先替换LLC中的死DRAM块和冷DRAM块，使得NVMM块和热DRAM块保持在保留区域中。通过这种方式, 与LRU算法相比,MALRU实现了高达228%的应用程序性能改进。这项工作展示了混合存储器系统如何影响片上缓存的架构设计。</p><p>为了提高NVMM的写入耐久性，我们提出了一种新的NVMM架构,以支持空间无关数据压缩和磨损均衡[119]。由于许多应用程序的内存块通常包含大量零字节和频繁值, 我们提出了零重复数据消除和频繁值压缩机制（称为ZD-FVC[119]）, 以减少NVMM上的位写入。ZD-FVC可以集成到NVMM模块中，并完全由硬件实现, 无需任何操作系统的干预。我们在Gem5和NVMain模拟器中实现了ZD-FVC[119]，并使用SPEC CPU2006中的几个程序对其进行了评估。实验结果表明,ZD-FVC比几种最先进的方法要好得多。特别是, 与频繁值压缩相比, DZ-FVC可以将数据压缩比提高1.5倍。与数据比较写入相比，ZD-FVC能够将NVMM上的位写入减少30%，并将NVMM的寿命平均提高5.8倍。相应地, ZD-FVC还平均减少了43%的NVMM写入延迟和21%的能耗。我们的设计以简单高效的方式为NVMM提供了细粒度数据压缩和磨损均衡解决方案。它是其他磨损均衡方案的补充，以进一步提高NVMM寿命。</p><h1 id="6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件"><a href="#6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件" class="headerlink" title="6.2 System Software for Hybrid Memories混合存储器的系统软件"></a>6.2 System Software for Hybrid Memories混合存储器的系统软件</h1><p>在本小节中,我们介绍了软件层混合内存系统的实践，包括对象级混合内存分配和迁移、NUMA感知页面迁移、超级页面支持和NVMM虚拟化机制。</p><h3 id="6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移"><a href="#6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移" class="headerlink" title="6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移"></a>6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移</h3><p>页面迁移技术已被广泛用于改善混合存储器系统中的系统性能和能量效率。然而,以前的页面迁移方案都依赖于OS层中昂贵的在线页面访问监控方案来跟踪页面访问的最近性或频率。此外,由于额外的内存带宽消耗和缓存/TLB一致性保证机制,页面粒度上的数据迁移通常会导致非平凡的性能开销。</p><p>为了减轻混合内存系统中数据迁移的性能开销，我们提出了更轻量级的面向对象内存分配和迁移机制,称为OAM[120]。OAM的框架如图7所示。与之前的研究[44 ,121]不同, 我们进一步分析了细粒度时隙中的对象访问模式, 这些研究仅在静态对象放置的全局视图中描述了内存访问行为。OAM利用编译框架LLVM以对象粒度描述应用程序内存访问模式，然后将应用程序的执行分为不同阶段。OAM利用性能能量集成模型来指导不同执行阶段的初始内存分配和运行时对象迁移, 而无需对硬件和操作系统进行侵入性修改以进行在线页面访问监控。我们通过扩展Glibc库和Linux内核开发了新的内存分配和迁移API。基于这些API ,程序员能够将DRAM或NVMM显式分配给不同的对象, 然后迁移访问模式在DRAM和NVMM。我们开发了一个静态代码插入工具,可以自动修改遗留应用程序的源代码,而无需程序员重新设计应用程序。与最先进的页面迁移方法CLOCK-DWF[33]和2PP[44]相比, 实验结果表明,OAM可以分别显著降低83%和69%的数据迁移成本,并实现约22%和10%的应用程序性能改进。以前的持久内存管理方案通常依赖内存访问评测来指导静态数据放置,以及页面迁移（代价高昂）技术来适应运行时的动态内存访问模式。OAM提供了一种更轻量级的混合内存管理方案,支持细粒度对象级内存分配和迁移。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/257cf52418304fce863269f59094a936.png"></p><h3 id="6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理"><a href="#6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理" class="headerlink" title="6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理"></a>6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理</h3><p>在非统一内存访问（NUMA）架构中，不同NUMA节点中应用程序观察到的内存访问延迟通常是不对称的。由于NVMM比DRAM慢几倍，混合存储器系统可以进一步扩大不同NUMA节点之间的性能差距。NUMA系统的传统内存管理机制在混合内存系统中不再有效,甚至可能降低应用程序性能。例如,自动NUMA平衡（ANB）策略总是将远程NUMA节点中的应用程序数据迁移到运行应用程序线程或进程的NUMA节点。然而，由于远程DRAM的访问性能可能甚至高于本地NVMM,ANB可能会错误地将应用数据移动到较慢的位置。为了解决这个问题，我们提出了HiNUMA[60]，这是一种用于混合内存管理的新NUMA抽象。当应用程序数据首次放置在混合存储器系统中时，HiNUMA将应用程序数据放置在NVMM和DRAM上，以分别平衡带宽敏感应用程序和延迟敏感应用程序的内存带宽利用率。总访问延迟。初始数据放置基于NUMA拓扑和混合内存访问性能。对于运行时混合内存管理,我们提出了一个新的NUMA平衡策略,名为HANB[60]，用于页面迁移。HANB能够通过考虑数据访问频率和内存带宽利用率来降低混合内存访问的总成本。我们在Linux内核中实现HiNUMA ,无需对硬件和应用程序进行任何修改。与NUMA架构中的传统内存管理策略和其他最先进的工作相比, HiNUMA可以通过有效利用混合内存来显著提高应用程序性能。从HiNUMA[60]中学到的经验教训也适用于配备真正IntelOptaneDCPMM设备的混合内存系统。</p><h3 id="6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储"><a href="#6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储" class="headerlink" title="6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储"></a>6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储</h3><p>随着应用程序占地面积和相应内存容量的快速增长,虚拟到物理地址转换已成为混合内存系统的新的性能瓶颈。在大内存系统中, 超页已被广泛用于减轻地址转换开销。然而,使用超级页面的副作用是，它们通常会阻碍轻量级内存管理,例如页面迁移,而页面迁移在混合内存系统中被广泛用于提高系统性能和能效。不幸的是,同时拥有超级页面和轻量级页面迁移是一个挑战。</p><p>为了解决这个问题,我们提出了一种新的混合内存管理系统Rainbow[41]以弥合超级页面和轻量级页面迁移之间的根本冲突。如图8所示，Rainbow以超页（2MB）的粒度管理NVMM,并将DRAM作为缓存管理以基本页的粒度（4KB）将热数据块存储在超级页中。为了加快地址转换, Rainbow使用了拆分TLB的现有硬件功能来支持超级页面和普通页面。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/13d8d4f2c76f4dc28be42a4424d5faee.png"></p><p>我们提出了一种两阶段页面访问监控机制来识别超级页面中的热基页面。在第一阶段，Rainbow记录所有超级页面的访问计数以识别前N个热超级页面。在第二阶段,我们逻辑上将这些热超页分割成基本页（4KB） , 并进一步监视它们以识别热基本页。这些方案显著减少了页访问计数器的SRAM存储开销和由于对热基页进行排序而导致的运行时性能开销。通过新的NVMM到DRAM地址重新映射机制, Rainbow能够将热基页迁移到DRAM,同时仍能保证超级页TLB的完整性。拆分的超页TLB和基本页TLB是并行查阅的。我们的地址重映射机制在逻辑上使用超页TLB作为基本页TLB的缓存。由于超级页TLB的命中率通常很高,Rainbow能够显著加快基本页地址转换。为了进一步提高TLB命中率, 我们还扩展Rainbow以支持多个页面大小,并一起迁移相邻的热基页面[42]。与不支持超级页面的最先进混合内存系统[18]相比,Rainbow通过同时使用超级页面和轻量级页面迁移的优势，可以将应用程序性能显著提高最多2.9倍。</p><p>这项工作提供了硬件/软件协同设计，以弥合超级页面和轻量级页面迁移技术之间的根本冲突。这可能是减轻大容量混合存储器系统中不断增加的虚拟到物理地址转换开销的一个有前途的解决方案。</p><h3 id="6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理"><a href="#6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理" class="headerlink" title="6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理"></a>6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理</h3><p>NVMM有望在云和数据中心环境中更受欢迎。然而,关于将NVMM用于虚拟机（VM）的研究很少。我们提出了HMvisor[61],一种管理程序/虚拟机协同混合内存管理系统, 以有效利用DRAM和NVMM。如图9所示, HMvisor利用伪NUMA机制来支持VM中的混合内存分配。由于VM中的虚拟NUMA节点可以映射到不同的物理NUMA节点, HMvisor可以将不同的内存区域映射到单个VM ,从而向VM暴露内存异质性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1402aec0f84e49268d98793dc3a37f4c.png"></p><p>为了支持VM中的轻量级页面迁移,HMvisor监控页面访问计数并进行虚拟机管理程序中的热页和冷页分类，然后VM通过域间通信机制周期性地收集热页面的信息。我们在VM中实现了一个可加载的驱动程序,以在DRAM和NVMM之间执行进程级页面迁移。由于HMvisor由VM本身执行页面迁移,因此HMvisor无需暂停VM进行页面迁移。HMvisor还提倡混合内存资源交易策略,以动态调整VM中NVMM和DRAM的大小。通过这种方式,HMvisor可以满足多样化应用程序的不同内存需求（容量或性能） ,同时保持VM的总货币成本不变。</p><p>HMvisor的原型在QEMU/KVM平台上实现。我们的评估表明,HMvisor能够以仅5%的性能开销为代价将NVMM写入流量减少50%。此外, 动态内存调整策略可以在VM承受高内存压力时显著减少VM中的主要页面错误,因此甚至可以将应用程序性能提高30倍。</p><p>这是一项在虚拟化环境中管理混合内存的早期系统工作。所提出的方案完全由软件实现, 因此也适用于新Intel Optane DCPMM设备的混合存储系统。</p><h2 id="6-3-NVMM-Supported-Applications-NVMM支持的应用程序"><a href="#6-3-NVMM-Supported-Applications-NVMM支持的应用程序" class="headerlink" title="6.3 NVMM-Supported Applications NVMM支持的应用程序"></a>6.3 NVMM-Supported Applications NVMM支持的应用程序</h2><p>由于混合存储器系统可以提供非常大容量的主存储器,因此它们已被广泛用于大数据应用,例如内存中的关键值KV存储和图形计算。在本小节中,我们介绍了NVMM支持的针对这些应用程序的系统优化实践。</p><p>具有大容量内存的内存KV存储可以在主内存中缓存更多的热数据,从而为应用程序提供更高的性能。然而,在混合内存系统中直接部署传统的KV存储（如memcached）存在若干挑战。例如，如何有效地识别热KV对象?如何重新设计NVMM友好的KV索引以减少NVMM写入?如何重新设计缓存替换算法以平衡混合内存系统中的对象访问频率和最近性?如何解决slab calcification问题[122] ,以在混合存储器系统中最佳地利用DRAM资源?</p><p>为了解决上述问题，我们提出HMCached[80] ,这是混合DRAM/NVMM系统的KV缓存（memcached）的扩展。图10显示了HMCached的系统架构。HM缓存跟踪KV对象访问并记录每个KV对的元数据结构中的进程计数, 因此HMCached可以轻松识别NVMM中频繁访问的对象,并将它们迁移到DRAM。这样, 我们逻辑上将DRAM用作NVMM的专用缓存,以避免更昂贵的NVMM访问。此外, 我们通过拆分基于哈希的KV索引来重新设计NVMM友好的KV数据结构, 以进一步减少NVMM访问。我们将KV对象的频繁更新元数据（例如,引用计数、时间戳和访问计数）放在DRAM中,其余部分（例如，键和值）放在NVMM中。我们利用多队列算法[118]来考虑DRAM缓存替换的对象访问频率和最近性。此外，我们建立了一个基于效用的性能模型来评估板类重新分配的效益。我们的动态slab重新分配策略能够有效解决slab calcification问题,并在数据访问模式发生变化时显著提高应用程序性能。与普通memcached相比HMCached可以显著减少70%的NVMM访问, 并实现大约50%的性能改进。此外，HMCached能够降低75%的DRAM成本,同时性能下降不到10%。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/76462968db584a9c8136d9c88d89346e.png"></p><p>据我们所知,我们是第一个探索混合存储系统中KV存储的对象级数据管理的。我们基于Memcached实现HMCached并开放源代码。我们发现，后来的研究（如flatstore[90]）也有类似的想法来解耦KV存储的数据结构。</p><p>今天,我们已经看到了许多内存中的图形处理系统,其中应用程序的性能与主内存的容量高度相关。高密度和低成本NVMM技术对于降低图形处理的I/O成本至关重要。如图11所示, 与基于SSD的存储系统相比,混合存储系统可以显著提高应用程序性能。图12显示了混合存储器系统和仅DRAM系统之间的应用程序性能差距。我们提出了NGraph，一种新的图形处理框架，专门设计用于更好地利用混合存储器。我们基于不同图形数据的访问模式开发混合内存感知数据放置策略,以减轻对NVMM的随机和频繁访问。通常，图形结构数据占总图形数据的大部分。NGraph根据目标顶点划分图形数据,并采用任务分解机制来避免多个处理器之间的数据争用。此外,NGraph采用了工作窃取机制，以最小化多核系统上并行图形数据处理的最大时间。我们称为ReRAM技术的。</p><p>基于图形处理框架Ligra[123]实现NGraph。与最先进的Ligra相比,NGraph可以将应用程序性能提高48%。从这项工作中获得的经验教训[91]可用于在配备真实PM设备的图形处理平台中进一步提高大规模图形分析的性能。</p><div class="note info no-icon"> 华科的工作关注的问题方向有的还挺小的，如果专注于某个问题的话，最后系统设计出来也是比较偏向于适合某个场景的。从后面第7章节来看，这个发展好像是走向越来越专业化的，不同的需求不同的架构和系统设计。 </div> <h1 id="7-Research-Directions研究方向"><a href="#7-Research-Directions研究方向" class="headerlink" title="7 Research Directions研究方向"></a>7 Research Directions研究方向</h1><p>NVMM技术的出现在材料、微电子、计算机架构、系统软件、编程模型和大数据应用领域引起了许多有趣的研究课题。随着IntelOptaneDCPMM等真正的NVMM设备越来越多地应用于数据中心环境,NVMM可能会改变数据中心的存储环境。我们的经<br>验和做法进行了一些初步和有趣的研究。在下文中,我们分享了NVMM未来研究方向的愿景,并分析了研究挑战和新机遇。图13说明了不同维度NVMM技术的未来趋势。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/21935e2720af4afeae966b3ce6e3724d.png"></p><ol><li><p>3D堆叠NVMM技术的发展仍在继续。NVMM有望提供更高的集成密度以降低成本。目前, 高端NVDIMM对于企业应用来说仍然过于昂贵。NVMM与传统DRAM和NAND闪存竞争的关键挑战是存储密度或每字节成本。NVMM技术主要有两种单片3D集成机制[124]。一种是将例如Intel/Micron 3DX点。另一种是垂直3D堆叠结构。然而, 3D集成技术尚未成熟。仍然存在许多挑战,如制造成本、柱电极电阻和潜路径问题。</p></li><li><p>NVMM越来越多地用于分布式共享存储器系统。随着NVMM的密度不断增加，单个服务器中的主内存容量可以达到数百TB。为了提高大容量NVMM的利用率,必须通过远程直接内存访问（RDMA）技术在多个服务器之间共享它们。使用NVMM的典型方法是将来自多个服务器的所有可共享内存聚合到混合共享内存资源池中, 例如Hotpot[49,125,126]。所有内存资源在全局内存空间中共享。有一些关于在数据中心和云环境中使用NVMM的初步研究[49,126,127]。使用PM的一个新趋势是将其作为分类内存进行管理,就像传统的分类存储系统一样。此型号与以前的共享PM系统不同, 在该系统中，PMDIMM分布在多个服务器中，由用户级应用程序共享。这些计算内存紧密耦合的体系结构在可管理性、可扩展性和资源利用率方面有几个缺点。相比之下，在少数存储器节点中配备有大量PM的分解PM系统可以由计算节点通过高速结构连接。这种计算/内存分类架构可以更容易地减轻数据中心环境中的上述挑战。然而,仍然存在许多挑战。例如,NVMM的持久性特性也应在分布式环境中得到保证。传统的PM管理指令（如clflush和mfence） 只能保证数据在单个服务器中持久化，但不能保证数据通过RDMA网络持久化到远程服务器。对于每个RDMA操作,一旦数据到达远程服务器中的网络接口卡（NIC），它就会向数据发送方发出确认。由于NIC中有数据缓冲区，数据不会立即存储到远程NVMM。如果此时发生电源故障,则无法保证数据持久性。因此,必须重新设计RDMA协议以支持flushing原语。此外,计算节点应支持对用户级应用程序透明的远程页面交换。为了支持这种机制应该重新设计传统的虚拟内存管理策略。另一方面,由于PM表现出类似内存的性能, 并且是字节可寻址的，因此需要对内存调度和管理进行新的设计，以适应分解的PM。</p></li></ol><p>3)基于NVMM的计算存储器集成计算机体系结构正在兴起。例如,新兴NVMM在存储器内处理（PIM）[95,96]和近数据处理（NDP）[128,129]架构中的应用正在兴起。PIM和NDP近年来已成为新的计算范式。NDP是指将处理器与存储器集成在单个芯片上,以便计算能够尽可能接近地访问存储器中的数据。NDP能够显著降低数据移动的成本。实现这一目标主要有两种方法。一种是将小型计算逻辑（如FPGA/ASIC）集成到存储器芯片中，以便在数据最终被提取到CPU之前对其进行预处理。另一种方法是将内存单元（HBM/HMC）集成到计算（CPU/GPGPU/FPGA）中。该模型通常用于许多处理器架构, 如IntelO Xeon PhiTM Knights Landing系列、NVIDIAO tesla V100和Google Tensor处理单元（TPU）。PIM指的是完全在计算机内存中处理数据。它通过在主存储器中执行计算,提供了高带宽、大规模并行性和高能量效率。使用NVMM（如ReRAM）的PIM通常可以并行计算两个或多个内存行的位逻辑, 并支持一步多行操作。该范例对于模拟计算方式中的矩阵向量乘法特别有效，并且可以实现极大程度的性能加速和节能。因此, PIM在加速机器学习算法（如卷积神经网络）中得到了广泛的研究（CNN）和深度神经网络（DNN）。尽管在PIM架构中.使用NVMM技术的兴趣越来越大[94-96 ,130]， 但目前的研究主要基于电模拟,没有一项研究可用于中型原型设计。</p><p>4)除了传统应用之外, 一些使用NVMM的新应用正在出现。尽管NVMM技术已初步应用于许多大数据应用, 如KV存储、图形计算和机器学习,但大多数编程框架/模型和运行时系统都是为磁盘设备和基于DRAM的主存储器而设计的, 它们在混合存储系统中并不有效。例如, 这些系统中广泛使用缓冲和延迟写入机制来隐藏I/O操作的高延迟。然而,混合存储器系统中可能不需要这些机制,甚至可能会损害应用程序性能。应重新设计Hadoop/Spark/GraphChi/Tensorflow等大数据处理平台,以适应NVMM技术的特点。除了这些传统应用之外,一些基于NVMM的新型应用正在出现。例如,有一些建议通过利用NVMM的切换过程的内在变化,将NVMM用作硬件安全原语, 如物理不可克隆函数（PUF）[131]。 PUF通常用于具有高安全性要求的应用,例如密码学。最近,已经提出了许多基于NVMM技术的逻辑电路并将其原型化[132-134]。例如，ReRAM技术被提议用作基于ReRAM的FPGA的可重构开关[133]。此外，STT-RAM技术被提出用于设计非易失性缓存或寄存器[135]。</p><h1 id="8-Conclusions结论"><a href="#8-Conclusions结论" class="headerlink" title="8 Conclusions结论"></a>8 Conclusions结论</h1><p>与传统DRAM技术相比, 新兴的NVMM技术具有许多良好的特性。它们有可能从根本上改变存储系统的面貌，甚至为计算机系统添加新的功能和特性。现在有很多机会重新思考当今计算机系统的设计,以实现系统性能和能耗的数量级改进。本文从内存体系结构、操作系统级内存管理和应用程序优化的角度全面介绍了最新的工作和我们的实践。我们还分享了我们对NVMM技术未来研究方向的展望。通过利用NVMM的独特特性, 有巨大的机会来创新未来的计算范式, 开发NVMM的多种新颖应用。</p><h2 id="积累"><a href="#积累" class="headerlink" title="积累"></a>积累</h2><p>一些可以进一步看一看的文献：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3 GB/s，而DRAM的读写带宽之间的差距要小得多(1.3x)。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。</p><div class="note danger"> 但是这个可能不是我想要的CPU和内存的延迟带宽之类的，可能是针对做持久性时的数据 </div> <p>Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面。</p><p>Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性.</p><p>“如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。”看看华科廖小飞团队在这个方面最近的工作。</p><p>“由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。”看一看这些有没有做内存的延迟和能耗的统计。</p><p>文中提到和写密集有关的（但是这个是store还是write还是要看看，有的需求write挺少的）CLOCK-DWF[33]，PDRAM[15]，RaPP[14]</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</title>
      <link href="/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/"/>
      <url>/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Adnan Maruf, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Ashikee Ghosh, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Janki Bhimani, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Daniel Campello, Google</li><li>Andy Rudoff, 英特尔公司</li><li>Raju Rangaswami, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>将PM作为第二级内存直接暴露给CPU是目前比较有希望的一个做法：如何把数据在正确时间放入正确分层中去。于是面临一个是大家关注的问题。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>动机：通过四个工作负载，统计归类50个采样页面的访问模式，得出层级友好页面是需要迁移的对象（空间局部性）。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b9a6c5150a4c455c9eca0fbbcd1aaf99.png" alt="热力图"><br>同时一段时间访问过的页面在下一段时间被访问概率也很大（时间局部性）<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cb7d1ef346b84032b6cae36568adf862.png" alt="相线图表示被访问的概率在不同时间窗口的变化"><br>然后说明了用frequency&amp;recency识别到的页面也具有层级友好的特征。相比于静态分层，说明了动态分层的必要。最后把整个比较模糊的大问题转化为：试图解决分层系统中：如何根据frequency&amp;recency来识别升级的热点页?如何在内核中设计一个简单、低开销而又高效的系统？</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>这些都是分层技术上的对比：<br>静态分层即一个内存页一旦被映射到一个分层，在其生命周期内就不会被重新分配到不同的分层。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9b9a94f135544a11a3dfdb7aca8ed615.png" alt="现有内存分层技术的比较"><br>[11]Nimble：只根据recency来选择页面，这篇的作者为了解决frequency决定把工作负载执行期分为观察窗口和性能窗口。得出观察窗口频率高的，在性能窗口概率也高。专注于透明大页（THP）迁移。应用程序需要通过Nimble的启动器运行以利用其页面迁移技术。MC在内置内核实现的功能。Nimble需要一个额外的启动器来运行内核上的任何工作负载。</p><p>[12]AutoTieringhint page fault的缺页异常来跟踪页面访问，并使用recency来识别热点页进行升级。尽管缺页异常可以提供高准确度的页面访问跟踪，但跟踪所有页面的成本很高，因为每一个页面故障都必须在访问页面之前进行处理。这是因为基于缺页的软件页面访问跟踪成本很高，而且跟踪页面历史位以识别冷页面的开销也很大。所以在后面工作负载测试中表现很差。</p><p>[19]Thermostat源代码不可用没有评估，通过poisoning页表项（PTE）和触发缺页异常来跟踪巨大的页面，并将冷页面迁移到较低的内存层。</p><p>[22]AMP（非统一内存访问架构NUMA，主板会分成不同的插槽，每一个插槽一组cpu，以及和这组cpu离得近的内存。）这在两个插座的NUMA机器中是不现实的，因为每个节点通常有自己的DRAM、PM和CPU。AMP使用一个节点，只用于DRAM的分配，其他节点只用于PM的分配。AMP是在Linux内核4.15版本上实现的，它不支持所需的KMEM DAX驱动（从内核v5.1开始提供），以便PM作为主内存在分层系统中使用。AMP的核心设计原则要求它扫描和剖析来自DRAM和PM层的所有内存页，这在实际系统的内核中是不现实的，因为在我们评估的工作负载中，内存页的数量可以增长到数亿。<br>对比于PM当前这款硬件的两种用法。</p><p>[7]Memory-mode：数据不能持久化，dram作为缓存不透明。缓存是需要从高层获取数据的，而这里提出的分层是两层都能被直接访问的。</p><p>[44]对象级需要改变应用程序API，而内核级别的修改不需要应用程序有何变化。</p><p>[32]提出了一种有效使用持久性内存作为NUMA节点的设计。这个分层设计同时意识到了DRAM和PM节点 ,并且只通过NUMA平衡处理匿名页面的升级/移动。</p><p>[33-36]不需要硬件，而且都是主存没有缓冲。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>设计MULTI-CLOCK的主要假设是，最近被访问过一次以上的页面，在不久的将来更有可能被访问。</p><p>具体升级降级要求门槛就是那张图。（但是频繁程度不够吧？）及时更新页面引用状态的方式根据对内存页的访问模式不同而不同。无监督式：CPU在进程的页表入口中设置的页面引用位。 <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ca0046254e904f9991e9b0242696b89f.png" alt="文章的idea"></p><p>与CLOCK类似，升级是由每次系统守护程序kpromoted去完成的。它定期被唤醒，以扫描列表，更新它们，并将最近由无监督访问产生的升级列表中页迁移到更高层的页。</p><p>降级机制基于今天的虚拟内存系统中的页面驱逐技术。当内存达到内存压力时（该层与系统总内存量来计算）会去扫描每个列表。活动列表中的页面相对于非活动列表的比例超过了一个与该层可用内存量相关的阈值时，那么在活动列表中没有被标记为引用的页面将被移至非活动列表。最后，非活动列表被扫描，以寻找未被标记为引用的页面，并将其迁移到较低的层级。当没有更低层级列表可以迁移，就写回块存储。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>评估的目的是确定MULTI-CLOCK是否、何时以及如何能够提高应用工作负载的性能。</p><p>使用雅虎云服务基准（YCSB）[13]的六个不同的工作负载和GAP基准套件(GAPBS) [14] 的六个工作负载来讨论我们的结果。Memcached[3]，一个使用大量主内存来维护其数据的内存缓存服务，作为YCSB的键值存储后端。 配置时内存要被全部消耗，并且消耗一部分PM。</p><p>MULTI-CLOCk在所有YSCB工作负载上都优于静态分层、Nimble、AT-CPM和AT-OPM。对GAPBS的执行时间也比其他方案减少。</p><p>分析了MULT-CLOCK和Nimble所升级的页面数量。MULTI-CLOCK每次扫描平均升级758页，最多扫描1024。而N是把1024作为固定值。如果那些将来不会再被重新访问的页面被提升到DRAM中，那么提升这些页面的开销会损害系统性能。第二次再次被访问的百分比比N高15%</p><p>实现了目标，低开销，特别是密集型应用。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ol><li>能耗方面摘要提了一下，后面也没说呀。</li><li>作者源码有一些问题，在运行高性能计算用MIP的时候会内核崩溃。</li><li>后面的性能评估对比了几种相关的工作，但是选择的workload也是比较局限的，GAPBS的论文都没有正式发出来。</li></ol><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><ol><li>在内核上修改代码，不需要其他程序有什么修改。</li><li>使用升级列表和频率可以很好的利用层级友好页面的局部性。</li></ol><h2 id="9-相关链接"><a href="#9-相关链接" class="headerlink" title="9. 相关链接"></a>9. 相关链接</h2><ul><li><a href="https://github.com/sylab/multi-clock">作者公布的源码</a></li><li><a href="https://docs.pmem.io/ndctl-user-guide/">ndctl用户手册</a></li><li><a href="https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/">PM的AD模式下的kmem模式设置</a></li><li><a href="https://github.com/brianfrankcooper/YCSB">YCSB工作负载源码</a></li><li><a href="https://cloud.tencent.com/developer/article/1004637">YCSB介绍与相关运行配置介绍</a></li><li><a href="https://github.com/pmem/ndctl/tree/main/Documentation/daxctl">ndctl下的daxctl安装源码下载</a></li><li><a href="https://memark.io/index.php/2021/04/09/pmem_intro/">持久内存开发资料汇总</a></li><li><a href="https://blog.csdn.net/qq_37858386/article/details/78444168">Linux驱动编程中EXPORT_SYMBOL介绍（因为作者公布的源码里有这个报错）</a></li><li><a href="https://github.com/pmem/ndctl/issues/108">daxctl fails to reconfigure to system-ram when DAX modules built-in</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/guide/qsg-intro-to-provisioning-pmem.html">持久内存配置简介，快速入门</a></li><li><a href="https://github.com/sbeamer/gapbs">GAPBS工作负载下载和使用</a></li></ul><h2 id="10-积累"><a href="#10-积累" class="headerlink" title="10. 积累"></a>10. 积累</h2><p> 当使用PM作为主存储器时，其持久性能力变得无关紧要（这里作者遵循原先的主存的设计，即易失性，抛弃了NVM数据持久的特点，后面提到的该产品Memory Mode，并且文献[10]说PMEP想要持久性靠的是<code>clflush</code>指令，这个指令由于要排序开销很大），从而完全避免了其最大的性能开销[10]</p><p> 有开源内核好像有一个选项可以计数随时间变化页面在层级间迁移的数量。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time Complexity Calculation</title>
      <link href="/2021/06/06/Time-Complexity-Calculation/"/>
      <url>/2021/06/06/Time-Complexity-Calculation/</url>
      
        <content type="html"><![CDATA[<h2 id="一、常见阶大小比较"><a href="#一、常见阶大小比较" class="headerlink" title="一、常见阶大小比较"></a>一、常见阶大小比较</h2><p>从大到小：  </p><ul><li>超指数阶：$n^n$，$n!$</li><li>指数阶：$9^{n/2}$,  $2^n$</li><li>多项式阶：$n^3$,   $n*log(n)$, $n^{1/2}$</li><li>对数阶：$log^2(n)$, $log(n)$, $log(log(n))$</li><li>常数阶：100, 1<br>下题需要保留阶最高的部分：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220748744.jpg"></li></ul><h2 id="二、算法复杂性估计函数"><a href="#二、算法复杂性估计函数" class="headerlink" title="二、算法复杂性估计函数"></a>二、算法复杂性估计函数</h2>$$\lim_{n \to \infty} \frac{f(n)}{g(n)}  =\begin{cases}(大于0的常数或)0       &amp;&amp;&amp; f(n)=O(g(n))上界&amp;-----f(n)\le cg(n)\\(大于0的常数或)无穷    &amp;&amp;&amp; f(n)= \Omega(g(n))下界&amp;-----f(n)\ge cg(n)\\大于0的常数            &amp;&amp;&amp; f(n)= \Theta(g(n))确切界&amp;-----f(n)=cg(n)\\0      &amp;&amp;&amp;f(n)=o(g(n))上界&amp;-----f(n)&lt; cg(n)\end{cases}$$<p>可以发现都是针对f(n)在讨论，很容易得出下题答案：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220628312.jpg"></p><h2 id="三、几个常用替换的式子"><a href="#三、几个常用替换的式子" class="headerlink" title="三、几个常用替换的式子"></a>三、几个常用替换的式子</h2><h3 id="1-Stirling公式："><a href="#1-Stirling公式：" class="headerlink" title="1.Stirling公式："></a>1.Stirling公式：</h3>$$n! \approx {(2 \pi n)}^{1/2}{(n/e)}^n$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605222019202.jpg"></p><h3 id="2-阶乘和二项式系数"><a href="#2-阶乘和二项式系数" class="headerlink" title="2.阶乘和二项式系数"></a>2.阶乘和二项式系数</h3>$$C_n^k = C_n^{n-k} \\ C_n^n = C_n^0 = 1 \\C_n^k = C_{n-1}^k +C_{n-1}^{k-1} $$<p>帕斯卡三角形可以辅助记二项式系数：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060522532381.jpg"></p>$$\sum_{j=0}^{n}{C_{n}^j}x^j= {(1+x)}^n $$<h3 id="3-和式"><a href="#3-和式" class="headerlink" title="3.和式"></a>3.和式</h3>$$\sum_{j=1}^{n}{a_{n-j}} = \sum_{j=0}^{n-1}{a_j} \\\sum_{j=0}^n{j \over 2^j} = \sum_{j=1}^n{j \over 2^j} = 2-{{(n+2)} \over {n^2}} = \Theta(1) \\\sum_{j=0}^njc^j = \sum_{j=1}^njc^j = \Theta(nc^n)$$<h3 id="4-定积分与和式转换"><a href="#4-定积分与和式转换" class="headerlink" title="4.定积分与和式转换"></a>4.定积分与和式转换</h3>$$\int_m^{n+1} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m-1}^nf(x)dx 递减函数 \\\int_{m-1}^{n} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m}^{n+1}f(x)dx  递增函数$$<p>可以采用画图的方法辅助记忆它的上下界：<br>以一个递增的函数为例，我们要求1（m）~6（n）他的面积，每一个小矩形$1*f(j)$如果我们积分每个点左边的矩形，那么总面积就是偏小的，积分右边矩形就会稍微偏大，这就找到了上下界，当函数平行于X轴时就会有等号。递减也是一个道理。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606094351964.jpg"><br>但是，如果是logn等函数会遇到定义域不存在的情况。我们应该从和式中把（在积分中）没有定义的点先拿出来，再去积分。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060610042349.jpg"></p><p>用代数方法证明就是放大缩小去求，用积分方法证明就是用上面那个公式。可以看到积分方法求时等式左边按公式应该为$\int_0^n jlogj !\ dj$定义域<br>不存在，所以对于右边：</p>$$\sum_{j=1}^njlogj=\sum_{j=2}^njlogj+1log1=\int_{2-1}^njlogj \!\ dj+1log1$$<h2 id="四、计算次数"><a href="#四、计算次数" class="headerlink" title="四、计算次数"></a>四、计算次数</h2><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法:COUNT输入:n=2k,k为正整数。输出: count的值 。count=0while n&gt;=1for j=1 to ncount=count+1n=n/2return countend COUNT<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>while要执行k+1次，$k=log_2n$.for循环在每次while的基础上执行n次所以(k+1)n即$nlog_2n$次计算</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法: MERGE输入:数组A[1..m]和它的三个索引p, q, r, 1&lt;=p&lt;=q&lt;r&lt;=m。两个子数组A[p..q]和A[q+1..r]各自按升序排列。输出:合并两个子数组A[p..q]和A[q+1..r]的升序数组A[p..r]for(s=p, t=q+1, k=p; S&lt;=q and t&lt;=r; k++)if A[s]&lt;=A[t] //两个指针从两个头开始排序B[k]=A[s]; //B[p..r]是个辅助数组S=S+1;elseB[k]=A[t];t=t+1;if s=q+1 B[k..r]=A[q+1..r] elseB[k..r]=A[s..q]A[p..r]=B[p..r]end MERGE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两段相邻数组分别有序，两个指针将两段变为有序的。2(r-p+1)先遍历一次排序，在从排好序的辅助数组移回来。</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">void insertion_ sort(Type *a, int n){// 代价 次数// ti: for的第i次while的循环次数for (int i=1; i&lt;n; i++){// c1   n(比较语句)key=a[i];// c2   n-1int j=i-1;// c3   n-1while( j&gt;=0 &amp;&amp; a[j]&gt;key ){  // c4   sum tia[j+1] = a[j];// c5   sum of (ti-1)j--;// c6sum of (ti-1)}a[j+1]=key; // c7   n-1}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>插入排序的思想是左手为空，右手的牌按序插入。这里t是个不确定的数，但是还是可以得出计算次数为：</p><p>$$c_1n+c_2(n-1)+c_3(n-1)+c_4\sum_{i=1}^{n-1}{t_i}+c_5\sum_{i=1}^{n-1}{(t_i-1)}+c_6\sum_{i=1}^{n-1}{(t_i-1)}+c_7(n-1)$$</p><p>最好情况就是已经排好序了，c5与c6都是0，每次for的while都只跑一次。<br>$$c_1n+c_2(n-1)+c_3(n-1)+c_4(n-1)+c_7(n-1) = O(n)$$<br>最坏情况就是倒序排的，n张牌每次都比上一次多查找一个。</p><p>$$\sum_{i=1}^{n-1}{(t_i-1)} = n(n-1)/2$$</p><p>复杂度$O(n^2)$</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">1. COUNT42.count &lt;-- 03.for i ←- 1 to Llogn」4.for j ←- i to i+55.for k ←- 1 to i^26.         count ←- count +17.end for8.  end for9.end for(a)第6步执行了多少次?(b)要表示算法的时间复杂性，用0和O哪个符号更合适?为什么?(c)算法的时间复杂性是什么?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>a）思路是把每次for循环乘起来。<br>$$\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}\sum_{k=1}^{i^2}{1}$$内层指的是从1到$i^2$个1相加$=\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}{i^2} = 6\sum_{i=1}^{\lfloor logn \rfloor}{i^2}$<br>利用平方和求和公式$n(n+1)(2n+1)/6$进一步化简$=\lfloor logn \rfloor(\lfloor logn \rfloor +1)(2\lfloor logn \rfloor+1)$<br>b） O 因为对于算出的确切的计算次数，这个用于表示算法时间复杂性的函数是它上界。<br>c）$O(log^3n)$</p><h2 id="五、解递归方程式"><a href="#五、解递归方程式" class="headerlink" title="五、解递归方程式"></a>五、解递归方程式</h2><h3 id="1-线性齐次递推式（二阶）"><a href="#1-线性齐次递推式（二阶）" class="headerlink" title="1.线性齐次递推式（二阶）"></a>1.线性齐次递推式（二阶）</h3><p>对于递推式:$$f(n)=a_1f(n-1)+a_2f(n-2)+…+a_kf(n-k)$$我们想要得到$f(n)$的确切解，它的解往往是$x^n$于是我们可以把这个递推式的等价于:$$x_n=a_1x^{n-1}+a_2x^{n-2}+…+a_kx^{n-k}$$将两边同时除以$x^{n-k}$并且移项可以得到与n无关还能解出x的式子$$x^k-a_1x^{k-1}-a_2x^{k-2}-…-a_k = 0$$这个就是常说的特征方程。</p><table><thead><tr><th align="center">步骤</th><th align="center">例1</th><th align="center">例2</th></tr></thead><tbody><tr><td align="center">序列</td><td align="center">1,4,16,64,256</td><td align="center">1,1,2,3,5,8(斐波拉契)</td></tr><tr><td align="center">递推关系</td><td align="center">f(n)=3f(n-1)+4f(n-2)</td><td align="center">f(n)=f(n-1)+f(n-2)</td></tr><tr><td align="center">特征方程</td><td align="center">$x^2-3x-4=0$</td><td align="center">$x^2-x-1=0$</td></tr><tr><td align="center">特征根</td><td align="center">$x_1=-1,x_2=4$</td><td align="center">$x_1= { {1+\sqrt5} \over 2},x_2={ {1-\sqrt5} \over 2}$</td></tr><tr><td align="center">通解</td><td align="center">$f(n) = c_1{(-1)}^n+c_24^n$</td><td align="center">$f(n)=c_1\left( { {1+\sqrt5} \over 2} \right)^n+c_2\left( { {1-\sqrt5} \over 2} \right)^n$</td></tr><tr><td align="center">带入序列中的点</td><td align="center">$c_1=0,c_2=1$</td><td align="center">$c_1={1\over {\sqrt5}},c_2=-{1\over {\sqrt5}}$</td></tr><tr><td align="center">最终解</td><td align="center">$f(n)=4^n$</td><td align="center">由于n无穷大$c_2\left( { {1-\sqrt5} \over 2} \right)^n$趋于0,$f(n)={1\over {\sqrt5}}\left( { {1+\sqrt5} \over 2} \right)^n$</td></tr></tbody></table><p>还有种特殊情况：$x_1=x_2=x$时$f(n)=c_1nx^n+c_2x^n$</p><h3 id="2-非齐次递推式"><a href="#2-非齐次递推式" class="headerlink" title="2.非齐次递推式"></a>2.非齐次递推式</h3><h4 id="2-1-f-n-f-n-1-g-n"><a href="#2-1-f-n-f-n-1-g-n" class="headerlink" title="2.1  f(n)=f(n-1)+g(n)"></a>2.1  f(n)=f(n-1)+g(n)</h4><p>对于这一类g(n)是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)+g(n) = \big(f(n-2)+g(n-1)\big)+g(n) = f(0)+ \sum_{j=1}^ng(j)$$<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606120205290.jpg"><br>这道题麻烦点在于怎么把前面的系数3搞没了。由1我们知道，这种递推式是$f(n)=x^n$变形令$f(n)=3^nq(n), f(0)=q(0)=3$于是乎原式变为：</p>$$3^nq(n)=3*3^{n-1}q(n-1)+2^n \\ q(n)=q(n-1)+{(2/3)}^n\\ q(n)=q(0)+\sum_{j=1}^n{{(2/3)}^n}\\ f(n)=3^n*\big(3+{(2/3)(1-{(2/3)}^n) \over {1-(2/3)}}\big) \\ f(n)=5*3^n+2^{n+1}$$<h4 id="2-2-f-n-f-n-1-g-n"><a href="#2-2-f-n-f-n-1-g-n" class="headerlink" title="2.2  f(n)=f(n-1)*g(n)"></a>2.2  f(n)=f(n-1)*g(n)</h4><p>对于这一类g(n)也是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)*g(n) = \big(f(n-2)*g(n-1)\big)+g(n) = f(0)\prod_{i=1}^ng(i)$$</p><h4 id="2-3-f-n-f-n-1-g-n-h-n"><a href="#2-3-f-n-f-n-1-g-n-h-n" class="headerlink" title="2.3  f(n)=f(n-1)*g(n)+h(n)"></a>2.3  f(n)=f(n-1)*g(n)+h(n)</h4><p>可以直接推，也可以带点技巧推。结果：$$=\prod_{i=1}^ng(i)\big( f(0)+\sum_{j=1}^n{h(j) \over{\prod_{i=1}^ng(i)} } \big)$$</p><h4 id="2-4-f-n-af-n-c-g-n"><a href="#2-4-f-n-af-n-c-g-n" class="headerlink" title="2.4 f(n)=af(n/c)+g(n)"></a>2.4 f(n)=af(n/c)+g(n)</h4>$$f(n)=\begin{cases} d&amp; \text{n=1}\\af({n\over c})+bn^x&amp; \text{n &gt;= 2} \end{cases}$$<p>其中d非负常量，g(n)非负函数，a，c正数。设$n=c^k$</p>$$f(n)=\begin{cases} bn^x*log_cn^x+dn^x&amp;{a=c^x}\\ \big(d+{{bc^x}\over{a-c^x}}\big)n^{log_ca}-\big({{bc^x}\over{a-c^x}}\big)n^x&amp; {a\neq c^x} \end{cases}$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606123724891.jpg"><br>这道题就没有为难菜鸡，直接把方向给了。这种做法在遇到f(n/2)的情况下叫做<strong>更换变元法</strong><br>$$f(2^k)=f(2^{k-1})+2^k=f(2^0)+\sum_{i=1}^k2^i=2^{k+1}-1 =2n-1\<br>g(2^k)=2g(2^{k-1})+1=\sum_{i=0}^k2^i=2^{k+1}-1=2n-1$$<br>也可以直接套公式:<br>对于f   $d=1,a=1,c=2,b=1$<br>对于g  $a=2,c=2,b=1,d=1$<br>这些公式太惨绝人寰了，其他还有一些和2.4一样复杂的，等遇到了再补充上去。</p><h2 id="六、p、np、np-hard、np-complete问题"><a href="#六、p、np、np-hard、np-complete问题" class="headerlink" title="六、p、np、np-hard、np-complete问题"></a>六、p、np、np-hard、np-complete问题</h2><p><a href="https://blog.csdn.net/birduncle/article/details/94646993">p、np、np-hard、np-complete问题</a>这篇文章讲的很清晰，附上一张从文章里拿的<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210704092654261.png"></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Advanced Mathematics </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
