<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2023</title>
      <link href="/2023/12/31/2023/"/>
      <url>/2023/12/31/2023/</url>
      
        <content type="html"><![CDATA[<p>按照重要程度写，这样就能尽力避免讨厌的无用功了（这些的背景就是5月来到翔安，在自己的科研上进展非常缓慢，找不到一些高效的方式方法，也找不到方向很近的充满热情的小伙伴经常交流。越努力，越感觉自己像个小丑，只是觉得读研3年不能这样，一定要有所改变。与其再把自己大部分精力放上面，不如分散点精力）。以下是对自己做出郑重的承诺，尽一切努力保证所有目标的完成。</p><ul><li><p><strong>早起早睡</strong>。虽然熬夜很酷。早上7:00到8:00能起，晚上11点过能睡，摆脱褪黑素。早上赖床可以试试听歌🎧</p></li><li><p><strong>多去跑步，最好每天都跑，对身体好</strong>。可以环校或者跑香山，300公里累计量，也就是一周7公里至少；争取400公里累计量。少一些下班后的骂骂咧咧。对实验失望的时候就去跑步吧。如果温度不合适去健身房跑也不错，不一定要纠结早上或者晚上。</p></li><li><p>提升当下的<strong>觉察能力</strong>，认识和清理被压抑的情绪和需求，有足够的力量去做“未完成事件”。现在只能“觉”那些情绪，但是没法“察”。或许可以找一些相关案例去带入。</p></li><li><p><strong>持续背单词和刷算法题</strong>。背单词的时候记得造句，雅思词汇6192过两遍，每天不足百个。早晚都得找工作面试，早晚都要刷，先刷剑指offer吧，然后力扣，因为主要还是为了工作刷，所以就功利一点，希望每周或者每两周会有关于算法的博客。</p></li><li><p><strong>巩固计算机专业基础</strong>。看了书也可以写写博客啥的。多读读内核源码也挺有收获的。24年专业相关博客希望上百篇，也就是3天会写一篇，一周2篇的样子。</p></li><li><p><strong>持续投简历</strong>。实习要趁早，最好是和自己的科研方向紧密相关的。</p></li><li><p>读5本哲学、社会学或者科普相关的书。（还要在不找男朋友这件事上和家长斗志斗勇，不要有一点点屈服）</p></li></ul><p>好的，图穷匕现。我还是想想，先想在这里，gjjxj。就算有时候很丧很丧，躺平是不可能的。23年确实与众不同，旅行路上还在谈未来打算的小哥因为新冠走了，大一一起打篮球的姐妹因为工作（数学老师）在精神病院治疗，收到舍友的结婚请柬，班里有一对也会在24年初领证。对于自己，好像我想的还不够清楚，可能更希望能找到一份让我一直有激情的“工作”吧，当然得是新资足够我慷慨的。</p><p>24年真的希望像王备战老师说的那样“每天都有进步，好好学习，天天向上”。</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perspective </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FlexHM</title>
      <link href="/2023/12/22/FlexHM/"/>
      <url>/2023/12/22/FlexHM/</url>
      
        <content type="html"><![CDATA[<h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><div class="note primary"><ul><li>文章来自ACM Transactions on Architecture and Code Optimization, (ATCO), 2022</li><li>FlexHM: A Practical System for Heterogeneous Memory with Flexible and Efficient Performance Optimizations</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>BO PENG, JIANGUO YAO, HAIBING GUAN. 上海交通大学。</li><li>YAOZU DONG, FENGGUANG WU. Intel Asia-Pacific Research and Development Ltd. (好像成都的数据中心就是做云服务相关的，听说工资不太高)</li></ul><h1 id="Abstract摘要"><a href="#Abstract摘要" class="headerlink" title="Abstract摘要"></a>Abstract摘要</h1><p>随着云计算的快速发展，众多的云服务、容器、虚拟机给现代数据中心带来了对高性能内存资源的巨大需求，异构内存，特别是新发布的Optane内存，为云中的DRAM提供了合适的替代方案。具有容量更大、采购成本更低、性能良好等优点，但云服务在使用混合DRAM和Optane内存时，实施起来非常不便，性能下降严重。优化所有虚拟机、容器和原生应用程序的内存访问性能。我们在 Linux 中展示了 FlexHM 的开源原型，其中有几个主要贡献。首先，FlexHM 提出了一种新颖的两级 NUMA 设计，将 DRAM 和 Optane 内存作为透明主内存进行管理。（这也没啥新意啊？）其次，FlexHM提供灵活高效的内存管理，通过定制的管理策略，帮助优化内存访问性能或节省差异化云服务的内存资源采购成本。最后，评估表明，FlexHM上使用50% Optane慢速内存的云工作负载使用全 DRAM 时可实现高达 93% 的性能，当工作负载使用相同比例的 DRAM 和 Optane 内存时，FlexHM 比之前的异构内存系统解决方案提供高达 5.8 倍的提升。</p><h1 id="1-Introduction简介"><a href="#1-Introduction简介" class="headerlink" title="1 Introduction简介"></a>1 Introduction简介</h1><p>内存是云基础设施中对性能最关键的资源之一，关系到众多内存密集型原生服务、容器和虚拟机 (VM) 的服务质量 (QoS, Quality of Service)。云服务提供商 (CSP, cloud service providers) 通常会持续超额订阅内存资源并增加每台云服务器上内存密集型云工作负载的密度以获得更高的利润。假设云服务的内存资源需求超过了一台云服务器的 DRAM 容量。在这种情况下，云服务提供商（CSP）只能转向分布式内存系统，而分布式系统与大规模本地内存系统相比，严重降低了内存资源的可扩展性和利用率[35]。随着Intel Optane DC内存（本文简称为Optane 内存）[19]，慢速主内存（SMM）可以提供可靠的主内存替代品，与 DRAM 结合形成云服务器的异构主内存（HMM）系统。Optane 内存是非易失性内存的一种实现，它包括字节寻址功能，以其容量大、单位容量购买成本低的优势丰富了内存层次结构。目前，Optane最大为512GB per blade，而商用最大DRAM为128GB. Optane每 GB 价格大约是 DRAM 的 1/2-1/3 [6,16]. 但如果用作主存，Optane 与 DRAM 的性能差距不可忽视；例如，Optane 的峰值读取带宽是 DRAM 的 1/3，峰值写入带宽是 DRAM 的 1/6 [20]。</p><p>考虑到异构内存的性能和价格，异构主存（HMM）系统设计最关心的问题是系统如何优化HMM资源利用率，以更实惠的内存资源成本达到良好的业务性能。 在物理服务器（硬件）中构建和运行的云原生应用程序、容器和虚拟机对HMM系统提出了几个基本要求。例如，<br>  -（1）他们更倾向于使用透明异构内存，而不是修改源代码并使用一些库或工具（如针对 Optane 的 PMDK [41]）调整编程模型，因为 PMDK 需要在 Optane 上进行繁重的编程和测试工作，以优化内存性能，而且它限制了框架移植到未来更多慢速内存设备（而不是 Optane）的便利性和灵活性。<br>  -（2）在使用 HMM 时，它们的各种内存使用目的和不同的内存访问模式，以及它们对内存容量和整体吞吐量及延迟性能的多级 QoS 或服务水平协议 (SLA, Service Levels of Agreement)，都大大增加了管理难度，并最终影响了它们的整体内存性能。<br>  -（3）在长时间运行过程中，它们通常会出现内存使用数据分布不平衡的情况，因此它们支付资源的意愿也不尽相同。例如，企业服务或虚拟机的大部分内存访问出现在每天的少数时间里[12]，而个人客户机则在其生命周期的大部分时间里空闲运行。</p><p>内存管理对于操作系统设计中的云内存性能保证和优化具有重要意义，特别是对于异构内存云系统而言，虚拟内存管理[7]、内存虚拟化[58]和非均匀内存访问（NUMA）[28]是基本且通用的操作系统内存管理技术。之前的许多工作通过使用 DRAM 模拟的非易失性内存 [1,4,10,25,30,33,38,55] 或内存控制器来研究现代系统的高效 HMM 管理解决方案模拟器[5,17,32,40,42,46,52]。然而，随着Optane内存的发布，最近的研究[34,49,57]已经证明，以前基于仿真的解决方案无法在实际应用中有效执行Optane 硬件以及那些模拟的内存设备因此，构建一个实用的系统来支持和管理真实的 DRAM 和 SMM 设备（例如 Optane）并优化本机内存工作负载、容器和来宾机器的性能和利用率引起了学术领域和操作系统开发者社区人们的关注。</p><p>在本文中，我们提出了 FlexHM，一种实用的异构主内存系统，具有灵活的 HMM 管理机制，因此云应用程序、容器和虚拟机可以透明地使用 HMM 并采用定制的管理策略来实现性能和价格高效的优化。FlexHM 构建了两个层NUMA（Distance and Heterogeneity Peer）在主流操作系统传统一层NUMA的基础上提供透明的HMM资源和内存虚拟化支持。然后FlexHM基于操作系统内核空间的内存性能监控与操作系统用户空间的使用分析和优化之间的周期性交互，对HMM进行管理。具体而言，我们研究了 FlexHM 的激进和保守管理策略，并证明 FlexHM 可以灵活高效地处理来自不同云工作负载的差异化 HMM 使用管理和性能优化需求。最后，我们通过微基准测试和应用程序基准测试（内存数据库和图计算基准测试）展示了 FlexHM 在综合内存密集型工作负载下的评估结果。例如，<br>-（1）使用 DRAM 和 Optane 慢速固定比例的微基准测试内存 (SMM) 的性能可达到 Thermostat 的 5.8 倍 [1]；<br>-（2） 使用 HMM (DRAM: Optane = 1:1) 的微基准测试可实现比全部使用 DRAM 的基准性能高达 93% 的性能；🤨😮<br>-（3） PageRank 应用程序基准测试可以使用较低百分比的 DRAM 资源（例如 DRAM:Optane = 1:3）来达到 5.7×性能提升与最先进的解决方案相比（使用 DRAM: Optane = 1:1），并节省 50% 昂贵的 DRAM 资源。😲</p><p>综上所述，本文做出以下贡献：</p><ul><li><p>我们提出了 FlexHM，这是一个实用的开源系统，可在云中实现灵活且内存高效的性能和利用率优化，所有来宾、容器和原生应用程序都可以透明且高效地使用异构主内存。</p></li><li><p>我们在 Linux 中实现了 FlexHM 原型，用于管理透明异构主内存，包括 DRAM 和真正的 NVM 硬件 Optane 内存，并为基本 HMM 管理添加必要的内核模块，<a href="https://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux.git">开源</a>。</p></li><li><p>我们在FlexHM上设计了灵活高效的HMM管理，可以灵活采用定制的HMM管理策略，高效处理差异化的HMM使用需求，并<a href="https://github.com/intel/memory-optimizer">开源</a>，供社区进一步开发更多HMM 管理策略。</p></li><li><p>我们进行了全面的评估，以证明 FlexHM 系统可以为本机或来宾内存密集型工作负载提供高效、灵活的 HMM 管理。</p></li><li><p>我们讨论了FlexHM选择激进或保守管理策略时的管理效率，以及FlexHM与Optane两级内存模式等硬件缓存的优缺点。</p></li></ul><p>本文接下来的内容安排如下： 第 2 节介绍了 FlexHM 的背景和动机。第 3 节介绍 FlexHM 系统的总体设计，第 4 节介绍用于 HMM 的双层 NUMA，第 5 节介绍灵活高效的 HMM 管理和性能优化。第 6 节介绍 Linux 原型的实现细节。第 7 节展示评估结果。第 8 节讨论了 FlexHM 的开销以及使用 DRAM 作为缓存的 HMM 系统之间的利弊。本文在第 9 节中结束。</p><h1 id="2-BACKGROUND-AND-MOTIVATION"><a href="#2-BACKGROUND-AND-MOTIVATION" class="headerlink" title="2 BACKGROUND AND MOTIVATION"></a>2 BACKGROUND AND MOTIVATION</h1><h2 id="2-1-Heterogeneous-Main-Memory"><a href="#2-1-Heterogeneous-Main-Memory" class="headerlink" title="2.1 Heterogeneous Main Memory"></a>2.1 Heterogeneous Main Memory</h2><p>Intel Optane DC持久内存（也称为Optane NVDIMM，本文简称为Optane内存）[19]是一种高性能商用NVM设备，Optane内存采用3D XPoint技术，内存颗粒密度更大，可以提供比 DRAM 更大容量、更经济实惠的主存资源。先前的研究[20, 56]表明，与 DRAM 相比，Optane 显示出相似的空闲延迟，但随机顺序加载延迟约为 DRAM 的 5 倍。我们还利用英特尔内存延迟检查器[51]进行了实验，并在图 1 中展示了测得的顺序和随机读写性能结果。从结果中我们可以发现，Optane 内存的随机读写性能比 DRAM 的顺序读写性能有更大的差距，而 Optane 甚至是最先进的慢速主存储器设备。因此，我们可以很明显地推断出，在操作系统中使用非易失性内存来取代易失性内存并重建当前的计算架构仍然是不现实和不可接受的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/c4d80314373749438ca300437c6ce50b.png" alt="MLC Load latency and throughput results of DRAM and Optane in one NUMA node （six DRAM and six Optane blades）"></p><p>Optane内存可以大幅提升一台云服务器的本地内存容量，提高服务、容器、虚拟机的密度，我们可以设想Optane以及未来的一些SMM设备可以与DRAM共存，作为云系统的异构主内存。目前Optane主要有两种使用模式，分别是APP Direct（AD）模式和内存（MM）模式，在AD模式下，操作系统最初将Optane内存设备视为字节可寻址但持久化的设备（不像主存，而是类似于闪存和磁盘）。在MM模式下，Optane内存使用来自相同内存通道的DRAM来构建包容性硬件缓存模型[21]，操作系统可以像易失性一样直接使用Optane. 因此，MM模式是现代系统的初步异构主存硬件解决方案，它可以通过固定的缓存替换算法（例如直接映射或LRU）从SMM加载和刷新DRAM缓存线。硬件解决方案存在三个主要缺点：（1）随着SMM容量的不断增长，硬件缓存实现将更加复杂和不切实际[32, 42]。（2）使用DRAM作为包容性硬件缓存会牺牲整个系统的总内存容量例如，当一台双路服务器配备 12 64 G DRAM blades和 12 512 GB Optane blades时，将浪费超过 10% 的总内存容量。 （3）硬件缓存只能采用刚性缓存替换策略（例如直接映射，伪LRU），在解决不同的云工作负载场景时不灵活。</p><blockquote><p>这里其实就说硬件缓存实现复杂、内存浪费多、缓存策略不灵活</p></blockquote><h2 id="2-2-Heterogeneous-Memory-Management"><a href="#2-2-Heterogeneous-Memory-Management" class="headerlink" title="2.2 Heterogeneous Memory Management"></a>2.2 Heterogeneous Memory Management</h2><p>异构主存资源的内存管理长期以来一直是研究热点[1,13,14,23,24,30,31,38,43,44,50,53,54,55]，我们表 1 总结了一些重要的研究工作。具体来说，Thermostat [1] 是一个典型的系统，它构建了一个应用程序透明的大页面感知机制，将具有在线页面热度分类的页面放置在 QEMU 的虚拟 NUMA 上。 SMM具有页面热度，但解决方案受到虚拟机的限制，无法满足原生云进程和容器的要求，而且这些相关工作的局限性在于它们通常使用DRAM来模拟慢速的NVM或使用内存模拟器来构建模拟异构内存环境，因此这些工作没有充分考虑真实 NVM 硬件的行为和性能来提供有效的优化。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/8acce03dbc7745f8b5efe852d10805cb.png"></p><p>具体来说，我们对这些先前的工作在实际 HMM 硬件上的表现进行了初步评估，我们选择来自 Github [37] 的 Thermostat 开源实现，构建系统，并使用 QEMU 的虚拟 NUMA 来运行 32- VCPU虚拟机，可以管理真实的DRAM和Optane内存作为其HMM资源。我们在这个VM中运行32线程Sysbench随机读写微基准测试，并比较Thermostat、MM模式（其中服务器的DRAM容量为其总Optane容量的1/4），并且是全DRAM基线。我们在图2中展示了内存访问吞吐量性能结果。从实验数据中我们可以发现，当我们让客户机使用 25% 的 DRAM 和 75% 的 Optane 内存作为其内存资源时（使用与 MM 模式环境相同的容量资源），与全部使用 DRAM 相比，客户机只能分别提供 18% 的随机读取性能和仅 3.1% 的随机写入性能。与使用 MM 模式为该虚拟机提供异构内存相比，Thermostat 的性能结果也要差得多。</p><p>我们进一步分析了Thermostat无法像在DRAM模拟的慢速内存上那样在真正的异构主硬件内存上高效工作的原因，根据参考文献[1]中的实现和评估环境，Thermostat的设计基于一个重要假设：异构内存性能，即慢速内存的延迟为约1𝜇. 但吞吐量并不比快速 DRAM 内存低。然而，尽管 Optane 内存是最先进的慢速内存硬件之一，并且它在延迟性能方面可以比假设更好地工作，但 Optane 和 DRAM 之间存在非常显着的吞吐量性能差距，特别是随机读写性能，由于Thermostat的性能优化是基于其系统内核中将热数据迁移到快内存、将冷数据迁移到慢内存，因此会在DRAM和Optane内存之间产生非常大的内存页面移动需求，因此当Thermostat使用非常激进的策略来进行异构内存页面的热度收集和迁移时，Optane的吞吐量性能可能会成为瓶颈，并且可能会比DRAM和Optane内存之间产生更多的内存访问和迁移开销。Thermostat可以对一些随机读写系统内存吞吐量较低的应用程序基准进行高效优化，不会给系统内核带来严重的热度收集和迁移开销，但它不能灵活地改变策略当密集型工作负载占用Optane内存峰值带宽时，在性能提升和开销之间取得更好的平衡。</p><p>因此，我们的目标是设计和实现FlexHM，为云计算提供灵活高效的HMM管理和性能优化。我们推断FlexHM必须具备以下功能，以处理来自原生服务、容器、以及云系统的虚拟机：</p><ul><li>1）支持透明 HMM 使用和完全 HMM 虚拟化： 我们的目标是为所有原生云工作负载、容器和客户机提供灵活的透明 HMM 容量资源，而无需对其源代码进行任何修改。</li><li>2）使用固定HMM比率优化工作负载的性能：我们的目标是为具有固定HMM比率的工作负载提供运行时内存管理，以动态地将热数据从其SMM区域移动到DRAM区域并交换冷数据进入SMM区以达到更优化的内存访问性能和资源利用率。</li><li>3）优化 HMM 利用率，以达到更实惠的价格和可接受的性能下降：我们的目标是通过轻微且可接受的性能牺牲来节省 HMM 工作负载的内存购买成本，但满足其 QoS。我们可能会稍微减少 DRAM 百分比，增加SMM（Optane NVM）百分比，动态挑选冷数据并将其交换到SMM中。<blockquote><p>这感觉也没说啥，都是这样想的啊不光云服务</p></blockquote></li></ul><h1 id="3-FLEXHM-OVERVIEW"><a href="#3-FLEXHM-OVERVIEW" class="headerlink" title="3 FLEXHM OVERVIEW"></a>3 FLEXHM OVERVIEW</h1><p>我们设计了FlexHM，一个实用的系统来管理异构主内存，具有高效灵活的运行时内存管理机制，以提供透明和快速的DRAM和其他SMM（例如Optane内存）并优化利用率。FlexHM架构的概述如图所示3，我们介绍了从硬件配置到系统内核以及用户和内核级协作的HMM管理的重要设计。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/3f7a437b0f0e4d81b480441ba52d12e1.png"></p><p>FlexHM硬件配置：FlexHM是一个建立在真实异构内存环境上的实用系统，例如使用DRAM作为快速主内存，使用Optane内存作为慢速主内存。与Optane的内存模式不同（Optane使用所有DRAM作为其硬件缓存），FlexHM系统使用的Optane资源在服务器BIOS中配置为APP Direct模式，因此FlexHM不会浪费DRAM容量，并且FlexHM内核可以将所有DRAM和Optane容量管理为所有进程的可用内存。这种配置有助于提高云服务器所有昂贵内存资源的更多效益。</p><p>FlexHM内核：我们基于Linux设计并实现了FlexHM内核功能，首先，FlexHM内核实现了异构内存硬件（例如DRAM和Optane）的透明资源管理，因为Optane本来就是作为一种更快的存储设备来使用的在配置APP Direct模式时，我们更新了Linux内核中原有的内存管理功能，将Optane作为缓慢但应用程序透明的内存资源进行管理，保持内核干净简洁，提高HMM管理和性能优化的效率，我们设计了一种用户和内核级协同的HMM管理机制来提高管理灵活性和效率，FlexHM内核为用户级管理器提供了基础支持，具体来说，我们<span class="label success">引入了“kvm-ept-idle”模块</span>用于将裸机原生服务和访客机的内存访问模式检查到FlexHM内核中。<span class="label success">该模块可以提供可靠的内存使用信息，但不会直接触发任何系统进程的任何内存管理操作</span>。FlexHM可以继承所有基本内存目前Linux系统的管理模块，我们只在FlexHM内核中引入少量内存页面管理策略来优化每个进程的异构内存分配性能。</p><p>FlexHM用户级管理器：我们在FlexHM的用户空间进一步设计了一个名为Memory-optimizer的HMM资源管理器，该管理器可以<span class="label success">确定每个系统进程的快慢内存利用率之间的任意比例</span>，并提供性能优化和利用率管理。可以<span class="label success">针对不同的云应用、容器或Guest采取不同的定制管理策略</span>，以满足其对HMM资源的不同需求或优化HMM资源利用率，具体可以利用来自内核的内存引用信息，分析工作负载的内存模式并触发异构内存之间的内存移动。如果工作负载希望在固定的 HMM 比率上优化性能，那么管理器可以动态地将其内存中的热数据选择到 DRAM 中，并将冷数据交换到 SMM 中。如果工作负载希望使用更多的 SMM 资源为了节省成本，它可以分析内存访问模式并提高其 SMM 使用百分比，而不会丢失 QoS。</p><p>（写到这里我终于知道作者干了什么事了，不过这些策略不是云相关的也可以用吧，好像没有太多云相关的应用特点被加入到作者的设计当中。）</p><h1 id="4-TRANSPARENT-HETEROGENEOUS-MEMORY-IN-FLEXHM"><a href="#4-TRANSPARENT-HETEROGENEOUS-MEMORY-IN-FLEXHM" class="headerlink" title="4 TRANSPARENT HETEROGENEOUS MEMORY IN FLEXHM"></a>4 TRANSPARENT HETEROGENEOUS MEMORY IN FLEXHM</h1><p>在 FlexHM 系统中，Optane 内存在服务器 BIOS 中配置为 APP Direct 模式。但是，当操作系统在 Optane 上构建文件系统时，采用 APP Direct 模式的 Optane 不是主内存，而是最初用作比 NVMe SSD 更快的存储设备。因此，我们必须为操作系统的内存管理功能添加必要的支持，以将DRAM和SMM（如Optane）作为透明主内存进行管理。</p><p>考虑到非均匀内存访问（NUMA）可以有效地管理来自每个服务器不同硬件插槽的内存，我们在FlexHM中设计了两层NUMA，将所有HMM资源作为应用程序透明的异构内存进行管理。（让我来看看他和传统使用方法到底有啥区别……）两层NUMA是一种扩展主流操作系统中传统 NUMA 的特点，传统 NUMA 是一层 NUMA，因为它只考虑 CPU 访问距离，与本地或远程访问延迟有关；也就是说，传统 NUMA 只有一个特性讲述硬件socket的信息，并且不能准确描述考虑到内存异构性的内存引用差异，因此，为了管理内存异构性，我们引入了一个名为 “NUMA peer”的概念，作为第一层 NUMA 访问距离特性之外的第二层 NUMA 特性。因此，如果只有 Optane NVDIMM，操作系统可以将来自同一硬件插槽内存控制器的所有异构内存设备分离成一个 DRAM 对等节点和一个 SMM 对等节点；如果未来服务器上安装了多个 SMM 硬件，则可以分离成多个不同的 SMM 对等节点。（就是再给NUMA节点加了一个属性，MC是在热插拔时加上去的，这个不知道会用什么手段去定义？）距离和对等功能共同构成了 FlexHM 中的双层 NUMA，一个 FlexHM 系统可以拥有插槽数*内存硬件数的 NUMA 节点。例如，我们在图 4 中演示了双插槽服务器中的双层 NUMA 如何管理 DRAM 和 Optane。如果未来出现其他字节可寻址的慢速主存储器设备，那么双层 NUMA 设计也能轻松管理 DRAM、Optane 和其他 SMM 等更复杂的内存异构。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/339a2cb87cdf4faf8f1f9f2e8a388711.png"></p><p>由于我们的FlexHM将所有DRAM和Optane作为两层NUMA的不同NUMA对等节点内的透明内存资源进行管理，因此我们引入了两个新概念：页面管理的升级和降级。具体来说，NUMA对等节点可以有升级和降级目标在FlexHM两层NUMA中，例如，内存页面可以从DRAM对等节点降级到SMM（Optane内存）对等节点，最后被回收到交换区域，反之亦然。如果我们有超过一个SMM，那么降级可以从DRAM开始到较快的SMM节点，然后到较慢的SMM节点，以此类推，升级反之亦然。升级/降级是FlexHM内存页面管理的基础，它给出了内核或用户级进程在不同 NUMA 对等节点之间移动内存页面的通用接口。</p><blockquote><p>（审稿人会接受这是新概念吗？啊哈？可能好像以前的工作也没有明说咱们要把所有慢速的都归为第二层，那么你后续的设计最好让我们能看出来，是有针对各种慢速内存构成的第二层的特有的策略。因为如果慢速层，你的设计只有一种和上文NUMA peer的功能描述对不上，那不就相当于传统分层）而且，谁说SMM都可以线性得出一个排序。总之这种给NUMA第二个特点定义的，笔者有想过，但是什么时候去定义，用什么方式定义，一直没有觉得比较合适的方法。再加上现在已知数据的混合内存的设备也就那几种。</p></blockquote><p>FlexHM 中的双层 NUMA 可以管理异构主内存的所有容量，并为所有系统进程提供透明的内存资源。此外，FlexHM 支持完整的 HMM 虚拟化，与使用硬件辅助虚拟化支持（如扩展页表 (EPT)）没有冲突。因此，从 CSP 的角度来看，每台运行 FlexHM 系统的服务器都可以提高云应用程序、容器和使用异构主内存的虚拟机的部署密度，而 CSP 也可以因为更好的内存资源可扩展性而提高利润。从云用户的角度来看，FlexHM 可以为他们提供极大的便利，让他们无需修改任何源代码就能使用更实惠的异构内存。此外，FlexHM 还可以通过使用一定比例的慢速内存来替代快速 DRAM，为他们的工作负载提供更实惠的内存资源选择。</p><blockquote><p>看到这里好像也没有介绍清楚NUMA peer的实现细节，后面说不定会写。笔者猜测在NUMA结构体加了一个属性，并且可以人为定义这个属性，整个物理地址仍然是平面且顺序的。</p></blockquote><h1 id="5-FLEXIBLE-AND-EFFICIENT-HMM-MANAGEMENT"><a href="#5-FLEXIBLE-AND-EFFICIENT-HMM-MANAGEMENT" class="headerlink" title="5 FLEXIBLE AND EFFICIENT HMM MANAGEMENT"></a>5 FLEXIBLE AND EFFICIENT HMM MANAGEMENT</h1><p>FlexHM现在可以支持透明的HMM使用以及基于两级NUMA设计的全HMM虚拟化，但是与使用所有快速内存（例如DRAM）相比，使用HMM的原生应用程序、容器和虚拟机将受到严重影响如果直接使用HMM而不进行内存管理，性能会下降，因此，参考我们在2.2节中推断的系统需求，FlexHM必须灵活处理不同云服务租户对异构内存使用和优化目标的复杂且差异化的要求。</p><blockquote><p>“不同云服务租户对异构内存使用和优化目标的复杂且差异化的要求”到底是怎么个差异和复杂化，这个问题反应的操作系统层面时又是什么问题，哪些瓶颈？”由于原生云系统上的应用程序、容器和虚拟机具有不同的内存资源需求和差异化的内存访问模式”云提供商在一台物理机上，应该是提供了差不多的应用的吧。</p></blockquote><p>HMM 管理的一个简单的想法是扩展当前的内核级内存管理策略来处理内存密集型云工作负载的 HMM 管理和优化需求。然而，这种设计可能有几个缺点。一方面，由于原生云系统上的应用程序、容器和虚拟机具有不同的内存资源需求和差异化的内存访问模式，操作系统内核很难设计一个全域策略来有效地管理和优化 HMM 差异化工作负载。如果我们在内核中设计并采用多种HMM管理策略，那么就会使原有的内核内存管理功能严重复杂化，牺牲代码维护的便利性，最终损害内核的稳定性。</p><p>在设计 FlexHM 时，我们选择了用户与内核级合作的 HMM 管理和优化设计。具体来说，我们只在 FlexHM 内核中添加了几个基本的 HMM 管理策略，以确保从操作系统的角度来看（第 5.1 节）内存分配和管理功能是正确的。内核 HMM 管理通常会在新应用程序、容器或虚拟机在 FlexHM 系统上初始化时触发，因此这种管理是针对每个工作负载本身的粗粒度管理，而不是运行时管理功能。此外，我们在 FlexHM 中设计了一种更精细的运行时 HMM 管理机制，通过用户空间管理器与内核之间的交互，提供灵活高效的 HMM 管理，并可采用定制的管理策略（第 5.2 节）。</p><h2 id="5-1-Memory-Management-in-Kernel"><a href="#5-1-Memory-Management-in-Kernel" class="headerlink" title="5.1 Memory Management in Kernel"></a>5.1 Memory Management in Kernel</h2><p>我们在 FlexHM 内核中添加了与两级 NUMA 设计相对应的三种基本内存管理策略，以便内核可以明智地进行内存分配和管理： (1) 如果新应用程序或来宾 VM 初始化，则内核默认从DRAM对等节点分配，而不是按照 NUMA 交错策略分配页面（具体来说，如果 DRAM 节点中没有内存，则页面分配直接落在 SMM 节点上。） (2) 运行在一个peer NUMA原生应用程序或VM显式地从peer NUMA节点中分配页面。如果此对等节点中没有内存，则内核将从具有相同 NUMA 距离的其他对等节点中分配内存。 (3) 页表PT被迫分配并保存在 DRAM 对等节点中。页表对性能至关重要，因为当应用程序对内存数据进行大量随机读/写操作时，页表会被频繁获取和更新，<span class="label warning">而在进行页表转换时，近一半的内存访问是由 TLB 错失引起的。</span></p><blockquote><p>策略2不也很容易没有吗？你这个一半为什么没有实验、没有引用就这样直说了。就算你分在PM node 多次访问也会被迁移去DRAM甚至LLC.</p></blockquote><p>而且，当内核发现 DRAM 对等节点没有可用空间时，会自动触发页面降级，类似于 kswapd 将页面从内存交换到磁盘上的交换分区，内核页面降级的频率与 kswapd 一样低，因此不会给内核内存管理带来严重的开销。如果操作系统有交换区，FlexHM还利用kswapd定期将SMM最不常用的内存数据移动到交换区。这有助于确保FlexHM上的进程不会分配即使操作系统的 DRAM 和 SMM 资源都被占用，也会影响交换区的内存资源，影响系统内存性能。</p><p>这些内核管理策略是简单、静态且通用的，适用于系统中的所有进程，这有助于保持操作系统内核的稳定。</p><h2 id="5-2-Runtime-Page-Management"><a href="#5-2-Runtime-Page-Management" class="headerlink" title="5.2 Runtime Page Management"></a>5.2 Runtime Page Management</h2><p>为了在FlexHM中进行更高效的HMM管理，除了内核管理之外，我们还需要更细粒度的管理机制。通常，在有内存页的操作系统中，每个内存页上每个数据的引用与每个CPU硬件的成本成正比当云应用程序长期对其内存页产生大量读/写操作时，缓存行会被加载和刷新，并且缓存行采用交错策略。因此，我们预测每个填充进程的内存读/写性能是相关的此外，由于当CPU缓存加载和刷新数据时，字节可寻址异构主存设备有不同的页面引用成本，因此整体性能与快速DRAM中页面引用成本的总和相关。因此，我们在FlexHM中设计了一种运行时页面管理机制，可以灵活决定快慢内存的使用百分比，并遵循定制的激进或保守的HMM管理策略。</p><p>在FlexHM中，我们设计了一个名为Memory-optimizer的用户级管理器来对每个原生进程、容器和虚拟机执行运行时细粒度的页面管理，该管理器以周期性过程分三个主要阶段执行页面管理：首先，在收集阶段，用户级管理器通过检查页面引用热度来计算页面引用频率；然后，在分析阶段，管理器进行页面热度分析，以预测运行时内存访问模式和数据引用最后，管理器根据资源利用率和定制的管理策略在HMM之间进行内存迁移，为Action阶段的不同工作负载场景提供优化。</p><p>FlexHM HMM页面管理的主要目标是，它可以根据收集的数据，将目标内存工作负载中最常引用的SMM页面识别为已识别的热SMM页面，将那些最空闲引用的DRAM页面识别为已识别的冷DRAM页面。所有参考信息尽可能正确。需要指出的是，识别的热SMM页面和识别的冷DRAM页面是测量值，可能与每个工作负载的真实热/冷数据存在一些差异。因此，FlexHM用户级别管理器旨在灵活地采取不同的管理策略，以提高运行时内存管理和性能优化过程中热/冷页监控、分析和识别的准确性。</p><blockquote><p>对虚拟机的热页面的收集，问题是在于本机的页表量是虚拟机页表量的好多倍，还是会多出很多页表的扫描哩。还麻烦的点在于你页面迁移，如果在关键路径，重新建立页表映射关系之类的，挺麻烦哩。</p></blockquote><p>我们用图 5 演示周期性页面管理程序。用户级管理器会在短时间 e 内定期重复该程序，包括收集、分析和行动阶段。具体来说，在每个 epoch e 中，管理器首先会以指定的时间间隔 i 进行热度收集，直到最大引用次数 r 为止，以监控和计算在一个 epoch e 中所有页面被引用的次数。r*i 时间（整个 epoch e 内）的页面热度信息，并预测目标工作负载和相同热/冷页面的数据分布。最后，管理器从相同的热 SMM 页面（以及相同的冷 DRAM 页面）中选择特定的内存大小，并进行页面晋升和降级，以优化目标内存的使用，从而达到自己的管理目标。最后，管理器休眠并等待下一个周期，以减少频繁的页面管理开销。下面三段将介绍更多细节。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/6d54bc30580e44c39fdcc59196fa40da.png"></p><p>首先介绍收集阶段，收集阶段对于监控页面引用信息至关重要，我们在FlexHM内核中设计了一个名为“kvm-ept-idle”的模块，将原来的管理更新为页表（PT）和扩展页表（EPT），具体增加了两个接口来检查和清除原生进程页面（包括原生应用程序和常规容器）或虚拟机（如QEMU）的匿名页面的PFN的Access位，接口可以调用分别来自内核空间和用户空间。因此，模块每次调用该接口时都会检查某个页面是否被引用。之后，如果该位为 1，内核模块会将 Access 位清为 0。每当重新检查该页面的 Access 位时，Access位的变化可以判断两次检查之间目标进程的页面是否被引用，在FlexHM设计中，用户级管理器以时间间隔i调用kvm-ept-idle模块的接口r次。我们在用户级管理器中为每个目标使用一个哈希映射来存储目标内存页面的HVA地址和对等节点信息，其中还记录了这些内存页面的页面引用类型（例如，读、写、执行） ）在Collection阶段，管理器可以根据目标进程的ProcMaps结构正确检查不同目标进程的内存页面，监控目标进程的页面引用信息。</p><p>其次，FlexHM将测量到的目标进程的页面引用信息进行拼接，并投影其页面引用数据分布，<span class="label primary">之前的工作已经证明，系统中，特别是云计算中的内存引用分布遵循Zipfian [15,47]，Gaussian [36]，或者Pareto[2]，如果页面引用遵循特定的数据分布，那么 FlexHM 就可以根据这种数据分布提高随机内存读写的性能</span>。此外，FlexHM 还能针对不同内存参考分布的各种云工作负载灵活采用定制的页面管理策略。在 FlexHM 中，用户级管理器为每个目标维护一个数据结构，用于存储所有目标工作负载内存页的引用计数。页面引用信息存储在用户级内存区域，因此不会对有限的内核内存资源造成严重的内存占用。在”Collection”阶段之后，管理器可以计算不同虚拟内存区域（VMA）中所有页面的页面引用计数，并将其从 r 到 0 排序。然后，FlexHM 可以分析包含最常引用页面的虚拟内存区域，并预测工作负载遵循一定的数据分布。然后，管理器会检查这些页面是否位于 DRAM 或 SMM 对等节点中，并分析确定的应移动的热 SMM（和冷 DRAM）页面的总大小。</p><p>最后，在操作阶段，管理器可以通过调用 DRAM 或 SMM 对等节点之间的 move_pages() 系统调用，将相同的热 SMM 页面提升到 DRAM 节点，并相应地将具有相同大小的相同冷 DRAM 页面降级到 SMM 节点。 DRAM 和 SMM NUMA 对等节点之间的通信会占用 DRAM 和 Optane 内存硬件的部分内存带宽，因此管理器会休眠并等待一段时间，直到下一个周期管理纪元，以避免较高的管理开销。</p><h2 id="5-3-Flexible-Management-Policies"><a href="#5-3-Flexible-Management-Policies" class="headerlink" title="5.3 Flexible Management Policies"></a>5.3 Flexible Management Policies</h2><p>在 FlexHM 运行时页面管理中，用户级管理器在 r∗i 时间的页面引用分布投影作为 epoch e 中热度分布的代表，并最终投影出目标工作负载的数据引用分布。<span class="label primary">如果工作负载在其生命周期内具有相对稳定和有规律的内存访问模式，那么每个时间点的投影将非常准确</span>。而如果工作负载由多个进程产生，且其内存模式在短期内不稳定，那么如果工作负载显示出非常不规则的内存访问模式，管理器可能会在不同的时间段显示不同的热/冷页面。因此，使用每个纪元的数据分布预测并不准确，而且会导致一些不必要的热/冷页面移动，从而无法有效优化性能，并带来高昂的管理开销。</p><p>FlexHM 为用户级管理器中的i、r、e参数的灵活调整机制，以便设计和定制不同的页面管理策略。就时间间隔而言，使用较小的时间间隔 i 可以增加页面热度收集的频率，并有助于提高热度测量信息的准确性。不过，这可能会给页表或 EPT 参考带来额外开销，并降低系统内存性能。同样，使用较小的 epoch 可以增加页面移动频率，从而更好地利用 HMM 内存资源，提高性能。不过，这可能会给系统内存带来额外开支，因为 DRAM 和 SMM 对等节点之间的页面移动会占用大量内存带宽。此外，较大的 r 参数有助于提高分析和描述页面引用分布特征的准确性，但使用较大但不合适的 r 会降低页面移动的灵敏度，损害管理效率。</p><p>一种是激进策略，尝试将所有已识别的热页从 SMM 提升到 DRAM 节点，并在每个 epoch 中专门降级已识别的冷页；另​​一种是保守策略，仅提升一定比例的已识别热页并降级已识别的冷页。每个epoch中冷页的比例相同。对于一些内存访问模式稳定的内存工作负载，激进策略可以立即达到异构内存的最佳资源利用率，并减少长期运行时页面管理开销。对于一些内存热度较高的密集工作负载是不规则的，选择保守的迁移策略优于激进的策略，主要原因如下：（1）大量页面的频繁页面迁移可以优化目标工作负载的页面引用延迟，但会产生严重的迁移延迟开销。（2）每个epoch收集到的页面热度可能会有很大差异，我们需要用它来预测未来的内存引用分布，保守的策略有助于提高预测精度，优化管理效率。（3）页面迁移过程会占用内存带宽，保守的页面迁移可以减少迁移过程与目标工作负载之间的带宽干扰。因此，FlexHM 的内存优化管理器还提供了一个参数 “max_migration_size”，用于决定每个 epoch 之后需要在 DRAM 和 SMM 之间迁移的已识别热/冷页面的百分比，以配合 i、r、e参数配合使用。我们将在第 6 节中介绍参数选择，并在第 7.5 节中通过实验来评估在相同冷/热页面的百分比方面，我们应该选择激进还是保守的策略。</p><h2 id="5-4-Memory-optimizer-task-refs-and-sys-refs-Tools"><a href="#5-4-Memory-optimizer-task-refs-and-sys-refs-Tools" class="headerlink" title="5.4 Memory-optimizer task-refs and sys-refs Tools"></a>5.4 Memory-optimizer task-refs and sys-refs Tools</h2><p>当优化目标工作负载是主机操作系统或客户计算机上的单个应用程序时，我们开发了用户级内存优化管理器的任务引用工具，用于内存性能和使用优化。针对进程（包括qemu-kvm进程），我们开发了sys-refs工具来提供多进程的优化，sys-refs还可以从全局角度管理FlexHM系统的所有进程。</p><p>task-refs工具利用目标进程的pid为单个原生工作负载或虚拟机工作负载提供定制化的性能和资源利用率优化，在启动task-refs工具之前，云管理员可以与云服务租户达成一致优化目标；例如，task-refs 可以通过在 HMM 页面上提供运行时性能监控并触发 DRAM 和 SMM 对等节点之间的页面迁移，使用固定比例的 HMM 来优化云工作负载的性能，或者可以从 DRAM 中移动冷数据将节点转换为SMM节点，以优化HMM资源利用率，并为每个云工作负载提供更实惠的价格。task-refs可以灵活接受间隔参数i，最大参考参数r，epoch参数e和“max_migration_size”来定制不同的管理策略。</p><p>如果系统面临多个应用程序或虚拟机产生的复杂内存工作负载，从而导致系统内存干扰和短缺，那么我们设计一个功能更强大的sys-refs工具来同时优化不同进程的内存，以在分配时达到全局优化结果多个工作负载的异构内存资源。算法1描述了sys-refs如何工作的基本过程。sys-refs可以接受多个目标工作负载的描述，例如“qemu-system-x86_64”，为所有虚拟机提供管理运行在FlexHM系统上，如果系统管理员没有指定任何目标，那么sys-refs可以根据pids为所有系统进程提供优化。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/00a60f25abe5493ab30338f3e09e962d.png"></p><p>与task-refs工具类似，sys-refs可以接受参数，针对不同的优化目标定制不同的HMM管理策略，并且sys-refs工具可以动态更新页表或EPT不同时刻的热度收集间隔参数i根据实际测得的每轮行走时间和页面热度变化进行行走，如果测得的行走时间远小于采集间隔，且每次页面扫描差异较小，那么sys-refs可以放大扫描间隔，因为整个系统是可能面临内存工作负载较低，或者如果一轮测量的行走时间很长，并且每次页面扫描在虚拟地址上显示出非常不同的热页分布，那么 sys-refs 可以缩小扫描间隔参数以获得更准确的参考热度。当内存密集型工作负载空闲且内存引用分布没有变化时，sys-refs工具可以动态放大 epoch 参数 e，以减少运行时 HMM 管理开销。</p><h1 id="6-FLEXHM-IMPLEMENTATION"><a href="#6-FLEXHM-IMPLEMENTATION" class="headerlink" title="6 FLEXHM IMPLEMENTATION"></a>6 FLEXHM IMPLEMENTATION</h1><p>FlexHM的实现在Linux系统上实用，并于2018年开源。具体来说，FlexHM内核是在Linux内核上修改的，<a href="https://git.kernel.org/pub/scm/linux/kernel/git/wfg/linux.git">开源于</a>，此外，FlexHM用户级管理器的<a href="https://github.com/intel/memory-optimizer">原型代码</a>。</p><p>内核实现对于Optane内存来说是实用的，并且基于Linux开源了1000多行代码，没有对硬件BIOS和用户空间应用程序或虚拟机进行任何修改，从而成功支持全虚拟化。具体来说，我们现在添加了Optane将内存作为慢速主内存类型放入 <code>e820表</code>中，并修改<code>acpi/numa</code>模块，以便我们可以将 SMM 设备存储到 NUMA 对等节点中并扩展<code>SRAT表</code>[48]。类似的实现后来已合并到 2020 年的 Linux 5.4 中。这两级 NUMA 设计基于这种透明的内存实现。此外，我们扩展了内核<code>mm/pgtable</code>内存管理，以强制<code>__get_free_pgtable_pages</code>在为新页表结构分配新内存时从 DRAM 对等节点获取页面。</p><blockquote><p>这样应该就不用热插拔了。</p></blockquote><p>我们在新的<code>kvm-ept-idle</code>内核模块中实现了内存访问信息的收集，对于原生应用程序，我们实现了一个名为<code>mm_idle_walk_range</code>的接口，该接口可以扫描页表并根据访问位检查所有页面的 Access 位。单个应用程序的虚拟地址区域，如果操作系统没有使用透明大页（THP）并且应用程序也没有使用大页，那么所有数据读/写操作都工作在4K页上，PTE的Access位可以表示参考信息。否则，1G 或2M PT条目的 Access 位可以代表数据读写操作的参考。由于 Linux 内核实现了 <code>walk_page_range</code> 函数，可以在其虚拟内存中递归地遍历进程的五级 PT地址范围 ，我们将 <code>walk_page_range</code> 的符号导出到用户空间并重用 <code>mm_idle_walk_range</code> 中的函数，以便接口可以扫描 PT 并检查 4K 页的 PFN 访问位，直到它触及页表项（PTE）（或 1G 页，直到页面上层目录（PUD），2M页面直到页面中层目录（PMD））。</p><p>对于guest机器，我们首先将匿名页面的HVA转换为GFN，然后使用EPT中的翻译信息将GFN转换为GPA，然后，我们实现一个<code>ept_idle_walk_hva_range</code>接口来接收匿名页面HVA作为输入，并检查匿名页面的引用信息kvm-ept-idle 中的 guest 页面。我们实现了一个<code>ept_page_range</code>，它在其虚拟地址范围内<span class="label success">递归地遍历 guest 的五级 EPT</span>，直到检查访问位修改。此外，当模块完成每一轮时，我们使 TLB 无效。 EPT遍历确保硬件正确地清除和设置超级热页的访问位.为了减少PT/EPT遍历的时间开销，如果发现高级PMD空闲，我们会跳过整个512个PTE。</p><p>为了灵活高效的HMM管理和性能优化，我们用3000多个c++代码实现了一个名为Memory-optimizer的全功能用户级管理器，该项目是开源的，它提供了运行时页面管理功能，用于优化单个工作负载（例如，应用程序或访客机器）或优化FlexHM系统上的所有进程。我们为不同的页面管理策略提供了多种参数选择。当遵循激进策略时，我们选择10作为参考参数r，0.1秒作为间隔i，并且10秒为一个epoch e，对于系统只使用4K页，没有大页（也没有THP配置）；对于使用2M/1G大页的系统，我们使用6作为参考参数r，以避免大多数情况下的写放大问题此外，由于管理器将提升所有相同的热 SMM 页面并降级在每个时期分析的所有相同的冷 DRAM 页面以进行积极的页面管理，因此需要移动页面的百分比 100% 等于相同的页面设计保守策略时，对于仅使用 4K 页的系统，我们选择 20 个引用作为 r，0.1 秒作为间隔 i，20 秒作为一个 epoch e；对于使用 2M/1G 大页的系统，我们选择使用 10 秒作为一个 epoch e。另外，当遵循保守的管理策略时，管理器在每个 epoch 中最多只选择 1/2 相同的热/冷页面进行页面移动。</p><h1 id="7-EVALUATION"><a href="#7-EVALUATION" class="headerlink" title="7 EVALUATION"></a>7 EVALUATION</h1><p>在评估部分，我们进行了全面的实验来评估FlexHM如何灵活高效地管理云工作负载的HMM资源。<br>评估内容有助于回答以下问题：</p><ul><li>与使用 DRAM 或 Optane 内存相比，在 FlexHM 上使用 HMM 资源时云工作负载的内存性能如何？ （第 7.2 节）</li><li>FlexHM的HMM利用率管理和性能优化效率如何？ （第 7.4 节）</li><li>与之前的HMM系统以及Optane的MM模式相比，FlexHM如何提供性能优化？ （第 7.3 节）</li><li>为什么要支持FlexHM中的灵活管理和优化策略适配（激进与保守策略的讨论）？ （第 7.5 节）</li></ul><h2 id="7-1-Experiment-Environment"><a href="#7-1-Experiment-Environment" class="headerlink" title="7.1 Experiment Environment"></a>7.1 Experiment Environment</h2><p>在实验中，我们设置了表 2 所示配置的硬件环境。所有实验中的 SMM 设备都是真正的英特尔 Optane 内存。我们在使用 Linux 4.20 内核的 Ubuntu 18.04 主机上编译了 kvm-ept-idle 内核模块。为了测试原生云进程的性能，我们直接在主机服务器上运行内存密集型基准测试。为了测试云虚拟机的性能，我们在使用通用内核的 Ubuntu 18.04 系统的 qemu-kvm 客户机中运行基准测试，因为 FlexHM 支持完全虚拟化，客户机无需修改内核。</p><blockquote><p>what?! 4.20也让过？</p></blockquote><p>我们将介绍评估实验中的微型基准和应用基准。首先，我们选择 Sysbench [27] 作为微基准，它可以产生<strong>非常密集的高并行随机读写操作</strong>，占用异构内存硬件的峰值带宽。Sysbench 可以对内存中的数据产生大量随机读/写操作，而且 Sysbench 内存测试的参数对我们评估 FlexHM 对差异化云工作负载的 HMM 管理效率非常友好。值得一提的是，Sysbench 有一个 <code>memory-block-size</code>参数，表示 Sysbench 在一个数据数组上创建随机内存访问，该数组的大小为 <code>memory-block-size</code>；它还有一个 <code>memory-total-size</code>参数，可以确定 Sysbench 在整个基准测试过程中随机读取或写入操作的内存总大小/内存块大小次数。我们的 Sysbench 基准的随机内存引用遵循近似生成的<strong>高斯分布</strong>，确保 80% 的随机引用位于数据分布期望值的 20% 对称区间内。我们在 32 核 68 G 客户机上运行了几组 1 线程和 32 线程 Sysbench 基准，以模拟对客户机工作负载的管理，并在服务器上执行了几组 1 线程和 32 线程本机 Sysbench 进程，以演示对原生服务或容器的管理。Sysbench 是使用 64G<code>memory-block-size</code>和 2T <code>memory-total-size</code>的<code>global</code>随机内存参考进行的随机读/写实验。</p><p>我们选择Memcached [8]（内存数据库）（使用YCSB框架[3]在Memcached上生成Zipfian分布工作负载）和Ligra [45]（图计算）作为应用程序基准测试的代表。</p><p>Memcached 是一个高性能的分布式内存缓存系统，旨在加速动态网络应用程序，提高数据库性能。我们将 Memcached 服务器设置在 64 G guest 中，作为云服务器基准，guest 内存由服务器 Socket 1 上的 NUMA 对等节点分配。我们在 Socket 0 上绑定了 YCSB 多线程生成器，并通过 40 Gb 双端口网卡向 Memcached 服务器发送操作。具体来说，YCSB 客户端生成的每个操作都在 100K K-V 条目上运行，我们在 Memcached 中存储了 500K 条目，因此所有数据都占用了 64G guest 中的 52G 内存（包含元数据）。对数据进行 K-V 操作的随机密钥选择遵循从 0 到 500K 的 Zipfian 分布，我们手动设置为 80% 的引用位于前 20% 的数据中，数据热度与 Sysbench 设置类似。</p><p>另一个应用程序基准是图计算框架 Ligra [45]。图计算是云计算中典型的内存密集型工作负载，图计算框架将非常大的图加载到内存中，然后运行计算算法。当在不同的迭代中计算时，大图中的数据可能会以不平衡的数据分布获取。我们分别在Ligra（一种用于共享内存的轻量级图处理框架）中运行BFS（广度优先搜索）、MIS（最大独立集）[22]和PR（PageRank）[39]。我们在 Ligra 中对具有 160M 顶点的随机生成图运行算法，该图在 64 G 虚拟机中消耗 62 G 内存用于内存计算。</p><h2 id="7-2-Overall-Performance-with-Different-Ratios-of-HMM"><a href="#7-2-Overall-Performance-with-Different-Ratios-of-HMM" class="headerlink" title="7.2 Overall Performance with Different Ratios of HMM"></a>7.2 Overall Performance with Different Ratios of HMM</h2><h3 id="7-2-1-Micro-Benchmark-–-Sysbench"><a href="#7-2-1-Micro-Benchmark-–-Sysbench" class="headerlink" title="7.2.1 Micro Benchmark – Sysbench"></a>7.2.1 Micro Benchmark – Sysbench</h3><p>我们在不同组中精确固定了 Sysbench 实验的 DRAM/SMM 使用比例，包括全 DRAM、DRAM: SMM = 1:1、1:2、1:4 和全 SMM（全 Optane）配置。值得一提的是，由于我们要将 FlexHM 的性能与基线进行比较，我们使用了 numactl 和 taskset 命令来绑定客户机或服务器 Socket 1 内本机 Sysbench 进程的 CPU 和内存。（虽然 FlexHM 上的内存优化器可以轻松设置 DRAM 和 Optane 的 HMM 资源比例，但原生 Ubuntu 系统基线需要设置一些假的 NUMA 节点[26]，然后使用 numactl 生成 1:1、1:2、1:4 的 HMM 比例）</p><blockquote><p>为啥要设置假的。这篇写的也太烂了吧，是给评委塞钱了吗……不翻译了。</p></blockquote><p>此外，FlexHM 上的本机 Sysbench 实验的性能结果如图 6(c) 和 6(d) 所示。原生 Sysbench 性能还表明，FlexHM 系统上的原生应用程序可以在不同的 HMM 比率下实现令人鼓舞的性能，例如，在 1:1 比率配置的 32 线程 Sysbench 中实现全 DRAM 基准的 93% 以上的读取性能。这些实验表明，如果想要节省昂贵的 DRAM 成本，FlexHM 上的内存工作负载可以通过选择不同比率的 HMM 来获得有希望的性能，而不会严重降低性能。总之，Sysbench 基准测试表明 FlexHM 系统可以为来宾或本机工作负载提供高效的 HMM 管理。</p><blockquote><p>总的来看，可能这篇文章由于是期刊的原因，大部分目光还是放在了傲腾PM上。同时提出的迁移策略和方案在近几年的会议论文中都有实现，所以现在来看创新性是欠缺的。另外作者的文章写的非常大概，对实现细节和策略设计都写得不太清楚，一些重复的“老生常谈”的话术倒是挺多。在评估方面，没有看出到底是作者的采样策略对热页面有效捕获、还是迁移很及时等能具体体现性能提升原因的评估实验。这篇文章区别于其他分层内存设计的最大亮点在于针对云计算（云原生、容器、虚拟机）来做设计，然后除了对虚拟机EPT的采样设计、对内存访问符合高斯分布的预测（也没说怎么实现，这个特性要怎么用，就是说的很大概）以外，在系统设计中并没有体现针对性，这也造成笔者对于到底性能怎么就提升了的疑惑。感觉是披了一层云计算外衣，做的还是分层内存的那几个操作。也可能笔者水平不足，还不能领悟其中精髓。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> Cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HugePage and THP in memory</title>
      <link href="/2023/12/21/hugepage-and-THP-in-memory/"/>
      <url>/2023/12/21/hugepage-and-THP-in-memory/</url>
      
        <content type="html"><![CDATA[<blockquote><p>虽然大页一直被人嫌弃，因为内存碎片化带来的页面规整开销和大页面分配困难、访问倾斜等。但是“存在即合理”。大页还是有些优势的比如在大内存中增加TLB的覆盖等。许多针对TLB和页面粒度大小的研究也一直没停过。PS. huge page 到底怎么写好像没有明确的说法。</p></blockquote><ul><li>参考<a href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">Transparent Hugepages: measuring the performance impact</a>解释了透明大页的优缺点，但是主要还是讲了3个测量大页性能的方法。最后面有一个测试案列不错。</li><li>了解透明巨页支持的最佳处是<a href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt">Linux内核网站上的官方文档</a>。</li><li>/proc/buddyinfo 让你<a href="http://andorian.blogspot.com/2014/03/making-sense-of-procbuddyinfo.html">了解 Linux 机器上的可用内存碎片</a>。您可以查看每个可用订单的空闲片段，每个numa节点的不同区域。</li></ul><h2 id="0-大页面"><a href="#0-大页面" class="headerlink" title="0. 大页面"></a>0. 大页面</h2><p><strong>地址转换</strong>逻辑（页表遍历）由<strong>CPU的内存管理单元（MMU）</strong> 实现。MMU还具有最近使用的页面的<strong>缓存</strong>。此缓存称为转换后备缓冲区（TLB） 。</p><p>虚拟地址需要转换为物理地址时，<strong>首先搜索TLB</strong>。如果找到匹配项（TLB命中），则返回物理地址，并且可以继续内存访问。但是，如果没有匹配项（称为TLB未命中），MMU通常会在<strong>页表中查找地址映射</strong>，以查看映射是否存在。<strong>页表遍历很昂贵</strong>，因为它可能<strong>需要多次内存访问</strong>（但它们可能会命中CPUL1/L2/L3缓存）。另一方面，<strong>TLB缓存大小有限，通常可以容纳数百页</strong>。</p><blockquote><p>在任何分页系统中，都需要考虑两个主要问题:<br>1)虚拟地址到物理地址的映射必须非常快。<br>2)如果虚拟地址空间很大，页表也会很大。<br>第一个问题是由于每次访向内存都需要进行虚拟地址到物理地址的映射，所有的指令最终都必须来自内存，并且很多指令也会访问内存中的操作数。因此，每条指令进行一两次或更多页表访问是必要的。如果执行一条指令需要1ns，页表查询必须在0.2ns之内完成，以避免映射成为一个主要瓶颈。<br>第二个问题来自现代计算机使用64位变得越来越普遍。假设页面大小为4KB，32位的地址空间将有100万页，而64位地址空间简直多到超乎想象。如果虚拟地址空间中有100万页，那么页表必然有100万条表项。另外请记住，每个进程都需要自己的页表（因为它有自己的虚拟地址空间）。（《现代操作系统》第4版，还有很多很清晰的讨论P114）<br>第一个是增加TLB大小，这很昂贵，而且不会有很大帮助（因为页表项增加后查找将变慢）。另一个是增加页面大小，因此要映射的页面更少。现代操作系统和CPU支持2MB甚至1GB的大页面。使用2MB的大页面，128GB内存变为64000页。</p></blockquote><p>使用更大的内存页作为映射单位有如下好处：</p><ol><li>减少 TLB（Translation Lookaside Buffer） 的失效情况（或者说减少page walk）。</li><li>减少 页表 的内存消耗。</li><li>减少 PageFault（缺页中断）的次数。</li></ol><p>有两个因素可以作为选择小页面的理由。随便选择一个数据很可能不会恰好装满整数个页面，多余的空间就被浪费掉了，这种浪费称为<strong>内存膨胀</strong>。另外分配大页需要连续的物理地址空间，多次分配后造成<strong>内部碎片</strong>（internal fragmentation）使得大页分配困难，需要额外后台线程做内存规整。 </p><h2 id="1-static-huge-page静态大页面"><a href="#1-static-huge-page静态大页面" class="headerlink" title="1. static huge page静态大页面"></a>1. static huge page静态大页面</h2><p>静态huge page是<a href="https://zhuanlan.zhihu.com/p/70964551">不支持swap操作</a>的，不能被换出到外部存储介质上。页面大小小于MAX_ORDER的大页称之为huge page，大于等于MAX_ORDER的大页称之为gigantic page，拿常用的举例，2MB是huge page，1GB是gigantic page。</p><p>huge page和gigantic page走的是不同的分配路径。huge page相对较小，所以通过伙伴系统来分配（注意：虽然走伙伴系统，但最终它的元数据设置和普通伙伴系统页面的设置是分开处理的）。分配走的是伙伴系统核心都是<code>alloc_huge_page</code>，在大页系统初始化的时候，也会通过<code>alloc_huge_page</code>接口，提前分配物理页。</p><p>在向伙伴系统申请一个大页时，如果是巨页，则优先通过CMA（Contiguous Memory Allocator）分配器分配，如果不支持CMA或者CMA内存不足，则通过指定范围连续物理页申请接口（<code>alloc_contig_pages</code>）而gigantic page页面较大，伙伴系统无法满足分配要求，所以通过连续内存分配接口<code>alloc_contig_range()</code></p><blockquote><p>标准大页存在两个严重的问题：<br>1、需要提前预分配好大页内存池，通过内核启动参数或者虚拟文件系统的方式。<br>2、对代码进行嵌入修改，例如使用mmap时，需要添加MAP_HUGETLB配置；或者需要挂载大页文件系统；或者使用libhugetlbfs提供的方法等。</p></blockquote><p>针对64位的x86-64系统，huge page的大小是2MB或者1GB，初始数目由启动时的”<code>vm.nr_hugepages</code>“ 内核参数确定，对于2MB的huge page，运行过程中可通过”<code>/proc/sys/vm/nr_hugepages</code>“修改。但对于1GB的huge page，就只能在启动时分配（且分配后不能释放），而不支持在运行时修改（系统起来后再要倒腾出1GB连续的物理内存，也怪难为内核的）</p><p><strong>预留大页面</strong>的方式需要在启动内核时应用配置优点是开机时就通过bootmem分配大页，不存在因为内存碎片导致分不出大页的情况，从而保证预留的成功。需要指出的是，系统能否支持大页，支持大页的大小为多少是由其使用的处理器决定的。通过在bootargs传参在系统启动过程中预留大页，bootargs参数：预分配大页数量hugepages= 和 预分配大页的大小hugepagesz= ，更详细的使用可参看内核文档kernel-parameters.txt</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 假如我们想保留64个大页面，每个2MB，就用下面的配置：</span>hugepagesz <span class="token operator">=</span> 2MB， hugepages <span class="token operator">=</span> <span class="token number">64</span><span class="token comment"># 还可以指定默认的大页大小。比如，我们想预留4GB内存作为大页使用，大页的大小为1GB，那么可以用以下的命令行：</span><span class="token assign-left variable">default_hugepagesz</span><span class="token operator">=</span>1G <span class="token assign-left variable">hugepagesz</span><span class="token operator">=</span>1G <span class="token assign-left variable">hugepages</span><span class="token operator">=</span><span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在Linux启动之后，如果想预留大页，则可以使用以下的方法来预留内存。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 在非NUMA系统中，可以使用以下方法预留2MB大小的大页。</span><span class="token builtin class-name">echo</span> <span class="token number">24</span> <span class="token operator">&gt;</span> /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages<span class="token comment"># 该命令预留24个大小为2MB的大页，也就是预留了2GB内存。</span><span class="token comment"># 如果是在NUMA系统中，假设有两个NODE的系统中，则可以用以下的命令：</span><span class="token builtin class-name">echo</span> <span class="token number">24</span> <span class="token operator">&gt;</span> /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages<span class="token builtin class-name">echo</span> <span class="token number">24</span> <span class="token operator">&gt;</span> /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages<span class="token comment"># 或者</span><span class="token builtin class-name">echo</span> <span class="token number">20</span> <span class="token operator">&gt;</span> /proc/sys/vm/nr_hugepages <span class="token comment">#设置20页大页</span><span class="token comment">#或者</span><span class="token builtin class-name">echo</span> <span class="token number">5</span> <span class="token operator">&gt;</span> /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages /** * 含义：通过sysfs下的文件节点申请和释放大页，保持系统中1GB的大页有5个。 * 若已经存在5个大页则什么都不做；若少于5个则分配够5个； * 若多于5个则释放多余的大页<span class="token punctuation">(</span>前提是未被使用<span class="token punctuation">)</span>。**/ <span class="token function">cat</span> /sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages// 含义：查看系统中空闲的的2MB大页的数量<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-THP"><a href="#2-THP" class="headerlink" title="2. THP"></a>2. THP</h2><p>复合页（Compound Page）就是将物理上连续的两个或多个页看成一个独立的大页，它能够用来创建hugetlbfs中使用的大页（hugepage）。也能够用来创建透明大页（transparent huge page）。可是它不能用在页缓存（page cache）中，这是由于页缓存中管理的都是单个页。分配一个复合页的方式是：使用<code>alloc_pages</code>函数，參数order至少为1，且设置<code>__GFP_COMP</code>标记。由于依据复合页的定义，它通常包含2个或多个连续的物理内存页，这是由它的实现决定的，因而order参数不可能为0。</p><p>transparent huge page透明大页。在Linux中自2.6.38版本开始支持THP。在应用需要huge page的时候，可通过memory compaction（内存规整）操作移动页面，以形成一个huge page，因为该过程不会被应用感知到，所以被称为”transparent”。</p><p>THP采用常规(“高阶”)内存分配路径，它<strong>要求操作系统能够找到连续且对齐的内存块</strong>。它与常规页面存在相同的问题，即碎片化。如果操作系统<strong>找不到连续的内存块，它将尝试压缩、回收或分页其他页面</strong>。**该过程成本高昂，可能会导致延迟峰值(长达几秒钟)**。此问题已在4.6 内核中得到解决(通过”延迟”选项)，如果操作系统无法分配大页面，则会回退到常规页面。</p><p>维护。即使应用程序只接触1字节的内存，它也会占用整个2MB的大页面。这显然是浪费内存。所以有一个名为<strong>khugepaged的后台内核线程</strong>。它<strong>扫描页面</strong>并尝试对其<strong>进行碎片整理并折叠成一个巨大的页面</strong>。尽管它是一个后台线程，<strong>但它会锁定它使用的页面</strong>，因此也可能导致<strong>延迟峰值</strong>。另一个陷阱在于<strong>大页面拆分</strong>，并非操作系统的所有部分都适用于大页面，例如Swap。操作系统将大页面拆分为常规页面。它还可能<strong>降低性能并增加内存碎片</strong>。</p><p>实现THP作为一个整体被swap out和swap in（<a href="https://link.zhihu.com/?target=https://lwn.net/Articles/758677/">参考这篇文章</a>）</p><p>运行以下命令检查透明大页的状态：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">cat</span> /sys/kernel/mm/transparent_hugepage/enabled<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>如果输出为 <code>[always] madvise never</code>，则透明大页功能已经开启。<br>如果输出为 <code>always madvise [never]</code>，则透明大页功能已经关闭。</p><p>如果透明大页功能未开启，可以通过以下命令启用它（需要管理员权限）</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">echo</span> always <span class="token operator">&gt;</span> /sys/kernel/mm/transparent_hugepage/enabled<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>调整透明大页配置，有两个可用的配置选项，分别是 <code>transparent_hugepage/defrag</code> 和 <code>transparent_hugepage/enabled</code>。<br><code>transparent_hugepage/defrag</code>：用于设置大页碎片整理的方式。<br><code>transparent_hugepage/enabled</code>：用于设置大页的启用方式。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">echo</span> always <span class="token operator">&gt;</span> /sys/kernel/mm/transparent_hugepage/defrag<span class="token builtin class-name">echo</span> always <span class="token operator">&gt;</span> /sys/kernel/mm/transparent_hugepage/enabled<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>参考<a href="http://cenalulu.github.io/linux/huge-page-on-numa/">Huge Page 是否是拯救性能的万能良药？</a>可以有参考。<br>参考<a href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">透明巨页：衡量性能影响</a></p><h2 id="3-TLB"><a href="#3-TLB" class="headerlink" title="3. TLB"></a>3. TLB</h2><p>通过编译优化来将频繁被访问的指令汇总到一起，放在二进制文件中的同一个地方，以提高空间局部性，这样就可以提高iTLB命中。这块放置频繁访问指令的区域，就叫热区域（Hot Text）。</p><p>通过在大页面上放置热文本，可以进一步提升iTLB命中率。使用大页面iTLB条目时，单个TLB条目覆盖的代码是标准4K页面的512倍。更重要的是，当代的CPU体系结构，通常为大页面提供一些单独的TLB条目，如果我们不使用大页面，这些条目将处于空闲状态。（是吗？待考证）所以，通过使用大页面，也可以充分利用那些TLB条目。</p><p>关于<a href="https://blog.csdn.net/hanzefeng/article/details/82893317">分支预测</a>的基础知识。如果从运行的二进制文件来优化iTLB，可以考虑<a href="https://lwn.net/Articles/680985/">LBR（Last Branch Record，最后分支记录）</a>为热函数创建优化表单<a href="https://github.com/facebook/hhvm/tree/master/hphp/tools/hfsort">HFSort</a>工具。</p><p>事实上，perf只支持所有事件的一个小子集，而CPU有数百个不同的计数器来监测性能。<br><strong>在页表中遍历所花费的 CPU 周期数</strong></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span>~<span class="token punctuation">]</span><span class="token comment"># perf stat -e cycles \</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-e</span> cpu/event<span class="token operator">=</span>0x08,umask<span class="token operator">=</span>0x10,name<span class="token operator">=</span>dcycles/ <span class="token punctuation">\</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-e</span> cpu/event<span class="token operator">=</span>0x85,umask<span class="token operator">=</span>0x10,name<span class="token operator">=</span>icycles/ <span class="token punctuation">\</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-a</span> <span class="token parameter variable">-I</span> <span class="token number">1000</span><span class="token comment">#           time             counts unit events</span>     <span class="token number">1.005079845</span>        <span class="token number">227,119</span>,840      cycles     <span class="token number">1.005079845</span>          <span class="token number">2,605</span>,237      dcycles     <span class="token number">1.005079845</span>            <span class="token number">806,076</span>      icycles<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>由 TLB 未命中引起的主内存读取次数</strong>;这些读取会错过 CPU 缓存，因此非常昂贵</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span>root@PRCAPISV0003L01 ~<span class="token punctuation">]</span><span class="token comment"># perf stat -e cache-misses \</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-e</span> cpu/event<span class="token operator">=</span>0xbc,umask<span class="token operator">=</span>0x18,name<span class="token operator">=</span>dreads/ <span class="token punctuation">\</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-e</span> cpu/event<span class="token operator">=</span>0xbc,umask<span class="token operator">=</span>0x28,name<span class="token operator">=</span>ireads/ <span class="token punctuation">\</span><span class="token operator">&gt;</span>   <span class="token parameter variable">-a</span> <span class="token parameter variable">-I</span> <span class="token number">1000</span><span class="token comment">#           time             counts unit events</span>     <span class="token number">1.007177568</span>             <span class="token number">25,322</span>      cache-misses     <span class="token number">1.007177568</span>                 <span class="token number">23</span>      dreads     <span class="token number">1.007177568</span>                  <span class="token number">5</span>      ireads<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-伙伴系统内存分配的碎片"><a href="#4-伙伴系统内存分配的碎片" class="headerlink" title="4. 伙伴系统内存分配的碎片"></a>4. 伙伴系统内存分配的碎片</h2><p>随着时间的推移，分配请求将拆分、合并、拆分…这个池，直到我们到达由于缺少连续内存块而可能不得不使请求失败的地步。在这种情况下，buddyinfo proc 文件将允许您查看内存的当前碎片状态。这是一个快速的python脚本。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment">#!/usr/bin/env python</span><span class="token comment"># vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4 textwidth=79 autoindent</span><span class="token triple-quoted-string string">"""Python source codeLast modified: 15 Feb 2014 - 13:38Last author: lmwangi at gmail  comDisplays the available memory fragmentsby querying /proc/buddyinfoExample:# python buddyinfo.py"""</span><span class="token keyword">import</span> optparse<span class="token keyword">import</span> os<span class="token keyword">import</span> re<span class="token keyword">from</span> collections <span class="token keyword">import</span> defaultdict<span class="token keyword">import</span> logging<span class="token keyword">class</span> <span class="token class-name">Logger</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> log_level<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>log_level <span class="token operator">=</span> log_level    <span class="token keyword">def</span> <span class="token function">get_formatter</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> logging<span class="token punctuation">.</span>Formatter<span class="token punctuation">(</span><span class="token string">'%(asctime)s - %(name)s - %(levelname)s - %(message)s'</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_handler</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> logging<span class="token punctuation">.</span>StreamHandler<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_logger</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Returns a Logger instance for the specified module_name"""</span>        logger <span class="token operator">=</span> logging<span class="token punctuation">.</span>getLogger<span class="token punctuation">(</span><span class="token string">'main'</span><span class="token punctuation">)</span>        logger<span class="token punctuation">.</span>setLevel<span class="token punctuation">(</span>self<span class="token punctuation">.</span>log_level<span class="token punctuation">)</span>        log_handler <span class="token operator">=</span> self<span class="token punctuation">.</span>get_handler<span class="token punctuation">(</span><span class="token punctuation">)</span>        log_handler<span class="token punctuation">.</span>setFormatter<span class="token punctuation">(</span>self<span class="token punctuation">.</span>get_formatter<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        logger<span class="token punctuation">.</span>addHandler<span class="token punctuation">(</span>log_handler<span class="token punctuation">)</span>        <span class="token keyword">return</span> logger<span class="token keyword">class</span> <span class="token class-name">BuddyInfo</span><span class="token punctuation">(</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""BuddyInfo DAO"""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> logger<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>BuddyInfo<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log <span class="token operator">=</span> logger        self<span class="token punctuation">.</span>buddyinfo <span class="token operator">=</span> self<span class="token punctuation">.</span>load_buddyinfo<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">parse_line</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">:</span>        line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">"Parsing line: %s"</span> <span class="token operator">%</span> line<span class="token punctuation">)</span>        parsed_line <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token keyword">match</span><span class="token punctuation">(</span><span class="token string">"Node\s+(?P&lt;numa_node&gt;\d+).*zone\s+(?P&lt;zone&gt;\w+)\s+(?P&lt;nr_free&gt;.*)"</span><span class="token punctuation">,</span> line<span class="token punctuation">)</span><span class="token punctuation">.</span>groupdict<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">"Parsed line: %s"</span> <span class="token operator">%</span> parsed_line<span class="token punctuation">)</span>        <span class="token keyword">return</span> parsed_line    <span class="token keyword">def</span> <span class="token function">read_buddyinfo</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        buddyhash <span class="token operator">=</span> defaultdict<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">)</span>        buddyinfo <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"/proc/buddyinfo"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> <span class="token builtin">map</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>parse_line<span class="token punctuation">,</span> buddyinfo<span class="token punctuation">)</span><span class="token punctuation">:</span>            numa_node <span class="token operator">=</span>  <span class="token builtin">int</span><span class="token punctuation">(</span>line<span class="token punctuation">[</span><span class="token string">"numa_node"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>            zone <span class="token operator">=</span> line<span class="token punctuation">[</span><span class="token string">"zone"</span><span class="token punctuation">]</span>            free_fragments <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">,</span> line<span class="token punctuation">[</span><span class="token string">"nr_free"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            max_order <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>free_fragments<span class="token punctuation">)</span>            fragment_sizes <span class="token operator">=</span> self<span class="token punctuation">.</span>get_order_sizes<span class="token punctuation">(</span>max_order<span class="token punctuation">)</span>            usage_in_bytes <span class="token operator">=</span>  <span class="token punctuation">[</span>block<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> block<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> block <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>free_fragments<span class="token punctuation">,</span> fragment_sizes<span class="token punctuation">)</span><span class="token punctuation">]</span>            buddyhash<span class="token punctuation">[</span>numa_node<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span>                <span class="token string">"zone"</span><span class="token punctuation">:</span> zone<span class="token punctuation">,</span>                <span class="token string">"nr_free"</span><span class="token punctuation">:</span> free_fragments<span class="token punctuation">,</span>                <span class="token string">"sz_fragment"</span><span class="token punctuation">:</span> fragment_sizes<span class="token punctuation">,</span>                <span class="token string">"usage"</span><span class="token punctuation">:</span> usage_in_bytes <span class="token punctuation">}</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> buddyhash    <span class="token keyword">def</span> <span class="token function">load_buddyinfo</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        buddyhash <span class="token operator">=</span> self<span class="token punctuation">.</span>read_buddyinfo<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log<span class="token punctuation">.</span>info<span class="token punctuation">(</span>buddyhash<span class="token punctuation">)</span>        <span class="token keyword">return</span> buddyhash    <span class="token keyword">def</span> <span class="token function">page_size</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> os<span class="token punctuation">.</span>sysconf<span class="token punctuation">(</span><span class="token string">"SC_PAGE_SIZE"</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">get_order_sizes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_order<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>page_size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">2</span><span class="token operator">**</span>order <span class="token keyword">for</span> order <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_order<span class="token punctuation">)</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__str__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        ret_string <span class="token operator">=</span> <span class="token string">""</span>        width <span class="token operator">=</span> <span class="token number">20</span>        <span class="token keyword">for</span> node <span class="token keyword">in</span> self<span class="token punctuation">.</span>buddyinfo<span class="token punctuation">:</span>            ret_string <span class="token operator">+=</span> <span class="token string">"Node: %s\n"</span> <span class="token operator">%</span> node            <span class="token keyword">for</span> zoneinfo <span class="token keyword">in</span> self<span class="token punctuation">.</span>buddyinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span>node<span class="token punctuation">)</span><span class="token punctuation">:</span>                ret_string <span class="token operator">+=</span> <span class="token string">" Zone: %s\n"</span> <span class="token operator">%</span> zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"zone"</span><span class="token punctuation">)</span>                ret_string <span class="token operator">+=</span> <span class="token string">" Free KiB in zone: %.2f\n"</span> <span class="token operator">%</span> <span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"usage"</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1024.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                ret_string <span class="token operator">+=</span> <span class="token string">'\t{0:{align}{width}} {1:{align}{width}} {2:{align}{width}}\n'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                        <span class="token string">"Fragment size"</span><span class="token punctuation">,</span> <span class="token string">"Free fragments"</span><span class="token punctuation">,</span> <span class="token string">"Total available KiB"</span><span class="token punctuation">,</span>                        width<span class="token operator">=</span>width<span class="token punctuation">,</span>                        align<span class="token operator">=</span><span class="token string">"&lt;"</span><span class="token punctuation">)</span>                <span class="token keyword">for</span> idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"sz_fragment"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                    ret_string <span class="token operator">+=</span> <span class="token string">'\t{order:{align}{width}} {nr:{align}{width}} {usage:{align}{width}}\n'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>                        width<span class="token operator">=</span>width<span class="token punctuation">,</span>                        align<span class="token operator">=</span><span class="token string">"&lt;"</span><span class="token punctuation">,</span>                        order <span class="token operator">=</span> zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"sz_fragment"</span><span class="token punctuation">)</span><span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span>                        nr <span class="token operator">=</span> zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"nr_free"</span><span class="token punctuation">)</span><span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span>                        usage <span class="token operator">=</span> zoneinfo<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"usage"</span><span class="token punctuation">)</span><span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">/</span> <span class="token number">1024.0</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> ret_string<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Main function. Called when this file is a shell script"""</span>    usage <span class="token operator">=</span> <span class="token string">"usage: %prog [options]"</span>    parser <span class="token operator">=</span> optparse<span class="token punctuation">.</span>OptionParser<span class="token punctuation">(</span>usage<span class="token punctuation">)</span>    parser<span class="token punctuation">.</span>add_option<span class="token punctuation">(</span><span class="token string">"-s"</span><span class="token punctuation">,</span> <span class="token string">"--size"</span><span class="token punctuation">,</span> dest<span class="token operator">=</span><span class="token string">"size"</span><span class="token punctuation">,</span> choices<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"B"</span><span class="token punctuation">,</span><span class="token string">"K"</span><span class="token punctuation">,</span><span class="token string">"M"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                      action<span class="token operator">=</span><span class="token string">"store"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token string">"choice"</span><span class="token punctuation">,</span> <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">"Return results in bytes, kib, mib"</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>options<span class="token punctuation">,</span> args<span class="token punctuation">)</span> <span class="token operator">=</span> parser<span class="token punctuation">.</span>parse_args<span class="token punctuation">(</span><span class="token punctuation">)</span>    logger <span class="token operator">=</span> Logger<span class="token punctuation">(</span>logging<span class="token punctuation">.</span>DEBUG<span class="token punctuation">)</span><span class="token punctuation">.</span>get_logger<span class="token punctuation">(</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Starting...."</span><span class="token punctuation">)</span>    logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Parsed options: %s"</span> <span class="token operator">%</span> options<span class="token punctuation">)</span>    <span class="token keyword">print</span> logger    buddy <span class="token operator">=</span> BuddyInfo<span class="token punctuation">(</span>logger<span class="token punctuation">)</span>    <span class="token keyword">print</span> buddy<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    main<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> THP </tag>
            
            <tag> HugePage </tag>
            
            <tag> TLB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices</title>
      <link href="/2023/12/21/Demystifying-CXL-Memory-with-Genuine-CXL-Ready-Systems-and-Devices/"/>
      <url>/2023/12/21/Demystifying-CXL-Memory-with-Genuine-CXL-Ready-Systems-and-Devices/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary no-icon"><ul><li>文章来自2023 MICRO,CCFA</li><li>Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li> Yan Sun, Zeduo Yu, Reese Kuper, Chihun Song, Jinghan Huang, Houxiang Ji, Siddharth Agarwal, Jiaqi Lou, Ipoom Jeong, Tianyin Xu, Nam Sung Kim. University of Illinois伊利诺伊大学.</li><li> Yifan Yuan, Ren Wang. Intel Labs.</li><li> Jung Ho Ahn. Seoul National University首尔大学. </li></ul><h2 id="2-评估"><a href="#2-评估" class="headerlink" title="2. 评估"></a>2. 评估</h2><p>这部分主要介绍真实CXL内存和模拟的之间的区别，以及和CPU之间的相互作用。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/07986173d7c846d2b14d4537f998d01e.png" alt="实验配置"><br>这里使用了不同制造商的3种CXL设备，对应的内存设备也不同。</p><h3 id="2-1-设备延迟"><a href="#2-1-设备延迟" class="headerlink" title="2.1 设备延迟"></a>2.1 设备延迟</h3><ul><li>MLC: Pointer-chasing (Latency of Serialized Memory Accesses) 顺序访问的延迟指不停的获得指针跳转到对应地址，最后才拿到数据。</li><li>Temporal Load (ld). 指CPU有规律的从内存读数据。</li><li>Non-Temporal Load (nt-ld). 指随机从内存读数据。</li><li>Temporal Store (st). <code>store</code>指令表示将寄存器或缓存中的数据写入内存，可能会在高速缓存中保留一份拷贝，以提高未来的读取性能。</li><li>Non-Temporal Store (nt-st). 写入没有规律。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/58fca036ffc442288fbe844bc234cc37.png"></p><ul><li>对比两种测试程序：MLC的串行内存访问没法使用UPI和CXL全双工的功能，使得MLC测得的延迟要大于MEMO测的。</li><li>对比四种设备：CXL-A 的 ld 延迟仅比 DDR5- R 多 35%，而 CXL-B 和 CXL-C 的 ld 延迟则分别长近 2 倍和 3 倍。即使采用相同的 DDR4 DRAM 技术，基于 DDR4-3200 的 CXL-C 比基于 DDR4-2400 的 CXLB 的延迟时间长 67%. 说明真实CXL设备的延迟和CXL控制器的设置是高度相关的。</li><li>模拟 CXL 内存延迟开销：在发出内存请求时，①本地 CPU 必须首先与远程 CPU 通信检查高速缓存一致性 。②内存请求还必须通过远程 CPU 内部较长的片内互连（芯片间 UPI 接口连接）才能到达其内存控制器。CPU 内核越多，即缓存越多，互连路径越长，这些开销就越大。这些造成延迟的问题在CXL上是如何解决的？CPU 实现了一种片上硬件结构，以促进对真正 CXL 内存访问的快速缓存一致性检查。此外，真正的 CXL 内存在 CXL 控制器内有一个较短的片内互连，以连接其内存控制器。</li></ul><h3 id="2-2-设备带宽"><a href="#2-2-设备带宽" class="headerlink" title="2.2 设备带宽"></a>2.2 设备带宽</h3><ul><li>从读写来看：将结果标准化为理想带宽。DDR5-4800、DDR4-3200 和 DDR4-2400 DRAM 技术可提供的最大顺序带宽值分别为每通道 38.4 GB/s、25.6 GB/s 和 19.2 GB/s. 全读操作时占DDR技术理想带宽百分比差异很大，CXL-A的内存控制器设计可以更高效处理交错的内存访问。在只读或只写内存访问方面的效率可能不如 CXL-B. CXL-C 非常差劲的带宽是 due to the FPGA-based implementation of the CXL controller. 从这些结果可以看到，CXL设备的带宽表现也和CXL控制器的设计紧密相关。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/ac23f88f04774eb5b5233bffa888969e.png"></p><ul><li>模拟 CXL 内存带宽利用率在写操作中低于真实CXL设备。对于st操作，仿真 CXL 内存的带宽效率下降幅度明显大于真实 CXL 内存，部分原因是仿真 CXL 内存承受了更多的高速缓存一致性检查开销。在store操作中真实CXL设备的带宽是有很大机会好于模拟的。</li></ul><h3 id="2-3-与CPU缓存间的相互作用"><a href="#2-3-与CPU缓存间的相互作用" class="headerlink" title="2.3 与CPU缓存间的相互作用"></a>2.3 与CPU缓存间的相互作用</h3><p>目前因特尔架构是所谓的非包容性架构Non-inclusive cache architecture (starting Intel Skylake CPU). LLC错误时，加载去L2而不是共享的LLC，这个时候就会从L2再驱逐一些到LLC。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/a7daca9d048047d7ba0de62f723af34d.png"></p><p>Each chiplet (or two chiplets) as a NUMA node in the SNC mode. 如上图所示，按理说这是一个socket，也应该是一个NUMA node，但是现在被切成4个了。当在SNC模式下LLC资源强制隔离之后，被驱逐只能驱逐到同一个SNC的LLC。但是CXL会打破这个隔离，core获得的LLC容量变多了，明显弥补了 CXL 内存较慢的访问延迟。</p><h3 id="2-4-应用程序的延迟"><a href="#2-4-应用程序的延迟" class="headerlink" title="2.4 应用程序的延迟"></a>2.4 应用程序的延迟</h3><p>P99延迟就是指99%的操作都会在那个延迟范围内完成，QPS代表“Queries Per Second”，即每秒查询数。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/8a612d1409ab411c892fd33afcd80b68.png"></p><ul><li>Redis：随QPS增加，差距增加，几乎两倍。</li><li>DSB：由3种组件组成，将CPU敏感的部分都放在DDR中运行，如表格所示。将内存敏感部分放入CXL和DDR做对比。<br>图d的黄色突起部分是由于那个阶段CXL解决了带宽瓶颈改善了延迟（作者这么解释的，感觉怪怪的）。之后两条线又重合了是CPU带来的瓶颈。Redis更敏感是微秒级的，但是DSB是毫秒级别的，所以两者对待这两类的延迟会有不同的表现。</li><li>TPP策略：全部分配去CXL，等TPP大概完成了只剩25%在CXL的迁移，再去测P99延迟。另一个是静态分配25%的页面去给TPP，竟然都比TPP要好。①将任何比例的页面分配到 CXL 内存都会相应增加要求 𝜇 级延迟的内存密集型应用的 p99 延迟（对内存访问延迟高度敏感）。②谨慎地将某些页面分配到 CXL 内存，并不会增加 𝑚 级延迟的应用的 p99 延迟。③即使是智能页面迁移策略，也会因为迁移页面的开销而进一步增加这类对延迟敏感的应用的 p99 延迟。作者认为主要是延迟敏感程序，页面迁移会阻碍关键路径上的内存读取请求。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/719c528bbbf74dad969d526a1517af1a.png"></p><h3 id="2-5-应用程序的吞吐量"><a href="#2-5-应用程序的吞吐量" class="headerlink" title="2.5 应用程序的吞吐量"></a>2.5 应用程序的吞吐量</h3><p>全部放在DDR5中，在20线程后吞吐量开始饱和。在这种情况下，将一定比例的页面分配到 CXL 内存可以进一步提高吞吐量，因为它可以补充 DDR 内存的带宽，增加 DLRM 可用的总带宽。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/23966f8f4f8744118792b3ca2ae733bd.png"><br>虽然 Redis 是一个对延迟敏感的应用，但其吞吐量也是一个重要的性能指标。对于内存带宽密集型应用（同时也延迟敏感）而言，可能会导致吞吐量低于将100%的页面分配到DDR内存，即使同时使用DDR内存和CXL内存不会带来更高的总带宽。由于 Redis 没有充分利用内存带宽，其吞吐量受到内存访问延迟的限制。 因此，将更多页面分配到 CXL 内存会降低 Redis 的吞吐量。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/6ac832524bcc47328c7bc9b2fb093550.png"></p><h2 id="3-CXL-MEMORY-AWARE的内存分配"><a href="#3-CXL-MEMORY-AWARE的内存分配" class="headerlink" title="3. CXL-MEMORY-AWARE的内存分配"></a>3. CXL-MEMORY-AWARE的内存分配</h2><p>一种动态页面分配策略，可以在运行时根据不同 CXL 内存设备的带宽能力和运行应用程序消耗的带宽，自动配置分配给 CXL 内存的页面比例。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/e98064a4bfdc42d4aa26bd6bed73277e.png"><br>运行了 DLRM，其吞吐量受内存带宽限制。然后，随着分配给 CXL 内存的页面百分比的变化，我们观察 DLRM 吞吐量（DDR的100%）与这些计数器值之间的相关性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/edbc336c66274e17b4a61748b2b79ace.png"><br>直接在用户层实现就好了，所以<a href="https://zenodo.org/record/8332543">作者的代码</a>大多python。</p><p>然后放一张性能提升的测试图：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/f9cad385e4c3483ea217982fa0686148.png"></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> CXL </tag>
            
            <tag> Memory </tag>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>memtis source code</title>
      <link href="/2023/12/15/memtis-source-code/"/>
      <url>/2023/12/15/memtis-source-code/</url>
      
        <content type="html"><![CDATA[<blockquote><p>memtis的源码分为很多个部分，对页表的修改，支持混合页面，硬件计数器采样，采样开销控制，页面迁移等。</p></blockquote><h1 id="1-硬件计数器采样"><a href="#1-硬件计数器采样" class="headerlink" title="1. 硬件计数器采样"></a>1. 硬件计数器采样</h1><h2 id="1-1-内核perf架构"><a href="#1-1-内核perf架构" class="headerlink" title="1.1 内核perf架构"></a>1.1 内核perf架构</h2><p><img src="https://terenceli.github.io/assets/img/perf/1.png" alt="the perf subsystem componenet"></p><p>写硬件计数器采样程序，涉及这幅图的软件、硬件事件，以及用户空间中的ring buffer.</p><p>唯一的用户态系统调用会返回一个perf事件的句柄，这样这个<code>perf_event</code>结构可以通过<code>read/write/ioctl/mmap</code>通用文件接口来操作。<code>perf_event_open(2)</code>的调用也是挺复杂的，详细操作可以看<a href="https://www.man7.org/linux/man-pages/man2/perf_event_open.2.html">perf_event_open(2) Linux manual page</a>，后面写采样后台线程用到了会详细说。</p><p><code>perf_event</code>也是内核中比较核心的结构体，性能事件有多种类型，例如跟踪点、软件、硬件。这些又具体表现为PMU结构体，每一个事件都有一个，比如software pmu：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">pmu</span> perf_swevent <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token punctuation">.</span>task_ctx_nr<span class="token operator">=</span> perf_sw_context<span class="token punctuation">,</span>    <span class="token punctuation">.</span>capabilities<span class="token operator">=</span> PERF_PMU_CAP_NO_NMI<span class="token punctuation">,</span>    <span class="token punctuation">.</span>event_init<span class="token operator">=</span> perf_swevent_init<span class="token punctuation">,</span>    <span class="token punctuation">.</span>add<span class="token operator">=</span> perf_swevent_add<span class="token punctuation">,</span>    <span class="token punctuation">.</span>del<span class="token operator">=</span> perf_swevent_del<span class="token punctuation">,</span>    <span class="token punctuation">.</span>start<span class="token operator">=</span> perf_swevent_start<span class="token punctuation">,</span>    <span class="token punctuation">.</span>stop<span class="token operator">=</span> perf_swevent_stop<span class="token punctuation">,</span>    <span class="token punctuation">.</span>read<span class="token operator">=</span> perf_swevent_read<span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果这个事件是硬件相关，那么这个PMU结构体还会有一个和架构相关的结构体，如下图的<code>struct x86_pmu</code>, 这个硬件相关结构体的作用就是读或者写MSR性能监视器。</p><p><img src="https://terenceli.github.io/assets/img/perf/2.png" alt="the abstract layer of perf"></p><p>perf event的组织方式是cpu维度或者task维度，这样采样才不是只有整个系统的。在manual page有写，<code>perf_event_open()</code>系统调用使用cpu、pid两个参数来指定<code>perf_event</code>的cpu、task维度。两种维度的关联是靠<code>perf_event_context</code>如下图：</p><p><img src="https://terenceli.github.io/assets/img/perf/3.png"><br>每个<code>perf_event</code>由<code>event_list</code>连接，而group的连接方式便于perf count功能一次性读出。</p><p>由于cpu维度的<code>perf_event</code>只要cpu online就会一直运行，task维度只有task被调度才会运行，这涉及perf驱动开关和任务调度。一个概括的函数调用图如下：<br><img src="https://terenceli.github.io/assets/img/perf/5.png"></p><p>Every PMU is registerd by calling ‘perf_pmu_register’.</p><p>每个pmu拥有一个per_cpu的链表，perf_event需要在哪个cpu上获取数据就加入到哪个cpu的链表上。如果event被触发，它会根据当前的运行cpu给对应链表上的所有perf_event推送数据。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdn.net/20180725121740849"></p><p>cpu维度的context：<code>this_cpu_ptr</code>(<code>pmu-&gt;pmu_cpu_context-&gt;ctx</code>)上链接的所有perf_event会根据绑定的pmu，链接到pmu对应的per_cpu的-&gt;perf_events链表上。<br>task维度的context：<code>this_cpu_ptr</code>(<code>pmu-&gt;pmu_cpu_context-&gt;task_ctx</code>)上链接的所有perf_event会根据绑定的pmu，链接到pmu对应的per_cpu的-&gt;perf_events链表上。perf_event还需要做cpu匹配，符合<code>event-&gt;cpu == -1 || event-&gt;cpu == smp_processor_id()</code>条件的event才能链接到pmu上。</p><p>参考<a href="https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2020/08/29/perf-arch">Linux kernel perf architecture</a><br>参考<a href="https://blog.csdn.net/pwl999/article/details/81200439">Linux perf 1.1、perf_event内核框架</a></p><h2 id="1-2-perf计数器模式"><a href="#1-2-perf计数器模式" class="headerlink" title="1.2 perf计数器模式"></a>1.2 perf计数器模式</h2><p>perf_event_open()有两个使用模式，一个叫做计数，一个叫做采样。计数事件会统计发生的总数，采样事件会定期写入缓冲区。下面来看一个非常简单的计数的代码段，每一秒获取刚刚过去的那一秒内的指令数：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;string.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdint.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/syscall.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/perf_event.h&gt;</span></span><span class="token comment">//目前perf_event_open在glibc中没有封装，需要手工封装一下</span><span class="token keyword">int</span> <span class="token function">perf_event_open</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span><span class="token class-name">pid_t</span> pid<span class="token punctuation">,</span><span class="token keyword">int</span> cpu<span class="token punctuation">,</span><span class="token keyword">int</span> group_fd<span class="token punctuation">,</span><span class="token keyword">unsigned</span> <span class="token keyword">long</span> flags<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token function">syscall</span><span class="token punctuation">(</span>__NR_perf_event_open<span class="token punctuation">,</span>attr<span class="token punctuation">,</span>pid<span class="token punctuation">,</span>cpu<span class="token punctuation">,</span>group_fd<span class="token punctuation">,</span>flags<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> attr<span class="token punctuation">;</span>    <span class="token function">memset</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//监测硬件</span>    attr<span class="token punctuation">.</span>type<span class="token operator">=</span>PERF_TYPE_HARDWARE<span class="token punctuation">;</span>    <span class="token comment">//监测指令数</span>    attr<span class="token punctuation">.</span>config<span class="token operator">=</span>PERF_COUNT_HW_INSTRUCTIONS<span class="token punctuation">;</span>    <span class="token comment">//初始状态为禁用</span>    attr<span class="token punctuation">.</span>disabled<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment">//创建perf文件描述符，其中pid=0,cpu=-1表示监测当前进程，不论运行在那个cpu上</span>    <span class="token keyword">int</span> fd<span class="token operator">=</span><span class="token function">perf_event_open</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>fd<span class="token operator">&lt;</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"Cannot open perf fd!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment">//启用（开始计数）</span>    <span class="token function">ioctl</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span>PERF_EVENT_IOC_ENABLE<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token class-name">uint64_t</span> instructions<span class="token punctuation">;</span>        <span class="token comment">//读取最新的计数值</span>        <span class="token function">read</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span><span class="token operator">&amp;</span>instructions<span class="token punctuation">,</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>instructions<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">//读取后清零，这样就不用手动去减了，否则会显示累计值</span>    <span class="token function">ioctl</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span>PERF_EVENT_IOC_RESET<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"instructions=%ld\n"</span><span class="token punctuation">,</span>instructions<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不需要任何的编译选项，直接gcc，然后运行（从上个图我们知道这是用户态的函数）：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">gcc single.c <span class="token parameter variable">-o</span> single<span class="token function">sudo</span> ./single<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>对于多个计数器不能说搞多个文件句柄去读取，这样read()函数调用开销还是有点大的，重复利用一个句柄，这样就成了前面提到的组的关系。主要有以下6点不同。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;string.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdint.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/syscall.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/perf_event.h&gt;</span></span><span class="token comment">//目前perf_event_open在glibc中没有封装，需要手工封装一下</span><span class="token keyword">int</span> <span class="token function">perf_event_open</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span><span class="token class-name">pid_t</span> pid<span class="token punctuation">,</span><span class="token keyword">int</span> cpu<span class="token punctuation">,</span><span class="token keyword">int</span> group_fd<span class="token punctuation">,</span><span class="token keyword">unsigned</span> <span class="token keyword">long</span> flags<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">return</span> <span class="token function">syscall</span><span class="token punctuation">(</span>__NR_perf_event_open<span class="token punctuation">,</span>attr<span class="token punctuation">,</span>pid<span class="token punctuation">,</span>cpu<span class="token punctuation">,</span>group_fd<span class="token punctuation">,</span>flags<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//1. 每次read()得到的结构体</span><span class="token keyword">struct</span> <span class="token class-name">read_format</span><span class="token punctuation">{</span>    <span class="token comment">//计数器数量（为2）</span>    <span class="token class-name">uint64_t</span> nr<span class="token punctuation">;</span>    <span class="token comment">//两个计数器的值</span>    <span class="token class-name">uint64_t</span> values<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> attr<span class="token punctuation">;</span>    <span class="token comment">// perf_event_attr structure provides detailed configuration information for the event being created.</span>    <span class="token comment">//————————————————————第一个计数器—————————————————</span>    <span class="token function">memset</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//监测硬件</span>    attr<span class="token punctuation">.</span>type<span class="token operator">=</span>PERF_TYPE_HARDWARE<span class="token punctuation">;</span>    <span class="token comment">//监测指令数</span>    attr<span class="token punctuation">.</span>config<span class="token operator">=</span>PERF_COUNT_HW_INSTRUCTIONS<span class="token punctuation">;</span>    <span class="token comment">//初始状态为禁用</span>    attr<span class="token punctuation">.</span>disabled<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment">//2. 每次读取一个组</span>    attr<span class="token punctuation">.</span>read_format<span class="token operator">=</span>PERF_FORMAT_GROUP<span class="token punctuation">;</span>    <span class="token comment">//创建perf文件描述符，其中pid=0,cpu=-1表示监测当前进程，不论运行在那个cpu上</span>    <span class="token keyword">int</span> fd<span class="token operator">=</span><span class="token function">perf_event_open</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>fd<span class="token operator">&lt;</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"Cannot open perf fd!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment">//————————————————————第二个计数器—————————————————</span>    <span class="token function">memset</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>size<span class="token operator">=</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//监测类型</span>    attr<span class="token punctuation">.</span>type<span class="token operator">=</span>PERF_TYPE_HARDWARE<span class="token punctuation">;</span>    <span class="token comment">//监测时钟周期数</span>    attr<span class="token punctuation">.</span>config<span class="token operator">=</span>PERF_COUNT_HW_CPU_CYCLES<span class="token punctuation">;</span>    <span class="token comment">//初始状态为禁用</span>    attr<span class="token punctuation">.</span>disabled<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token comment">//3. 创建perf文件描述符，但是不同的是要传入上次的句柄</span>    <span class="token keyword">int</span> fd2<span class="token operator">=</span><span class="token function">perf_event_open</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>fd<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span><span class="token punctuation">(</span>fd2<span class="token operator">&lt;</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token function">perror</span><span class="token punctuation">(</span><span class="token string">"Cannot open perf fd2!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment">//4. 启用（开始计数），注意PERF_IOC_FLAG_GROUP标志</span>    <span class="token function">ioctl</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span>PERF_EVENT_IOC_ENABLE<span class="token punctuation">,</span>PERF_IOC_FLAG_GROUP<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        <span class="token keyword">struct</span> <span class="token class-name">read_format</span> aread<span class="token punctuation">;</span>        <span class="token comment">//5.读取最新的计数值，每次读取一个结构体，每个计数器的读取和加入组的顺序是一致的。</span>        <span class="token function">read</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span><span class="token operator">&amp;</span>aread<span class="token punctuation">,</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">read_format</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"instructions=%ld,cycles=%ld\n"</span><span class="token punctuation">,</span>aread<span class="token punctuation">.</span>values<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>aread<span class="token punctuation">.</span>values<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">//6. 清空组内计数器</span>        <span class="token function">ioctl</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span>PERF_EVENT_IOC_RESET<span class="token punctuation">,</span>PERF_IOC_FLAG_GROUP<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="1-3-perf采样"><a href="#1-3-perf采样" class="headerlink" title="1.3 perf采样"></a>1.3 perf采样</h2><p>如果机器只运行一个程序，那么使用计数的方式也是可以的吧。但是如果要在多个里面追踪一个进程或特定的core那就得采样了，而且采样的好处在于可以获得更多的信息。<br><a href="https://zhou-yuxin.github.io/articles/2017/Linux%20perf%E5%AD%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E9%87%87%E6%A0%B7%EF%BC%88signal%E6%96%B9%E5%BC%8F%EF%BC%89/index.html">采样的模板</a>主要在于：1、采样需要设置触发源，也就是告诉kernel何时进行一次采样；2、采样需要设置信号，也就是告诉kernnel，采样完成后通知谁；3、采样值的读取需要使用mmap，因为采样有异步性，需要一个环形队列，另外也是出于性能的考虑。通过轮询或者响应信号判断是否采样完这一轮。</p><p>采样事件准备工作：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">//这些的定义得看cpu的架构手册</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">DRAM_LLC_LOAD_MISS</span>  <span class="token expression"><span class="token number">0x1d3</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">REMOTE_DRAM_LLC_LOAD_MISS</span>   <span class="token expression"><span class="token number">0x2d3</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NVM_LLC_LOAD_MISS</span>   <span class="token expression"><span class="token number">0x80d1</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">ALL_STORES</span>    <span class="token expression"><span class="token number">0x82d0</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">ALL_LOADS</span>    <span class="token expression"><span class="token number">0x81d0</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">STLB_MISS_STORES</span>    <span class="token expression"><span class="token number">0x12d0</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">STLB_MISS_LOADS</span>    <span class="token expression"><span class="token number">0x11d0</span></span></span><span class="token keyword">static</span> __u64 <span class="token function">get_pebs_event</span><span class="token punctuation">(</span><span class="token keyword">enum</span> <span class="token class-name">events</span> e<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">switch</span> <span class="token punctuation">(</span>e<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token keyword">case</span> DRAMREAD<span class="token operator">:</span>    <span class="token keyword">return</span> DRAM_LLC_LOAD_MISS<span class="token punctuation">;</span><span class="token keyword">case</span> NVMREAD<span class="token operator">:</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>htmm_cxl_mode<span class="token punctuation">)</span>    <span class="token keyword">return</span> NVM_LLC_LOAD_MISS<span class="token punctuation">;</span>    <span class="token keyword">else</span>    <span class="token keyword">return</span> N_HTMMEVENTS<span class="token punctuation">;</span><span class="token keyword">case</span> MEMWRITE<span class="token operator">:</span>    <span class="token keyword">return</span> ALL_STORES<span class="token punctuation">;</span><span class="token keyword">case</span> CXLREAD<span class="token operator">:</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>htmm_cxl_mode<span class="token punctuation">)</span>    <span class="token keyword">return</span> REMOTE_DRAM_LLC_LOAD_MISS<span class="token punctuation">;</span>    <span class="token keyword">else</span>    <span class="token keyword">return</span> N_HTMMEVENTS<span class="token punctuation">;</span><span class="token keyword">default</span><span class="token operator">:</span>    <span class="token keyword">return</span> N_HTMMEVENTS<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token keyword">struct</span> <span class="token class-name">perf_event</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token operator">*</span>mem_event<span class="token punctuation">;</span> <span class="token comment">//perf_event结构体数组，*mem_event是数组指针，因为是二维数组所以3*</span><span class="token keyword">static</span> <span class="token keyword">int</span> <span class="token function">pebs_init</span><span class="token punctuation">(</span><span class="token class-name">pid_t</span> pid<span class="token punctuation">,</span> <span class="token keyword">int</span> node<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> cpu<span class="token punctuation">,</span> event<span class="token punctuation">;</span> <span class="token comment">//要采样的CPU数和事件数</span><span class="token comment">//存放perf事件的空间被申请，是一个二维数组，每个core有一个专门存放的地方。</span>    <span class="token comment">//kzalloc是内核空间内存申请函数，可以保证是连续的物理地址，但不能超过128KB，与kmalloc非常相似，只是这个会将申请到的内存清零。malloc是用户空间的。</span>    mem_event <span class="token operator">=</span> <span class="token function">kzalloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event</span> <span class="token operator">*</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">*</span> CPUS_PER_SOCKET<span class="token punctuation">,</span> GFP_KERNEL<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>cpu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> cpu <span class="token operator">&lt;</span> CPUS_PER_SOCKET<span class="token punctuation">;</span> cpu<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>mem_event<span class="token punctuation">[</span>cpu<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">kzalloc</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event</span> <span class="token operator">*</span><span class="token punctuation">)</span> <span class="token operator">*</span> N_HTMMEVENTS<span class="token punctuation">,</span> GFP_KERNEL<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>        <span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"pebs_init\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>       <span class="token keyword">for</span> <span class="token punctuation">(</span>cpu <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> cpu <span class="token operator">&lt;</span> CPUS_PER_SOCKET<span class="token punctuation">;</span> cpu<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token keyword">for</span> <span class="token punctuation">(</span>event <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> event <span class="token operator">&lt;</span> N_HTMMEVENTS<span class="token punctuation">;</span> event<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">get_pebs_event</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span> <span class="token operator">==</span> N_HTMMEVENTS<span class="token punctuation">)</span> <span class="token punctuation">{</span> <span class="token comment">//这里是在枚举有多少个要收集的数据，根据系统不同有的采样用不到，所以需要将这个指针变为NULL</span>mem_event<span class="token punctuation">[</span>cpu<span class="token punctuation">]</span><span class="token punctuation">[</span>event<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span><span class="token keyword">continue</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">__perf_event_open</span><span class="token punctuation">(</span><span class="token function">get_pebs_event</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> cpu<span class="token punctuation">,</span> event<span class="token punctuation">,</span> pid<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">//这一步获得每个perf事件对应的文件描述符</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">htmm__perf_event_init</span><span class="token punctuation">(</span>mem_event<span class="token punctuation">[</span>cpu<span class="token punctuation">]</span><span class="token punctuation">[</span>event<span class="token punctuation">]</span><span class="token punctuation">,</span> BUFFER_SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">//这一步将每个事件和ring buffer对应</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这一步就是要创建perf文件描述符，这里重写了一个名为<code>htmm__perf_event_open</code>的<code>perf_event_open</code>，但是核心操作是差不多的。从调用入口来看，传入的参数依次是<code>要采样事件的宏定义</code>，<code>0</code>这里值config2不用，<code>第几个cpu</code>，<code>第几个perf event</code>，<code>进程的pid</code></p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">/********************/</span><span class="token comment">/*传入要采样事件的宏定义，config1=0，cpu个数，事件个数，pid（为0就监控所有）；因为这里是在循环时open，每个cpu都有一个文件操作符在做这件事*/</span><span class="token comment">/*******************/</span><span class="token keyword">static</span> <span class="token keyword">int</span> <span class="token function">__perf_event_open</span><span class="token punctuation">(</span>__u64 config<span class="token punctuation">,</span> __u64 config1<span class="token punctuation">,</span> __u64 cpu<span class="token punctuation">,</span>__u64 type<span class="token punctuation">,</span> __u32 pid<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> attr<span class="token punctuation">;</span> <span class="token comment">// 函数需要的结构体，告诉这个文件描述符该怎么创建，因为采样不同的事件最后传回的perf_event结构体也不一样。</span>      <span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">;</span> <span class="token comment">// 已打开的文件在内核中用file结构体表示，文件描述符表中的指针指向file结构体。</span>    <span class="token keyword">int</span> event_fd<span class="token punctuation">,</span> __pid<span class="token punctuation">;</span> <span class="token comment">// 我们要接收的文件句柄</span>    <span class="token function">memset</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>type <span class="token operator">=</span> PERF_TYPE_RAW<span class="token punctuation">;</span> <span class="token comment">// 要检测的类型有硬件、软件等等咯。This indicates a "raw" implementation-specific event in the config field.</span>    attr<span class="token punctuation">.</span>size <span class="token operator">=</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>config <span class="token operator">=</span> config<span class="token punctuation">;</span> <span class="token comment">//要监测的采样事件</span>    <span class="token comment">/* 但是我们可以发现，这个事件传入的宏定义是作者自己定义的，不是系统有的默认的宏定义。If type is PERF_TYPE_RAW, then a custom "raw" config value is needed.  Most CPUs support events that are not covered by the "generalized" events.  These are implementation defined; see your CPU manual (for example the Intel Volume 3B documentation or the AMD BIOS and Kernel Developer Guide).  The libpfm4 library can be used to translate from the name in the architectural manuals to the raw hex value perf_event_open() expects in this field.*/</span>    attr<span class="token punctuation">.</span>config1 <span class="token operator">=</span> config1<span class="token punctuation">;</span> <span class="token comment">// 这是用作扩展用的</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>config <span class="token operator">==</span> ALL_STORES<span class="token punctuation">)</span>attr<span class="token punctuation">.</span>sample_period <span class="token operator">=</span> htmm_inst_sample_period<span class="token punctuation">;</span> <span class="token comment">//采样事件间隔</span>    <span class="token keyword">else</span>attr<span class="token punctuation">.</span>sample_period <span class="token operator">=</span> <span class="token function">get_sample_period</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    attr<span class="token punctuation">.</span>sample_type <span class="token operator">=</span> PERF_SAMPLE_IP <span class="token operator">|</span> PERF_SAMPLE_TID <span class="token operator">|</span> PERF_SAMPLE_ADDR<span class="token punctuation">;</span> <span class="token comment">//采样目标IP寄存器、TID实际上是内核（线程）中可调度对象的标识符（当一个进程只有一个线程时，pid和tid总是相同的）、地址</span>    attr<span class="token punctuation">.</span>disabled <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> <span class="token comment">// 初始状态为启用</span>    attr<span class="token punctuation">.</span>exclude_kernel <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* don't count kernel */</span>    attr<span class="token punctuation">.</span>exclude_hv <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* don't count hypervisor */</span>    attr<span class="token punctuation">.</span>exclude_callchain_kernel <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* exclude kernel callchains */</span>    attr<span class="token punctuation">.</span>exclude_callchain_user <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* exclude user callchains */</span>    attr<span class="token punctuation">.</span>precise_ip <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* skid constraint，默认是2咦 */</span>    attr<span class="token punctuation">.</span>enable_on_exec <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment">/* next exec enables */</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>pid <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>__pid <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token keyword">else</span>__pid <span class="token operator">=</span> pid<span class="token punctuation">;</span>    event_fd <span class="token operator">=</span> <span class="token function">htmm__perf_event_open</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>attr<span class="token punctuation">,</span> __pid<span class="token punctuation">,</span> cpu<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//创建文件描述符</span>    <span class="token comment">//htmm__perf_event_open在\kernel\events\core.c ，A call to perf_event_open() creates a file descriptor that allows measuring performance information. Each file descriptor corresponds to one event that is measured; these can be grouped together to measure multiple events simultaneously.只不过这里的是修改版的，那么两个文件有些什么区别主要添加了什么呢？</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>event_fd <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"[error htmm__perf_event_open failure] event_fd: %d\n"</span><span class="token punctuation">,</span> event_fd<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment">// 这里的读句柄的方式和上面计数用到的不一样。判断是不是写入到了文件，然后保留这个文件的private_data成员指针。private_data指针的指向会根据驱动不同而不同，这里可以获得perf_event指针。</span>    file <span class="token operator">=</span> <span class="token function">fget</span><span class="token punctuation">(</span>event_fd<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>file<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"invalid file\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    mem_event<span class="token punctuation">[</span>cpu<span class="token punctuation">]</span><span class="token punctuation">[</span>type<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">fget</span><span class="token punctuation">(</span>event_fd<span class="token punctuation">)</span><span class="token operator">-&gt;</span>private_data<span class="token punctuation">;</span>     <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分析作者自己封装的<code>htmm__perf_event_open</code>函数，到底有什么区别，因为封装前的<code>__NR_perf_event_open</code>这个系统调用才是主角。但是这里作者封装后的代码并没有这个主角。所以笔者尝试<a href="https://zhuanlan.zhihu.com/p/487648323">对比一下这个系统调用</a>。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">htmm__perf_event_open</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_attr</span> <span class="token operator">*</span>attr_ptr<span class="token punctuation">,</span> <span class="token class-name">pid_t</span> pid<span class="token punctuation">,</span><span class="token keyword">int</span> cpu<span class="token punctuation">,</span> <span class="token keyword">int</span> group_fd<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">long</span> flags<span class="token punctuation">)</span><span class="token punctuation">{</span> ……<span class="token comment">/*err = perf_copy_attr(attr_ptr, &amp;attr);if (err)return err;*/</span>attr <span class="token operator">=</span> <span class="token operator">*</span>attr_ptr<span class="token punctuation">;</span>……<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从结果来看，这里只有一个区别，指针赋值方式。重点在于这个函数没有被系统调用了，少了软中断，由原来的内核态切换到用户态执行。但是系统调用不是说拿到用户态就可以的，接着分析一下整个后台线程中其他的操作。发现作者还有添加新的系统调用：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">/* CONFIG_HTMM */</span>asmlinkage <span class="token keyword">long</span> <span class="token function">sys_htmm_start</span><span class="token punctuation">(</span><span class="token class-name">pid_t</span> pid<span class="token punctuation">,</span> <span class="token keyword">int</span> node<span class="token punctuation">)</span><span class="token punctuation">;</span>asmlinkage <span class="token keyword">long</span> <span class="token function">sys_htmm_end</span><span class="token punctuation">(</span><span class="token class-name">pid_t</span> pid<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">/***************/</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">else</span></span><span class="token function">SYSCALL_DEFINE2</span><span class="token punctuation">(</span>htmm_start<span class="token punctuation">,</span><span class="token class-name">pid_t</span><span class="token punctuation">,</span> pid<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">ksamplingd_init</span><span class="token punctuation">(</span>pid<span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">SYSCALL_DEFINE1</span><span class="token punctuation">(</span>htmm_end<span class="token punctuation">,</span><span class="token class-name">pid_t</span><span class="token punctuation">,</span> pid<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token function">ksamplingd_exit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也就是说虽然这里perf采样不是单独被系统调用的，但是整个后台的采样线程的启动都是被系统调用的，都是运行在内核态的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/d2fa3fb0e7784a43b931f86abb6f61e4.png" alt="后台采样线程流程图"><br>到目前为止，后台采样线程的文件描述符已经实现了。采样值的读取需要使用<code>mmap()</code>直接在用户空间操作，少一次复制，因为采样有异步性，需要一个环形队列，这是一个共享内存,一个多生产者单消费者（MPSC）队列。环形缓冲区的头是<code>struct perf_event_mmap_page</code>，记录<a href="https://github.com/torvalds/linux/blob/master/tools/perf/design.txt">共享环形缓冲区</a>的特点，这个头大小是一个页面大小。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">/* * Structure of the page that can be mapped via mmap */</span><span class="token keyword">struct</span> <span class="token class-name">perf_event_mmap_page</span> <span class="token punctuation">{</span>        __u32   version<span class="token punctuation">;</span>                <span class="token comment">/* version number of this structure */</span>        __u32   compat_version<span class="token punctuation">;</span>         <span class="token comment">/* lowest version this is compat with */</span>        <span class="token comment">/*         * Bits needed to read the hw counters in user-space.         *         *   u32 seq;         *   s64 count;         *         *   do {         *     seq = pc-&gt;lock;         *         *     barrier()         *     if (pc-&gt;index) {         *       count = pmc_read(pc-&gt;index - 1);         *       count += pc-&gt;offset;         *     } else         *       goto regular_read;         *         *     barrier();         *   } while (pc-&gt;lock != seq);         *         * NOTE: for obvious reason this only works on self-monitoring         *       processes.         */</span>        __u32   lock<span class="token punctuation">;</span>                   <span class="token comment">/* seqlock for synchronization */</span>        __u32   index<span class="token punctuation">;</span>                  <span class="token comment">/* hardware counter identifier */</span>        __s64   offset<span class="token punctuation">;</span>                 <span class="token comment">/* add to hardware counter value */</span>        <span class="token comment">/*         * Control data for the mmap() data buffer.         *         * User-space reading this value should issue an rmb(), on SMP capable         * platforms, after reading this value -- see perf_event_wakeup().         */</span>        __u32   data_head<span class="token punctuation">;</span>              <span class="token comment">/* head in the data section */</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后来看ring buffer结构体的信息：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">PERF_RECORD_MISC_KERNEL</span>          <span class="token expression"><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">PERF_RECORD_MISC_USER</span>            <span class="token expression"><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">1</span><span class="token punctuation">)</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">PERF_RECORD_MISC_OVERFLOW</span>        <span class="token expression"><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> <span class="token number">2</span><span class="token punctuation">)</span></span></span><span class="token keyword">struct</span> <span class="token class-name">perf_event_header</span> <span class="token punctuation">{</span>        __u32   type<span class="token punctuation">;</span>        __u16   misc<span class="token punctuation">;</span>        __u16   size<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">enum</span> <span class="token class-name">perf_event_type</span> <span class="token punctuation">{</span>        <span class="token comment">/*         * The MMAP events record the PROT_EXEC mappings so that we can         * correlate userspace IPs to code. They have the following structure:         *         * struct {         *      struct perf_event_header        header;         *         *      u32                             pid, tid;         *      u64                             addr;         *      u64                             len;         *      u64                             pgoff;         *      char                            filename[];         * };         */</span>        PERF_RECORD_MMAP                 <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>        PERF_RECORD_MUNMAP               <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>        <span class="token comment">/*         * struct {         *      struct perf_event_header        header;         *         *      u32                             pid, tid;         *      char                            comm[];         * };         */</span>        PERF_RECORD_COMM                 <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>        <span class="token comment">/*         * When header.misc &amp; PERF_RECORD_MISC_OVERFLOW the event_type field         * will be PERF_RECORD_*         *         * struct {         *      struct perf_event_header        header;         *         *      { u64                   ip;       } &amp;&amp; PERF_RECORD_IP         *      { u32                   pid, tid; } &amp;&amp; PERF_RECORD_TID         *      { u64                   time;     } &amp;&amp; PERF_RECORD_TIME         *      { u64                   addr;     } &amp;&amp; PERF_RECORD_ADDR         *         *      { u64                   nr;         *        { u64 event, val; }   cnt[nr];  } &amp;&amp; PERF_RECORD_GROUP         *         *      { u16                   nr,         *                              hv,         *                              kernel,         *                              user;         *        u64                   ips[nr];  } &amp;&amp; PERF_RECORD_CALLCHAIN         * };         */</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>接下来将文件描述符和ring buffer相关联。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">htmm__perf_event_init</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">perf_event</span> <span class="token operator">*</span>event<span class="token punctuation">,</span> <span class="token keyword">unsigned</span> <span class="token keyword">long</span> nr_pages<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">struct</span> <span class="token class-name">perf_buffer</span> <span class="token operator">*</span>rb <span class="token operator">=</span> <span class="token constant">NULL</span><span class="token punctuation">;</span>    <span class="token keyword">int</span> ret <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> flags <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>event<span class="token operator">-&gt;</span>cpu <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span> <span class="token operator">&amp;&amp;</span> event<span class="token operator">-&gt;</span>attr<span class="token punctuation">.</span>inherit<span class="token punctuation">)</span><span class="token keyword">return</span> <span class="token operator">-</span>EINVAL<span class="token punctuation">;</span>    ret <span class="token operator">=</span> <span class="token function">security_perf_event_read</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>ret<span class="token punctuation">)</span><span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token comment">// 采用伙伴系统分配的话得是2的幂次方</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>nr_pages <span class="token operator">!=</span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span><span class="token function">is_power_of_2</span><span class="token punctuation">(</span>nr_pages<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">return</span> <span class="token operator">-</span>EINVAL<span class="token punctuation">;</span>    <span class="token function">WARN_ON_ONCE</span><span class="token punctuation">(</span>event<span class="token operator">-&gt;</span>ctx<span class="token operator">-&gt;</span>parent_ctx<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 加上互斥锁，避免被并发访问</span>    <span class="token function">mutex_lock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>event<span class="token operator">-&gt;</span>mmap_mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">WARN_ON</span><span class="token punctuation">(</span>event<span class="token operator">-&gt;</span>rb<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 分配一个环形缓冲区</span>    rb <span class="token operator">=</span> <span class="token function">rb_alloc</span><span class="token punctuation">(</span>nr_pages<span class="token punctuation">,</span>    event<span class="token operator">-&gt;</span>attr<span class="token punctuation">.</span>watermark <span class="token operator">?</span> event<span class="token operator">-&gt;</span>attr<span class="token punctuation">.</span>wakeup_watermark <span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>    event<span class="token operator">-&gt;</span>cpu<span class="token punctuation">,</span> flags<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>rb<span class="token punctuation">)</span> <span class="token punctuation">{</span>ret <span class="token operator">=</span> <span class="token operator">-</span>ENOMEM<span class="token punctuation">;</span><span class="token keyword">goto</span> unlock<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token comment">// 这三个是perf子系统自带的函数，用于实现既定的功能</span><span class="token comment">// 将分配的环形缓冲区与 event 结构体关联起来。主要实现是rcu_assign_pointer(event-&gt;rb, rb);</span>    <span class="token function">ring_buffer_attach</span><span class="token punctuation">(</span>event<span class="token punctuation">,</span> rb<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 用于初始化环形缓冲区的头就是初始化perf_event_mmap_page。</span>    <span class="token function">perf_event_init_userpage</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">// 更新环形缓冲区头的信息，其中包括对性能事件的统计信息的更新。</span>    <span class="token function">perf_event_update_userpage</span><span class="token punctuation">(</span>event<span class="token punctuation">)</span><span class="token punctuation">;</span>unlock<span class="token operator">:</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>ret<span class="token punctuation">)</span> <span class="token punctuation">{</span>    <span class="token comment">// 增加 event-&gt;mmap_count 的计数。</span><span class="token function">atomic_inc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>event<span class="token operator">-&gt;</span>mmap_count<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token comment">// 解锁</span>    <span class="token function">mutex_unlock</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>event<span class="token operator">-&gt;</span>mmap_mutex<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> read the source code </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tiered Memory Management for Virtual Machines</title>
      <link href="/2023/12/07/vTMM/"/>
      <url>/2023/12/07/vTMM/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>(EuroSys), 2023</li><li>vTMM: Tiered Memory Management for Virtual Machines</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Sai Sha, Chuandong Li, Yingwei Luo, Xiaolin Wang, 终于读到北京大学的了</li><li>Zhenlin Wang, Michigan Technological University</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>内存虚拟化的技术一直在发展，那么以下罗列了几种比较典型的内存虚拟化方式。</p><h3 id="2-1-多次地址转换"><a href="#2-1-多次地址转换" class="headerlink" title="2.1 多次地址转换"></a>2.1 多次地址转换</h3><p>内存软件虚拟化：<br>虚拟机的虚拟地址（Guest Virtual Address, GVA）<br>虚拟机的物理地址（Guest Physical Address, GPA）<br>Host 虚拟地址（Host Virtual Address）<br>Host 的物理地址（Host Physical Address, HPA）</p><p>虚拟到物理都是传统的页表完成地址转换。虚拟机和物理机的转换VMM需要 intercept （截获）虚拟机的内存访问指令，VMM定义的映射表是kvm_memory_slot的数据结构，需要经过多次地址转换。</p><h3 id="2-2-影子页表"><a href="#2-2-影子页表" class="headerlink" title="2.2 影子页表"></a>2.2 影子页表</h3><p>通过影子页表，则可以实现客户机虚拟地址到宿主机物理地址的直接转换。必须为 Guest 的系统页表设计一套对应的影子页表，然后将影子页表装入 Host 的 MMU 中，这样当 Guest 访问 Host 内存时，就可以根据 MMU 中的影子页表映射关系，完成 GVA 到 HPA 的直接映射。而维护这套影子页表的工作则由 VMM 来完成。由于 Guest 中的每个进程都有自己的虚拟地址空间，这就意味着 VMM 要为 Guest 中的每个进程页表都维护一套 对应的影子页表，当 Guest 进程访问内存时，才将该进程的影子页表装入 Host 的 MMU 中，完成地址转换。这种方式虽然减少了地址转换的次数，但本质上还是纯软件实现的，效率还是不高，而且 VMM 承担 了太多影子页表的维护工作。</p><h3 id="2-3-EPT"><a href="#2-3-EPT" class="headerlink" title="2.3 EPT"></a>2.3 EPT</h3><p>EPT(Extended Page Table) 在原有 CR3 页表地址映射的基础上，引入了 EPT 页表来实现另一层映射，这样，GVA-&gt;GPA-&gt;HPA 的两次地址转换都由硬件来完成。CPU 首先会访问 Guest 中的 CR3 页表来完成 GVA 到 GPA 的转换，如果 GPA 不为空，则 CPU 接着通过 EPT 页表来实现 GPA 到HPA 的转换（实际上，CPU 会首先查看硬件 EPT TLB 或者缓存，如果没有对应的转换，才会进一步查看 EPT 页表），如果 HPA 为空呢，则 CPU 会抛出 EPT Violation 异常由 VMM 来处理。</p><p>当VM中的GuestOS在执行内存访问操作时，会通过EPT-page-structure将linear-address先转换成guest-physical-address,然后再将guest-physical-address转换成实际的物理地址physical-address；得到了最终的实际物理地址。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/1c30ff693cf54e42adb9a0c59db030e8.png"></p><p>PML是个记录脏页GPA的log，会有一块buff专门存，这个的设计和硬件的设计有关。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>分层内存在虚拟机应用中的性能提升。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/0d64837bec664b199ea625e0c343a989.png" alt="motivation"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/1a5b60a538304935bd2f31f799cfa995.png"></p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>首先是采样方面，并不去采样整个页表，甚至GPT比EPT要小也不去采样，直接在PML脏页面的范围里采样。有一个时间窗口，上次PML记录的地址下次就被访问。因为访问后需要刷新AD位，这个也比较耗时间，于是乎有一个多级列表，在列表越靠前获得AD位刷新免死金牌越多。当然详细算法设置还是看论文。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/c4735ac9101d461e9da762cb5b580a86.png"><br>在区分页面冷热方面，由于读写造成的延迟不同，于是上一步采样到的读写次数在这里加权之后去表示页面的冷热。桶排序由于时间复杂度低，用于筛选出要进行迁移的页面。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/b2269c8c7ab04dc18953bb7b0c072505.png"><br>迁移涉及到脏页那么如何去保护一致性，在这里作者还对比了以往的迁移方法，说明在虚拟机里不太合适以往的方式。这里是复制的方法，不过由于这些页面会在PML里也有存，所以利用起来保持一致性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/direct/f28cf860d41a40e3ad10ee5160fde179.png"><br>除此之外，当一台物理机运行多台虚拟机时，每台机器相比较其实最热的页面热度也有差别。同时，快速内存层资源紧张，于是建立内存池，对快速的内存资源进行有效调度。这里有个难点，虚拟机的内存资源按理说都得隔离开，怎么去动态调度的呢？</p><h2 id="6-缺陷和改进思路"><a href="#6-缺陷和改进思路" class="headerlink" title="6. 缺陷和改进思路"></a>6. 缺陷和改进思路</h2><p>主要问题还是这样的物理机会有运行多少虚拟机，以及他的应用场景是什么？</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> Virtual Machines </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DAMON</title>
      <link href="/2023/11/22/DAMO/"/>
      <url>/2023/11/22/DAMO/</url>
      
        <content type="html"><![CDATA[<p>这个采样方式是一个区域内选一个页面，称为样本页</p><p>首先是都得在root用户或者sudo下运行，否则会有难以下手的bug。damo start –help之类的方式可以获得所提供命令的介绍。</p><p>画图相关命令需要安装gnuplot命令如<code>sudo apt install gnuplot-x11</code></p><h2 id="用于记录访问监控结果并可视化这些结果"><a href="#用于记录访问监控结果并可视化这些结果" class="headerlink" title="用于记录访问监控结果并可视化这些结果"></a>用于记录访问监控结果并可视化这些结果</h2><h3 id="record"><a href="#record" class="headerlink" title="record"></a>record</h3><p><code>sudo damo record $(pidof pr)</code><br>得到damon.data文件，包括不同时间不同区域的访问频率。会统计访问事件和工作负载大小的情况。会在工作负载停止时自动停止。但是需要额外的代码去可视化。</p><p>但是还是太粗粒度了 <span class="github-emoji"><span>😕</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f615.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 通过 <code>damo record --help</code>可以获得更多的选项设置，nice</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">damo record <span class="token parameter variable">--help</span>positional arguments:  <span class="token operator">&lt;</span>command, pid, special keywords, or kdamonds json spec<span class="token operator">&gt;</span>                        the implicit monitoring requestsoptional arguments:  -h, <span class="token parameter variable">--help</span>            show this <span class="token builtin class-name">help</span> message and <span class="token builtin class-name">exit</span>  <span class="token parameter variable">-s</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>, <span class="token parameter variable">--sample</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>                        sampling interval <span class="token punctuation">(</span>us <span class="token number">1,000</span>,000<span class="token punctuation">)</span>                        采样间隔是有用且需要的。只会检查这个区域的一个样本页面。  <span class="token parameter variable">-a</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>, <span class="token parameter variable">--aggr</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>                        aggregate interval <span class="token punctuation">(</span>us<span class="token punctuation">)</span>这个是采样之后聚合用，多次采样间隔的值放在一个时间段去聚合显示，这样配合采样间隔使用图片就不会那么黑了。  <span class="token parameter variable">-u</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>, <span class="token parameter variable">--updr</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>                        regions update interval <span class="token punctuation">(</span>us<span class="token punctuation">)</span>采样时将监控目标区域划分为多个子区域，子区域由访问频率相似的相邻页面组成。每个子区域大小是不一样的，但是程序运行中，原先满足条件的子区域可能变了，此时需要更新子区域。  <span class="token parameter variable">-n</span> <span class="token operator">&lt;</span><span class="token comment"># regions&gt;, --minr &lt;# regions&gt;</span>                        minimal number of regions  <span class="token parameter variable">-m</span> <span class="token operator">&lt;</span><span class="token comment"># regions&gt;, --maxr &lt;# regions&gt;</span>                        maximum number of regions  <span class="token parameter variable">--monitoring_nr_regions_range</span> <span class="token operator">&lt;</span>min<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>max<span class="token operator">&gt;</span>                        min/max number of monitoring regions                        这个指每次采样会有多少个监控区域，增多监控区域，每个区域的大小自然就小了。而且这个工具如果你一个地方访问频率相似，就会把采样粒度扩大的。区域多当然开销就多。  <span class="token parameter variable">--damos_sz_region</span> <span class="token operator">&lt;</span>min<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>max<span class="token operator">&gt;</span>                        min/max size of damos target regions <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span>                        指定采样区域的大小粒度，哇塞，4096就是4KB，8129就是两个页面和起来监控。  <span class="token parameter variable">-o</span> <span class="token operator">&lt;</span>file path<span class="token operator">&gt;</span>, <span class="token parameter variable">--out</span> <span class="token operator">&lt;</span>file path<span class="token operator">&gt;</span>                        output <span class="token function">file</span> path   目前没啥用的功能，但是保留在这几介绍   <span class="token parameter variable">--ops</span> <span class="token punctuation">{</span>vaddr,paddr,fvaddr<span class="token punctuation">}</span>                        monitoring operations <span class="token builtin class-name">set</span>  <span class="token parameter variable">--target_pid</span> <span class="token operator">&lt;</span>pid<span class="token operator">&gt;</span>    pid of monitoring target process  <span class="token parameter variable">-r</span> <span class="token string">"&lt;start&gt;-&lt;end&gt; ..."</span>, <span class="token parameter variable">--regions</span> <span class="token string">"&lt;start&gt;-&lt;end&gt; ..."</span>                        monitoring target address regions                        这是指地址区域，一般也不知道这个进程在哪个地址啊？  <span class="token parameter variable">--numa_node</span> <span class="token operator">&lt;</span>node id<span class="token operator">&gt;</span>                        <span class="token keyword">if</span> target is <span class="token string">'paddr'</span>, limit it to the numa <span class="token function">node</span>  <span class="token parameter variable">--damos_action</span> <span class="token operator">&lt;</span>action<span class="token operator">&gt;</span>                        damos action to apply to the target regions  <span class="token parameter variable">--monitoring_intervals</span> <span class="token operator">&lt;</span>sample<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>aggr<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>update<span class="token operator">&gt;</span>                        monitoring intervals <span class="token punctuation">(</span>us<span class="token punctuation">)</span>                        组合的采样间隔用不到这么复杂的                      <span class="token parameter variable">--damos_access_rate</span> <span class="token operator">&lt;</span>min<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>max<span class="token operator">&gt;</span>                        min/max access rate of damos target regions <span class="token punctuation">(</span>percent<span class="token punctuation">)</span>  <span class="token parameter variable">--damos_age</span> <span class="token operator">&lt;</span>min<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>max<span class="token operator">&gt;</span>                        min/max age of damos target regions <span class="token punctuation">(</span>microseconds<span class="token punctuation">)</span>  <span class="token parameter variable">--damos_apply_interval</span> <span class="token operator">&lt;</span>microseconds<span class="token operator">&gt;</span>                        the apply interval <span class="token keyword">for</span> the scheme应用隔离？？？  <span class="token parameter variable">--damos_quotas</span> <span class="token operator">&lt;</span>time <span class="token punctuation">(</span>ms<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>size <span class="token punctuation">(</span>bytes<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>reset interval <span class="token punctuation">(</span>ms<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>size priority weight <span class="token punctuation">(</span>permil<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>access rate priority weight<span class="token operator">&gt;</span> <span class="token punctuation">(</span>permil<span class="token punctuation">)</span> <span class="token operator">&lt;</span>age priority weight<span class="token operator">&gt;</span> <span class="token punctuation">(</span>permil<span class="token punctuation">)</span>                        damos quotas给damo的资源使用配额？  <span class="token parameter variable">--damos_wmarks</span> <span class="token operator">&lt;</span>metric <span class="token punctuation">(</span>none<span class="token operator">|</span>free_mem_rate<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>interval <span class="token punctuation">(</span>us<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>high mark <span class="token punctuation">(</span>permil<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>mid mark <span class="token punctuation">(</span>permil<span class="token punctuation">)</span><span class="token operator">&gt;</span> <span class="token operator">&lt;</span>low mark <span class="token punctuation">(</span>permil<span class="token punctuation">)</span><span class="token operator">&gt;</span>                        damos watermarks  <span class="token parameter variable">--damos_filter</span> <span class="token operator">&lt;</span>filter argument<span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token operator">&lt;</span>filter argument<span class="token operator">&gt;</span> <span class="token punctuation">..</span>.<span class="token punctuation">]</span>                        damos filter <span class="token punctuation">(</span>type, matching, and optional arguments                        看来就是过滤某些地址咯，可以设置多个过滤器  <span class="token parameter variable">--kdamonds</span> <span class="token operator">&lt;</span>json string or file<span class="token operator">&gt;</span>                        json <span class="token function">format</span> kdamonds specification to run DAMON <span class="token keyword">for</span>  <span class="token parameter variable">--damon_interface</span> <span class="token punctuation">{</span>sysfs,debugfs,auto<span class="token punctuation">}</span>                        underlying DAMON interface to use <span class="token punctuation">(</span><span class="token operator">!</span><span class="token operator">!</span> DEPRECATED<span class="token punctuation">)</span>                        ！！！思考报错的原因，有点意思  <span class="token parameter variable">--debug_damon</span>         Print debugging log  <span class="token parameter variable">--output_type</span> <span class="token punctuation">{</span>json_compressed,json,perf_script,perf_data<span class="token punctuation">}</span>                        output file's <span class="token builtin class-name">type</span>  <span class="token parameter variable">--output_permission</span> OUTPUT_PERMISSION                        permission of the output <span class="token function">file</span>  <span class="token parameter variable">--perf_path</span> PERF_PATH                        path of perf tool  <span class="token parameter variable">--include_child_tasks</span>                        record accesses of child processes                        将子进程也一起监控<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>执行指定的监控程序有3种方式，首先在命令后面使用command 比如”sleep 5”; 第二种方式获取pid（比较好奇MPI程序那么多进程会怎样）; 第三种方式指定的物理地址（监视目标区域默认为 /proc/iomem 文件中指定的最大“System RAM”区域。）。</p></blockquote><h3 id="report"><a href="#report" class="headerlink" title="report"></a>report</h3><h4 id="heats"><a href="#heats" class="headerlink" title="heats"></a>heats</h4><p><code>sudo damo report heats --heatmap 图片名</code><br>好的这个直接将可视化的都展示了，完全OK了，It supports pdf, png, jpeg, and svg.如果需要指定区域，那么可以使用–resol组合来做。<code>--time_ range &lt;time&gt; &lt;time&gt;</code>和<code>--address_ range &lt;address&gt; &lt;address&gt;</code>都可以指定更细节的范围。如果想要显示绝对地址和时间作为横纵坐标，那么可以使用<code>--abs_time</code>和<code>--abs_addr</code>.设置图片主题颜色有3种可选gray、flame、emotion<code>--stdout_heatmap_color</code>. <code>-i</code> 选择指定文件名。</p><p><code>sudo damo report heats --resol 2 4</code><br>–resol是指分辨率，具体来说后面根的两个参数表示<strong>输出时间段只有2段，输出地址范围有4个</strong>，会按照总的时间和地址范围大概平均分的亚子。<br><strong>输出依次表示以纳秒为单位的时间、以字节为单位的地址、观察到的访问频率</strong>。<br><strong>这个命令输出的数据可以支持画热图</strong>。</p><p><code>sudo damo report heats --heatmap stdout</code><br><strong>需要和damon.data在一个文件夹下</strong>就能直接运行(x-axis) what memory region (y-axis) is how frequently accessed (color)</p><h4 id="wss（working-set-size）"><a href="#wss（working-set-size）" class="headerlink" title="wss（working set size）"></a>wss（working set size）</h4><p><code>sudo damo report wss --range 0 101 1</code><br>the distribution of the working set size.这个是终端打印出来的图像，不太行。而且这个统计图横纵坐标也不太清楚。</p><p><code>sudo damo report wss --range 0 101 1 --sortby time --plot 图片名</code><br>how the working set size has changed by time.这个是曲线图，统计的信息和样式上没我自己做的好，可能这个开销小一些。</p><h4 id="raw"><a href="#raw" class="headerlink" title="raw"></a>raw</h4><p><code>sudo damo report raw &gt;&gt; pr.txt</code><br>可以将。data这个二进制文件的内容读出来，空白行隔开每一次的采样。每次采样的内容包括<strong>采样开始、结束、持续时间、此次监控区域的数量、每个监控区域的起始地址、地址长度、访问频率、age</strong>.</p><h2 id="用于快照和可视化-DAMON-的监控结果和运行状态"><a href="#用于快照和可视化-DAMON-的监控结果和运行状态" class="headerlink" title="用于快照和可视化 DAMON 的监控结果和运行状态"></a>用于快照和可视化 DAMON 的监控结果和运行状态</h2><h3 id="show"><a href="#show" class="headerlink" title="show"></a>show</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># damo start 我的workload</span><span class="token comment"># damo show</span><span class="token number">0</span>   addr <span class="token punctuation">[</span><span class="token number">4.000</span> GiB   , <span class="token number">16.245</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.245</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">7</span> m <span class="token number">32.100</span> s<span class="token number">1</span>   addr <span class="token punctuation">[</span><span class="token number">16.245</span> GiB  , <span class="token number">28.529</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.284</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">12</span> m <span class="token number">40.500</span> s<span class="token number">2</span>   addr <span class="token punctuation">[</span><span class="token number">28.529</span> GiB  , <span class="token number">40.800</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.271</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">15</span> m <span class="token number">10.100</span> s<span class="token number">3</span>   addr <span class="token punctuation">[</span><span class="token number">40.800</span> GiB  , <span class="token number">52.866</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.066</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">15</span> m <span class="token number">58.600</span> s<span class="token number">4</span>   addr <span class="token punctuation">[</span><span class="token number">52.866</span> GiB  , <span class="token number">65.121</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.255</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">16</span> m <span class="token number">15.900</span> s<span class="token number">5</span>   addr <span class="token punctuation">[</span><span class="token number">65.121</span> GiB  , <span class="token number">77.312</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.191</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">16</span> m <span class="token number">22.400</span> s<span class="token number">6</span>   addr <span class="token punctuation">[</span><span class="token number">77.312</span> GiB  , <span class="token number">89.537</span> GiB <span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.225</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">16</span> m <span class="token number">24.200</span> s<span class="token number">7</span>   addr <span class="token punctuation">[</span><span class="token number">89.537</span> GiB  , <span class="token number">101.824</span> GiB<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">12.287</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">16</span> m <span class="token number">25</span> s<span class="token number">8</span>   addr <span class="token punctuation">[</span><span class="token number">101.824</span> GiB , <span class="token number">126.938</span> GiB<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token number">25.114</span> GiB <span class="token punctuation">)</span> access <span class="token number">0</span> %   age <span class="token number">16</span> m <span class="token number">25.300</span> stotal size: <span class="token number">122.938</span> GiB<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个Region信息包含该Region的监控结果，包括内存Region的起始地址、结束地址、nr_accesses、age等，每个快照的Region数量取决于DAMON参数min_nr_regions和max_nr_regions，实际情况监控目标地址空间的数据访问模式。</p><h3 id="status"><a href="#status" class="headerlink" title="status"></a>status</h3><h2 id="运行控制指令"><a href="#运行控制指令" class="headerlink" title="运行控制指令"></a>运行控制指令</h2><h3 id="start"><a href="#start" class="headerlink" title="start"></a>start</h3><p>这就是怎么跑都会被报错的！！！</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#比如这里希望隔5s就有快照</span><span class="token function">sudo</span> damo start <span class="token parameter variable">--target_pid</span> <span class="token variable"><span class="token variable">$(</span>pidof masim<span class="token variable">)</span></span><span class="token keyword">while</span> <span class="token builtin class-name">:</span><span class="token punctuation">;</span> <span class="token keyword">do</span> <span class="token function">sudo</span> damo show<span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">5</span><span class="token punctuation">;</span> <span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>相关报错但不限于：</p><pre class="line-numbers language-none"><code class="language-none">could not turn DAMON on (cannot apply kdmonds from args (DAMON debugfs doesn't support online staging))vaddr region install failed (staging updates failed (DAMON debugfs doesn't support online staging))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>试了试<code>sudo mount -t debugfs none /sys/kernel/debug</code>也没用</p><h3 id="物理地址和虚拟地址的查询"><a href="#物理地址和虚拟地址的查询" class="headerlink" title="物理地址和虚拟地址的查询"></a>物理地址和虚拟地址的查询</h3><p>damo使用的是将16进制地址变成10进制后的地址，无论是作图还是输入地址范围。</p><p><code>sudo cat /proc/iomem</code>可以得到物理地址的范围，对于物理采样damo只接受整个System RAM区域的地址，就是说你不能把一个System RAM的地址拆开给他。这个地址的DRAM一般出现在比较靠后的位置，可以根据<code>free -h</code>或者<code>numactl -H</code>算一算大小对不对。</p><pre class="line-numbers language-none"><code class="language-none">00000000-00000fff : Reserved00001000-0008efff : System RAM0008f000-0008ffff : Reserved00090000-0009ffff : System RAM000a0000-000fffff : Reserved  000a0000-000bffff : PCI Bus 0000:00  000c8000-000cffff : PCI Bus 0000:00  000f0000-000fffff : System ROM00100000-46cc501f : System RAM46cc5020-46cf6a5f : System RAM46cf6a60-46cf701f : System RAM46cf7020-46d29c5f : System RAM46d29c60-46d2a01f : System RAM46d2a020-46d5cc5f : System RAM46d5cc60-46d5d01f : System RAM46d5d020-46d6505f : System RAM46d65060-49e76fff : System RAM49e77000-4ae76fff : ACPI Non-volatile Storage4ae77000-4b2cefff : System RAM4b2cf000-4b54dfff : ACPI Tables4b54e000-4bd93fff : System RAM4bd94000-4be99fff : Reserved4be9a000-4cdfffff : System RAM4ce00000-4cffffff : Reserved4d000000-5eefdfff : System RAM5eefe000-6e3fefff : Reserved  69569020-6956906f : APEI ERST  69569078-6956907f : APEI ERST  69569080-6956b01f : APEI ERST6e3ff000-6f3fefff : ACPI Non-volatile Storage6f3ff000-6f7fefff : ACPI Tables6f7ff000-6f7fffff : System RAM6f800000-8fffffff : Reserved  80000000-8fffffff : PCI MMCONFIG 0000 [bus 00-ff]90000000-9b7fffff : PCI Bus 0000:00  90000000-900fffff : PCI Bus 0000:04    90000000-9003ffff : 0000:04:00.0    90040000-9007ffff : 0000:04:00.1  91000000-91ffffff : PCI Bus 0000:02    91000000-91ffffff : PCI Bus 0000:03      91000000-91ffffff : 0000:03:00.0        91000000-912fffff : efifb  92000000-928fffff : PCI Bus 0000:02    92000000-928fffff : PCI Bus 0000:03      92000000-927fffff : 0000:03:00.0      92808000-9280bfff : 0000:03:00.0  92900000-929fffff : PCI Bus 0000:04    92900000-9290ffff : 0000:04:00.1      92900000-9290ffff : tg3    92910000-9291ffff : 0000:04:00.1      92910000-9291ffff : tg3    92920000-9292ffff : 0000:04:00.1      92920000-9292ffff : tg3    92930000-9293ffff : 0000:04:00.0      92930000-9293ffff : tg3    92940000-9294ffff : 0000:04:00.0      92940000-9294ffff : tg3    92950000-9295ffff : 0000:04:00.0      92950000-9295ffff : tg3  92a00000-92afffff : 0000:00:02.4  92b00000-92b7ffff : 0000:00:17.0    92b00000-92b7ffff : ahci  92b80000-92bfffff : 0000:00:11.5    92b80000-92bfffff : ahci  92c80000-92cfffff : 0000:00:02.1  92d00000-92d7ffff : 0000:00:02.1  92dc0000-92ddffff : 0000:00:02.4  92de0000-92deffff : 0000:00:14.0    92de0000-92deffff : xhci-hcd  92df0000-92df3fff : 0000:00:02.4  92df4000-92df7fff : 0000:00:1f.2  92df8000-92df9fff : 0000:00:17.0    92df8000-92df9fff : ahci  92dfa000-92dfbfff : 0000:00:11.5    92dfa000-92dfbfff : ahci  92dfc000-92dfdfff : 0000:00:02.0  92dfe000-92dfe0ff : 0000:00:1f.4  92dff000-92dfffff : 0000:00:16.4  92e00000-92e00fff : 0000:00:16.1  92e01000-92e01fff : 0000:00:16.0  92e02000-92e02fff : 0000:00:14.2    92e02000-92e02fff : Intel PCH thermal driver  92e04000-92e040ff : 0000:00:17.0    92e04000-92e040ff : ahci  92e05000-92e050ff : 0000:00:11.5    92e05000-92e050ff : ahci  9b7fc000-9b7fcfff : dmar99b800000-a63fffff : PCI Bus 0000:16  9b800000-9b9fffff : PCI Bus 0000:17    9b800000-9b8fffff : 0000:17:00.0      9b800000-9b8fffff : megasas: LSI    9b900000-9b9fffff : 0000:17:00.0  9ba00000-9bafffff : PCI Bus 0000:17    9ba00000-9bafffff : 0000:17:00.0  a63fc000-a63fcfff : dmar5a6400000-b0ffffff : PCI Bus 0000:30  b0ffc000-b0ffcfff : dmar6b1000000-bbbfffff : PCI Bus 0000:4a  bbbfc000-bbbfcfff : dmar7bbc00000-c5ffffff : PCI Bus 0000:64  c5ffc000-c5ffcfff : dmar8c6800000-d0ffffff : PCI Bus 0000:80  c6800000-c68fffff : 0000:80:02.4  c6980000-c69fffff : 0000:80:02.1  c6a00000-c6a7ffff : 0000:80:02.1  c6a80000-c6a9ffff : 0000:80:02.4  c6aa0000-c6aa3fff : 0000:80:02.4  c6aa4000-c6aa5fff : 0000:80:02.0  d0ffc000-d0ffcfff : dmar0d1000000-dbbfffff : PCI Bus 0000:97  dbbfc000-dbbfcfff : dmar1dbc00000-e67fffff : PCI Bus 0000:b0  e67fc000-e67fcfff : dmar2e6800000-f13fffff : PCI Bus 0000:c9  f13fc000-f13fcfff : dmar3f1400000-fb7fffff : PCI Bus 0000:e2  fb7fc000-fb7fcfff : dmar4fd000000-fdabffff : pnp 00:04fdad0000-fdadffff : pnp 00:04fdb00000-fdffffff : pnp 00:04fe000000-fe010fff : Reserved  fe000000-fe00ffff : pnp 00:04  fe010000-fe010fff : PCI Bus 0000:00    fe010000-fe010fff : 0000:00:1f.5fe011000-fe01ffff : pnp 00:04fe036000-fe03bfff : pnp 00:04fe03d000-fe3fffff : pnp 00:04fe410000-fe7fffff : pnp 00:04fec00000-fecfffff : PNP0003:00  fec00000-fec003ff : IOAPIC 0fed00000-fed003ff : HPET 0  fed00000-fed003ff : PNP0103:00ff000000-ffffffff : pnp 00:01100,000,000-207fffffff : System RAM-------------------------------------------  b6be00000-b6cc025c6 : Kernel code  b6ce00000-b6d321fff : Kernel rodata  b6d400000-b6d761d3f : Kernel data  b6da4e000-b6dffffff : Kernel bss2080000000-11c7fffffff : Persistent Memory  2080000000-22781fffff : namespace0.0  2280000000-9e7fffffff : dax0.0    2,280,000,000-9,e7f,fff,fff : System RAM (kmem) -------------------------------  9e80000000-9e801fffff : namespace1.0  9e80200000-11c7fffffff : dax1.01ffc00000000-1fffffffffff : Reserved  1ffffff84000-1ffffff84fff : ndbus0  1ffffff85000-1ffffff85fff : ndbus0  1ffffff86000-1ffffff86fff : ndbus0  1ffffff87000-1ffffff87fff : ndbus0  1ffffff94000-1ffffff94fff : ndbus0  1ffffff95000-1ffffff95fff : ndbus0  1ffffff96000-1ffffff96fff : ndbus0  1ffffff97000-1ffffff97fff : ndbus0200000000000-20ffffffffff : PCI Bus 0000:00210000000000-21ffffffffff : PCI Bus 0000:16  21fffff00000-21fffff1ffff : 0000:16:02.0220000000000-22ffffffffff : PCI Bus 0000:30230000000000-23ffffffffff : PCI Bus 0000:4a240000000000-24ffffffffff : PCI Bus 0000:64250000000000-25ffffffffff : PCI Bus 0000:80260000000000-26ffffffffff : PCI Bus 0000:97270000000000-27ffffffffff : PCI Bus 0000:b0280000000000-28ffffffffff : PCI Bus 0000:c9290000000000-29ffffffffff : PCI Bus 0000:e2<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>另外有时也会用到虚拟地址，通过查找文件Documentation/x86/x86_64/mm.rst</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTE </tag>
            
            <tag> 采样 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux内核常用内核调优</title>
      <link href="/2023/11/19/Linux%E5%86%85%E6%A0%B8%E5%B8%B8%E7%94%A8%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/"/>
      <url>/2023/11/19/Linux%E5%86%85%E6%A0%B8%E5%B8%B8%E7%94%A8%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/</url>
      
        <content type="html"><![CDATA[<h2 id="0-相关理论"><a href="#0-相关理论" class="headerlink" title="0.相关理论"></a>0.相关理论</h2><ul><li>80%结果由20%原因引起</li><li>先找木桶短板再优化</li><li>采样-&gt;分析瓶颈和热点-&gt;定位问题-&gt;优化-&gt;测试</li></ul><h2 id="1-内核自带统计信息"><a href="#1-内核自带统计信息" class="headerlink" title="1.内核自带统计信息"></a>1.内核自带统计信息</h2><h3 id="1-1-proc文件系统"><a href="#1-1-proc文件系统" class="headerlink" title="1.1 proc文件系统"></a>1.1 proc文件系统</h3><p>让用户和内核内部数据结构交互，并不是真正意义上的文件系统，在内存且不占用磁盘，文件本身显示0字节，但查看就会返回大量信息。ps、top等shell命令就是从proc文件系统中读取信息的。</p><ul><li><code>/proc/pagetypeinfo</code> 这是URL链表上有多少活跃和不活跃页面的统计</li><li><code>/proc/zoneinfo</code> zone结构体的相关信息，按照node来显示的，会显示伙伴系统管理的页面数量、各种水位线页面的数量等</li><li><code>/proc/slabinfo</code> slab分配的小的内存的信息</li><li><code>/proc/cpuinfo</code> processer如果是7则是指有8个核心</li><li><code>/proc/[pid]/maps</code> 或者 <code>pmap pid</code> 命令来查看进程用户态虚拟内存空间的实际分布</li><li><code>/proc/iomem</code> 命令来查看进程内核态虚拟内存空间的的实际分布，还能查看内存的物理地址，不用sudo竟然地址全是0</li><li><code>/proc/buddyinfo</code>命令来查看NUMA节点中不同内存区域zone的伙伴系统当前状态，迁移类型，链表成员数量</li><li><code>/proc/meminfo</code>显示的是当前时刻物理页的信息，包括LRU的、slab的、hugepage的等等，<code>meminfo_proc_show</code>函数实现</li><li><code>proc/[pid]/status | grep -E 'Name|Pid|Vm*|Rss*|Vm*|Hu*'</code>是进程和内存相关的信息。数据结构<code>mm_struct</code>成员<code>rss_stat</code>会记录进程内存的使用情况。（VmRSS进程使用的最大物理内存、RssAnon进程使用的匿名页、RssFile、RssShmem共享页、VmPTE页表大小、VmSwap交换区大小等）</li><li><code>proc/mounts</code>已加载的文件系统列表</li><li><code>proc/modules</code>已加载的模块</li><li><code>proc/cmdline</code>系统启动时输入的内核命令行参数</li><li><code>proc/kmsg</code>内核日志信息</li></ul><p>写一个内核模块在/proc创建benshushu目录，通过read节点读取内核模块某个全局变量值。通过write修改某个全局变量值</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">//procfs的常用API在fs/proc/internal.h文件中有定义</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/proc_fs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/uaccess.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/init.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NODE</span> <span class="token string">"benshushu/my_proc"</span></span><span class="token keyword">static</span> <span class="token keyword">int</span> param <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">proc_dir_entry</span> <span class="token operator">*</span>my_proc<span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">proc_dir_entry</span> <span class="token operator">*</span>my_root<span class="token punctuation">;</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KS</span> <span class="token expression"><span class="token number">32</span></span></span><span class="token keyword">static</span> <span class="token keyword">char</span> kstring<span class="token punctuation">[</span>KS<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">/* should be less sloppy about overflows :) */</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span><span class="token function">my_read</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span> <span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> nbytes <span class="token operator">=</span> <span class="token function">sprintf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token function">simple_read_from_buffer</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> kstring<span class="token punctuation">,</span> nbytes<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">my_write</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span><span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token class-name">ssize_t</span> rc<span class="token punctuation">;</span>rc <span class="token operator">=</span> <span class="token function">simple_write_to_buffer</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">sscanf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"param has been set to %d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> rc<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//文件函数操作集</span><span class="token keyword">static</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">file_operations</span> my_proc_fops <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span>owner <span class="token operator">=</span> THIS_MODULE<span class="token punctuation">,</span><span class="token punctuation">.</span>read <span class="token operator">=</span> my_read<span class="token punctuation">,</span><span class="token punctuation">.</span>write <span class="token operator">=</span> my_write<span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">int</span> __init <span class="token function">my_init</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token comment">//proc_mkdir在父目录中创建一个benshushu的目录，这里父目录NULL那就是指在/proc根目录下创建了</span>my_root <span class="token operator">=</span> <span class="token function">proc_mkdir</span><span class="token punctuation">(</span><span class="token string">"benshushu"</span><span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>my_root<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">pr_err</span><span class="token punctuation">(</span><span class="token string">"I failed to make benshushu dir\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span>    <span class="token comment">//proc_create创建一个新的文件节点，名称是NODE宏定义，节点访问权限0(UGO模式)，父进程的proc_dir_entry对象NULL，my_proc_fops指向文件的操作函数。</span>my_proc <span class="token operator">=</span> <span class="token function">proc_create</span><span class="token punctuation">(</span>NODE<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>my_proc_fops<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>my_proc<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">pr_err</span><span class="token punctuation">(</span><span class="token string">"I failed to make %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"I created %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> __exit <span class="token function">my_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">if</span> <span class="token punctuation">(</span>my_proc<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">proc_remove</span><span class="token punctuation">(</span>my_proc<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">proc_remove</span><span class="token punctuation">(</span>my_root<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"Removed %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function">module_init</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">module_exit</span><span class="token punctuation">(</span>my_exit<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_LICENSE</span><span class="token punctuation">(</span><span class="token string">"GPL"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-Makefile" data-language="Makefile"><code class="language-Makefile">BASEINCLUDE ?= /lib/modules/`uname -r`/buildproc-test-objs := proc_test.o obj-m:=   proc-test.oall : $(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean:$(MAKE) -C $(BASEINCLUDE) M=$(PWD) clean;rm -f *.ko;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-2-sys文件系统"><a href="#1-2-sys文件系统" class="headerlink" title="1.2 sys文件系统"></a>1.2 sys文件系统</h3><p>子系统、设备驱动程序将sysfs作为与用户空间交互的接口。<br>功能和上一个一样，sysfs的API又有所不同。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/proc_fs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/uaccess.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/init.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/device.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/platform_device.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/sysfs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NODE</span> <span class="token string">"benshushu/my_sysfs"</span></span><span class="token keyword">static</span> <span class="token keyword">int</span> param <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">proc_dir_entry</span> <span class="token operator">*</span>my_sysfs<span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">proc_dir_entry</span> <span class="token operator">*</span>my_root<span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">platform_device</span> <span class="token operator">*</span>my_device<span class="token punctuation">;</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KS</span> <span class="token expression"><span class="token number">32</span></span></span><span class="token keyword">static</span> <span class="token keyword">char</span> kstring<span class="token punctuation">[</span>KS<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">/* should be less sloppy about overflows :) */</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span><span class="token function">my_read</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span> <span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> nbytes <span class="token operator">=</span> <span class="token function">sprintf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token function">simple_read_from_buffer</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> kstring<span class="token punctuation">,</span> nbytes<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">my_write</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span><span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token class-name">ssize_t</span> rc<span class="token punctuation">;</span>rc <span class="token operator">=</span> <span class="token function">simple_write_to_buffer</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">sscanf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"param has been set to %d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> rc<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//文件的操作集</span><span class="token keyword">static</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">file_operations</span> my_proc_fops <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span>owner <span class="token operator">=</span> THIS_MODULE<span class="token punctuation">,</span><span class="token punctuation">.</span>read <span class="token operator">=</span> my_read<span class="token punctuation">,</span><span class="token punctuation">.</span>write <span class="token operator">=</span> my_write<span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">data_show</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">device</span> <span class="token operator">*</span>d<span class="token punctuation">,</span><span class="token keyword">struct</span> <span class="token class-name">device_attribute</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span>  <span class="token keyword">char</span> <span class="token operator">*</span>buf<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">return</span> <span class="token function">sprintf</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> <span class="token string">"%d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">data_store</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">device</span> <span class="token operator">*</span>d<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">device_attribute</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> count<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">sscanf</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> <span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">dev_dbg</span><span class="token punctuation">(</span>d<span class="token punctuation">,</span> <span class="token string">": write %d into data\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token function">strnlen</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> count<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token function">DEVICE_ATTR_RW</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//用于描述文件属性的数据结构</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">attribute</span> <span class="token operator">*</span>ben_sysfs_entries<span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token operator">&amp;</span>dev_attr_data<span class="token punctuation">.</span>attr<span class="token punctuation">,</span><span class="token constant">NULL</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token comment">//一组属性类型，这个数据结构定义在include/linux/sysfs.h</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">attribute_group</span> mydevice_attr_group <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span>name <span class="token operator">=</span> <span class="token string">"benshushu"</span><span class="token punctuation">,</span><span class="token punctuation">.</span>attrs <span class="token operator">=</span> ben_sysfs_entries<span class="token punctuation">,</span> <span class="token comment">//attribute数据结构</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">int</span> __init <span class="token function">my_init</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> ret<span class="token punctuation">;</span>my_root <span class="token operator">=</span> <span class="token function">proc_mkdir</span><span class="token punctuation">(</span><span class="token string">"benshushu"</span><span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span>my_sysfs <span class="token operator">=</span> <span class="token function">proc_create</span><span class="token punctuation">(</span>NODE<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>my_proc_fops<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>my_sysfs<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">pr_err</span><span class="token punctuation">(</span><span class="token string">"I failed to make %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token function">PTR_ERR</span><span class="token punctuation">(</span>my_sysfs<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"I created %s on procfs\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//kobject_create_and_add()生成kobject数据结构，注册到sysfs文件系统，参数为：文件名、父目录的kobject</span>my_device <span class="token operator">=</span> <span class="token function">platform_device_register_simple</span><span class="token punctuation">(</span><span class="token string">"benshushu"</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>my_device<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"platfrom device register fail\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>ret <span class="token operator">=</span> <span class="token function">PTR_ERR</span><span class="token punctuation">(</span>my_device<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">goto</span> proc_fail<span class="token punctuation">;</span><span class="token punctuation">}</span>    <span class="token comment">//（属性集合，一组属性类型）</span>ret <span class="token operator">=</span> <span class="token function">sysfs_create_group</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>my_device<span class="token operator">-&gt;</span>dev<span class="token punctuation">.</span>kobj<span class="token punctuation">,</span> <span class="token operator">&amp;</span>mydevice_attr_group<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>ret<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"create sysfs group fail\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">goto</span> register_fail<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"create sysfs node done\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>register_fail<span class="token operator">:</span><span class="token function">platform_device_unregister</span><span class="token punctuation">(</span>my_device<span class="token punctuation">)</span><span class="token punctuation">;</span>proc_fail<span class="token operator">:</span><span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> __exit <span class="token function">my_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">if</span> <span class="token punctuation">(</span>my_sysfs<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">proc_remove</span><span class="token punctuation">(</span>my_sysfs<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">proc_remove</span><span class="token punctuation">(</span>my_root<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"Removed %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">sysfs_remove_group</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>my_device<span class="token operator">-&gt;</span>dev<span class="token punctuation">.</span>kobj<span class="token punctuation">,</span> <span class="token operator">&amp;</span>mydevice_attr_group<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">platform_device_unregister</span><span class="token punctuation">(</span>my_device<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">module_init</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">module_exit</span><span class="token punctuation">(</span>my_exit<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_LICENSE</span><span class="token punctuation">(</span><span class="token string">"GPL"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-Makefile" data-language="Makefile"><code class="language-Makefile">BASEINCLUDE ?= /lib/modules/`uname -r`/buildsysfs-test-objs := sysfs_test.o obj-m:=   sysfs-test.oall : $(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean:$(MAKE) -C $(BASEINCLUDE) M=$(PWD) clean;rm -f *.ko;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-3-debugfs文件系统"><a href="#1-3-debugfs文件系统" class="headerlink" title="1.3 debugfs文件系统"></a>1.3 debugfs文件系统</h3><p>调试内核的内存文件系统，在运行中修改某些内核数据，将关心的数据映射到用户空间。<code>mount -t debugfs none /sys/kernel/debug</code></p><p>相关API定义在<code>include/linux/debuggfs.h</code>中</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/proc_fs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/uaccess.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/init.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/debugfs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">NODE</span> <span class="token string">"benshushu"</span></span><span class="token keyword">static</span> <span class="token keyword">int</span> param <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">;</span><span class="token keyword">struct</span> <span class="token class-name">dentry</span> <span class="token operator">*</span>debugfs_dir<span class="token punctuation">;</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KS</span> <span class="token expression"><span class="token number">32</span></span></span><span class="token keyword">static</span> <span class="token keyword">char</span> kstring<span class="token punctuation">[</span>KS<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token comment">/* should be less sloppy about overflows :) */</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span><span class="token function">my_read</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span> <span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> nbytes <span class="token operator">=</span> <span class="token function">sprintf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d\n"</span><span class="token punctuation">,</span> param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token function">simple_read_from_buffer</span><span class="token punctuation">(</span>buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> kstring<span class="token punctuation">,</span> nbytes<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">my_write</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span><span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token class-name">ssize_t</span> rc<span class="token punctuation">;</span>rc <span class="token operator">=</span> <span class="token function">simple_write_to_buffer</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> lbuf<span class="token punctuation">,</span> ppos<span class="token punctuation">,</span> buf<span class="token punctuation">,</span> lbuf<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">sscanf</span><span class="token punctuation">(</span>kstring<span class="token punctuation">,</span> <span class="token string">"%d"</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>param<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//pr_info("param has been set to %d\n", param);</span><span class="token keyword">return</span> rc<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">file_operations</span> mydebugfs_ops <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span>owner <span class="token operator">=</span> THIS_MODULE<span class="token punctuation">,</span><span class="token punctuation">.</span>read <span class="token operator">=</span> my_read<span class="token punctuation">,</span><span class="token punctuation">.</span>write <span class="token operator">=</span> my_write<span class="token punctuation">,</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">int</span> __init <span class="token function">my_init</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">struct</span> <span class="token class-name">dentry</span> <span class="token operator">*</span>debug_file<span class="token punctuation">;</span>debugfs_dir <span class="token operator">=</span> <span class="token function">debugfs_create_dir</span><span class="token punctuation">(</span>NODE<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>debugfs_dir<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"create debugfs dir fail\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span>EFAULT<span class="token punctuation">;</span><span class="token punctuation">}</span>debug_file <span class="token operator">=</span> <span class="token function">debugfs_create_file</span><span class="token punctuation">(</span><span class="token string">"my_debug"</span><span class="token punctuation">,</span> <span class="token number">0444</span><span class="token punctuation">,</span>debugfs_dir<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span>mydebugfs_ops<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">IS_ERR</span><span class="token punctuation">(</span>debug_file<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"create debugfs file fail\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">debugfs_remove_recursive</span><span class="token punctuation">(</span>debugfs_dir<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span>EFAULT<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"I created %s on debugfs\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> __exit <span class="token function">my_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">if</span> <span class="token punctuation">(</span>debugfs_dir<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">debugfs_remove_recursive</span><span class="token punctuation">(</span>debugfs_dir<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"Removed %s\n"</span><span class="token punctuation">,</span> NODE<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function">module_init</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">module_exit</span><span class="token punctuation">(</span>my_exit<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_LICENSE</span><span class="token punctuation">(</span><span class="token string">"GPL"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-Makefile" data-language="Makefile"><code class="language-Makefile">BASEINCLUDE ?= /lib/modules/`uname -r`/builddebugfs-test-objs := debugfs_test.o obj-m:=   debugfs-test.oall : $(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean:$(MAKE) -C $(BASEINCLUDE) M=$(PWD) clean;rm -f *.ko;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-查看系统内存信息工具"><a href="#2-查看系统内存信息工具" class="headerlink" title="2.查看系统内存信息工具"></a>2.查看系统内存信息工具</h2><h3 id="2-1-top"><a href="#2-1-top" class="headerlink" title="2.1 top"></a>2.1 top</h3><p>前几行是<code>free -h</code>的信息，还有些可以交互的命令，比如<code>f</code>是显示隐藏的统计信息比如缺页等，<code>M</code>是按照使用内存排序<br>top显示的排序是:</p><ul><li>PR进程优先级</li><li>NI nice值</li><li>VIRT进程使用的虚拟内存总量</li><li>RES进程使用的没有被换出的物理内存大小</li><li>SHR共享内存大小</li><li>S进程状态（D不可中断睡眠，R运行，S睡眠，T跟踪/停止，Z僵尸）</li><li>%MEM进程使用物理内存百分比</li><li>%CPU上次更新到现在的CPU时间占用百分比</li><li>TIME+ 进程使用CPU的时间总计，单位10ms</li><li>CMOMAND命令名或者命令行</li></ul><h3 id="2-2-vmstat"><a href="#2-2-vmstat" class="headerlink" title="2.2 vmstat"></a>2.2 vmstat</h3><p>第一个参数是时间间隔，第二个参数是采样次数。<br><code>-S M</code>指按照兆字节来显示。</p><ul><li>r正在执行和等待的进程数</li><li>b阻塞的进程</li><li><a href="https://farseerfc.me/zhs/in-defence-of-swap.html">swpd</a>通常而言，最优内存管理所需的最小交换空间取决于程序固定在内存中而又很少访问到的匿名页面的数量， 以及回收这些匿名页面换来的价值。后者大体上来说是问哪些页面不再会因为要保留这些很少访问的匿名页面而 被回收掉腾出空间。</li><li>free空闲物理内存</li><li>buff用于磁盘缓存的大小</li><li>cache用于页面缓存的内存大小</li><li>si从磁盘换入的内存大小，so交换到磁盘的内存大小。页的换入换出</li><li>磁盘读写块数量bibo从块设备</li><li>每秒中断次数算不算太大in</li><li>cs <strong>每秒上下文切换次数数百到一万以内</strong>都是正常的，例如我们调用系统函数，就要进行上下文切换，线程的切换，也要进程上下文切换，这个值要越小越好，太大了，要考虑调低线程或者进程的数目,例如<strong>在apache和nginx这种web服务器中，我们一般做性能测试时会进行几千并发甚至几万并发的测试</strong>，选择web服务器的进程可以由进程或者线程的峰值一直下调，压测，直到cs到一个比较小的值，这个进程和线程数就是比较合适的值了。<strong>系统调用也是，每次调用系统函数，我们的代码就会进入内核空间，导致上下文切换，这个是很耗资源</strong>，也要尽量避免频繁调用系统函数。上下文切换次数过多表示你的CPU大部分浪费在上下文切换，导致CPU干正经事的时间少了，CPU没有充分利用，是不可取的。<a href="https://www.cnblogs.com/emars/p/13787492.html">下图来自这篇，很详细的找瓶颈</a> <strong>线程数不要超过物理机的2-3倍</strong>。</li><li>内核系统进程sy和用户进程us执行时间百分比</li><li>wa是CPU的I/O等待时间百分比</li><li>id空闲CPU百分比</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/68d632bf8f0743cbbe0b5c3978dfb54b.png"></p><p><a href="https://linux.die.net/man/8/vmstat">man手册</a></p><h3 id="2-3-iostat"><a href="#2-3-iostat" class="headerlink" title="2.3 iostat"></a>2.3 iostat</h3><ul><li>iowait CPU 等待磁盘 IO 操作的时间</li><li>rrqm/s：每秒合并的读请求数。这表示磁盘在读取数据时是否进行了请求的合并操作。较高的值可能表示磁盘正在优化读取性能。</li><li>wrqm/s：每秒合并的写请求数。类似于rrqm/s，这表示磁盘在写入数据时是否进行了请求的合并操作。</li><li>r/s：每秒完成的读请求数。这表示磁盘每秒实际执行的独立的读取操作数量。</li><li>w/s：每秒完成的写请求数。这表示磁盘每秒实际执行的独立的写入操作数量。</li><li>rkB/s：每秒读取的数据量（以KB为单位）。表示从磁盘读取的数据量。</li><li>wkB/s：每秒写入的数据量（以KB为单位）。表示向磁盘写入的数据量。</li><li>avgrq-sz：平均请求大小（以扇区为单位）。表示平均每个I/O请求的大小。</li><li>avgqu-sz：平均I/O队列长度。表示等待磁盘服务的请求的平均数量。</li><li>await：平均每个I/O请求的等待时间（以毫秒为单位）。较低的值通常表示更好的性能。</li><li>svctm：平均每个I/O请求的服务时间（以毫秒为单位）。这表示磁盘实际执行请求所需的时间。</li><li>%util：磁盘利用率。表示磁盘的负载程度，较高的值表示磁盘正在饱和。<br><a href="https://www.cnblogs.com/happy-king/p/9234122.html#:~:text=delta%20%28ruse+wuse%29/delta%20%28io%29%20=%20await%20=%2078.21%20=%3E,=78.21%20*%20delta%20%28io%29/s%20=%2078.21*28.57%20=%202232.8%EF%BC%8C%E8%A1%A8%E6%98%8E%E6%AF%8F%E7%A7%92%E5%86%85%E7%9A%84I/O%E8%AF%B7%E6%B1%82%E6%80%BB%E5%85%B1%E9%9C%80%E8%A6%81%E7%AD%89%E5%BE%852232.8ms%E3%80%82">iowait过高的查找和解决方案</a></li></ul><h2 id="3-printk输出函数和动态输出"><a href="#3-printk输出函数和动态输出" class="headerlink" title="3. printk输出函数和动态输出"></a>3. printk输出函数和动态输出</h2><h3 id="3-1-printk最简单有效的调试方法"><a href="#3-1-printk最简单有效的调试方法" class="headerlink" title="3.1 printk最简单有效的调试方法"></a>3.1 printk最简单有效的调试方法</h3><p>在Linux内核中，<code>printk</code>函数可以使用不同的优先级来记录日志消息。以下是一些常用的<code>printk</code>优先级：</p><ol><li><strong>KERN_EMERG</strong>：紧急情况。用于记录最紧急的系统消息，通常表示系统崩溃或无法继续运行的情况。这个优先级的消息通常会导致系统宕机。</li><li><strong>KERN_ALERT</strong>：警报。用于记录需要立即采取行动的情况，通常表示严重的系统问题，但不一定导致宕机。</li><li><strong>KERN_CRIT</strong>：关键。用于记录严重的系统问题，但没有导致系统崩溃。这可以是一个需要紧急关注的问题。</li><li><strong>KERN_ERR</strong>：错误。用于记录错误消息，表示系统遇到了一个问题，但仍然可以继续运行。</li><li><strong>KERN_WARNING</strong>：警告。用于记录警告消息，表示可能存在一些问题，但系统仍在正常运行。</li><li><strong>KERN_NOTICE</strong>：通知。用于记录一般信息，通常表示系统中发生的一些重要事件。</li><li><strong>KERN_INFO</strong>：信息。用于记录普通信息，通常用于调试和跟踪系统行为。</li><li><strong>KERN_DEBUG</strong>：调试。用于记录调试信息，通常只在开发和调试阶段使用。</li></ol><p>这些优先级可以与<code>printk</code>函数一起使用，例如：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token function">printk</span><span class="token punctuation">(</span>KERN_ERR <span class="token string">"This is an error message.\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>在这个例子中，<code>KERN_ERR</code>表示这是一个错误消息，该消息将被记录到系统日志中。</p><p>对于新驱动程序，建议使用更方便的打印API,这些API在其名称中嵌入了日志级别。这些打印辅助函数包括<code>pr_emerg、pr_alert、pr_crit、pr_err、pr_warning、pr_warn、pr_notice、pr_info、pr_debug或pr_dbg</code>。<br>除了比等效的printk()调用更简洁外，它们还可以通过pr_fmt()宏 对格式字符串使用<br>通用定义，例如，在源文件的顶部(在任何#include 指令之前)定义它:</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">pr_fmt</span><span class="token expression"><span class="token punctuation">(</span>fmt<span class="token punctuation">)</span> </span><span class="token string">"%s:%s: "</span> <span class="token expression">fmt<span class="token punctuation">,</span> KBUILD_MODNAME<span class="token punctuation">,</span> <span class="token constant">__func__</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这将在该文件中的每个<code>pr_*</code>消息前加上产生该消息的模块和函数名称。内核得是用DEBUG编译的，否则它们被替换为空语句。</p><h3 id="3-2动态输出"><a href="#3-2动态输出" class="headerlink" title="3.2动态输出"></a>3.2动态输出</h3><p>由系统维护者动态地打开和关闭指定的printk输出。配置内核时打开<code>CONFIG_DYNAMIC_DEBUG</code>宏，挂载<code>debugfs</code>文件系统。</p><p>动态输出语句由<code>pr_debug()/dev_dbg()</code>这些函数定义。</p><p>具体来说动态输出会在debugfs文件系统的control文件节点被记录，输出包括文件路径名、输出语句所在行号、模块名（函数名）、将要输出的语句。<code>cat /sys/kernel/debug/dynamic_debug/control</code></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment">#打开svcsock.c文件中的所有动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token string">'file svcsock.c +p'</span> <span class="token operator">&gt;</span> /sys/kernel/debug/dynamic_debug/control<span class="token comment">#打开usbcore模块中的所有动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token string">'module usbcore +p'</span> <span class="token operator">&gt;</span> /sys/kernel/debug/dynamic_debug/control<span class="token comment">#打开svc_process()函数中的所有动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token string">'func svc_process +p'</span> <span class="token operator">&gt;</span> /sys/kernel/debug/dynamic_debug/control<span class="token comment">#关闭svc_process()函数中的所有动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token string">'func svc_process -p'</span> <span class="token operator">&gt;</span> /sys/kernel/debug/dynamic_debug/control<span class="token comment">#打开文件路径中包含usb的文件里的所有动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token parameter variable">-n</span> <span class="token string">'*usb* +p'</span> <span class="token operator">&gt;</span> /sys/kernel/debug/dynamic_debug/control<span class="token comment">#打开系统所有的动态输出语句</span><span class="token builtin class-name">echo</span> <span class="token parameter variable">-n</span> <span class="token string">'+p'</span> <span class="token operator">&gt;</span> / sys/kernel/debug/dynamic_debug/control<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>p打开关闭动态输出语句</li><li>l输出行号</li><li>f输出函数名</li><li>m输出模块名</li><li>t输出线程ID</li></ul><p>对于那些开机就已经完成运行的代码，调试方式是在qemu启动时加入命令，比如SMP topology的动态输出语句打开方式在qemu启动命令加上<code>topology.dyndbg=+plft</code>这样输入<code>dmesg | grep 'cluster' </code>就能看见输出。</p><h2 id="4-cppcheck"><a href="#4-cppcheck" class="headerlink" title="4. cppcheck"></a>4. cppcheck</h2><p>这是一个用于静态检查代码错误的工具，可以避免越界访问等一些问题，减少后期在内核调试的成本。在改改内核的时候<a href="https://cppcheck.sourceforge.io/">用起来</a>！<a href="https://cppcheck.sourceforge.io/manual.pdf">操作手册</a></p><h2 id="5-ftrace"><a href="#5-ftrace" class="headerlink" title="5. ftrace"></a>5. ftrace</h2><p>使用set_ftrace_pid</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># cd /sys/kernel/debug/tracing/</span><span class="token comment"># cat set_ftrace_pid</span>no pid<span class="token comment"># echo 3111 &gt; set_ftrace_pid //跟踪PID为3111的进程</span><span class="token comment"># cat set_ftrace_pid</span><span class="token number">3111</span><span class="token comment"># echo function &gt; current_tracer</span><span class="token comment"># echo 1 &gt; tracing_on</span><span class="token comment"># usleep 1</span><span class="token comment"># echo 0 &gt; tracing_on</span><span class="token comment"># cat trace</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>frace还支持种更为直观的跟踪器，名为function_gaph,使用方法和function跟踪器类似。</p><p><strong>动态ftrace</strong>：<br>只要在配置内核时打开CONFIG_DYNAMIC_FTRACE选项，就可以支持动态ftrace功能。set_ftrace_filter和set_ftrace_notrace这两个文件可以配对使用，其中，前者设置要跟踪的函数，后者指定不要跟踪的函数。如果函数太多,需要设置跟踪函数的过滤器,过滤器还支持通配符。available_filter_functions文件可以列出当前系统支持的所有函数，假如现在我们只想关注sys_nanosleep()和hrtimer_interrupt()这两个函数。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># cd /sys/kernel/debug/tracing/</span><span class="token comment"># echo sys_nanosleep hrtimer_interrupt &gt; set_ftrace_filter</span><span class="token comment"># echo function &gt; current_tracer</span><span class="token comment"># echo 1 &gt; tracing_on</span><span class="token comment"># usleep 1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>也可以将要跟踪的事件添加到相应文件，差不多的方式。例子在samples/trace_events目录里面。</p><p><strong>kernelshark图形化工具</strong><br><code>sudo apt-get install trace-cmd kernelshark</code><br>分析ftrace数据用</p><h2 id="6-kdump"><a href="#6-kdump" class="headerlink" title="6. kdump"></a>6. kdump</h2><p>内核转储工具。kdump核心实现基于kexec, kexec 的全称是kernel execution,非常类似于Linux内核中的exec系统调用。kexec可以快速启动新的内核，并且跳过BIOS或bootloader等引导程序的初始化阶段。这个特性可以让系统上崩溃时快速切换到备份的内核，这样第一个内核的内存就得到了保留。在第二个内核中，可以对第一个内核产生的崩溃数据进行继续分析。</p><p>kdump会在内存中保留一块区域，这块区域用来存放捕获内核。当生产内核在运行过程中遇到崩溃等情况时，kdump会通过kexec机制自动启动到捕获内核，这时会绕过BIOS,以免破坏第一个内核的内存，然后把生产内核的完整信息(包括CPU寄存器、栈数据等)转储到指定文件中。接着，使用crash工具分析这个转储文件，就可以快速定位宕机问题了。</p><p>kdump+crash工具的适用范围：主要用来分析系统宕机黑屏、无响应(unresponsive)等问题，比如SSH、串口、鼠标键盘无响应等。有一类宕机情况kdump无能为力，比如因硬件错误导致CPU崩溃。也就是说，系统不能正常地热重启，只能通过重新关闭和开启电源才能启动，这种情况下kdump就不适用了。因为kdump需要在系统崩溃的时候快速启动到捕获内核，但前提条件就是系统能热启动，并且内存中的内容不会丢失。</p><h2 id="7-常见的Linux性能测试工具"><a href="#7-常见的Linux性能测试工具" class="headerlink" title="7. 常见的Linux性能测试工具"></a>7. 常见的Linux性能测试工具</h2><table><thead><tr><th>工具</th><th>描述</th></tr></thead><tbody><tr><td>kernel-selftests</td><td>内核源代码目录自带的测试程序</td></tr><tr><td>perf-bench</td><td>perf工具自带的测试程序，包含对内存、调度等的测试</td></tr><tr><td>phoronix-test-suit</td><td>综合性能测试程序</td></tr><tr><td>sysbench</td><td>综合性能测试套件，包含对CPU、内存、多线程等的测试</td></tr><tr><td>unixbench</td><td>综合性能测试套件，UNIX系统的一套传统的测试程序</td></tr><tr><td>pmbench</td><td>用来测试内存性能的工具</td></tr><tr><td>iozone</td><td>用来测试文件系统性能的工具</td></tr><tr><td>AIM7</td><td>一套来自UNIX系统的测试系统底层性能的工具</td></tr><tr><td>iperf</td><td>用来测试网络性能的工具</td></tr><tr><td>linpack</td><td>用来测试CPU的浮点运算的性能</td></tr><tr><td>vm-scalability</td><td>用来测试Linux内核内存管理模块的扩展性</td></tr><tr><td>glbenchmark</td><td>用来测试GPU性能</td></tr><tr><td>GFXbenchmark</td><td>用来测试GPU性能</td></tr><tr><td>DBENCH</td><td>用来测试I/O性能</td></tr></tbody></table><p>内容来自《奔跑吧Linux内核基础篇》这些工具记录在这里，有一部分也没用过。</p><h2 id="8-perf和火焰图"><a href="#8-perf和火焰图" class="headerlink" title="8. perf和火焰图"></a>8. perf和火焰图</h2><blockquote><p><strong>性能计数器是 CPU 硬件寄存器</strong>，用于计算硬件事件，例如执行的指令、遭受的缓存未命中或错误预测的分支。利用这些数据可以跟踪动态控制流和识别热点。Perf 提供每个任务、每个 CPU 和每个工作负载计数器、基于这些计数器的采样和源代码事件注释。</p></blockquote><blockquote><p><strong>跟踪点</strong>是放置在代码中的逻辑位置的检测点，例如用于系统调用、TCP/IP 事件、文件系统操作等。这些在不使用时开销可以忽略不计，并且可以通过 perf 命令启用以收集包括时间戳和堆栈跟踪在内的信息。Perf 还可以使用 kprobes 和 uprobes 框架动态创建跟踪点，用于内核和用户空间动态跟踪。</p></blockquote><p>perf 功能强大：它可以检测 CPU 性能计数器、跟踪点、kprobes 和 uprobes（动态跟踪）。它能够进行轻量级分析。它包含在 Linux 内核中的 tools/perf 下，并且经常更新和增强。对于perf子系统更深入的剖析在笔者另一篇读源码的文章中。这里主要讲怎么使用封装好的用户空间的perf工具。以page fault为例子：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># -I 1000 每1000ms输出一次，</span><span class="token comment"># -a 采集全部CPU上的事件</span><span class="token comment"># -p &lt;pid&gt; 可以指定进程</span><span class="token operator">&gt;</span> perf <span class="token function">stat</span> <span class="token parameter variable">-e</span> page-faults <span class="token parameter variable">-I</span> <span class="token number">1000</span> <span class="token parameter variable">-a</span><span class="token operator">&gt;</span> perf <span class="token function">stat</span> <span class="token parameter variable">-e</span> page-faults <span class="token parameter variable">-I</span> <span class="token number">1000</span> <span class="token parameter variable">-a</span> <span class="token parameter variable">-p</span> <span class="token number">10102</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用FlameGraph更加直观的看到各部分代码触发pagefault的比例</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 采集进程10102的30秒pagefault触发数据</span><span class="token operator">&gt;</span> perf record <span class="token parameter variable">-e</span> page-faults <span class="token parameter variable">-a</span> <span class="token parameter variable">-p</span> <span class="token number">10102</span> <span class="token parameter variable">-g</span> -- <span class="token function">sleep</span> <span class="token number">30</span><span class="token comment"># 导出原始数据，此步必须在采集机器上进行，因为需要解析符号。</span><span class="token operator">&gt;</span> perf script <span class="token operator">&gt;</span> out.stacks<span class="token comment"># 下面的步骤就可以移动到其他机器上了</span><span class="token operator">&gt;</span> ./FlameGraph/stackcollapse-perf.pl <span class="token operator">&lt;</span> out.stacks <span class="token operator">|</span> ./FlameGraph/flamegraph.pl <span class="token parameter variable">--color</span><span class="token operator">=</span>mem <span class="token punctuation">\</span>    <span class="token parameter variable">--title</span><span class="token operator">=</span><span class="token string">"Page Fault Flame Graph"</span> <span class="token parameter variable">--countname</span><span class="token operator">=</span><span class="token string">"pages"</span> <span class="token operator">&gt;</span> out.svg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>stat 收集性能计数器的统计数据</li><li>record Run a command and record its profile into perf.data</li><li> -e 事件选择器。使用 “perf list “来列出可用的事件。</li><li> -p 对现有进程ID的统计事件</li><li> -a 从所有CPU收集全系统的信息</li><li> -i -I 子任务不继承计数器</li><li> -o 输出文件名和文件路径</li></ul><blockquote><p>dTLB是数据TLB，iTLB是指令TLB，因为CPU在处理指令和数据时访问方式不同，所以指令页和数据页是分开访问的。<br>TLB是用于快速查找虚拟地址到物理地址的映射关系的，他们分别映射的是指令页和数据页的物理地址。<br>指令和数据都存放在主存，它们都以二进制代码形式出现，对于CPU而言，区分的方法为：（1）取指令或数据时所处的机器周期不同：取指周期取出的是指令；分析、取数或执行周期取出的是数据。（2）取指令或数据时地址的来源不同：指令地址来源于程序计数器；数据地址来源于地址形成部件。<br>当CPU执行一条指令时，会将下一条指令的地址预取到缓存中，这个预取的过程称为指令预取。iTLB用于缓存指令页的映射信息，可以加速指令预取和指令执行的过程。<br>常用事件的解释<a href="https://zhuanlan.zhihu.com/p/445260558">参考博客</a>：</p></blockquote><ul><li>branches ：这段时间内发生分支预测的次数。现代的CPU都有分支预测方面的优化</li><li>branch-instructions：分支预测成功次数</li><li>branch-misses ：这段时间内分支预测失败的次数，这个值越小越好</li><li>cache-references：cache命中次数</li><li>cache-misses：cache失效次数</li><li>context-switches：下文切换次数，前半部分是切换次数，后面是平均每秒发生次数（M是10的6次方）</li><li>cpu-cycles：统计cpu周期数，cpu周期：指一条指令的操作时间。</li><li>cycles：程序消耗的处理器周期数</li><li>instructions：执行的指令条数，insns per cycle: 即IPC，每个cpu周期执行的指令条数，IPC比上面的CPU使用率更能说明CPU的使用情况，（很多指令需要多个处理周期才能执行完毕），IPC越大越好，说明程序充分利用了处理器的特征。</li><li>L1-dcache-loads ：一级数据缓存读取次数</li><li>L1-dcache-load-missed ：一级数据缓存读取失败次数</li><li>LLC-loads：last level cache 读取次数</li><li>LLC-load-misses：last level cache 读取失败次数</li><li>major-faults：页错误，内存页已经被swap到硬盘上，需要I/O换回</li><li>minor-faults ：页错误，内存页在物理内存中，只是没有和逻辑页进行映射</li><li>page-faults：缺页异常的次数。当应用程序请求的页面尚未建立、请求的页面不在内存中，或者请求的页面虽然在内存中，但物理地址和虚拟地址的映射关系尚未建立时，都会触发一次缺页异常。另外TLB不命中，页面访问权限不匹配等情况也会触发缺页异常。</li><li>stalled-cycles-frontend和stalled-cycles-backend表示CPU停滞统计</li><li>task-clock (msec): cpu处理task所消耗的时间，表示目标任务真正占用处理器的时间，单位ms，CPUs utilized表示cpu使用率， 该值越高代表程序是CPU bound（计算密集型）而非IO bound（I/O密集型）</li></ul><p>具体命令写法<a href="https://perf.wiki.kernel.org/index.php/Tutorial">参考文档</a>里面写了各种可以测量的事件。比如：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">perf <span class="token function">stat</span> <span class="token parameter variable">-e</span> dTLB-loads,dTLB-load-misses,iTLB-load-misses,cpu-cycles,cache-misses,cache-references,page-faults,major-faults,minor-faults,LLC-loads,LLC-loads-misses <span class="token parameter variable">-a</span> ./test.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>确定 CPU 繁忙的原因是一项性能分析的例行任务，这通常涉及分析堆栈跟踪。火焰图是采样堆栈跟踪的可视化，可以快速识别热代码路径。<a href="https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html">火焰图可以与任何操作系统上的任何 CPU 分析器一起使用</a>。我在这里的例子使用Linux perf （perf_events），DTrace，SystemTap和ktap。有关其他探查器示例，<a href="https://www.brendangregg.com/flamegraphs.html#Updates">请参阅更新列表</a>，有关火焰图软件，<a href="https://github.com/brendangregg/FlameGraph">请参阅 github</a>。<a href="https://www.cnblogs.com/xuxh120/p/15990446.html">火焰图使用</a>不同工具获得的信息在生成火焰图时命令不同，<a href="https://blog.csdn.net/SaberJYang/article/details/123964439">参考博客</a>。</p><p><strong>y轴</strong>表示<strong>调用栈</strong>， 每一层都是一个函数。 调用栈越深， 火焰就越高， 顶部就是正在执行的函数， 下方都是它的父函数。<br><strong>x轴</strong>表示<strong>抽样数</strong>， 如果一个函数在x轴占据的宽度越宽， 就表示它被抽到的次数多， <strong>即执行的时间长</strong>。 注意x轴不代表时间， 而是所有的调用栈合并后， 按字母顺序排列的。<br>火焰图就是看<strong>顶层的哪个函数占据的宽度最大</strong>。 只要有 “平顶”(plateaus)， 就表示该函数可能存在性能问题。<br>颜色没有特殊含义， 因为火焰图表示的是CPU的繁忙程度， 所以一般选择暖色调。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 采样，1s10次</span><span class="token function">sudo</span> perf recoed <span class="token parameter variable">-F</span> <span class="token number">10</span> <span class="token parameter variable">-o</span> <span class="token variable">${LOG_DIR}</span>/perf.data <span class="token parameter variable">-a</span> <span class="token parameter variable">-g</span> <span class="token variable">${BENCH_RUN}</span><span class="token comment"># 画图，因为写入是默认名字所以这里处理的输入也就默认名</span><span class="token function">sudo</span> perf script <span class="token operator">|</span> ./tools/FlameGraph/stackcollapse-perf.pl <span class="token operator">|</span> ./tools/FlameGraph/flamegraph.pl <span class="token operator">&gt;</span> perf.svg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="9-Intel-MLC-（Memory-Latency-Checker）"><a href="#9-Intel-MLC-（Memory-Latency-Checker）" class="headerlink" title="9. Intel MLC （Memory Latency Checker）"></a>9. Intel MLC （Memory Latency Checker）</h2><p>决定应用程序性能的一个重要因素是应用程序<strong>从处理器的缓存层次结构和内存子系统获取数据所需的时间</strong>。在启用非统一内存访问 (NUMA) 的多插槽系统中，<strong>本地内存延迟和跨插槽内存延迟</strong>会有很大差异。除了延迟之外，带宽 (b/w) 在决定性能方面也发挥着重要作用。因此，测量这些<strong>延迟和带宽</strong>对于为被测系统和性能分析建立基线非常重要。</p><p>英特尔® 内存延迟检查器（英特尔® MLC）是一种用于测量内存延迟和带宽以及它们<strong>如何随着系统负载增加而变化</strong>的工具。它还为更细粒度的调查提供了多种选项，还可以测量从一组特定核心到缓存或内存的带宽和延迟。</p><p>root权限来运行<code>./mlc &gt;&gt; /home/xmu3/Documents/res/mlc.out </code>不加后缀的话是都可以测量的。</p><p>这些命令在运行工作负载和空机运行时应该会有所不同，<strong>用ycsb的数据对比一下</strong>。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">mlc <span class="token parameter variable">--latency_matrix</span><span class="token comment"># 打印本地和跨插槽理想情况下内存延迟的矩阵，请求来自每个socket并且发送到每个空闲socket</span>mlc <span class="token parameter variable">--bandwidth_matrix</span><span class="token comment"># 打印本地和跨插槽内存带宽的矩阵</span>mlc <span class="token parameter variable">--peak_injection_bandwidth</span><span class="token comment"># 针对所有本地访问的各种读写比率打印峰值内存带宽（核心以尽可能快的速率生成请求）</span>mlc <span class="token parameter variable">--max_bandwidth</span><span class="token comment"># 针对所有本地访问的各种读写比率打印最大内存带宽（通过自动改变负载注入速率）</span>mlc <span class="token parameter variable">--idle_latency</span><span class="token comment"># 打印平台的空闲内存延迟</span>mlc <span class="token parameter variable">--loaded_latency</span><span class="token comment"># 打印平台加载的内存延迟</span>mlc <span class="token parameter variable">--c2c_latency</span><span class="token comment"># 打印平台的缓存到缓存传输延迟（cache-to-cache transfer latencies ）</span>mlc <span class="token parameter variable">-e</span><span class="token comment"># 不要修改预取器设置</span>mlc <span class="token parameter variable">--memory_bandwidth_scan</span> <span class="token comment"># 打印整个内存中每个 1 GB 地址范围的内存带宽</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>默认命令的解释：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">Intel<span class="token punctuation">(</span>R<span class="token punctuation">)</span> Memory Latency Checker - v3.8*** Unable to modify prefetchers <span class="token punctuation">(</span>try executing <span class="token string">'modprobe msr'</span><span class="token punctuation">)</span>*** So, enabling random access <span class="token keyword">for</span> latency measurementsMeasuring idle latencies <span class="token punctuation">(</span>in ns<span class="token punctuation">)</span><span class="token punctuation">..</span>.<span class="token comment"># 理想情况下内存延迟的矩阵，请求来自每个socket并且发送到每个可用socket。mlc --latency_matrix</span>Numa <span class="token function">node</span>Numa <span class="token function">node</span>     <span class="token number">0</span>     <span class="token number">1</span>     <span class="token number">2</span>     <span class="token number">3</span>       <span class="token number">0</span>  <span class="token number">40.0</span>  <span class="token number">61.5</span> <span class="token number">108.5</span> <span class="token number">139.5</span>       <span class="token number">1</span>  <span class="token number">61.8</span>  <span class="token number">39.8</span> <span class="token number">141.8</span> <span class="token number">108.5</span>Measuring Peak Injection Memory Bandwidths <span class="token keyword">for</span> the systemBandwidths are <span class="token keyword">in</span> MB/sec <span class="token punctuation">(</span><span class="token number">1</span> MB/sec <span class="token operator">=</span> <span class="token number">1,000</span>,000 Bytes/sec<span class="token punctuation">)</span>Using all the threads from each core <span class="token keyword">if</span> Hyper-threading is enabledUsing traffic with the following read-write ratios<span class="token comment"># 针对具有不同读取和写入量的请求（每个核心尽可能快地生成请求）测量峰值注入内存带宽（对本地内存的所有访问）。mlc --peak_injection_bandwidth</span>ALL Reads        <span class="token builtin class-name">:</span><span class="token number">161501.6</span><span class="token number">3</span>:1 Reads-Writes <span class="token builtin class-name">:</span><span class="token number">194996.6</span><span class="token number">2</span>:1 Reads-Writes <span class="token builtin class-name">:</span><span class="token number">211301.9</span><span class="token number">1</span>:1 Reads-Writes <span class="token builtin class-name">:</span><span class="token number">256226.9</span>Stream-triad like:<span class="token number">147516.0</span><span class="token comment"># 源自每个socket并寻址到每个可用socket的请求的内存带宽矩阵。mlc --bandwidth_matrix</span>Measuring Memory Bandwidths between nodes within system Bandwidths are <span class="token keyword">in</span> MB/sec <span class="token punctuation">(</span><span class="token number">1</span> MB/sec <span class="token operator">=</span> <span class="token number">1,000</span>,000 Bytes/sec<span class="token punctuation">)</span>Using all the threads from each core <span class="token keyword">if</span> Hyper-threading is enabledUsing Read-only traffic <span class="token builtin class-name">type</span>Numa <span class="token function">node</span>Numa <span class="token function">node</span>     <span class="token number">0</span>     <span class="token number">1</span>     <span class="token number">2</span>     <span class="token number">3</span>       <span class="token number">0</span><span class="token number">80686.1</span><span class="token number">54665.8</span><span class="token number">25508.8</span> <span class="token number">2529.8</span>       <span class="token number">1</span><span class="token number">54657.7</span><span class="token number">80956.5</span> <span class="token number">2571.2</span><span class="token number">25510.1</span>Measuring Loaded Latencies <span class="token keyword">for</span> the systemUsing all the threads from each core <span class="token keyword">if</span> Hyper-threading is enabledUsing Read-only traffic <span class="token builtin class-name">type</span>InjectLatencyBandwidth<span class="token comment"># 不同带宽点的延迟</span>Delay<span class="token punctuation">(</span>ns<span class="token punctuation">)</span>MB/sec<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span> 00000<span class="token number">455.70</span> <span class="token number">161519.3</span> 00002<span class="token number">458.47</span> <span class="token number">161556.4</span> 00008<span class="token number">454.24</span> <span class="token number">161501.8</span> 00015<span class="token number">448.57</span> <span class="token number">161506.9</span> 00050<span class="token number">405.38</span> <span class="token number">161417.2</span> 00100<span class="token number">396.14</span> <span class="token number">161388.8</span> 00200<span class="token number">135.20</span> <span class="token number">152952.1</span> 00300 <span class="token number">62.77</span> <span class="token number">116188.2</span> 00400 <span class="token number">53.61</span>  <span class="token number">88853.4</span> 00500 <span class="token number">51.12</span>  <span class="token number">71974.5</span> 00700 <span class="token number">48.78</span>  <span class="token number">52310.7</span> 01000 <span class="token number">47.38</span>  <span class="token number">37255.9</span> 01300 <span class="token number">46.68</span>  <span class="token number">29088.7</span> 01700 <span class="token number">45.94</span>  <span class="token number">22646.6</span> 02500 <span class="token number">45.75</span>  <span class="token number">15891.3</span> 03500 <span class="token number">45.14</span>  <span class="token number">11785.4</span> 05000 <span class="token number">44.83</span>   <span class="token number">8691.3</span> 09000 <span class="token number">44.16</span>   <span class="token number">5489.5</span> <span class="token number">20000</span> <span class="token number">42.18</span>   <span class="token number">3338.6</span>  <span class="token comment"># 处理器中缓存之间的延迟。mlc --c2c_latency</span>Measuring cache-to-cache transfer latency <span class="token punctuation">(</span>in ns<span class="token punctuation">)</span><span class="token punctuation">..</span>.Local Socket L2-<span class="token operator">&gt;</span>L2 HIT  latency<span class="token number">35.8</span>Local Socket L2-<span class="token operator">&gt;</span>L2 HITM latency<span class="token number">59.9</span>Remote Socket L2-<span class="token operator">&gt;</span>L2 HITM latency <span class="token punctuation">(</span>data address homed <span class="token keyword">in</span> writer socket<span class="token punctuation">)</span>Reader Numa NodeWriter Numa Node     <span class="token number">0</span>     <span class="token number">1</span>            <span class="token number">0</span>     -  <span class="token number">54.5</span>            <span class="token number">1</span>  <span class="token number">54.7</span>     -Remote Socket L2-<span class="token operator">&gt;</span>L2 HITM latency <span class="token punctuation">(</span>data address homed <span class="token keyword">in</span> reader socket<span class="token punctuation">)</span>Reader Numa NodeWriter Numa Node     <span class="token number">0</span>     <span class="token number">1</span>            <span class="token number">0</span>     -  <span class="token number">53.7</span>            <span class="token number">1</span>  <span class="token number">55.3</span>     -<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="10-使用SystemTap软件跟踪内核函数"><a href="#10-使用SystemTap软件跟踪内核函数" class="headerlink" title="10. 使用SystemTap软件跟踪内核函数"></a>10. 使用SystemTap软件跟踪内核函数</h2><p>衡量 TPH 如何影响性能和延迟的另一种有效方法是跟踪/探测 Linux 内核功能。为此使用<strong>SystemTap</strong>，它是“用于动态检测正在运行的生产Linux内核操作系统的工具”。这个工具有它自己的脚本语言，使用非常灵活。</p><p>第一个函数是<code>__alloc_pages_slowpath</code>。当没有可用于分配的连续内存块时，将执行它。反过来，此函数调用可能导致延迟峰值的昂贵页面压缩和回收逻辑。第二个函数是<code>khugepaged_scan_mm_slot</code>。它由后台“khugepaged”内核线程执行。它扫描巨大的页面并试图将它们折叠成一个。<br>脚本如下：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#! /usr/bin/env stap</span>global start, intervalsprobe <span class="token variable">$1</span> <span class="token punctuation">{</span> start<span class="token punctuation">[</span>tid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> gettimeofday_us<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">}</span>probe <span class="token variable">$1</span>.return<span class="token punctuation">{</span>  t <span class="token operator">=</span> gettimeofday_us<span class="token punctuation">(</span><span class="token punctuation">)</span>  old_t <span class="token operator">=</span> start<span class="token punctuation">[</span>tid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span>old_t<span class="token punctuation">)</span> intervals <span class="token operator">&lt;&lt;&lt;</span> t - old_t  delete start<span class="token punctuation">[</span>tid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">}</span>probe timer.ms<span class="token punctuation">(</span><span class="token variable">$2</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>@count<span class="token punctuation">(</span>intervals<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span>    <span class="token punctuation">{</span>        printf<span class="token punctuation">(</span><span class="token string">"%-25s:<span class="token entity" title="\n">\n</span> min:%dus avg:%dus max:%dus count:%d <span class="token entity" title="\n">\n</span>"</span>, tz_ctime<span class="token punctuation">(</span>gettimeofday_s<span class="token punctuation">(</span><span class="token punctuation">))</span>,             @min<span class="token punctuation">(</span>intervals<span class="token punctuation">)</span>, @avg<span class="token punctuation">(</span>intervals<span class="token punctuation">)</span>, @max<span class="token punctuation">(</span>intervals<span class="token punctuation">)</span>, @count<span class="token punctuation">(</span>intervals<span class="token punctuation">))</span>        print<span class="token punctuation">(</span>@hist_log<span class="token punctuation">(</span>intervals<span class="token punctuation">))</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>__alloc_pages_slowpath</code>函数结果示例</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span>~<span class="token punctuation">]</span><span class="token comment"># ./func_time_stats.stp 'kernel.function("__alloc_pages_slowpath")' 1000</span>Thu Aug <span class="token number">17</span> 09:37:19 <span class="token number">2017</span> CEST: min:0us avg:1us max:23us count:1538value <span class="token operator">|</span>-------------------------------------------------- count    <span class="token number">0</span> <span class="token operator">|</span>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  <span class="token number">549</span>    <span class="token number">1</span> <span class="token operator">|</span>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@  <span class="token number">541</span>    <span class="token number">2</span> <span class="token operator">|</span>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                 <span class="token number">377</span>    <span class="token number">4</span> <span class="token operator">|</span>@@@@                                                <span class="token number">54</span>    <span class="token number">8</span> <span class="token operator">|</span>@                                                   <span class="token number">12</span>   <span class="token number">16</span> <span class="token operator">|</span>                                                     <span class="token number">5</span>   <span class="token number">32</span> <span class="token operator">|</span>                                                     <span class="token number">0</span>   <span class="token number">64</span> <span class="token operator">|</span>                                                     <span class="token number">0</span><span class="token punctuation">..</span>.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://sourceware.org/systemtap/getinvolved.html">SystemTap软件用于收集运行的Linux的信息</a></p><h2 id="Over-View"><a href="#Over-View" class="headerlink" title="Over View"></a>Over View</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cd67339d6b6a477e9587e4c5a3f43707.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/fbbab3ca9a084f5aa0986216daf20f38.png"></p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 虚拟文件系统 </tag>
            
            <tag> proc </tag>
            
            <tag> memoey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>与内存管理相关的进程和文件知识</title>
      <link href="/2023/11/17/Linux%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%90%8E%E5%8F%B0%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/"/>
      <url>/2023/11/17/Linux%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%90%8E%E5%8F%B0%E5%AE%88%E6%8A%A4%E8%BF%9B%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>文件可以看做相对磁盘等存储设备的地址空间。内存设备抽象作做进程地址空间。</p><p>文件可以看做线性字节数组，每字节可以读取写入。（当然是被掩盖了的）</p><p>目录记录文件的位置，目录包括文件或者其他子目录，目录本身也是文件。</p><p>文件系统的重要任务就是分配和索引数据块。</p><p>虚拟文件系统就是为了将各种文件系统统一 然后抽象成一个通用的层 。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/57d7f3138355481fabd774aa88d22ba9.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/fe5bb756987a4b1ca6fcf7fb398991a9.jpeg"></p><hr><p>内核线程和后台守护进程被广泛用在分层内存的一些创新策略上，那么这些创建的一般流程是怎样的？</p><h2 id="内核线程"><a href="#内核线程" class="headerlink" title="内核线程"></a>内核线程</h2><p>内核线程其实是运行在内核地址空间中的进程，没有独立的地址空间，共享内核的地址空间。Linux内核其实没有专门的线程，线程都当作普通的进程。<code>task_struct-&gt;mm == NULL</code></p><p>Linux内核有多个函数接口创建内核线程，<code>kthread_create()</code>新创建的不能运行，还需要手动调用<code>wake_up_process</code>唤醒进程加入就绪队列。<code>kthread_run()</code>创建的马上就能直接运行。</p><p>下面这个模块，每个CPU一个内核线程，输出当前CPU状态，以及当前进程的优先级等信息。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/init.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/kthread.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/delay.h&gt;</span></span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">task_struct</span> <span class="token operator">*</span>tsk<span class="token punctuation">[</span>NR_CPUS<span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">show_reg</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> spsr<span class="token punctuation">,</span> sp<span class="token punctuation">,</span> el<span class="token punctuation">;</span><span class="token keyword">asm</span><span class="token punctuation">(</span><span class="token string">"mrs %0, spsr_el1"</span> <span class="token operator">:</span> <span class="token string">"=r"</span> <span class="token punctuation">(</span>spsr<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token operator">:</span> <span class="token string">"cc"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">asm</span><span class="token punctuation">(</span><span class="token string">"mov %0, sp"</span> <span class="token operator">:</span> <span class="token string">"=r"</span> <span class="token punctuation">(</span>sp<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token operator">:</span> <span class="token string">"cc"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">asm</span><span class="token punctuation">(</span><span class="token string">"mrs %0, CurrentEL"</span> <span class="token operator">:</span> <span class="token string">"=r"</span> <span class="token punctuation">(</span>el<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token operator">:</span> <span class="token string">"cc"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"spsr:0x%x, sp:0x%x, el=%d\n"</span><span class="token punctuation">,</span> spsr<span class="token punctuation">,</span> sp<span class="token punctuation">,</span> el <span class="token operator">&gt;&gt;</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">show_prio</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">struct</span> <span class="token class-name">task_struct</span> <span class="token operator">*</span>task <span class="token operator">=</span> current<span class="token punctuation">;</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"%s pid:%d, nice:%d prio:%d static_prio:%d normal_prio:%d\n"</span><span class="token punctuation">,</span>task<span class="token operator">-&gt;</span>comm<span class="token punctuation">,</span> task<span class="token operator">-&gt;</span>pid<span class="token punctuation">,</span><span class="token function">PRIO_TO_NICE</span><span class="token punctuation">(</span>task<span class="token operator">-&gt;</span>static_prio<span class="token punctuation">)</span><span class="token punctuation">,</span>task<span class="token operator">-&gt;</span>prio<span class="token punctuation">,</span> task<span class="token operator">-&gt;</span>static_prio<span class="token punctuation">,</span>task<span class="token operator">-&gt;</span>normal_prio<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span>s<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">preempt_disable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"%s cpu=%d.\n"</span><span class="token punctuation">,</span> s<span class="token punctuation">,</span> <span class="token function">smp_processor_id</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">preempt_enable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">int</span> <span class="token function">thread_fun</span><span class="token punctuation">(</span><span class="token keyword">void</span> <span class="token operator">*</span>t<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">do</span> <span class="token punctuation">{</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"SLEEP in Thread Function "</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">msleep_interruptible</span><span class="token punctuation">(</span><span class="token number">2000</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"msleep over in Thread Function"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"running"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">show_reg</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">show_prio</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token function">kthread_should_stop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//入口</span><span class="token keyword">static</span> <span class="token keyword">int</span> __init <span class="token function">my_init</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> i<span class="token punctuation">;</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"Loading module"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">for_each_online_cpu</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span>tsk<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token function">kthread_create</span><span class="token punctuation">(</span>thread_fun<span class="token punctuation">,</span> <span class="token constant">NULL</span><span class="token punctuation">,</span> <span class="token string">"kdemo/%d"</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>tsk<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"Failed to generate a kernel thread\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">kthread_bind</span><span class="token punctuation">(</span>tsk<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"About to wake up and run the thread for cpu=%d\n"</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">wake_up_process</span><span class="token punctuation">(</span>tsk<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">"Staring thread for cpu %d"</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"on"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> __exit <span class="token function">my_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> i<span class="token punctuation">;</span><span class="token function">for_each_online_cpu</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">pr_info</span><span class="token punctuation">(</span><span class="token string">" Kill Thread %d"</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">kthread_stop</span><span class="token punctuation">(</span>tsk<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">print_cpu</span><span class="token punctuation">(</span><span class="token string">"Kill was done on "</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token function">module_init</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">module_exit</span><span class="token punctuation">(</span>my_exit<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_AUTHOR</span><span class="token punctuation">(</span><span class="token string">"Ben ShuShu"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_LICENSE</span><span class="token punctuation">(</span><span class="token string">"GPL v2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-Makefile" data-language="Makefile"><code class="language-Makefile">BASEINCLUDE ?= /lib/modules/`uname -r`/buildkthread_test-objs := kthread.oobj-m:=   kthread_test.oall :$(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean:$(MAKE) -C $(BASEINCLUDE) M=$(PWD) clean;rm -f *.ko;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="后台守护进程"><a href="#后台守护进程" class="headerlink" title="后台守护进程"></a>后台守护进程</h2><p>创建一个守护进程，每隔5s查看当前内核的日志中是否有Oops错误。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdlib.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;time.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;fcntl.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;string.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/stat.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;sys/klog.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">FALLBACK_KLOG_BUF_SHIFT</span> <span class="token expression"><span class="token number">17</span>  </span><span class="token comment">/* CONFIG_LOG_BUF_SHIFT in kernel */</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">FALLBACK_KLOG_BUF_LEN</span>   <span class="token expression"><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">&lt;&lt;</span> FALLBACK_KLOG_BUF_SHIFT<span class="token punctuation">)</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_CLOSE</span>         <span class="token expression"><span class="token number">0</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_OPEN</span>          <span class="token expression"><span class="token number">1</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_READ</span>          <span class="token expression"><span class="token number">2</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_READ_ALL</span>      <span class="token expression"><span class="token number">3</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_READ_CLEAR</span>    <span class="token expression"><span class="token number">4</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_CLEAR</span>         <span class="token expression"><span class="token number">5</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_CONSOLE_OFF</span>   <span class="token expression"><span class="token number">6</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_CONSOLE_ON</span>    <span class="token expression"><span class="token number">7</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_CONSOLE_LEVEL</span> <span class="token expression"><span class="token number">8</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_SIZE_UNREAD</span>   <span class="token expression"><span class="token number">9</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">KLOG_SIZE_BUFFER</span>   <span class="token expression"><span class="token number">10</span></span></span><span class="token comment">/* we use 'Linux version' string instead of Oops in this lab */</span><span class="token comment">//#define OOPS_LOG  "Oops" </span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">OOPS_LOG</span>  <span class="token string">"Linux version"</span> </span><span class="token keyword">int</span> <span class="token function">save_kernel_log</span><span class="token punctuation">(</span><span class="token keyword">char</span> <span class="token operator">*</span>buffer<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">char</span> path<span class="token punctuation">[</span><span class="token number">128</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token class-name">time_t</span> t<span class="token punctuation">;</span><span class="token keyword">struct</span> <span class="token class-name">tm</span> <span class="token operator">*</span>tm<span class="token punctuation">;</span><span class="token keyword">int</span> fd<span class="token punctuation">;</span>t <span class="token operator">=</span> <span class="token function">time</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>tm <span class="token operator">=</span> <span class="token function">localtime</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>t<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">snprintf</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token string">"/mnt/%d.%d.%d.%d.%d.%d.log"</span><span class="token punctuation">,</span> tm<span class="token operator">-&gt;</span>tm_year<span class="token operator">+</span><span class="token number">1900</span><span class="token punctuation">,</span>tm<span class="token operator">-&gt;</span>tm_mon<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> tm<span class="token operator">-&gt;</span>tm_mday<span class="token punctuation">,</span> tm<span class="token operator">-&gt;</span>tm_hour<span class="token punctuation">,</span>tm<span class="token operator">-&gt;</span>tm_min<span class="token punctuation">,</span> tm<span class="token operator">-&gt;</span>tm_sec<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"%s\n"</span><span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">;</span>        fd <span class="token operator">=</span> <span class="token function">open</span><span class="token punctuation">(</span>path<span class="token punctuation">,</span> O_WRONLY<span class="token operator">|</span>O_CREAT<span class="token punctuation">,</span> <span class="token number">0644</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span><span class="token punctuation">(</span>fd <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"open error\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span>        <span class="token function">write</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span> buffer<span class="token punctuation">,</span> <span class="token function">strlen</span><span class="token punctuation">(</span>buffer<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token function">close</span><span class="token punctuation">(</span>fd<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">check_kernel_log</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">char</span> <span class="token operator">*</span>buffer<span class="token punctuation">;</span><span class="token keyword">char</span> <span class="token operator">*</span>p<span class="token punctuation">;</span><span class="token class-name">ssize_t</span> klog_size<span class="token punctuation">;</span><span class="token keyword">int</span> ret <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token keyword">int</span> size<span class="token punctuation">;</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"start kernel log\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>klog_size <span class="token operator">=</span> <span class="token function">klogctl</span><span class="token punctuation">(</span>KLOG_SIZE_BUFFER<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>klog_size <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>klog_size <span class="token operator">=</span> FALLBACK_KLOG_BUF_LEN<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"kernel log size: %d\n"</span><span class="token punctuation">,</span> klog_size<span class="token punctuation">)</span><span class="token punctuation">;</span>buffer <span class="token operator">=</span> <span class="token function">malloc</span><span class="token punctuation">(</span>klog_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>buffer<span class="token punctuation">)</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span>size <span class="token operator">=</span> <span class="token function">klogctl</span><span class="token punctuation">(</span>KLOG_READ_ALL<span class="token punctuation">,</span> buffer<span class="token punctuation">,</span> klog_size<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>size <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"klogctl read error\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">goto</span> done<span class="token punctuation">;</span><span class="token punctuation">}</span>buffer<span class="token punctuation">[</span>size<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token char">'\0'</span><span class="token punctuation">;</span><span class="token comment">/* check if oops in klog */</span>p <span class="token operator">=</span> <span class="token function">strstr</span><span class="token punctuation">(</span>buffer<span class="token punctuation">,</span>OOPS_LOG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>p<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"we found '%s' on kernel log\n"</span><span class="token punctuation">,</span> OOPS_LOG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">save_kernel_log</span><span class="token punctuation">(</span>buffer<span class="token punctuation">)</span><span class="token punctuation">;</span>ret <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span> done<span class="token operator">:</span><span class="token function">free</span><span class="token punctuation">(</span>buffer<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">if</span><span class="token punctuation">(</span><span class="token function">daemon</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"daemon error"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">check_kernel_log</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 后台守护进程 </tag>
            
            <tag> 内核线程 </tag>
            
            <tag> 文件系统 </tag>
            
            <tag> 虚拟文件系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Quantitative Approach for Adopting Disaggregated Memory in HPC Systems</title>
      <link href="/2023/11/14/A-Quantitative-Approach-for-Adopting-Disaggregated-Memory-in-HPC-Systems/"/>
      <url>/2023/11/14/A-Quantitative-Approach-for-Adopting-Disaggregated-Memory-in-HPC-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>The International Conference for High Performance Computing, Networking, Storage and Analysis, (SC), 2023</li><li>A Quantitative Approach for Adopting Disaggregated Memory in HPC Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jacob Wahlgren, Gabin Schieffer, Ivy Peng, KTH瑞典皇家理工学院</li><li>Maya Gokhale, 劳伦斯利弗莫尔国家实验室（LLNL, 美国能源部所属国家研究机构）</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h3 id="2-1内存分解"><a href="#2-1内存分解" class="headerlink" title="2.1内存分解"></a>2.1内存分解</h3><p>最近的研究发现，大量节点内存仍未得到利用[24]. 在HPC环境中，资源分配更加粗粒度，因为作业通常不共享节点。加上每个节点的高内存容量，这意味着内存通常是未充分利用的资源。最近对领先设施的研究发现，只有不到15%的作业使用超过75%的节点内存，并且50%的时间使用的内存不足总内存的12%[8,33,36,37]。另一方面，作业不用使用多于配置的特定数量的内存资源，就会OOM。（很少量的作业需要大内存，而且需要大内存的和时间是短短暂的。）</p><p>使用内存分解来提高内存利用率，其想法是通过远程内存池按需提供一部分内存资源。在基于内存池的系统中，每个节点都有固定的本地内存和可变数量的结构连接远程内存，实际上是多层内存系统的一种形式[14]。之前的工作已经探索使用节点本地持久内存（例如英特尔傲腾DC PM）作为第二内存层[12,38]。由于Optane已停产，并且随着缓存一致性互连协议（即Compute Express Link（CXL）3类设备）的最新进展，<span class="label primary">内存池成为实现第二层的新兴选项</span>[44]。</p><p>虽然分解内存是一种多层内存，但它与传统的非均匀内存访问（NUMA）系统不同，后者是对称的，每个NUMA域包含相同的计算和内存资源，优化重点是将计算转移到与访问内存页位于同一域的core上。相反，分层内存系统可能是不对称的，其中某些层无需CPU，并且优化重点是将热页放置在更快的层中[24]。</p><p>通过共享内存池干扰可能会导致一项作业由于其他节点上的作业而导致不可预测的性能下降。例如，性能变化的原因之一是不同节点上不相关的作业竞争计算节点和内存池之间的链路带宽。对多层内存的误解包括内存带宽低于同构内存。事实上，从硬件角度来看，<span class="label primary">添加额外的内存层（通道）可以增加聚合带宽</span>。另一个误解是共享内存池应用程序性能总是会下降。分布式内存HPC应用程序可以选择通过扩展到更多计算节点来最大限度地减少对池化内存层的暴露。</p><div class="note info">其实有个大前提，HPC本质还是计算密集型的，但是由于计算规模变大对内存需求也就大了，于是会在大内存里讨论HPC，而使用的很多方法是偏向于以前HPC的比如数据结构、对象级粒度、Roofline，这些笔者感觉在纯做内存那条线好像考虑的比较少，而涉及到HPC就讨论非常频繁了。</div><h3 id="2-2-Roofline模型"><a href="#2-2-Roofline模型" class="headerlink" title="2.2 Roofline模型"></a>2.2 Roofline模型</h3><p>Roofline模型[51]被提出来对具有算术强度𝐼的程序可达到的峰值性能𝑃进行建模。X轴为算术强度𝐼（Arithmetic Intensity）定义为从主内存传输的每个字节的浮点运算数量<strong>Flops/Bytes</strong>, 这个值越高，代表运算的需求越高（每比特浮点运算）。Y轴是<strong>Flop/s</strong> = Min（peak Flops峰值计算能力𝐹， 𝐼 * peak bandwidth峰值内存带宽𝐵（B/s））</p><p>Roofline模型可以进一步扩展以模拟其他限制因素。例如，可以调整存储器带宽的斜率以包括存储器干扰的影响。图5中的虚线表示如果向基准系统添加额外的内存层，则总内存带宽会增加。为了进行细粒度测量，我们将分析器设置为量化每秒浮点数的吞吐量和每秒字节数的内存访问量。图5报告了表2中评估的应用程序中每个阶段的测量算术强度和获得的吞吐量。如图5所示，每个应用程序通常至少包含两个阶段，其中第一阶段（表示为p1）表示初始化阶段。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/7429c10833e141138561ba6fdbe89610.png"></p><p>这个曲线其实反应的是当前的短板。先分析线条</p><ul><li>在曲线上升期，peak Flops计算能力𝐹是大的值，不是短板。只要程序每比特含的计算量大，带宽峰值一定，计算吞吐量就能持续增加。</li><li>曲线平缓后说明计算能力𝐹成为了短板，应该考虑将任务分给其他计算节点之类的操作了。</li><li>对于虚线，如果程序包含的每比特浮点计算量一定，增加带宽的操作也可以提高程序吞吐量。</li><li>曲线平缓后会是一个固定的值，再增加吞吐量或者程序包含的每比特浮点计算量都无济于事。</li></ul><p>对于图中的点，workload的特征表现如下：</p><ul><li>前提是CPU的计算资源是没办法池化的，但是内存可以池化，你就可选择更多的通道，增加聚合带宽。</li><li>在斜线范围内，对于没有靠近屋檐的程序，是由于没有获得足够的带宽。因为这时计算能力并不是整个系统的瓶颈，如果能达到峰值的带宽，吞吐量还能提升。</li><li>对于横线范围下的程序，计算能力成为瓶颈后且不想加入新的计算节点，那最好的情况是min里两个者相等，这是这个程序所需带宽最大值了。<br>（但值得注意的是，这只是一个理想模型，他还有很多其他因素没有考虑进去，比如HPC在同步点的通信开销之类的）</li></ul><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>如何在HPC环境中使用内存池。云环境的内存池大多和虚拟化有关HPC不需要，且云提供商的目的是服务质量和成本，HPC计算性能是首要目标。这使得使用内存池也得满足</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>一些现有的工作侧重于了解性能下降或优化分解内存上的数据放置[18,24,30,39,48]。然而，本文并没有提供数据放置的其他优化工作，而是构建了一个<span class="label primary">通用框架来研究应用程序的内存行为并确定优化优先级和限制</span>，以及影响部署选项。移植工作量低，因为架构变化导致程序重写之类的开销。性能变化低。计算设施收益：可持续性、组件单独升级、低成本。</p><h2 id="5-实验平台"><a href="#5-实验平台" class="headerlink" title="5. 实验平台"></a>5. 实验平台</h2><p>我们为图2中的内存池配置了一个仿真平台。该方法使用双插槽系统中的插槽互连来仿真一致的分解内存系统。[24,30,53]中使用了类似的方法。默认的首次接触页面分配策略将分配分配到本地NUMA节点上，直到满为止，然后再溢出到其他NUMA节点（远程内存）。</p><p>该仿真平台使用具有两个插槽和每个插槽一个NUMA节点的Intel Xeon测试台。如图3所示，一个socket代表一个计算节点，另一个socket上的内存代表一个内存池。第二个socket上的核心未使用。它们之间的UPI互连代表远程链路。Socket内带宽为73GB/s，延迟为111ns，socket间带宽为34GB/s，延迟为202ns。 Linux内核版本是5.14。为了获得一致的结果，我们禁用NUMA平衡和透明大页面（THP）。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2385095697584c9383e915b8ec52e02c.png" alt="图3"></p><h2 id="6-围绕该问题作者如何构建解决思路"><a href="#6-围绕该问题作者如何构建解决思路" class="headerlink" title="6. 围绕该问题作者如何构建解决思路"></a>6. 围绕该问题作者如何构建解决思路</h2><p>我们采用了三层自上而下的方法:</p><ul><li>在第一层，我们确定了应用程序对内存系统的内在要求，与确切的系统架构无关。这些内在要求包括算术强度、内存容量使用、带宽使用和访问模式。</li><li>第二层将应用需求扩展到一般多层内存系统，并利用内存roofline模型。 </li><li>最后一个层次是在图2所示的高性能计算架构上，研究下层由内存池支持时的特定内存干扰挑战。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e77b0cd0f54b49b3aa03a5be7577ff32.png" alt="图2"><br>图4展示了实验工作流程，其中每个执行层次都收集了剖析信息，这些信息可分别进行可视化显示。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/42372e86afbe44e3858ce05ec18a507d.png" alt="A step-by-step overview of the profiling workflow with example commands."><br>使用硬件性能计数器捕获需要的指标，比如内存访问。如果架构改变或者多层存储器改变，只需要改变选定的性能事件。</p><h3 id="6-1工作负载特征-容量带宽弹性"><a href="#6-1工作负载特征-容量带宽弹性" class="headerlink" title="6.1工作负载特征-容量带宽弹性"></a>6.1工作负载特征-容量带宽弹性</h3><p>感兴趣的是<span class="label primary">识别并抽象出应用程序在不同内存配置的系统上执行时仍能保持的特性</span>。表2总结了本文使用的评估应用程序和输入问题列表。identifying and abstracting  the properties that persist even when the application is executed on  systems with different memory configurations.</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e1a92a68adf249a5bf48e61fb000b334.png" alt="用于分析的workloads"></p><p>在部署高性能计算应用程序时，容量和带宽是与内存相关的两个主要考虑因素。在典型的决策流程中，用户需要估算作业的总内存占用和每个节点的峰值内存使用量，然后将其与每个计算节点的内存容量进行比较，以确定所需的最小节点数。</p><p><span class="label info">当内存带宽成为限制因素时，用户可能会决定进一步增加节点数量，以获得更高的总内存带宽</span>。我们提出了内存带宽-容量缩放曲线（如图6所示），以帮助用户<span class="label primary">量化容量和带宽使用之间的关系</span>。该曲线是利用我们的剖析器中的内存访问采样建立的。在按页面测量并汇总内存访问次数后，我们将页面按访问次数降序排序。然后，我们建立<strong>访问量的累积分布</strong>，与内存占用百分比进行比较。每个应用程序都使用3个大小变化约为2倍的输入问题进行测试。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/fc92947cae88430d82c45ee935b3a692.png"><br>这种缩放曲线分析是一个简洁的可视化工具，可以帮助HPC用户统一内存需求的两个维度，即带宽和容量，并估计更大规模问题的需求。</p><ul><li>HPL和Hypre在整个内存占用空间中表现出相对均匀的内存访问。（随着被访问的内存逐渐增加，访问量也是平缓增加的）这种特性与传统的数值代码一致，几乎所有的主存对象都被访问来进行计算。</li><li>BFS和XSBench在执行过程中只有一小部分内存占用被主动访问。我们检查源代码发现BFS分配了大的图结构，而在执行过程中只会访问邻接数据。类似地，XSBench分配大型网格结构，而仅查找采样点。</li><li>NekRS、HPL、Hypre和XSBench这四个应用程序对于1×、2×和4×输入问题具有重叠的缩放曲线，说明输入的问题规模不影响他们的表现模式。</li><li>当输入问题增加时，BFS的缩放曲线向x轴左侧移动，表明访问分布变得更加倾斜，具有更多访问的页面百分比更少。</li><li>SuperLU的变化表明热点页的分布从偏斜分布转向更均匀的分布。</li></ul><p>（事实上这个和带宽的关系是指访问量，还有些牵强啊。）</p><h3 id="6-2工作负载特征-预取"><a href="#6-2工作负载特征-预取" class="headerlink" title="6.2工作负载特征-预取"></a>6.2工作负载特征-预取</h3><p>预取的适用性是由应用程序的算法和访问模式决定的。应用程序在不同的系统上执行，预取的适用性也得以保留。在分解内存的情况下，预取掩盖更高的访问延迟。这里主要讨论对于HPC来说是否开启预取，因为在同样内存分解的云环境中预取是损害性能的。</p><p>量化预取的适用性使用之前提出的两个指标，即准确率和覆盖率[29]. 1.准确度衡量的是程序实际访问的预取缓存行的比率。2.覆盖率衡量在按需访问之前预取的缓存行访问的比率。</p><p>分析基于L2高速缓存，core硬件预检器位于L2高速缓存，LLC是独占的L3高速缓存。作者的测量结果就说明预取有好处，但是会增加流量，不过仍然有好处。</p><h3 id="6-3多层内存-分层访问"><a href="#6-3多层内存-分层访问" class="headerlink" title="6.3多层内存-分层访问"></a>6.3多层内存-分层访问</h3><p>量化每层内存容量、带宽和访问的比率。柱状图图是远端访问的内存占总内存访问的百分比。均匀的虚线是远端和近端容量占比。不均匀的虚线是一个固定值，因为带宽峰值和硬件相关， $R_{BW}^{remort}$ 是内存访问瓶颈的转折点。</p><ul><li>其中 $R_{access}^{remort}$ 值低于 $R_{BW}^{remort}$ 表示内存访问瓶颈受到快速层带宽的限制。理想情况应该是两层的这两个比是一样的，才能充分利用流量。</li><li>如果 $R_{access}^{remort}$ 值高于 $R_{BW}^{remort}$ ，则表示对较慢层的内存访问过多，导致较慢层成为内存限制访问性能。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/59098ca6575a41439361eb768ae89495.png"></p><p>通过利用分析器中的两个参考点和内存访问指标，可以识别多层内存的优化机会并确定优先级。</p><ul><li>图9a两条参考线很接近，大多数应用程序（HPL和XS Bench除外）的远程内存访问比率接近两条优化参考线，表明优化空间很小，用户不应该花费精力进行优化数据放置。</li><li>相比之下，图9c提供了两条参考线的大量优化机会。例如，根据容量比，HPL、NekRS和BFS都表现出高于𝑅𝐶𝑎𝑝（以红色虚线表示）之上的过度访问。如果热度分布均匀应该是更按照容量比来访问的，这表明在PM的那部分更热。</li><li>图9c几乎所有应用程序的第二阶段 (p2) 都大大高于带宽比 (𝑅𝐵𝑊) 参考值。（这也可以理解，首先你远端层本身访问就多了，再加上你带宽本来比别人低，所以在这个指标上偏离的更加遥远了）虽然这表明还有很大的优化空间，但也可能意味着内存层的设计不平衡。</li><li>XSBench在所有应用程序中脱颖而出，因为所有配置中的远程访问比率都很低（低于6%）。这意味着远程存储器的额外带宽没有被利用。然而，考虑到XSBench的预取覆盖率&lt;1%，应用程序对内存延迟的增加高度敏感。因此，通过最小化远程内存暴露来减少延迟比增加聚合内存带宽更重要。（作者这个解释笔者不太认可啊。前面说XSBench是大型网络结构只有一小部分被访问，那说明那部分都被分配去本地了呀）</li></ul><h3 id="6-4多层内存-访问的阶段性变化"><a href="#6-4多层内存-访问的阶段性变化" class="headerlink" title="6.4多层内存-访问的阶段性变化"></a>6.4多层内存-访问的阶段性变化</h3><p>高性能计算应用通常由多个阶段组成。如图5和图9所示，应用程序中各阶段的运算强度和内存访问模式可能截然不同。一个core的数据放置方案可能会损害其他core。这种全局优化是一个NP完全背包问题。 </p><ul><li>静态解决方案利用离线剖析来修改分配位置，将内存变量直接放置在合适的内存层上。需要大量的移植工作，通常无法适应新的输入问题或系统架构。</li><li>动态解决方案则通过运行时检测访问模式，将性能关键的内存页迁移到快速层。方法对用户透明。但是，高性能计算终端用户通常无法接受性能变化和不确定性能。此外，在高端HPC硬件上，应用程序中的各个阶段通常时间很短，而运行时解决方案需要时间来适应。</li></ul><h3 id="6-5对内存池的干扰-干扰敏感度"><a href="#6-5对内存池的干扰-干扰敏感度" class="headerlink" title="6.5对内存池的干扰-干扰敏感度"></a>6.5对内存池的干扰-干扰敏感度</h3><p>由于多个节点可能共享内存池，因此一个独特的挑战是来自其他节点上共同运行的作业的内存干扰。我们使用LBench来量化表2中的应用程序的内存干扰的影响，主要用于注入链路的拥塞。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/dcfdd670c5e442c5aaa9ca7de6c174d0.png"></p><p>从这里来看多多少少都会有影响。应用程序对内存池内存干扰的敏感度由其远程内存访问引起，并与其运算强度成反比。应用程序用户需要量化应用程序在目标分解内存系统上对内存干扰的敏感度，以确定其在系统上的部署配置。<strong>对于内存干扰敏感度较低的应用，用户可以配置作业部署，充分利用内存池的更多容量，减少支持作业所需的计算节点数量。对于高度敏感的应用，用户可以选择部署具有更多计算节点的作业，以减少远程内存访问，甚至完全避免远程内存，从而满足其性能要求。</strong>我们的量化方法为高性能计算终端用户提供了一个做出明智决策和权衡的工具，提高了用户对新系统架构的信心。</p><h3 id="6-5对内存池的干扰-应用程序造成的内存干扰"><a href="#6-5对内存池的干扰-应用程序造成的内存干扰" class="headerlink" title="6.5对内存池的干扰-应用程序造成的内存干扰"></a>6.5对内存池的干扰-应用程序造成的内存干扰</h3><p>我们通过与闲置系统相比，通过与LBENCE共同运行应用程序并计算相对放缓的应用程序来量化应用程序系统上应用程序的干扰系数。图11的右面板报告了50％内存池设置的应用程序的测量干扰系数。结果表明，NEKR和Hypre可以将最大的内存干扰引入其他共同运行的作业，而HPL和XSBench引入了最低的干扰。图11中的传播突出了由工作量引起的干扰系数的差异。例如，在HYPRE中，计算阶段会导致高干扰系数，而初始化阶段会导致低干扰系数。</p><h2 id="7-具体实现方式"><a href="#7-具体实现方式" class="headerlink" title="7. 具体实现方式"></a>7. 具体实现方式</h2><p>第一层级的分析中，使用硬件计数器收集每秒浮点数计算的次数、加载的字节数<code>OFFCORE_RESPONSE:L3_MISS</code>，使用Roofline模型计算。内存使用量通过<code>procfs</code>中的<code>numa_maps</code>文件采样进行测量，在用户态来做就相当于<code>cat /proc/[pid]/numa_maps</code>进程的NUMA内存策略和分配的信息。内存访问模式1.基于精确事件的采样（PEBS）可以记录缺失的虚拟地址。2.硬件计数器和预取相关的部分。</p><p>第二层的分析中考虑1.远程容量比率（远程容量/总容量）作者意思是<code>numa_maps</code>文件也有2.远程访问比率（访问下一层内存数/总访问数）访问数是通过非core事件<code>LOCAL_DRAM</code>和<code>REMOTE_DRAM</code>采样得到的。</p><p>第三层干扰敏感度决定了由于远程内存干扰而导致的性能下降，而引起的干扰决定了应用程序对其他应用程序造成的干扰程度。使用Intel PCM中的UPI计数器sktXtraffic测量系统级别的注入流量。<span class="github-emoji"><span>👍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 开发了一个名为LBench的基准测试，用于注入和量化内存池链接上的干扰。该基准测试在内存池上分配一个数组，并在该数组上执行类似于 Empirical Roofline Toolkit[51]的简单数值内核。我们将干扰级别（表示为𝐿𝑜𝐼）定义为生成的链路流量与峰值链路流量相比的百分比。𝐿𝑜𝐼通过改变内核中每个数组元素的浮点运算数量来配置。</p><p>讨论预取收益时使用的硬件计数器：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d9a1181add3d47e4b7a0e27b6f41fddb.png"></p><h2 id="8-从结果看，作者如何有力证明他解决了问题"><a href="#8-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="8. 从结果看，作者如何有力证明他解决了问题"></a>8. 从结果看，作者如何有力证明他解决了问题</h2><p>在两个案例研究中，我们表明我们的方法的结果可以指导应用程序级的数据放置优化，减少50%的远程访问并将BFS的性能提高13%，并指导系统级调度以减少同一地点工作的绩效差异。</p><h2 id="9-缺陷和改进思路"><a href="#9-缺陷和改进思路" class="headerlink" title="9. 缺陷和改进思路"></a>9. 缺陷和改进思路</h2><p>很难评，最后案例分析的意思是一边跑代码，一边调参数？</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> Memory Pool </tag>
            
            <tag> Disaggregated Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Memtis: Efficient Memory Tiering with Dynamic Page Classification and Page Size Determination</title>
      <link href="/2023/11/10/Memtis/"/>
      <url>/2023/11/10/Memtis/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ACM Symposium on Operating Systems Principles, (SOSP), 2023</li><li>Memtis: Efficient Memory Tiering with Dynamic Page Classification and Page Size Determination</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>ATaehyung Lee, Young Ik Eom,（韩国）成均馆大学(SKKU)</li><li>Sumit Kumar Monga,弗吉尼亚理工大学（六所提供预备役军官训练团项目的美国高等军事院校之一）</li><li>Changwoo Min,Igalia公司（Open Source Consultancy and Development）</li></ul><h2 id="2-其他学者解决这个问题的思路和缺陷"><a href="#2-其他学者解决这个问题的思路和缺陷" class="headerlink" title="2. 其他学者解决这个问题的思路和缺陷"></a>2. 其他学者解决这个问题的思路和缺陷</h2><p>以前做分层内存主要考虑2点：1.怎么样去监控目前有哪些页面是热页面，因为热页面会随着时间而产生改变，同时局部性也不一定有效。2.什么时候什么办法将热/冷页面迁移到它该在的地方。</p><p>以前大家的方法基本分成以下3种来监控热页面：</p><ul><li>缺页异常时将页面进行迁移或者收集数据，但是缺页异常在程序执行的关键路径上，此时再去执行其他的操作很容易产生更高的延迟。</li><li>基于页表的页面扫描，在虚拟地址和物理地址转换的过程中会使用页表，页表上有一个A位的访问位，每次页面被访问后都会置位。通过定期扫描页表，扫描后再清空置位来搜集页面访问信息。但是如下图所示，你对页表的扫描间隔、扫描的采样粒度会影响CPU开销以及准确度。</li><li>基于硬件的采样，不同设备都会有硬件计数器PMU以及PEBS，这些采样时可以记录物理地址和pid等信息，也会随着采样粒度更小而开销更大。除此之外，如果硬件设备更换,它的性能监控计数器（PMC）的寄存器地址可能会有变化。这时需要重新查看新硬件的编程手册,确定PMC寄存器的新地址,然后修改软件程序中的寄存器访问代码,将其映射到新的地址上。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cad3789c063a409294252b6a18309f12.png"></li></ul><p>页面放置也可以大致分为3种类型：</p><ul><li>最近访问的页面，这依赖于时间局部性，但是它往往被记录的时间不够就不能了解这个页面很长一段时间的访问历史。</li><li>想要捕获页面的访问历史，目前的方法还比较局限，大家会设计位图或者一定的数据结构去记录，或者干脆是改进LRU.</li><li>还有的方法通过设置一个阈值来表示页面到底是冷的或者是热的，这就会容易出现如下图的情况，不同工作负载运行在上面，但是他们在不同时刻你定义的热要么多于DRAM（我们高性能但有限的快速内存，要么远远少于DRAM），这使得很多不同程度冷的页面也混合到了快速内存层，这就产生了无效的页面分类。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d46af68f858d4fdc95a3c8d81589c840.png"></li></ul><p>另外一点是地址转换的开销，当内存变得很大之后，TLB很小缓存的地址转换的条目相对覆盖率就低了。转换开销主要是由于TLB未命中时缓慢的page table walk造成的。TLB的命中速度很快，但page table walk可能需要四次内存访问才能查询分层页表。较新的处理器由于采用了更深的页表结构，需要多达五次内存访问。大页面通过两种方式减少转换开销。1.由于大页面的单个条目映射了更大的地址范围，因此可以通过增加TLB覆盖率来降低TLB未命中的频率。2.通过减少需要访问的页表级数，加快单个walk速度。<br>但是大家在使用大页面的时候往往一个大页面只有很小一部分小页面被访问，迁移大页面又必须承受昂贵的开销。比如下图横坐标是0到511，每个点代表一个大页面，（a）右上角的点表示一个大页中机会所有的小页面都被访问到了，同时这个大页总的被访问次数也很大。最坏的情况是（b）的右上角，那些大页面只有很少的小页面被访问，同时整体访问频率又很高，不如放它们做小页面单飞更好。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4d169f4812364c309dbd3b1a82623bb9.png"></p><p><span class="label danger">事实上笔者自己在测这个地址转换开销的时候，并没有发现会大到哪里去，也可能是我打开方式不对</span></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>The biggest limitation of prior systems （tiered memory systems） is their inability to effectively classify page hotness across diverse memory configurations and workloads. </p><h2 id="4-围绕该问题作者如何构建解决思路"><a href="#4-围绕该问题作者如何构建解决思路" class="headerlink" title="4. 围绕该问题作者如何构建解决思路"></a>4. 围绕该问题作者如何构建解决思路</h2><p>先放一个总体设计图，还是那几步，采样页面信息、分析、然后迁移。多了对于大页面的分析，分析大页面被拆分的好处。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/73f7df30c4e14cd18ace9258ab6aed35.png"></p><p>PEBS采样的好处就是可以直接区分目前的地址是被当作小页还是大页了。同时这里硬件采样只采样了两个指令（事件），就还好一般不会找不到映射。这个后台线程根据单核CPU性能的3%控制采样的间隔。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a6823d64ee6142a493598c4e8eabc25c.png"></p><p>为了平衡小页和大页记录热度的不公平（毕竟大页大一点被访问的概率也大一些），作者设计了热度因子。主要笔者觉得非常棒的点在于这种zipf分布的直方图，符合访问到热度分布，然后又将页面热度做了很多分层，适合不同的快速层内存容量。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/54629799caed455cb4bd9a7155ca571d.png"></p><p>设计冷热阈值是根据系统目前DRAM的含量来做的，从值最大的bin开始加，一直到装满DRAM则前一个bin就是热阈值，后面就是温阈值，由于温阈值只是作为热页面候选页，所以冷阈值就直接-1得到，只给温页面一个bin。<span class="label danger">不太理解为啥只给一个bin，这显得这个阈值一点也不动态，其实可能根本不需要这个T阈值，应该可以做出更灵活的方法</span>同时如果热的bins加上最后一个bin马上就满了，那么选择不要最后那个bin了，给页面动态迁移留出空间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9ddb01fa0d014f06be208f1e23e204e5.png"></p><p>笔者以前也想过这个问题，我收集了一批又一批数据，可是现有的热度会变啊，我以前的数据怎么办呢？这里作者将他们以到后一个bin做变冷的处理，很简单机智。同时页面迁移交给采样线程和迁移线程来做，做的时机就是采样和冷处理后。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/7cdea7dfced74152bad98f62799c9b32.png"></p><p>最后是设计了大页面的拆分，绿色部分是收益，我们可以从箭头看到，白色直方图统计大页和小页，灰色直方图只统计小页，那么之间的差值就是如果他不是大页，那么热的bin还会增加多少。差值越大说明越不该用大页。于是设计了一个拆分大页面的阈值。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c79d262302d14c8892e6a1dcc44089b8.png"></p><h2 id="5-从结果看，作者如何有力证明他解决了问题"><a href="#5-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="5. 从结果看，作者如何有力证明他解决了问题"></a>5. 从结果看，作者如何有力证明他解决了问题</h2><p>作者实验环境：</p><ul><li>DRAM + NVM （Optane DCPMM, load: 300ns）</li><li>DRAM + emulated CXL memory （cross NUMA DRAM with increased latency, load: 177ns）</li><li>A single socket for our evaluations to avoid NUMA effects</li><li>The ratio of fast to capacity tier memory size as 1:2, 1:8, and 1:16（In the 1:2 configuration, the fast tier size is set to 33% （1/3） of the resident set size （RSS）     while in the 1:16 configuration, it is reduced to 5.9%（1/17）.）</li></ul><p>性能评估图显示好于7个分层内存，他们大多都发表于A类。</p><h2 id="6-缺陷和改进思路"><a href="#6-缺陷和改进思路" class="headerlink" title="6. 缺陷和改进思路"></a>6. 缺陷和改进思路</h2><p>在分层中使用混合页面的工作比较少，主要在于大页面带来的内存膨胀、内存碎片等问题，解决这些问题可能比使用大页面节省的开销更多。这篇工作只讨论拆分大页面，没有启用大页面的设计。同时在使用大页面的评估方面，还有好多前人的工作没讨论到。</p><p>采样时CPU的消耗、直方图的数目、计算阈值时的α、直方图的降温周期、页面拆分的参数β，对于参数敏感性的评估不充分。</p><p>作者好像没说清楚他每个数据是怎么存的？就是怎么做到知道哪个页面当前在哪个bin的，因为你迁移的时候需要用到，你拆分大页面的时候也需要用到。</p><h2 id="7-创新点"><a href="#7-创新点" class="headerlink" title="7. 创新点"></a>7. 创新点</h2><p>对现有的分层内存系统进行了比较全面的分析和总结。</p><p>Memtis的设计解决了作者发现的一个问题，即目前热页面采样不准确开销大、迁移并没有完全发挥DRAM效率。在不考虑大页面的情况下，Memtis相比于其他工作是有明显的优势的。</p><h2 id="8-积累"><a href="#8-积累" class="headerlink" title="8. 积累"></a>8. 积累</h2><ul><li>性能评估那张图可以多看看，和自己的工作看有没有出入。</li><li>性能评估分类比较完整的，workload可以借鉴借鉴。</li><li><a href="https://github.com/cosmoss-jigu/memtis/tree/main/memtis-userspace#tips-for-setting-other-systems">开源</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux内核字符设备驱动</title>
      <link href="/2023/11/10/Linux%E5%86%85%E6%A0%B8%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8/"/>
      <url>/2023/11/10/Linux%E5%86%85%E6%A0%B8%E8%AE%BE%E5%A4%87%E9%A9%B1%E5%8A%A8/</url>
      
        <content type="html"><![CDATA[<p>根据设备的共性大致分为3类</p><ul><li>字符设备，以字节为单位的I/O传输，比如鼠标、键盘、触摸屏（CXL.mem也是以字节为单位传输，但是不是字符数据流）。</li><li>块设备，以块为传输单位，比如SSD、磁盘。</li><li>网络设备，<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/27aa9de078ec4377aa2cde3c33bce0ba.jpeg" alt="Linux驱动设备"></li></ul><h3 id="字符设备的抽象"><a href="#字符设备的抽象" class="headerlink" title="字符设备的抽象"></a>字符设备的抽象</h3><p>Linux如何抽象和描述这些设备的呢？这里主要讨论字符设备<code>include\linux\cdev.h</code>：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">struct</span> <span class="token class-name">cdev</span> <span class="token punctuation">{</span><span class="token keyword">struct</span> <span class="token class-name">kobject</span> kobj<span class="token punctuation">;</span>  <span class="token comment">//该驱动所属的kobject，可以通过sysfs接口将其挂载到所需要的sysfs节点，注意需要驱动开发人员手动去挂载，本身字符驱动不会进行挂载只会对kobj初始化，需要相应具体设备驱动进行挂载。</span><span class="token keyword">struct</span> <span class="token class-name">module</span> <span class="token operator">*</span>owner<span class="token punctuation">;</span>  <span class="token comment">//字符设备驱动所在的内核模块对象指针。</span><span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">file_operations</span> <span class="token operator">*</span>ops<span class="token punctuation">;</span> <span class="token comment">//文件操作 openc、read、write等，和应用程序交互的枢纽。</span><span class="token keyword">struct</span> <span class="token class-name">list_head</span> list<span class="token punctuation">;</span>  <span class="token comment">//用于将字符设备串成链表。</span><span class="token class-name">dev_t</span> dev<span class="token punctuation">;</span>  <span class="token comment">//设备号（主设备和次设备组成）</span><span class="token keyword">unsigned</span> <span class="token keyword">int</span> count<span class="token punctuation">;</span>  <span class="token comment">//同属主设备号的次设备号个数</span><span class="token punctuation">}</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以下是一个简单字符设备demo只包含字符驱动的框架：</p><h3 id="内核中创建字符驱动"><a href="#内核中创建字符驱动" class="headerlink" title="内核中创建字符驱动"></a>内核中创建字符驱动</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/module.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/fs.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/uaccess.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/init.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;linux/cdev.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">DEMO_NAME</span> <span class="token string">"my_demo_dev"</span>  <span class="token comment">//设备名</span></span><span class="token keyword">static</span> <span class="token class-name">dev_t</span> dev<span class="token punctuation">;</span>  <span class="token comment">//设备号</span><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">cdev</span> <span class="token operator">*</span>demo_cdev<span class="token punctuation">;</span>  <span class="token comment">//静态全局变量的方式创建字符设备</span><span class="token keyword">static</span> <span class="token keyword">signed</span> count <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span><span class="token comment">//打开设备</span><span class="token keyword">static</span> <span class="token keyword">int</span> <span class="token function">demodrv_open</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">inode</span> <span class="token operator">*</span>inode<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> major <span class="token operator">=</span> <span class="token function">MAJOR</span><span class="token punctuation">(</span>inode<span class="token operator">-&gt;</span>i_rdev<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">int</span> minor <span class="token operator">=</span> <span class="token function">MINOR</span><span class="token punctuation">(</span>inode<span class="token operator">-&gt;</span>i_rdev<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"%s: major=%d, minor=%d\n"</span><span class="token punctuation">,</span> <span class="token constant">__func__</span><span class="token punctuation">,</span> major<span class="token punctuation">,</span> minor<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">// 释放设备</span><span class="token keyword">static</span> <span class="token keyword">int</span> <span class="token function">demodrv_release</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">inode</span> <span class="token operator">*</span>inode<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//读取设备</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span><span class="token function">demodrv_read</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> lbuf<span class="token punctuation">,</span> <span class="token class-name">loff_t</span> <span class="token operator">*</span>ppos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"%s enter\n"</span><span class="token punctuation">,</span> <span class="token constant">__func__</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//写入设备</span><span class="token keyword">static</span> <span class="token class-name">ssize_t</span><span class="token function">demodrv_write</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">file</span> <span class="token operator">*</span>file<span class="token punctuation">,</span> <span class="token keyword">const</span> <span class="token keyword">char</span> __user <span class="token operator">*</span>buf<span class="token punctuation">,</span> <span class="token class-name">size_t</span> count<span class="token punctuation">,</span> <span class="token class-name">loff_t</span> <span class="token operator">*</span>f_pos<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"%s enter\n"</span><span class="token punctuation">,</span> <span class="token constant">__func__</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//定义操作集合</span><span class="token keyword">static</span> <span class="token keyword">const</span> <span class="token keyword">struct</span> <span class="token class-name">file_operations</span> demodrv_fops <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">.</span>owner <span class="token operator">=</span> THIS_MODULE<span class="token punctuation">,</span><span class="token punctuation">.</span>open <span class="token operator">=</span> demodrv_open<span class="token punctuation">,</span><span class="token punctuation">.</span>release <span class="token operator">=</span> demodrv_release<span class="token punctuation">,</span><span class="token punctuation">.</span>read <span class="token operator">=</span> demodrv_read<span class="token punctuation">,</span><span class="token punctuation">.</span>write <span class="token operator">=</span> demodrv_write<span class="token punctuation">}</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">int</span> __init <span class="token function">simple_char_init</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">int</span> ret<span class="token punctuation">;</span>ret <span class="token operator">=</span> <span class="token function">alloc_chrdev_region</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>dev<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> count<span class="token punctuation">,</span> DEMO_NAME<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//自动分配主设备号，避免重复</span><span class="token keyword">if</span> <span class="token punctuation">(</span>ret<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"failed to allocate char device region"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token punctuation">}</span>demo_cdev <span class="token operator">=</span> <span class="token function">cdev_alloc</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//动态创建字符设备</span><span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>demo_cdev<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"cdev_alloc failed\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">goto</span> unregister_chrdev<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">cdev_init</span><span class="token punctuation">(</span>demo_cdev<span class="token punctuation">,</span> <span class="token operator">&amp;</span>demodrv_fops<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//初始化cdve结构体，建立设备与驱动操作方法集file_operations之间的链接关系</span>ret <span class="token operator">=</span> <span class="token function">cdev_add</span><span class="token punctuation">(</span>demo_cdev<span class="token punctuation">,</span> dev<span class="token punctuation">,</span> count<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//将字符设备添加到系统（注册字符设备）[字符设备结构体，设备号，多少次设备号]</span><span class="token keyword">if</span> <span class="token punctuation">(</span>ret<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"cdev_add failed\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">goto</span> cdev_fail<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"succeeded register char device: %s\n"</span><span class="token punctuation">,</span> DEMO_NAME<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"Major number = %d, minor number = %d\n"</span><span class="token punctuation">,</span><span class="token function">MAJOR</span><span class="token punctuation">(</span>dev<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">MINOR</span><span class="token punctuation">(</span>dev<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>cdev_fail<span class="token operator">:</span><span class="token function">cdev_del</span><span class="token punctuation">(</span>demo_cdev<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//从系统中删除这个结构体</span>unregister_chrdev<span class="token operator">:</span><span class="token function">unregister_chrdev_region</span><span class="token punctuation">(</span>dev<span class="token punctuation">,</span> count<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//释放主设备号</span><span class="token keyword">return</span> ret<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token keyword">static</span> <span class="token keyword">void</span> __exit <span class="token function">simple_char_exit</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token function">printk</span><span class="token punctuation">(</span><span class="token string">"removing device\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">if</span> <span class="token punctuation">(</span>demo_cdev<span class="token punctuation">)</span><span class="token function">cdev_del</span><span class="token punctuation">(</span>demo_cdev<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">unregister_chrdev_region</span><span class="token punctuation">(</span>dev<span class="token punctuation">,</span> count<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">module_init</span><span class="token punctuation">(</span>simple_char_init<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">module_exit</span><span class="token punctuation">(</span>simple_char_exit<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_AUTHOR</span><span class="token punctuation">(</span><span class="token string">"kylin"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_LICENSE</span><span class="token punctuation">(</span><span class="token string">"GPL v2"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">MODULE_DESCRIPTION</span><span class="token punctuation">(</span><span class="token string">"simpe character device"</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>编译这个内核模块：</p><pre class="line-numbers language-Makefile" data-language="Makefile"><code class="language-Makefile">BASEINCLUDE ?= /lib/modules/`uname -r`/buildmydemo-objs := simple_char.o obj-m:=   mydemo.oall : $(MAKE) -C $(BASEINCLUDE) M=$(PWD) modules;clean:$(MAKE) -C $(BASEINCLUDE) M=$(PWD) clean;rm -f *.ko;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>make</code>编译后生成<code>mydemo.ko</code>, 然后加载这个模块</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> insmod mydemo.ko<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载内核模块后可以在<code>dmesg</code>命令输出看到<code>simple_char_init</code>函数成功初始化的输出。<br><code>/proc/devices</code>可以看到所有的设备名称和设备编号。</p><h3 id="设备节点"><a href="#设备节点" class="headerlink" title="设备节点"></a>设备节点</h3><p>内核模块加载后还需要在<code>/dev/</code>目录下生成对应的节点，249是刚刚加载这个内核模块时分配的主设备号。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">mknod</span> /dev/demo_drv c <span class="token number">249</span> <span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>设备节点属于特殊的文件，也叫设备文件。连接内核空间驱动和用户空间应用程序的桥梁，用户操作硬件设备变成了操作文件。设备节点的生成方式有<code>mknod</code>手动生成，也可以<code>udev</code>机制动态生成。<br><code>ls -l</code>可以查看<code>/dev/</code>目录的情况，其中<code>c</code>表示字符设备，<code>b</code>表示块设备。</p><h3 id="用户空间对字符设备的调用"><a href="#用户空间对字符设备的调用" class="headerlink" title="用户空间对字符设备的调用"></a>用户空间对字符设备的调用</h3><p>内核完成对这个设备加载后，用户空间操作这个字符驱动设备</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;fcntl.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;unistd.h&gt;</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">DEMO_DEV_NAME</span> <span class="token string">"/dev/demo_drv"</span></span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token keyword">char</span> buffer<span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">int</span> fd<span class="token punctuation">;</span>fd <span class="token operator">=</span> <span class="token function">open</span><span class="token punctuation">(</span>DEMO_DEV_NAME<span class="token punctuation">,</span> O_RDONLY<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//打开设备[设备文件名刚刚手动挂载的那个名字，文件打开属性]</span><span class="token keyword">if</span> <span class="token punctuation">(</span>fd <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"open device %s failded\n"</span><span class="token punctuation">,</span> DEMO_DEV_NAME<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">return</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token function">read</span><span class="token punctuation">(</span>fd<span class="token punctuation">,</span> buffer<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//调用读函数</span><span class="token function">close</span><span class="token punctuation">(</span>fd<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token comment">//关闭设备</span><span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这里read的调用是来自之前提到的<code>file_operations</code>，这个方法集结构体是由<code>cdev_init</code>方法和用户空间建立联系的。执行成功返回一个文件描述符（文件句柄）。应用程序的<code>open</code>方法执行时会通过系统调用进入内核空间，内核空间的虚拟文件系统层经过复杂的转换，最后会调用设备驱动的<code>file_operations</code>方法集里的<code>open</code>方法。 </p><h3 id="misc机制"><a href="#misc机制" class="headerlink" title="misc机制"></a>misc机制</h3><p>misc device被称为杂项设备，Linux内核把一些不符合预先确定的字符设备划分为杂项设备，这类设备的主设备号是10，次设备号动态分配，Linux内核使用struct miscdevice数据结构描述这类设备。使用misc机制创建驱动的过程和上面差不多，其中misc的设备文件可以自己建好，和设备名一样（之前手动建的可以换名字）</p><blockquote><p>还有IO阻塞，异步通信、IO复用等，但是和CXL.mem应该关系不大，所以就简单看看，不写笔记了。</p></blockquote><h2 id="Memory-Pooling-With-CXL"><a href="#Memory-Pooling-With-CXL" class="headerlink" title="Memory Pooling With CXL"></a>Memory Pooling With CXL</h2><div class="note primary">- Donghyun Gouk , Miryeong Kwon , and Hanyeoreum Bae, KAIST- Sangwon Lee and Myoungsoo Jung , KAIST and Panmnesia</div> <p>这篇和22年他们发的分解内存相似度极高。程序通过<code>malloc</code>申请内存，这个c语言库函数有个阈值是128k。大于128k调用<code>mmap</code>，小于调用<code>brk</code>（不过不同glibc的这个值可能不太一样）。如果申请的是比较大块的内存（超过 128K）时，会调用<code>mmap</code>在虚拟内存空间中的文件映射与匿名映射区创建出一块<code>VMA</code>内存区域，用<code>struct anon_vma</code>结构表示。在这篇论文里<code>mmap</code>的映射是文件映射。就是说将进程虚拟内存空间中某一段虚拟内存区域与磁盘中某个文件的某段区域进行映射。通过<code>mmap</code>只是给程序的某个进程分配了虚拟内存，还没完。这些虚拟地址和物理地址的关系是怎么建立的？以前通过<code>mmap</code>映射的是磁盘上的一个文件，那么就需要通过参数<code>fd</code>来指定要映射文件的描述符（file descriptor）。现在是去访问一个设备。一般来说内核访问一个设备需要设备驱动程序进行一系列操作包括在<code>/dev</code>目录下创建一个字符设备文件或块设备文件。<code>/dev</code>下的PCIe设备文件抽象了应用程序与底层硬件的交互,通过文件操作的统一接口完成对PCIe设备的访问。而这篇文章里DIRECTCXL驱动程序将基址寄存器和HDM映射到主机的保留系统内存空间中。Host-Managed Device Memory技术让设备内存访问对主机来说就像访问系统内存一样,通过加载/存储指令直接操作。对主机来说以前PM的物理地址只是排在DRAM后面的一段线性地址，那现在又如何看CXL设备的内存？从论文的描述来看CXL内存并不对主机透明，而是需要多少就现成分多少，DIRECTCXL驱动程序就创建多少<code>/dev/xx</code>。这篇文章里CXL内存设备的虚拟地址地址和物理地址的映射更像传统段式内存。如下图展示了程序该怎么使用CXL设备内存的整个流程。无论是NVM还是CXL作为物理内存块，它们首先都是由namespace定义一套内存的抽象（这个过程不是我们讨论的范围）。只是以前PM将namespace通过热插拔设置为node，而现在是设置为如下图的段式内存。另一个问题为什么放在<code>/dev/directcxl</code>之类的地方？因为他实现CXL的方式是基于PCIe的，是个设备。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cd7ecaf067cf4e8b92b034b2f5f6984c.png"></p><p>字符设备图片出处<br><a href="https://jerrysheh.com/post/2720656c.html">https://jerrysheh.com/post/2720656c.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设备驱动 </tag>
            
            <tag> Memory Pool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data tiering in heterogeneous memory systems</title>
      <link href="/2023/11/09/Data-tiering-in-heterogeneous-memory-systems/"/>
      <url>/2023/11/09/Data-tiering-in-heterogeneous-memory-systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>(EuroSys), 2016</li><li>Data tiering in heterogeneous memory systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Subramanya R Dulloor, Amitabha Roy, Narayanan Sundaram, Nadathur Satish, Rajesh Sankaran, Jeff Jackson, Intel Labs</li><li>Subramanya R Dulloor, Karsten Schwan, 佐治亚理工学院</li><li>Zheguang Zhao, 布朗大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>NVM的一个关键使用模型是更便宜的高容量易失性存储器。数据中心运营商一定会问，这种使用NVM取代大部分DRAM内存的模式是否会导致其应用程序大幅放缓？回答这个问题至关重要，因为巨大的性能影响将成为采用此类系统的障碍。</p><p><strong>模拟器的设置：</strong><br>该系统上DRAM的延迟和带宽分别为150ns和40GB/s。NVM技术正在不断发展，并且可能会表现出一系列性能特征，具体取决于技术和控制器设计的细节。因此，我们改变NVM模拟器[39]的参数来反映这种情况，首先将延迟从300ns改变到600ns，同时保持带宽与DRAM相同，然后将带宽从40GB/s改变（与DRAM相同）。）降至5GB/s（DRAM的12.5%），同时将延迟固定在最坏情况下的600ns。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d373fef339cc4bdc8c903861be56f70c.png"><br>从图上来看，横坐标是延迟和带宽，设置的是随着延迟增加，带宽也逐渐增加的情况。</p><p>表明将数据结构放置在特定类型的内存中对性能的影响取决于对其的访问模式。这种依赖性是由于<span class="label danger">不同的内存访问模式（顺序、随机和指针跳转）在现代超标量乱序处理器上具有非常不同的有效延迟</span>。访问应用程序数据结构的频率本身不足以确定这些数据结构的相对“热度”。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/816dca9ab7984868b2173b1e3371476e.png"></p><p>将访问模式分为3种来测量：</p><ul><li>指针跳转会经历最大的停顿延迟，等于内存的实际物理延迟。</li><li>随机访问，乱序执行可以有效隐藏一些内存延迟。</li><li>流式访问，预取器在隐藏内存延迟方面更加有效。<br>这些访问模式的性能的数量级差异验证了我们的观察，即仅访问频率不足以确定数据结构的“热度”。</li></ul><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p><span class="label primary">代表性应用程序进行了全面研究</span>，包括键值存储 (MemC3)、内存数据库 (VoltDB) 和图形分析框架 (GraphMat)（在模拟器上运行）。我们的结论是，确实可以混合使用<span class="label primary">少量快速DRAM和大量较慢的NVM，而不会对应用程序的性能产生成比例的影响</span>。需要注意的是，这个结果只能通过仔细放置数据结构来实现。本文的贡献是设计和实现了一组库和自动工具，使程序员能够以最少的努力实现最佳的数据放置。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>为了克服“仅NVM”系统中的性能下降问题，未来的系统可能会将NVM与少量DRAM结合起来，从而形成真正的异构内存架构。处理此类系统中DRAM和NVM不同特性的一个简单方法是将DRAM视为NVM的缓存，并应用经典的paging技术按需在它们之间动态移动数据。不幸的是，由于多种原因，该解决方案会导致性能不佳。</p><p>首先，它忽略了具有不同访问特性的对象可以位于同一页面上的事实，迫使位于同一位置的对象面临paging系统的相同处理。其次，paging会带来较高的开销[17]，这通常是不可接受的，特别是在直接连接的NVM环境中，数据不需要paging即可访问。事实上，本文考虑的大多数应用程序都明确禁止对其整个内存数据进行paging（例如使用mlock），强调需要比传统分页更复杂的解决方案。</p><blockquote><p>paging是啥意思？应该是指按照4KB来组织页面。因为这边的引用在讨论加入临界于传统存储和内存设备之间的设备后，页面回收的效率将可能没有原来那么好了。</p></blockquote><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d849fa0d1a4b4da19b8e2c7b9a7cc4cd.png" alt="设计总览"></p><p>根据上面的分析，作者认为内存访问的延迟是来自CPU对不同访问模式的延迟。所以数据的放置首先需要识别他们的数据结构，标记数据结构的分配，运行时得到数据结构优先级，然后结束分析转为运行时确定最佳放置位置。（有点奇怪）</p><p>本例中的应用程序使用XMem API分配三个数据结构（树、哈希和缓冲区）。每个数据结构的内存分配均由单独的<code>jemalloc</code>实例管理。应用程序使用的标签代表（T1、T2 和 T3）被分配内部优先级，用于决定放置。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4668cf102ce144f996786f1c04e2c51f.png"></p><p>用PIN工具记录访问的信息，在缓冲区扫描。如果一个值是一下个访问的地址，那么就是指针跳转类型的。如果这次访问和下次访问靠近，连续元素相差相同数量的位置，那么认为是流式访问。总数减去这两者，得到随机访问的数量。</p><p>然后根据一系列建模，用贪心算法去决定放置。</p><p>修改应用程序以使用X-Mem API只需要在任何应用程序中编写很少（10到50）行代码。对于每个应用程序，我们首先确定随着应用程序数据大小而增长并占据应用程序内存占用的重要部分的数据结构。只有这样大的数据结构才使用X-Mem API进行分配；其他数据继续使用默认系统分配器在DRAM中分配。每个数据结构都被赋予一个唯一的标签。我们正在探索自动化为每个数据结构附加唯一标签的过程的选项，也许作为编译器扩展。</p><h2 id="6-环境配置"><a href="#6-环境配置" class="headerlink" title="6. 环境配置"></a>6. 环境配置</h2><p>混合内存仿真平台（HMEP）[39]由英特尔构建，用于支持混合内存系统软件堆栈的探索。 HMEP通过实施以下方式实现混合内存与实际应用的研究：(i)DRAM和模拟NVM的独立物理内存范围（使用自定义 BIOS），以及(ii)可配置延迟和时间的细粒度模拟NVM的带宽（使用自定义CPU微代码）。HMEP之前已在其他研究中使用过 [20,26,27,43,44,56]，并在其他地方详细描述过[39]。</p><p>我们展示了六种HMEP配置的结果，代表了NVM的性能范围（表1）。在这些配置中，NVM的延迟范围为DRAM延迟的2倍到4倍（300ns、450ns和600ns），NVM的带宽范围为DRAM带宽的1/4倍到1/8倍（10GB/s到5GB/s）。此外，我们还评估了系统中NVM与DRAM大小的不同比例。我们使用1/T来指代DRAM大小为系统中总可用内存的1/T的设置。在所有测试中，我们为<strong>应用程序提供固定数量的总内存</strong>（等于DRAM和NVM的总和）。与这些设置相对应，我们还考虑了总内存的成本。一般来说，NVM成本源自之前的研究，预计基于NVM（PCM）的SSD比企业MLC NAND闪存SSD贵4倍[33]。基于DDR的DRAM[19]的普遍价格以及保守估计，本工作中假设的可直接寻址NVM设备将比基于NVM的SSD更昂贵（至少在最初），我们假设DRAM是5×按每比特成本计算，比NVM更贵。由于本文感兴趣的大型内存系统中的内存成本明显高于其他服务器组件，因此我们使用上述成本估算作为服务器总成本的代理。</p><h2 id="7-从结果看，作者如何有力证明他解决了问题"><a href="#7-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="7. 从结果看，作者如何有力证明他解决了问题"></a>7. 从结果看，作者如何有力证明他解决了问题</h2><p>评估有两个目标。首先是表明所有三个应用程序的性能很大程度上取决于其中每个数据结构的放置选择。其次，我们的目标是证明我们的放置技术可以正确识别每个数据结构的访问模式，并通过X-Mem实现最佳数据放置。为了证明所有情况下的结果，我们提供了两个基线进行比较。第一种是纯NVM——其中应用程序的内存完全从NVM分配。仅NVM描述了混合内存系统中未经修改的应用程序的最坏情况性能。第二种是仅DRAM，它代表当所有访问的数据都分配在快速DRAM中时应用程序的最佳性能。</p><p>红色柱状图是开销。</p><h3 id="7-1-MLP-GraphMat"><a href="#7-1-MLP-GraphMat" class="headerlink" title="7.1 MLP GraphMat"></a>7.1 MLP GraphMat</h3><blockquote><p>MLP(memory-level parallelism)可以帮助CPU更好地利用系统内存层次结构，从而减少内存访问延迟并提高计算效率。MLP的内存层次通常包括寄存器、高速缓存、主内存和可能还有其他级别的内存。当CPU执行指令时，它会不断地从不同的内存层次中获取数据，而不仅仅依赖于主内存。这可以通过预取数据到高速缓存、使用多级高速缓存、乱序执行指令等技术来实现。通过充分利用MLP，CPU可以在等待内存数据返回时执行其他指令，从而提高整体性能。MLP的具体实现和性能提升取决于处理器的架构和设计，以及编写的程序的特性。</p></blockquote><p>GraphMat采用顶点程序，并在后端将其映射为高度优化、可扩展的稀疏矩阵操作。GraphMat以迭代广义稀疏矩阵-稀疏向量乘法（或SPMV）的方式实现图算法，更新与顶点相关的一些数据（如 Pagerank）。下图是在代表Twitter follower网络[34]的大图上执行Pagerank算法[23]，32G. Graphmat具有三种主要数据结构：<strong>稀疏向量、顶点数据的稠密向量，最后是压缩稀疏行格式的图的邻接矩阵</strong>。顶点数据的稀疏向量是在每次迭代上构建的，并且由计算中活动顶点的数据组成。通过读取和写入稀疏向量而不是顶点的全稠密向量Graphmat实现了更好的缓存效率。从X-Mem的API可以发现对数据结构的访问量是很有倾向性的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a03fdc9475da4447b0e1544061a016d8.png"></p><p>图8显示了运行测试时GraphMat的带宽要求和有效延迟。有效延迟（显示为实际物理内存延迟的百分比）通过测量每次最后一级缓存缺失时由于待读取而导致的core停滞，近似于应用程序中的平均内存读取延迟。GraphMat可以实现很高的带宽使用率（读取带宽高达25GB/s，写入带宽高达15GB/s），<span class="label primary">但由于GraphMat能够利用CPU的MLP（内存级并行性），其有效延迟很低（大多低于20%）。 因此，GraphMat的性能对较低的带宽高度敏感，但对较高的延迟只有一定程度的敏感。</span>这就解释了为什么GraphMat的纯NVM性能会随着带宽的降低而降低，而不是随着延迟的增加而增加（图9a最高的那条线）。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a916317248884134b9aea76b6b686724.png"></p><p>GraphMat仅适用于NVM的性能比仅适用于DRAM的性能差2.6倍至5.9倍，并且在较低峰值NVM带宽下性能下降尤其急剧。大部分性能差距可归因于稀疏向量，稀疏向量仅占GraphMat占用空间（32GB）的3.5%（但是他很热）。图9b描述了使用稀疏向量的SPMV操作对GraphMat整体运行时间的巨大贡献。超过1/16，性能会受到对大型邻接矩阵的统一（带宽限制）访问的限制。因此，1/2的性能是仅DRAM的1.09倍到1.29倍，比 1/16 略有改进，但成本更高。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/54382511dea94faeb3fd6d56ab3d74f4.png"></p><h3 id="7-2-VoltDB"><a href="#7-2-VoltDB" class="headerlink" title="7.2 VoltDB"></a>7.2 VoltDB</h3><p>内存数据库利用架构和应用程序趋势来避免许多通常与基于磁盘的<strong>OLTP数据库</strong>相关的开销[12,13,15,51]。VoltDB就是这样的内存数据库之一，它是一种符合ACID的分布式主内存关系数据库，它实现了H-Store的无共享架构 [51]。将仓库数量设置为512个，VoltDB中的站点数量设置为8个，从而产生40GB内存占用。<strong>TPC-C</strong>吞吐量以每秒总事务数报告。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/197dc904cf5a493d9fc9d29a08d367f7.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/254aa90c9ef94399a85c33f3f940ab51.png"></p><p>图11显示，VoltDB的性能只对内存延迟敏感，这可以解释为VoltDB中的随机访问（尤其是对索引的访问）造成了较高的有效延迟，有时甚至超过50%（图10b）。在TPC-C工作负载中，VoltDB的平均读取带宽为2.3GB/s，写入带宽仅为1.3GB/s（图10a），因此对较低的带宽并不敏感。</p><p>VoltDB中的TPC-C包含28个此类数据结构，其中前10个数据结构（按大小）约占总内存占用的99%。表4说明了X-Mem的放置算法在这前10个数据结构中的应用。中间结果和其他数据结构使用默认的系统分配器在DRAM中分配。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a6ec47fb78054c699cc750e00110fe6c.png"></p><p>1/16的性能比纯NVM提高了24%到48%。STOCK表和小型CUSTOMER表-在DRAM中。1/8（此处未显示以避免混乱）和1/4的性能并不比1/16好多少。在1/2时，X-Mem能够将VoltDB的所有数据结构放入DRAM中，除了非常大但不经常使用的“CUSTOMER NAME”.图11b显示了<strong>TPC-C事务</strong>的延迟分布，因为它是OLTP工作负载的重要指标。当我们转向NVM中包含更多数据的配置时，平均事务延迟会增加。然而，这种增加受到布局算法的调节，导致事务延迟接近DRAM，即使对于1/16配置也是如此。</p><h3 id="7-3-MemC3"><a href="#7-3-MemC3" class="headerlink" title="7.3 MemC3"></a>7.3 MemC3</h3><p>作为大型数据中心中的缓存，键值存储变得越来越重要[21]。我们研究了Memcached[10] 的一个变体，称为MemC3[28]，这是开源Memcached的最新进展，它比现有实现提高了内存效率和吞吐量。 MemC3实现了布谷鸟哈希cuckoo hashing的变体，以避免Memcached中常见的昂贵的随机相关访问。此外，MemC3用乐观并发控制方案取代了Memcached的独占全局锁定。在加载阶段，MemC3服务器填充了大约64GB的数据集。密钥大小固定为16字节，值的大小范围从16B到8K，大致遵循Facebook发布的数据中ETC跟踪中值的分布 [21]。超过90%的值的大小为512字节或更小。然而，大于512字节的值几乎占据了缓存空间的50%工作负载的总内存占用量约为100GB。</p><p>在执行阶段，客户端再次根据ETC请求跟踪中的值分布生成5000万个随机查询。工作负载读取繁重（95%GET和5%SET），并且当前不包含任何DELETE请求（和ycsb差不多竟然）。我们报告完成测试的总时间。图12显示MemC3的性能特征，客户端每次请求一种大小的值（从16B到8K）。图12a显示MemC3的读取带宽范围为1.8 GB/s至14GB/s，写入带宽范围为1GB/s至6.5GB/s，具体取决于值的大小。由于顺序性能的提高，有效延迟在较大值时减少（从48%到4%）（图12b）。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2d40a9406d02470295476136fabd73cf.png"></p><p>MemC3对<span class="label primary">较高延迟而不是较低带宽的敏感性</span>（如图13a中仅NVM的性能所示）可以通过工作负载中较小值的较大偏差来解释。图13b通过分解不同值大小的NVM-only开销进一步说明了这一事实。MemC3在启动时为cuckoo哈希表分配内存，并使用动态slab分配器来分配值，slab类型由slab分配的大小决定。哈希表和slabs均使用xmalloc进行分配，因此X-Mem中总共有9个数据结构（表5），因为每个板类型都被视为单独的数据结构。表5显示了由X-Mem放置算法确定的这些数据结构的优先级。请注意，X-Mem为各种板类型确定的优先级完全遵循图13b中仅NVM开销的值细分，这证明了X-Mem放置模型的准确性。在1/8分层中，只有12.5%的应用程序数据位于DRAM中，性能比仅NVM提高了8%到17%，因为X-Mem分配了MemC3的布谷鸟哈希DRAM。 1/4的表现甚至更好，将性能提高到仅DRAM的6%到13%，主要是因为X-Mem现在能够将大量小值放入DRAM中，并减少对NVM的随机访问次数。</p><p><span class="label primary">表5还显示了为什么仅凭数据结构的访问频率不足以确定将其置于更快内存中的好处。例如，布谷鸟哈希和256B在访问频率和内存占用方面都比较接近，但由于访问模式不同，在每个内存区域的收益方面却形成了鲜明对比。</span></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d50257a5b9004017867774b9e337a2dd.png"></p><h3 id="7-4-实际部署"><a href="#7-4-实际部署" class="headerlink" title="7.4 实际部署"></a>7.4 实际部署</h3><p>虽然用xmalloc和xfree替换malloc和free很简单，但根据实现情况，为各个数据结构分配唯一的标记可能需要更多的工作。在我们的例子中，修改VoltDB需要付出最多的努力，但仍然涉及不到50行代码的更改。对于面向对象的程序来说，工作量要低得多，因为可以定义特定于类的分配器来调用xmalloc，而不是将这些调用分散在代码中。虽然这个过程并不繁重，但仍然需要手动操作和对应用程序源代码的充分理解。我们计划探索自动标记应用程序数据结构的动态分配的技术，至少在托管语言和运行时的上下文中[24,32,50]。<span class="github-emoji"><span>👍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>👍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> <span class="github-emoji"><span>👍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><p>在剖析步骤中进行内存跟踪（使用Pin）会大大降低应用程序的运行速度（在我们的测试中最高可达40倍），因此只适合离线使用，不适用于生产。我们考虑过的一个方案是采用基于采样的剖析[55]。由于担心采样剖析器可能无法捕捉到指针追逐和strided扫描等紧密间隔的内存访问之间的关系，这个想法被搁置了。我们目前正在探索对x86 ISA的扩展，以实现必要的剖析，但无需昂贵的跟踪每次内存访问的过程。不过，剖析器仍然是初步部署的解决方案。我们还发现，<strong>在保留各种数据结构之间关系的同时，缩小工作负载的规模也很有用，这样可以减少剖析所花费的时间</strong>。</p><p>当内存区域从DRAM迁移到NVM时，X-Mem通过等待mbind调用返回来执行“同步”迁移。在我们的实验中，这种迁移的开销可以忽略不计，因为与应用程序的整体运行时间相比，分配所花费的时间相对较少。表6显示了区域大小范围从64M到1G的迁移的原始开销。 1G区域可以比较小区域摊销5%到8%的迁移成本。然而，**使用1G区域的真正好处是它允许在大多数64位服务器上使用大型（1GB）硬件页面；与4K页的基准相比，迁移开销进一步减少了60%**。虽然我们了解在内存占用较大的应用程序中使用具有大页面的1G区域的好处[27]，但X-Mem的当前实现使用64 MB区域（默认情况下）来减少内部碎片。此外，由于Linux内核的限制，X-Mem使用4K硬件页面。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/7974e5f219954ae0b7e2eecd5050c5db.png"></p><h2 id="8-缺陷和改进思路"><a href="#8-缺陷和改进思路" class="headerlink" title="8. 缺陷和改进思路"></a>8. 缺陷和改进思路</h2><p>这里作者只对堆进行分配，现在人们在匿名和文件映射区也有关注。</p><p>具体作者讲了很多从jmalloc来做的事情，没有看懂，再看看吧，emmmmmmmmmm</p><h2 id="9-创新点"><a href="#9-创新点" class="headerlink" title="9. 创新点"></a>9. 创新点</h2><p>这个都想到了：<br>数据中心通常在部署之前使用影子基础设施来测试其代码。一种选择是定期分析此影子环境中的工作负载，并相应地重新分类应用程序数据。</p><h2 id="10-积累"><a href="#10-积累" class="headerlink" title="10. 积累"></a>10. 积累</h2><p><a href="https://lwn.net/Articles/636972/">惊, 原来论坛里MC的想法很早之前就讨论过了, 使用大页面和是否用LRU这些问题也都有讨论</a></p><p>讨论了关于价格的事</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Working Set Analytics</title>
      <link href="/2023/11/01/Working-Set-Analytics/"/>
      <url>/2023/11/01/Working-Set-Analytics/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">- 文章来自ACM Comput. Surv. 2021- Working Set Analytics</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>PETER J. DENNING（ACM前任主席，虚拟内存方面发明了程序行为的工作集模型解决了抖动，等其他贡献）, 海军研究生院（美国海军的公立研究生院）美国加利福尼亚州蒙特雷市</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><blockquote><p>工作集是指程序在最近一次执行中访问过的页面(内存块)集合。工作集算法可以根据程序的访问局部性原理来提高内存利用率,减少页面置换的次数,从而提高系统性能。与全局置换算法(如FIFO)相比,工作集算法更能适应不同进程的内存访问行为。</p></blockquote><h2 id="3-文章概要"><a href="#3-文章概要" class="headerlink" title="3. 文章概要"></a>3. 文章概要</h2><p>程序行为的工作集模型于1965年发明。它在虚拟内存管理领域经受了50多年的时间考验。它被认为是管理操作系统和缓存中的内存的理想选择。它的优越性能是基于同时发现的局域性原理；局部性是观察到的程序在较长时间内使用其页面的不同子集的趋势。</p><p>本教程追溯了工作集理论从起源到现在的发展。</p><ol><li>我们将讨论局部性原理及其实验验证。</li><li>我们将展示为什么工作集内存管理可以抵抗颠簸并生成接近最佳的系统吞吐量。</li><li>我们将介绍强大的线性时间算法，用于计算工作集统计数据并将其应用于内存系统的设计。</li><li>我们将揭穿有关存储系统局部性和性能的几个神话。</li><li>最后我们将讨论工作集模型在并行系统、现代共享 CPU 缓存、网络边缘缓存以及库存和物流管理中的应用。</li></ol><h2 id="4-虚拟内存的发展之痛"><a href="#4-虚拟内存的发展之痛" class="headerlink" title="4. 虚拟内存的发展之痛"></a>4. 虚拟内存的发展之痛</h2><p>曼彻斯特的设计师推出了四项创新，这些创新很快就被采纳为计算机体系结构和操作系统的标准，从那时起到今天。</p><ol><li>Page，一种固定大小的存储和传输单元。程序和数据被分成页面并存储在称为页框的主存储器插槽中，并在辅助存储器上复制副本。</li><li>addresses（值的名称）和locations（保存值的内存插槽）之间的区分。地址空间是一个大型的线性地址序列，主存储器（RAM）是一个页框池，每个页框都可以容纳地址空间中的任何页面。当CPU将地址生成到其地址空间时，硬件内存映射单元使用将页面与页框相关联的页表将addresses转换为locations。</li><li>page fault，当执行程序向映射单元提供页面不在主存中的地址时，操作系统会触发中断。操作系统在辅助内存中找到丢失的页面，选择一个已加载的页面进行逐出，并将丢失的页面转移到空出的页面框架中。</li><li>页面替换策略，该算法选择必须从主内存中逐出哪个页面并将其返回到辅助内存以为传入页面让路。未命中率函数（产生页面错误的引用的比例）是虚拟内存的关键性能指标。</li></ol><p>操作系统的性能一直是一个大问题。为了被操作系统接受，虚拟内存系统必须高效。效率低下的两个潜在来源是地址映射和页面传输。它们可以称为寻址问题和替换问题。</p><p>有效的解决方案。TLB是内存映射单元中的一个小型高速缓存，它将地址映射的减慢限制为正常 RAM 访问时间的 3%。就所有好处而言，这是可以接受的成本。虚拟内存确实使程序员的生产力提高了一倍或三倍。它还消除了必须针对每个不同大小的主内存重新制定手工传输计划的恼人问题。更重要的是，虚拟内存可以将主内存划分为不相交的集合，每个地址空间对应一个集合，从而支持防止正在执行的作业相互干扰的主要目标。这对于早期操作系统的设计者和用户来说都是好消息。</p><blockquote><p>太长啦，正在努力读……</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> O </tag>
            
            <tag> Working Set </tag>
            
            <tag> Memory Management </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux内核常用C语言技巧数据结构和算法</title>
      <link href="/2023/10/31/Linux%E5%86%85%E6%A0%B8%E5%B8%B8%E7%94%A8C%E8%AF%AD%E8%A8%80%E6%8A%80%E5%B7%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"/>
      <url>/2023/10/31/Linux%E5%86%85%E6%A0%B8%E5%B8%B8%E7%94%A8C%E8%AF%AD%E8%A8%80%E6%8A%80%E5%B7%A7%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Linux内核中常用C语言技巧"><a href="#1-Linux内核中常用C语言技巧" class="headerlink" title="1.Linux内核中常用C语言技巧"></a>1.Linux内核中常用C语言技巧</h2><p>GCC的C编译器对C语言进行了很多扩充，称为GUN C语言。</p><h3 id="1-1语句表达式"><a href="#1-1语句表达式" class="headerlink" title="1.1语句表达式"></a>1.1语句表达式</h3><p>语句表达式可以使用循环、跳转和局部变量，通常用在宏定义中。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">define</span> <span class="token macro-name function">max</span><span class="token expression"><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token punctuation">(</span>b<span class="token punctuation">)</span> <span class="token operator">?</span> <span class="token punctuation">(</span>a<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span></span></span><span class="token comment">/***调用 max(i++, j++),宏的作用是文本替换得到(i++) &gt; (j++) ? (i++) : (j++)在条件部分被计算一次，结果部分还被计算一次，会导致安全问题。***/</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当知道ab类型时的写法</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">maxint</span><span class="token expression"><span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span> </span><span class="token punctuation">\</span>  <span class="token expression"><span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token keyword">int</span> _a<span class="token operator">=</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">,</span> _b<span class="token operator">=</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span> </span><span class="token punctuation">\</span>  <span class="token expression">_a <span class="token operator">&gt;</span> _b <span class="token operator">?</span> _a <span class="token operator">:</span> _b<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>不知道ab类型, <code>typeof</code>可以构造新的类型</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>kernel<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">min</span><span class="token expression"><span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">{</span> </span><span class="token punctuation">\</span>  <span class="token expression"><span class="token keyword">typeof</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> _min1 <span class="token operator">=</span> <span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span> </span><span class="token punctuation">\</span>  <span class="token expression"><span class="token keyword">typeof</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> _min2 <span class="token operator">=</span> <span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span> </span><span class="token punctuation">\</span>  <span class="token expression"><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>_min1 <span class="token operator">==</span> <span class="token operator">&amp;</span>_min2<span class="token punctuation">)</span><span class="token punctuation">;</span> \  </span></span>  <span class="token comment">/***  &amp;_min1得到min1的指针，编译时，如果两个指针类型不同会报错。  void只是想利用语句表达式进行类型检查,而不需要比较的结果。  为什么要比较地址，不直接比较值？  claude的回答：  即使编译器对_min1和_min2做优化,转换为寄存器变量,指针地址比较仍可用；  指针地址比较也可以作用于结构体等非基本类型。  ***/</span>  _min1 <span class="token operator">&lt;</span> _min2 <span class="token operator">?</span> _min1 <span class="token operator">:</span> _min2<span class="token punctuation">;</span> \<span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-2标号元素"><a href="#1-2标号元素" class="headerlink" title="1.2标号元素"></a>1.2标号元素</h3><p>标准C语言要求数组或者结构体初始化必须固定顺序。但GUN C可以指定要初始化的，没有初始化的为0或者NULL，而且顺序可以不固定。以下是C内核中的一段代码示例:</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">static</span> <span class="token keyword">struct</span> <span class="token class-name">usb_driver</span> usb_storage_driver <span class="token operator">=</span> <span class="token punctuation">{</span>       <span class="token punctuation">.</span>owner         <span class="token operator">=</span> THIS_MODULE<span class="token punctuation">,</span>        <span class="token punctuation">.</span>name          <span class="token operator">=</span> <span class="token string">"usb-storage"</span><span class="token punctuation">,</span>         <span class="token punctuation">.</span>probe         <span class="token operator">=</span> storage_probe<span class="token punctuation">,</span>        <span class="token punctuation">.</span>disconnect    <span class="token operator">=</span> storage_disconnect<span class="token punctuation">,</span>        <span class="token punctuation">.</span>id_table      <span class="token operator">=</span> storage_usb_ids<span class="token punctuation">,</span> <span class="token punctuation">}</span><span class="token punctuation">;</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-3可变参数的宏"><a href="#1-3可变参数的宏" class="headerlink" title="1.3可变参数的宏"></a>1.3可变参数的宏</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>printk<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">define</span> <span class="token macro-name function">pr_debug</span><span class="token expression"><span class="token punctuation">(</span>fmt<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> \ </span><span class="token comment">//...是可变化参数</span></span>  <span class="token function">dynamic_pr_debug</span><span class="token punctuation">(</span>fmt<span class="token punctuation">,</span> ##__VA_ARGS__<span class="token punctuation">)</span> <span class="token comment">//__VA_ARGS__编译器保留字段</span><span class="token comment">//预处理时参数传给宏，调用宏时传给dynamic_pr_debug函数</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-4函数属性、变量属性核类型属性"><a href="#1-4函数属性、变量属性核类型属性" class="headerlink" title="1.4函数属性、变量属性核类型属性"></a>1.4函数属性、变量属性核类型属性</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">//属性语法格式</span><span class="token keyword">__attribute__</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>attribute<span class="token operator">-</span>list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">void</span> <span class="token keyword">__attribute__</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>noreturn<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">die</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//告诉die函数，从不返回值</span><span class="token keyword">static</span> <span class="token keyword">inline</span> u32 __attribute_const__ <span class="token function">read_cpuid_cachetype</span><span class="token punctuation">(</span><span class="token keyword">void</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token keyword">return</span> <span class="token function">read_cpuid</span><span class="token punctuation">(</span>CTR_EL0<span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token comment">//只调用函数一次，再次调用时返回第一次结果就行</span><span class="token keyword">struct</span> <span class="token class-name">xx</span><span class="token punctuation">{</span>  <span class="token comment">//表示变量或者结构体成员的最小对齐格式</span>  <span class="token comment">//以8字节对齐方式分配xx数据结构</span><span class="token punctuation">}</span><span class="token function">__aligned</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>不过还有更复杂的函数属性。函数属性的宏定义在<code>compiler-gcc.h</code>中，在使用时它们不一定都有<code>__attribute</code>字符串在。</p><h3 id="1-5内建函数"><a href="#1-5内建函数" class="headerlink" title="1.5内建函数"></a>1.5内建函数</h3><p>以<code>_builtin</code>作为前缀（当然有的已经被宏定义过了）</p><ul><li><code>__builtin_constant_p(x)</code>判断在编译时是否就能确定为常量，返回1或0</li><li><code>__builtin_expect(exp, c)</code>表示<code>exp==c</code>的概率很大，引导GCC做条件分支预测，提高CPU预取指令正确率<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">LIKELY</span><span class="token expression"><span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token function">__builtin_expect</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token operator">!</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> </span><span class="token comment">//x很可能为真</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">UNLIKELY</span><span class="token expression"><span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token function">__builtin_expect</span><span class="token punctuation">(</span><span class="token operator">!</span><span class="token operator">!</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> </span><span class="token comment">//x很可能为假</span></span><span class="token comment">//!!是为了确保是bool值</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li><code>__builtin_prefetch(const void *addr, int rw, int locality)</code>主动预取，在使用<code>addr</code>的值之前把这个值加入到cache，rw指读写属性，locality指局部性强弱<pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">prefetch</span><span class="token expression"><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token function">__builtin_prefetch</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">prefetchw</span><span class="token expression"><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token function">__builtin_prefetch</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><h3 id="1-6-UL"><a href="#1-6-UL" class="headerlink" title="1.6 UL"></a>1.6 UL</h3><p>在数字定义时在后面加上<code>UL</code>后缀，这会把<code>int</code>类型强制转化为<code>unsigned long</code>，防止两个<code>int</code>相加溢出。</p><h2 id="2-Linux内核中常用数据结构和算法"><a href="#2-Linux内核中常用数据结构和算法" class="headerlink" title="2.Linux内核中常用数据结构和算法"></a>2.Linux内核中常用数据结构和算法</h2><p>Linux内核有的数据结构非常常用因此封装好了一系列的数据结构以及和相关函数操作。</p><h3 id="2-1链表"><a href="#2-1链表" class="headerlink" title="2.1链表"></a>2.1链表</h3><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>types<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token keyword">struct</span> <span class="token class-name">list_head</span><span class="token punctuation">{</span>  <span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>next<span class="token punctuation">,</span> <span class="token operator">*</span>prev<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>因为链表的数据区是各不相同的，所以是纯链表封装没有数据区。使用方式是嵌入到其他数据结构，如在<code>page</code>中嵌入lru链表的节点这样就加入了LRU链表。</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>mm_types<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token keyword">struct</span> <span class="token class-name">page</span><span class="token punctuation">{</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token keyword">struct</span> <span class="token class-name">list_head</span> lru<span class="token punctuation">;</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>链表的初始化是将两个指针指向自身</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>list<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token comment">//动态初始化</span><span class="token keyword">static</span> <span class="token keyword">inline</span> <span class="token keyword">void</span> <span class="token function">INIT_LIST_HEAD</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>list<span class="token punctuation">)</span><span class="token punctuation">{</span>  list<span class="token operator">-&gt;</span>next <span class="token operator">=</span> list<span class="token punctuation">;</span>  list<span class="token operator">-&gt;</span>next <span class="token operator">=</span> list<span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment">//静态初始化，宏定义的展开结果是一个包含两个指针的结构，这两个指针都指向name自身</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">LIST_HEAD_INIT</span><span class="token expression"><span class="token punctuation">(</span>name<span class="token punctuation">)</span> <span class="token punctuation">{</span><span class="token operator">&amp;</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">&amp;</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">}</span> </span></span><span class="token comment">//然后把上个结构体封装</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">LIST_HEAD</span><span class="token expression"><span class="token punctuation">(</span>name<span class="token punctuation">)</span> <span class="token keyword">struct</span> <span class="token class-name">list_head</span> name <span class="token operator">=</span> <span class="token function">LIST_HEAD_INIT</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>inline是一个关键字，通常用于指示编译器在编译时将函数的代码插入到函数调用的地方，而不是像普通函数那样进行函数调用。因为函数调用通常涉及一定的开销，如参数传递、栈帧设置等，而inline函数可以减少这些开销。这样的函数被声明为内联函数，编译器会尝试在函数调用的地方直接插入函数体的代码。这可以提高函数调用的效率，特别是对于小而简单的函数。如果函数体过于复杂，内联可能会导致代码膨胀，反而降低性能。不过，最终是否内联函数由编译器决定。</p></blockquote><p>添加节点到链表</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>list<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token comment">//添加到表头</span><span class="token keyword">void</span> <span class="token function">list_add</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>new<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>head<span class="token punctuation">)</span><span class="token comment">//添加到表尾</span><span class="token keyword">void</span> <span class="token function">list_add_tail</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>new<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>head<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>遍历节点接口</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">list_for_each</span><span class="token expression"><span class="token punctuation">(</span>pos<span class="token punctuation">,</span> head<span class="token punctuation">)</span> </span><span class="token punctuation">\</span>  <span class="token expression"><span class="token keyword">for</span> <span class="token punctuation">(</span>pos<span class="token operator">=</span><span class="token punctuation">(</span>head<span class="token punctuation">)</span><span class="token operator">-&gt;</span>next<span class="token punctuation">;</span> pos<span class="token operator">!=</span><span class="token punctuation">(</span>head<span class="token punctuation">)</span><span class="token punctuation">;</span> pos<span class="token operator">=</span>pos<span class="token operator">-&gt;</span>next<span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>但是主要还是访问这个节点嵌入的数据结构，比如前面提到的page结构体里的其他成员该怎么办？</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">list_entry</span><span class="token expression"><span class="token punctuation">(</span>ptr<span class="token punctuation">,</span> type<span class="token punctuation">,</span> member<span class="token punctuation">)</span> <span class="token function">container_of</span><span class="token punctuation">(</span>ptr<span class="token punctuation">,</span> type<span class="token punctuation">,</span> member<span class="token punctuation">)</span></span></span><span class="token comment">/*container_of宏定义在kernel头文件中container_of先将0地址转化为type结构体指针，_mptr=ptr，类比page里lru成员的地址（因为假设在遍历LRU链表）获取member在type结构体中的偏移量，type—&gt;member，即page结构体中lru的偏移量ptr-offset得到这个结构体的真实地址，现在这个指针指向page了 */</span>  <span class="token comment">//真实案例</span> <span class="token operator">&lt;</span>drivers<span class="token operator">/</span>block<span class="token operator">/</span>osdblk<span class="token punctuation">.</span>c<span class="token operator">&gt;</span> <span class="token comment">//这个函数传入的参数不用管</span> <span class="token keyword">static</span> <span class="token class-name">ssize_t</span> <span class="token function">class_osdblk_list</span><span class="token punctuation">(</span><span class="token keyword">struct</span> <span class="token class-name">class</span> <span class="token operator">*</span>c<span class="token punctuation">,</span>                <span class="token keyword">struct</span> <span class="token class-name">class_attribute</span> <span class="token operator">*</span>attr<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>data<span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>    <span class="token keyword">struct</span> <span class="token class-name">list_head</span> <span class="token operator">*</span>tmp<span class="token punctuation">;</span> <span class="token comment">//当前遍历到的节点</span>    <span class="token function">list_for_each</span><span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> <span class="token operator">&amp;</span>osdblkdev_list<span class="token punctuation">)</span><span class="token punctuation">{</span>      <span class="token keyword">struct</span> <span class="token class-name">osdblk_device</span> <span class="token operator">*</span>osdev<span class="token punctuation">;</span> <span class="token comment">//这个链表本来串连起来的结构体</span>      osdev <span class="token operator">=</span> <span class="token function">list_entry</span><span class="token punctuation">(</span>tmp<span class="token punctuation">,</span> <span class="token keyword">struct</span> <span class="token class-name">osdblk_device</span><span class="token punctuation">,</span> node<span class="token punctuation">)</span><span class="token punctuation">;</span>      <span class="token comment">//链表的一个节点作为osdblk_device的结构体成员，名称是node</span>      <span class="token comment">//现在可以对拿到的结构体做一系列操作了</span>      n <span class="token operator">+=</span> <span class="token function">sprintf</span><span class="token punctuation">(</span>data<span class="token operator">+</span>n<span class="token punctuation">,</span> <span class="token string">"%d %d ..."</span><span class="token punctuation">,</span> osdev<span class="token operator">-&gt;</span>id<span class="token punctuation">,</span> osdev<span class="token operator">-&gt;</span>major<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> n<span class="token punctuation">;</span>  <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2无锁环形缓冲区"><a href="#2-2无锁环形缓冲区" class="headerlink" title="2.2无锁环形缓冲区"></a>2.2无锁环形缓冲区</h3><p>环形缓冲区属于生产者-消费者模型的经典算法，好处是当一个元素被消耗后，其余元素不需要移动存储位置，减少复制操作。通过移动读指针和写指针实现缓冲区数据的读取和写入。内核使用KFIFO无锁环形缓冲区“first in first out”.</p><p>创建KFIFO</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token operator">&lt;</span>include<span class="token operator">/</span>linux<span class="token operator">/</span>kfifo<span class="token punctuation">.</span>h<span class="token operator">&gt;</span><span class="token comment">//动态分配</span><span class="token keyword">int</span> <span class="token function">kfifo_alloc</span><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> size<span class="token punctuation">,</span> gfp_mask<span class="token punctuation">)</span><span class="token comment">/*创建大小为size的环形缓冲区fifo指向kfifo数据结构gfp_mask是内存分配的掩码 */</span><span class="token comment">//静态分配</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">DEFINE_KFIFO</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> type<span class="token punctuation">,</span> size<span class="token punctuation">)</span></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">INIT_KFIFO</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>入列</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token keyword">int</span> <span class="token function">kfifo_in</span><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> buf<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token comment">//将buf指针指向的n个元素复制到环形缓冲区</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>出列</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_out</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> buf<span class="token punctuation">,</span> n<span class="token punctuation">)</span></span></span><span class="token comment">//从fifo复制n个到buf</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>获取缓冲区大小</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_size</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">)</span> </span><span class="token comment">//缓冲区大小</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_len</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">)</span>  </span><span class="token comment">//有多少有效数据元素</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_is_empty</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">)</span> </span><span class="token comment">//是否为空</span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_is_full</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">)</span> </span><span class="token comment">//是否已满</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>与用户空间交互</p><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_from_user</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> from<span class="token punctuation">,</span> len<span class="token punctuation">,</span> copied<span class="token punctuation">)</span></span></span><span class="token comment">//将from指向的用户空间中len个数据元素复制到KFIFO中，最后一个参数copied表示成功复制了几个元素</span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name function">kfifo_to_user</span><span class="token expression"><span class="token punctuation">(</span>fifo<span class="token punctuation">,</span> to<span class="token punctuation">,</span> len<span class="token punctuation">,</span> copied<span class="token punctuation">)</span></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="2-3红黑树"><a href="#2-3红黑树" class="headerlink" title="2.3红黑树"></a>2.3红黑树</h3><p>优点是所有重要操作可以在 $O(log_2n)$的时间内完成。<br><a href="https://www.jianshu.com/p/e136ec79235c">这是算法介绍</a><br><a href="https://www.cnblogs.com/hellokitty2/p/15362630.html">这是linux文档里Documentation/Rbtree.txt红黑树的翻译</a></p><hr><blockquote><p>大部分是《奔跑吧Linux内核入门篇（第二版）》第二章的阅读笔记。谢谢欣欣帮忙答疑</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C syntax </tag>
            
            <tag> Data Structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Direct Access, High-Performance Memory Disaggregation with DIRECTCXL</title>
      <link href="/2023/10/29/Direct-Access-High-Performance-Memory-Disaggregation-with-DIRECTCXL/"/>
      <url>/2023/10/29/Direct-Access-High-Performance-Memory-Disaggregation-with-DIRECTCXL/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary no-icon"><ul><li>文章来自2022 USENIX Annual Technical Conference,CCFA</li><li>Direct Access, High-Performance Memory Disaggregation with DIRECTCXL</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li> Donghyun Gouk, Sangwon Lee, Miryeong Kwon, Myoungsoo Jung. KAIST CAMELab.</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h3 id="Memory-disaggregation是什么？"><a href="#Memory-disaggregation是什么？" class="headerlink" title="Memory disaggregation是什么？"></a>Memory disaggregation是什么？</h3><p>一种服务器内存架构,其核心思想是将服务器内存从计算节点（CPU）上分离出来,形成一个可独立扩展的内存资源池,然后再根据需要将内存资源动态分配给各个计算节点使用。（将内存资源解耦，使多个计算节点可以访问一个集中式内存池，而不必依赖本地内存。）</p><h3 id="远端直接内存访问RDMA怎么做的？"><a href="#远端直接内存访问RDMA怎么做的？" class="headerlink" title="远端直接内存访问RDMA怎么做的？"></a>远端直接内存访问RDMA怎么做的？</h3><p>对于后端网络控制，大多数分解工作采用远程直接内存访问（RDMA）[4,5,11–13,15,16]或类似的定制DMA协议[7,9,10]。图1显示了RDMA式数据传输（单侧RDMA）的工作原理。对于主机和内存节点端，RDMA都需要RDMA NIC（RNIC[23]）等硬件支持，其设计目的是尽可能消除网络软件堆栈的干预。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9128ea4c40a043d9bd581017c6274724.png"><br>为了在它们之间移动数据，每一侧的进程首先需要定义一个或多个存储区域（MR）并让MR到达底层RNIC。在此期间，RNIC驱动程序检查与MR页面关联的所有物理地址，并将它们注册到RNIC的内存转换表（MTT）。</p><p>由于这两个RNIC还在初始化时交换其MR的虚拟地址，因此主机可以简单地将内存节点的目标虚拟地址与数据一起发送以进行写入。然后，远程节点通过参考其MTT来转换地址，并将传入数据复制到MR的目标位置。通过RDMA的读取也可以以类似的方式执行。请注意，除了内存复制操作（对于DMA）之外，每一方的应用程序还需要准备数据到MR中或从MR中检索数据以进行数据传输，从而在本地DRAM中引入额外的数据副本[24]。</p><p>（说到底就是需要复制很多次吧）</p><h3 id="基于页面的内存池——SWAP"><a href="#基于页面的内存池——SWAP" class="headerlink" title="基于页面的内存池——SWAP"></a>基于页面的内存池——SWAP</h3><p>基于页面的内存分解[4-10]通过依赖虚拟内存系统实现内存弹性。具体来说，这种方法会在出现缺页异常时将数据交换到远程内存节点而不是底层存储。主机kswapd下的分解驱动程序将传入的块地址转换为内存节点的虚拟地址，目标页面复制到RNIC的MR并向内存节点发出相应的RDMA请求。</p><p>当对远程内存的请求过多时，由于页面错误处理、I/O 放大和上下文切换的开销，基于页面的系统会遭受性能下降[16]。</p><h3 id="基于对象的存储池——KVS"><a href="#基于对象的存储池——KVS" class="headerlink" title="基于对象的存储池——KVS"></a>基于对象的存储池——KVS</h3><p>基于对象的内存分解系统[11-16]使用自己的数据库（例如键值存储（KVS））直接干预RDMA数据传输。基于对象的系统为主机和内存节点端创建两个MR，每个MR处理缓冲区数据和提交/完成队列（SQ/CQ）。采用KV哈希表，其条目指向相应的（远程）内存对象。每当应用程序发出Put（或Get）请求时，系统会将相应的值放入主机的缓冲区MR中，并通过RDMA写入SQ MR的远程端来提交它。由于内存节点不断轮询SQ MR，因此它可以识别该请求。然后，内存节点通过RDMA将值复制到其缓冲区MR，并通过写入主机的CQ MR来完成请求。</p><p>这种方法受到限制，因为它需要对遗留应用程序进行大量的源代码级修改。而且访问不是直接的，是通过提交完成队列间接交互的。</p><h3 id="CXL优势"><a href="#CXL优势" class="headerlink" title="CXL优势"></a>CXL优势</h3><p>FlexBus指的是CXL定义的互连架构。CXL.mem允许主机通过PCIe总线直接访问底层内存,而不是通过像PCIe这样的外设互连协议。（CXL基于PCI Express 5.0物理层，速度高达 32GT/s。）</p><p>CXL最初的设计目的是实现跨不同处理器复合体的异构管理的卓越性，但业界和学术界都预计其缓存一致性能力可以帮助提高内存利用率并以低延迟缓解内存过度配置[20-22]。</p><h2 id="3-其他学者解决这个问题的思路和缺陷"><a href="#3-其他学者解决这个问题的思路和缺陷" class="headerlink" title="3. 其他学者解决这个问题的思路和缺陷"></a>3. 其他学者解决这个问题的思路和缺陷</h2><p>我们可以根据现有的内存分解运行时如何管理主机和内存服务器之间的数据，将其大致分为两种不同的方法：i）基于页面和ii）基于对象。基于页面的方法[4-10]利用虚拟内存技术来使用分解内存，而无需更改代码。在发生页面错误时，它通过网络将驻留在主机本地DRAM上的页面缓存数据与远程内存系统进行交换。另一方面，基于对象的方法使用自己的数据库（例如键值存储）从远程处理分解内存，而不是利用虚拟内存系统[11-16]。这种方法可以解决地址转换带来的挑战（例如，页面错误、上下文切换和写放大），但它需要大量的源代码级别修改和接口更改。</p><p>虽然有很多变体，但所有现有方法都需要通过远程直接内存访问（RDMA）[4,5,11–13,15,16]（或类似的细粒度网络接口 [7,9,10,17]）将数据从远程内存移动到主机内存。此外，它们甚至需要管理主机或内存节点中本地缓存的数据。不幸的是，数据移动及其伴随操作（例如页面缓存管理）引入了冗余内存副本和软件结构干预，这使得分解内存的延迟比本地DRAM访问的延迟长多个数量级。</p><h2 id="4-解决了什么问题"><a href="#4-解决了什么问题" class="headerlink" title="4. 解决了什么问题"></a>4. 解决了什么问题</h2><p>尽管CXL展现出以低成本和高性能实现内存分解的巨大潜力，但它尚未投入生产，并且没有平台将内存集成到内存池网络中。基于CXL的内存分解与直接访问。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="5-1架构设计"><a href="#5-1架构设计" class="headerlink" title="5.1架构设计"></a>5.1架构设计</h3><p>CXL设备设计和实现为每个模块都能够拥有许多带有自己的硬件控制器的DRAM DIMM。我们的CXL设备采用多个DRAM控制器，通过传统DDR接口连接DRAM DIMM。然后，其CXL控制器通过许多PCIe通道将内部DRAM模块暴露给FlexBus。</p><p>在当前架构中，设备的CXL控制器解析传入的基于PCIe的CXL数据包（称为CXL flits），将其信息（地址和长度）转换为DRAM请求，并使用DRAM控制器从底层DRAM为它们提供服务。</p><h3 id="5-2系统集成"><a href="#5-2系统集成" class="headerlink" title="5.2系统集成"></a>5.2系统集成</h3><p>图2显示了CXL设备的内部DRAM如何通过CXL映射（暴露）到主机的内存空间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/62b69a72893c4da1a015bbc2c157204c.png"><br>主机CPU的系统总线包含一个或多个CXL根端口（RP），它们连接一个或多个作为端点（EP）设备的CXL设备。</p><ol><li>我们的主机端内核驱动程序首先通过PCIe事务查询基址寄存器（BAR）及其内部存储器（称为主机管理设备存储器（HDM））的大小来枚举CXL设备。</li><li>根据检索到的大小，内核驱动程序将BAR和HDM映射到主机的保留系统内存空间中，并让底层CXL设备知道它们的BAR和HDM（基地址）在主机系统内存中的映射位置。</li><li>当主机CPU通过load/store指令访问HDM系统内存时，请求会被传递到相应的RP，RP将请求转换为CXL flit。由于HDM映射到系统内存的不同位置，因此HDM的内存地址空间与EP内部DRAM的内存地址空间不同。因此，CXL控制器通过简单地从中扣除HDM的基地址来转换传入的地址，并将转换后的请求发送到底层DRAM控制器。结果通过CXL交换机和FlexBus返回至主机。请注意，由于HDM访问没有软件干预或内存数据副本，因此DIRECTCXL可以以较低的访问延迟向主机公开CXL设备的内存资源。</li></ol><p>（意思是通过这一步，主机系统映射CXL.mem的地址了。）</p><h3 id="5-3CXL网络交换机"><a href="#5-3CXL网络交换机" class="headerlink" title="5.3CXL网络交换机"></a>5.3CXL网络交换机</h3><p>图3a说明了DIRECTCXL如何使用一个或多个CXL设备从主机中分解内存资源，图3b显示了其中我们的CXL交换机组织。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1788e055a03e4a3d94cc4cae173984f9.png"></p><p>主机的CXL RP直接连接到CXL交换机或CXL设备的上游端（USP）。CXL交换机的下游端口（DSP）还连接另一个CXL交换机的USP或CXL设备。请注意，我们的CXL交换机采用多个USP和DSP。通过设置内部路由表，我们的CXL交换机的结构管理器（FM）重新配置交换机的交叉开关，将每个USP连接到不同的 DSP，从而创建从根（主机）到终端（CXL设备）的虚拟层次结构。由于CXL设备可以采用一个或多个控制器和许多DRAM，因此它还可以定义多个逻辑设备，每个逻辑设备将其自己的HDM暴露给主机。这样，不同的主机可以连接到CXL交换机和CXL设备。请注意，每个CXL虚拟层次结构仅提供从一个到另一个的路径，以确保没有主机共享HDM。</p><h3 id="5-4程序怎么看CXL的内存"><a href="#5-4程序怎么看CXL的内存" class="headerlink" title="5.4程序怎么看CXL的内存"></a>5.4程序怎么看CXL的内存</h3><p>与RDMA相比，一旦主机和CXL设备之间建立了虚拟层次结构，主机上运行的应用程序就可以通过引用HDM的内存空间直接访问CXL设备。然而，它需要软件运行时/驱动程序来管理底层CXL设备并在应用程序的内存空间中公开其HDM。因此，我们支持DIRECTCXL 运行时，它将HDM的地址空间简单地分成多个段，称为 <code>cxl-namespace</code>。然后允许应用程序将每个CXL命名空间作为内存映射文件（mmap）进行访问。</p><p>为啥是映射文件？？？？</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c6fa0871c9e64650a0a23a410649c60c.png"></p><p>图4显示了运行时的软件堆栈以及应用程序如何通过cxl命名空间使用分解内存。当检测到CXL设备时（在PCIe枚举时），DIRECTCXL驱动程序会创建一个入口设备（例如 <code>/dev/directcxl</code>）以允许用户通过<code>ioctl</code>管理<span class="label primary">cxl命名空间</span>。如果用户向<code>/dev/directcxl</code>请求cxl命名空间，则驱动程序会通过引用其HDM段表来检查HDM上的（物理）连续地址空间，该表的条目包括段的偏移量、大小和引用计数（记录如何许多cxl命名空间指示此段）。由于多个进程可以访问该表，因此其标头还保留必要的信息，例如自旋锁、读/写锁和表条目摘要（例如，有效条目号）。一旦DIRECTCXL驱动程序根据用户请求分配了一个段，它就会为mmap创建一个设备（例如 <code>/dev/cxl-ns0</code>）并更新段表。用户程序可以使用<code>mmap</code>和<code>vm_area_struct</code>将<code>cxl-namespace</code>映射到其进程虚拟内存空间。</p><p>请注意，DIRECTCXL软件运行时是为直接访问CXL设备而设计的，这与持久内存开发工具包（PMDK[25]）的内存映射文件管理的概念类似。然而，它的命名空间管理比PMDK更简单、更灵活。例如，PMDK的命名空间与NVMe命名空间的理念非常相似，由文件系统或具有固定大小的DAX进行管理[26]。相比之下，我们的cxl命名空间更类似于传统的内存段，它直接暴露给应用程序而无需使用文件系统。</p><h3 id="5-5原型实现"><a href="#5-5原型实现" class="headerlink" title="5.5原型实现"></a>5.5原型实现</h3><p>图5a说明了我们用于分解内存资源的CXL网络拓扑设计，实际系统中的相应实现如图5b所示。有n台计算主机通过CXL交换机连接m台CXL设备；在我们的原型中，n和m为4，但这些数字可以通过使用更多CXL开关来扩展。具体来说，每个CXL设备原型均基于我们定制的附加卡（AIC）CXL内存刀片构建，该刀片采用16nm FPGA和8个不同的DDR4 DRAM模块（64GB）。在FPGA中，我们制造了一个CXL控制器和八个DRAM控制器，每个控制器管理CXL端点和内部DRAM通道。到目前为止，还没有支持CXL的处理器架构，我们还使用RISC-V ISA构建自己的内部主机处理器，该处理器采用四个乱序核心，<span class="label primary">其末级缓存（LLC）实现CXL RP</span>。每个支持CXL的主机处理器都在高性能数据中心加速卡中实现，充当主机的角色，可以单独运行Linux 5.13和DIRECTCXL。我们通过PCIe背板向四台主机公开四个CXL设备（32个DRAM模块）。我们通过多一个实现DIRECTCXL的CXL交换机的加速器卡来扩展背板。该交换机实现了FM，可以创建多个虚拟层次结构，每个虚拟层次结构以灵活的方式连接主机和CXL设备。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/eed160e3d6e0444aacccb166c4ab9ab5.png"></p><p>处理器端的CXL引擎和CXL交换机还没有商业化的CXL2.0 IP。因此，我们从头开始构建了所有DIRECTCXL IP。主机端处理器需要高级配置和电源接口（ACPI[27]）来进行CXL2.0枚举（例如RP位置和RP的保留地址空间）。由于RISC-V尚不支持 ACPI，因此我们通过将此类信息添加到设备树中来启用CXL枚举[28]。</p><blockquote><p>咦~</p></blockquote><h2 id="6-积累"><a href="#6-积累" class="headerlink" title="6. 积累"></a>6. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> RDMA </tag>
            
            <tag> CXL </tag>
            
            <tag> 内存分解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>cgroup和namespace</title>
      <link href="/2023/10/29/%E5%B0%8F%E5%AE%9E%E9%AA%8C-cgroup%E9%99%90%E5%88%B6%E5%92%8Cnamespace/"/>
      <url>/2023/10/29/%E5%B0%8F%E5%AE%9E%E9%AA%8C-cgroup%E9%99%90%E5%88%B6%E5%92%8Cnamespace/</url>
      
        <content type="html"><![CDATA[<h2 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1. 介绍"></a>1.1. 介绍</h2><p>内核控制组<code>cgroups</code>是一项内核功能，允许为<strong>进程</strong>分配和限制硬件和系统资源。</p><p><strong>创建</strong>：在使用<code>systemd</code>（systemd是System Management Daemon的简写（在UNIX系统中，后台进程都按照惯例以d结尾，因此当你看到一个进程的名字以d结尾，那它极有可能是一个后台进程），它是Linux系统中启动的第一个进程）的操作系统中，<code>/sys/fs/cgroup</code>目录都是<strong>由systemd在系统启动的过程中挂载</strong>的，并且挂载为只读的类型（创建cgroup除了使用cgroup的工具的命令以外，还可以直接在目录里创建相应memory或者cpu的目录，之后默认的cgroup文件会在里面自动生成，但是有的情况下这样的方式创建是不被允许不能成功的即使是sudo）。</p><p><strong>结构</strong>：每个进程都分配有一个管理cgroup。cgroup以分层树结构排序。可以为单个进程或层次结构的整个分支设置资源限制。使用cgroups将所有进程组织成组，这称为切片slice，slice是一棵树或者一棵子树。scope和service是slice中的一个节点，scope是以资源管理为目的的比如我有一组不太相关但是都需要被限制CPU使用的进程；service是一组相关的进程一起工作，形成一个服务单元。默认情况下, systemd会自动创建slice, scope和service单位的层级,来为cgroup树提供统一结构。</p><p><strong>记账</strong>：记账（Accounting）功能是指cgroups可以跟踪和记录组内任务（进程或线程）使用资源的情况。收集有关特定进程组（cgroup）使用的CPU时间、内存占用、磁盘I/O等方面的信息。这有助于识别资源密集型任务，管理系统负载，并进行资源分配和优化。记账具有相对较小但非零的开销，其影响取决于工作负载。请注意，打开一个单元的记账也会隐式地为直接包含在同一切片中的所有单元及其所有父切片和单元打开记账。直接包含在其中，因此会计成本不属于单个单位。</p><p><strong>内核配置</strong>：每个控制组子系统都依赖于相关的配置选项。例如，cpuset子系统应通过CONFIG_CPUSETS内核配置选项启用，io子系统应通过CONFIG_BLK_CGROUP内核配置选项启用等。要启用cgroups中的memory子系统,需要在内核配置时将CONFIG_MEMCG选项设置为y. All of these kernel configuration options may be found in the General setup → Control Group support menu.</p><h2 id="1-2-diff-V1-V2"><a href="#1-2-diff-V1-V2" class="headerlink" title="1.2. diff V1 V2"></a>1.2. diff V1 V2</h2><p>/usr/src/linux/Documentation/admin-guide/cgroup-v1<br>/usr/src/linux/Documentation/admin-guide/cgroup-v2.rst.</p><p>cgroup v1 主要包括以下几个概念：</p><ul><li>subsystem（子系统）：内核模块，具体的资源控制器，可以被关联到cgroup树。不同资源的控制器不同，例如，内存子系统控制内存资源，CPU子系统控制CPU资源。</li><li>cgroup（控制组）：资源隔离的最小单位，表示一组进程和cgroup子系统的关联，例如：通过内存子系统限制一组进程的可用内存资源总量。进程可以加入某个cgroup，也可以从一个cgroup迁移到另一个cgroup，同一进程不能同时存在同类型的两个cgroup中。</li><li>hierarchy（层级）：由一系列cgroup按照树状结构排列，每个节点都是一个cgroup，子cgroup默认继承父cgroup的参数和配置。系统可以有多个层级（cgroup树），每个层级可以和不同的subsystem关联，每个subsystem只能和一个层级关联，同一进程可以属于多个层级，但在每个层级中只能属于一个cgroup节点。</li></ul><p>在cgroup v2中，去掉了层级（hierarchy）的概念，只有一个层级，所有cgroup在该层级中以树形的方式组织，每个cgroup可以管理多种资源。在cgroup v1中，每种资源一个层级。对于bg和adhoc两个cgroup，bg需要限制blkio和memory两种资源，adhoc需要限制memory和pids两种资源。cgroup v1场景的视图如下：<br><img src="https://pic3.zhimg.com/80/v2-8097cbafea04c6cd4a3325600a435d12_1440w.webp"></p><p>对于cgroup v2，由于所有资源都在同一个cgroup下管理，通过cgroup.sub_controller控制子cgroup启用的controller，这样一来，视图以cgroup为单位，更加清晰和便于管理。<br><img src="https://pic1.zhimg.com/80/v2-6afa56ea78c4af88ffaea2057690f0c4_1440w.webp"></p><p>在cgroup v2中，每个cgroup目录下有个名为cgroup.controllers的可读文件，记录了当前cgroup启用的controller。根目录cgroup.controllers文件的内容记录了当前系统支持的所有controller。</p><p>在cgroup v2中，每个cgroup目录下有个名为cgroup.controllers的可读文件，记录了当前cgroup启用的controller。根目录下cgroup.controllers文件的内容记录了当前系统支持的所有controller。修改父cgroup的cgroup.subtree_control，增加cgroup controller：</p><pre class="line-numbers language-none"><code class="language-none">echo "+memory" &gt; test/cgroup.subtree_controlcat test/child/cgroup.controllersmemory<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>删除子cgroup中的controller，也通过修改父cgroup的cgroup.subtree_control实现：</p><pre class="line-numbers language-none"><code class="language-none">echo "-memory" &gt; test/cgroup.subtree_controlcat test/child/cgroup.controllers<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>当前cgroup中存在进程时，写入不会成功；写入当前cgroup没有启用的controller时，不会成功。</p><p><a href="https://zhuanlan.zhihu.com/p/643942399">参考</a></p><h2 id="1-3-Unified-Control-Group-Hierarchy"><a href="#1-3-Unified-Control-Group-Hierarchy" class="headerlink" title="1.3. Unified Control Group Hierarchy"></a>1.3. Unified Control Group Hierarchy</h2><p>节点存在以下面3种cgroups模式，选择不同模式,会影响到用户空间的 cgroup使用与兼容性：</p><ul><li>legacy：只支持cgroup V1</li><li>hybrid：同时支持cgroup V1和cgroup V2</li><li>unified：只支持cgroup V2又叫Unified Control Group Hierarchy </li></ul><p>判断目前是哪种模式<a href="https://zhuanlan.zhihu.com/p/643942399">参考</a>：</p><pre class="line-numbers language-none"><code class="language-none">[ $(stat -fc %T /sys/fs/cgroup/) = "cgroup2fs" ] &amp;&amp; echo "unified" || ( [ -e \/sys/fs/cgroup/unified/ ] &amp;&amp; echo "hybrid" || echo "legacy")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>使用unified模式（注意cgroup_no_v1=all）：</p><pre class="line-numbers language-none"><code class="language-none">GRUB_CMDLINE_LINUX="cgroup_enable=memory \systemd.unified_cgroup_hierarchy=1 \systemd.legacy_systemd_cgroup_controller=0 cgroup_no_v1=all"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用 legacy 模式：</p><pre class="line-numbers language-none"><code class="language-none">GRUB_CMDLINE_LINUX="cgroup_enable=memory \systemd.unified_cgroup_hierarchy=0 \systemd.legacy_systemd_cgroup_controller=1"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用 hybrid 模式：</p><pre class="line-numbers language-none"><code class="language-none">GRUB_CMDLINE_LINUX="cgroup_enable=memory \systemd.unified_cgroup_hierarchy=1 \systemd.legacy_systemd_cgroup_controller=1"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>主要是开启GRUB_CMDLINE_LINUX= “systemd.unified_cgroup_hierarchy=yes”才能使用V2版本，有时候内核可能没有默认。</p><p>以及貌似在内核编译时也有选项：</p><pre class="line-numbers language-none"><code class="language-none"># Only unified hierarchy is supportedCONFIG_CGROUP_UNIFIED_SYSTEMD=y<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="2-1-创建一个有内存限制的cgroup"><a href="#2-1-创建一个有内存限制的cgroup" class="headerlink" title="2.1.创建一个有内存限制的cgroup"></a>2.1.创建一个有内存限制的cgroup</h2><p>为了创建、管理和监控cgroup，我们需要另一个名为cgroup-tools的包<code>sudo apt install cgroup-tools</code></p><p>创建内存类型的cgroup命名为memhog-limiter<code>sudo cgcreate -g memory:memhog-limiter</code>实际操作是创建了一个目录。<code>ls -la /sys/fs/cgroup/memhog-limiter/</code></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4acc17383e0e4fb1b4cd544b495488f1.png" alt="系统不同目录位置会不同"></p><p>设置内存限制，这里有新版的cgroup的软限制是memory.high,硬限制是menory.max<code>sudo cgset -r memory.max=50M memhog-limiter</code>设置完成后可以输出查看<code>cat  /sys/fs/cgroup/memhog-limiter/memory.max</code></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/100b93b3167145538f316f4a5fe29082.png"></p><h2 id="2-2-去限制内存"><a href="#2-2-去限制内存" class="headerlink" title="2.2. 去限制内存"></a>2.2. 去限制内存</h2><blockquote><p>memhog：用于测试系统内存管理的工具，它通常用于在Linux系统上进行内存压力测试。它的主要作用是 通过分配大量的内存来模拟内存压力。</p></blockquote><p>创建一个吃内存的脚本</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span><span class="token keyword">while</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token keyword">do</span> memhog 100M<span class="token punctuation">;</span> <span class="token assign-left variable">id</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span>pgrep memhog<span class="token variable">)</span></span> <span class="token punctuation">;</span> <span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$id</span>"</span> <span class="token punctuation">;</span> <span class="token function">sleep</span> <span class="token number">20</span><span class="token punctuation">;</span> <span class="token keyword">done</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>运行脚本<code>sudo cgexec -g memory:memhog-limiter ./memhog.sh</code>查看这个内存控制组里的pid<code>cat  /sys/fs/cgroup/memhog-limiter/cgroup.procs</code></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/98414239c59242a1b9a5d84302694121.png"></p><p>显然没有控制住，emmmm到底什么原因？后来发现还是得在grub设置<code>systemd.unified_cgroup_hierarchy=1</code>才行。</p><h2 id="2-3-创建namespace"><a href="#2-3-创建namespace" class="headerlink" title="2.3. 创建namespace"></a>2.3. 创建namespace</h2><p><code>sudo cgexec -g memory:memhog-limiter unshare -fp --mount-proc</code></p><p>To verify that you are in a new namespace<code>ps -ef</code></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token environment constant">UID</span>          PID    <span class="token environment constant">PPID</span>  C STIME TTY          TIME CMDroot           <span class="token number">1</span>       <span class="token number">0</span>  <span class="token number">0</span> 08:00 pts/0    00:00:00 <span class="token parameter variable">-bash</span>root          <span class="token number">12</span>       <span class="token number">1</span>  <span class="token number">0</span> 08:00 pts/0    00:00:00 <span class="token function">ps</span> <span class="token parameter variable">-ef</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>新命名空间中，只有bash作为PID 1运行。因此，现在启动的每个服务都将在这个cgroup上下文中启动。</p><p><code>./memhog.sh</code>应该也是被之前那样限制住的情况。</p><p><a href="https://facebookmicrosites.github.io/cgroup2/docs/create-cgroups.html">这是facebook关于cgroup2的文档，他们的创建方式不太一样，也可以试试</a></p><h2 id="2-4-namespace又是什么？"><a href="#2-4-namespace又是什么？" class="headerlink" title="2.4. namespace又是什么？"></a>2.4. namespace又是什么？</h2><p>Kernel space - 在某个特定namespace内,操作系统内核执行的内存区域。<br>User space - 在某个特定namespace内,用户模式程序执行的内存区域。</p><p>Namespace的类型有:<br>PID namespace - 进程ID的隔离Namespace<br>Network namespace - 网络设备和协议栈的隔离Namespace<br>Mount namespace - 文件系统挂载点的隔离Namespace<br>IPC namespace - 进程间通信的隔离Namespace<br>UTS namespace - 主机名和域名的隔离Namespace<br>User namespace - 用户和用户组的隔离Namespace</p><p>通过namespace技术,内核可以为不同的用户空间提供隔离的系统资源,增强安全性和容器化。所以在讨论用户空间和内核空间时,需要明确是在哪个namespace环境下。比如docker容器内的用户空间和内核空间就与宿主机隔离开来。</p><p>命名空间是Linux内核用来隔离内核资源的方式。让一些进程只能看到和自己相关的一部分资源。两组感觉不到对方存在。<br><a href="https://zhuanlan.zhihu.com/p/593351621">更多实验参考</a></p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>x86架构内核调试环境搭建QEMU+GDB</title>
      <link href="/2023/10/29/Linux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BAQEMU-GDB/"/>
      <url>/2023/10/29/Linux%E5%86%85%E6%A0%B8%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BAQEMU-GDB/</url>
      
        <content type="html"><![CDATA[<h2 id="1-起因"><a href="#1-起因" class="headerlink" title="1. 起因"></a>1. 起因</h2><p>太多文章是基于arm架构在讲，而基于x86架构的大部分都是最小根文件的内核调试，而对内核修改的范围往往比这个大。此文记录这些困扰的解决方案。</p><h2 id="2-最小根文件的编译环境"><a href="#2-最小根文件的编译环境" class="headerlink" title="2. 最小根文件的编译环境"></a>2. 最小根文件的编译环境</h2><ol><li>编译选项里有的还需要加上调试信息</li><li>需要一个根文件系统来运行刚刚编译好的内核，运行busybox会在_install目录下得到一个最小的文件系统的一部分，还需要进行下面的操作：<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">rsync</span> <span class="token parameter variable">-ar</span> busybox-1.33.0/_install/* rootfs<span class="token builtin class-name">cd</span> rootfs<span class="token function">sudo</span> <span class="token function">chown</span> root:root * <span class="token parameter variable">-R</span><span class="token function">sudo</span> <span class="token function">mkdir</span> <span class="token parameter variable">-p</span> proc sys dev etc/init.d<span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">622</span> dev/console c <span class="token number">5</span> <span class="token number">1</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">666</span> dev/null c <span class="token number">1</span> <span class="token number">3</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">666</span> dev/zero c <span class="token number">1</span> <span class="token number">5</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">666</span> dev/ptmx c <span class="token number">5</span> <span class="token number">2</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">666</span> dev/tty c <span class="token number">5</span> <span class="token number">0</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">444</span> dev/random c <span class="token number">1</span> <span class="token number">8</span><span class="token function">sudo</span> <span class="token function">mknod</span> <span class="token parameter variable">-m</span> <span class="token number">444</span> dev/urandom c <span class="token number">1</span> <span class="token number">9</span><span class="token function">sudo</span> <span class="token function">chown</span> root:tty dev/<span class="token punctuation">{</span>console,ptmx,tty<span class="token punctuation">}</span><span class="token function">sudo</span> <span class="token function">sh</span> <span class="token parameter variable">-c</span> <span class="token string">'echo "#!/bin/sh\nmount -t proc none /proc\nmount -t sysfs none /sys\nln -s /dev/null /dev/tty2\nln -s /dev/null /dev/tty3\nln -s /dev/null /dev/tty4\nexit 0" &gt; etc/init.d/rcS'</span><span class="token function">sudo</span> <span class="token function">chmod</span> a+x etc/init.d/rcS<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol> <div class="note info no-icon">但是我的要求应该不止最小根文件系统，还需要更丰富的根文件系统。对于4.15用于采样的那个内核最小根文件系统cgroup文件夹就是没法挂载上的。一些可能有效的方法：- 2.1在创建文件镜像的时候加入那些目录。- 2.2使用-append "init=/xx.sh"写一个脚本告诉内核启动阶段需要创建的目录和文件       但这两种方式不一定能行，因为这里面的文件应该是内核自己挂载上去的才对，最初img里面的文件只有一层。而有趣的是最小的根文件系统创建的时候，有4个文件创建空文件夹，由内核启动时挂载上去就好。当需要更多文件系统的功能时，需要自己从内核文件去拿编译了的，并且创建新的文件映射。详见下面这部分的文件有没有被改。</div> <p>这里参考的链接如下,谢谢慷慨的前辈们：<br><a href="https://www.cnblogs.com/haiyonghao/p/14440249.html">利用qemu+gdb在ubuntu下搭建调试kernel的环境</a><br><a href="https://blog.cyyself.name/setup-linux-kernel-debug-environment/">Linux内核调试环境的搭建QEMU+GDB</a><br><a href="https://www.ebpf.top/post/qemu_gdb_busybox_debug_kernel/">使用GDB+Qemu调试Linux内核</a>  </p><ol start="3"><li>启动命令<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">qemu-system-x86_64 <span class="token punctuation">\</span><span class="token parameter variable">-kernel</span> /home/yi/Downloads/kernel/longterm/linux-5.4.259/arch/x86_64/boot/bzImage <span class="token punctuation">\</span><span class="token parameter variable">-nographic</span> <span class="token punctuation">\</span><span class="token parameter variable">-append</span> <span class="token string">"nokaslr console=ttyS0 root=/dev/sda rw"</span> <span class="token punctuation">\</span><span class="token parameter variable">-drive</span> <span class="token assign-left variable">file</span><span class="token operator">=</span>/home/yi/Documents/kernel.img,format<span class="token operator">=</span>raw,id<span class="token operator">=</span>hd0 <span class="token punctuation">\</span><span class="token parameter variable">-m</span> 8G <span class="token punctuation">\</span><span class="token parameter variable">-s</span> <span class="token punctuation">\</span> <span class="token parameter variable">-S</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><h2 id="3-更丰富的调试环境"><a href="#3-更丰富的调试环境" class="headerlink" title="3. 更丰富的调试环境"></a>3. 更丰富的调试环境</h2><p><span class="label primary">Ubuntu系统提供debootstrap工具快速创建指定架构的根文件系统。</span></p><h3 id="3-1-根文件系统的制作"><a href="#3-1-根文件系统的制作" class="headerlink" title="3.1 根文件系统的制作"></a>3.1 根文件系统的制作</h3><p>只使用最小的根文件系统是很有局限性的，很多调试的工具都没法用，现在试图做一个更丰富的根文件。<br>这些文件来自<a href="https://github.com/figozhang/runninglinuxkernel_5.0">笨叔的奔跑吧</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">cat</span> rootfs_debian_x86_64.part0* <span class="token operator">&gt;</span> rootfs_debian_x86_64.tar.xz<span class="token function">sudo</span> <span class="token function">tar</span> Jxf rootfs_debian_x86_64.tar.xz<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance<span class="token function">sudo</span> <span class="token function">cp</span> <span class="token parameter variable">-a</span> rootfs_debian_x86_64/* rootfsAdvance/<span class="token function">sudo</span> <span class="token function">rm</span> <span class="token parameter variable">-rf</span> /home/yi/Documents/rootfs_debian_x86_64<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-2-编译内核"><a href="#3-2-编译内核" class="headerlink" title="3.2 编译内核"></a>3.2 编译内核</h3><p>进入编译内核的那个文件里</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">make</span> <span class="token parameter variable">-j20</span><span class="token function">sudo</span> <span class="token function">make</span> modules_install <span class="token parameter variable">-j</span> <span class="token number">20</span><span class="token function">make</span> headers_install<span class="token function">make</span> <span class="token parameter variable">-s</span> kernelrelease<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>得到结果5.4.259qemulearn</p><h3 id="3-3-完善准备要制作的文件系统"><a href="#3-3-完善准备要制作的文件系统" class="headerlink" title="3.3 完善准备要制作的文件系统"></a>3.3 完善准备要制作的文件系统</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance/usr/src/linux/<span class="token function">rm</span> /home/yi/Documents/rootfsAdvance/lib/modules/5.4.259qemulearn/build<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance/lib/modules/5.4.259qemulearn/build<span class="token function">cp</span> <span class="token parameter variable">-a</span> include /home/yi/Documents/rootfsAdvance/usr/src/linux/<span class="token function">cp</span> Makefile .config Module.symvers System.map /home/yi/Documents/rootfsAdvance/usr/src/linux/<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance/usr/src/linux/arch/x86/<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance/usr/src/linux/arch/x86/kernel/<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /home/yi/Documents/rootfsAdvance/usr/src/linux/scripts<span class="token function">cp</span> <span class="token parameter variable">-a</span> arch/x86/include /home/yi/Documents/rootfsAdvance/usr/src/linux/arch/x86/<span class="token function">cp</span> <span class="token parameter variable">-a</span> arch/x86/Makefile /home/yi/Documents/rootfsAdvance/usr/src/linux/arch/x86/<span class="token function">cp</span> <span class="token parameter variable">-a</span> scripts /home/yi/Documents/rootfsAdvance/usr/src/linux/<span class="token function">ln</span> <span class="token parameter variable">-s</span> /usr/src/linux /home/yi/Documents//lib/modules/5.4.259qemulearn/build<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-4-制作流程"><a href="#3-4-制作流程" class="headerlink" title="3.4 制作流程"></a>3.4 制作流程</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">dd</span> <span class="token assign-left variable">if</span><span class="token operator">=</span>/dev/zero <span class="token assign-left variable">of</span><span class="token operator">=</span>/home/yi/Documents/rootfsAdvance.ext4 <span class="token assign-left variable">bs</span><span class="token operator">=</span>1M <span class="token assign-left variable">count</span><span class="token operator">=</span><span class="token number">8192</span>mkfs.ext4 /home/yi/Documents/rootfsAdvance.ext4<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> tmpfs<span class="token function">sudo</span> <span class="token function">mount</span> <span class="token parameter variable">-t</span> ext4 /home/yi/Documents/rootfsAdvance.ext4 tmpfs/ <span class="token parameter variable">-o</span> loop<span class="token function">sudo</span> <span class="token function">cp</span> <span class="token parameter variable">-af</span> /home/yi/Documents/rootfsAdvance/* tmpfs/<span class="token function">umount</span> tmpfs<span class="token function">chmod</span> <span class="token number">777</span> /home/yi/Documents/rootfsAdvance.ext4<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-5-启动"><a href="#3-5-启动" class="headerlink" title="3.5 启动"></a>3.5 启动</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">sudo</span> qemu-system-x86_64 <span class="token parameter variable">-m</span> 10G<span class="token punctuation">\</span><span class="token parameter variable">-nographic</span> <span class="token parameter variable">-smp</span> <span class="token number">10</span> <span class="token punctuation">\</span><span class="token parameter variable">-kernel</span> /home/yi/Downloads/kernel/longterm/linux-5.4.259/arch/x86_64/boot/bzImage <span class="token punctuation">\</span><span class="token parameter variable">-append</span> <span class="token string">"console=ttyS0 root=/dev/sda rw"</span> <span class="token punctuation">\</span><span class="token parameter variable">-hda</span> /home/yi/Documents/rootfsAdvance.ext4 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-gdb断点调试"><a href="#4-gdb断点调试" class="headerlink" title="4. gdb断点调试"></a>4. gdb断点调试</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">在有vmlinux的目录下进入gdb<span class="token function">file</span> vmlinuxtarget remote:1234hb start_kernel <span class="token comment">#如果断点不停止，第一个断点需要下硬断点而不是软断点;同时还需要在启动命令加上nokaslr关闭地址随机化</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/03c49184e12d49609da4d119d33ab0a2.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/dd4051a0e5474b1ba326ad35d6641735.png"></p><h2 id="5-vscode连接上虚拟机"><a href="#5-vscode连接上虚拟机" class="headerlink" title="5. vscode连接上虚拟机"></a>5. vscode连接上虚拟机</h2><p>需要安装一个gdb debug的扩展<br><a href="https://github.com/Dreamacro/clash/releases">用到的网络工具</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token builtin class-name">cd</span> clash-linux-amd64-v1.18.0/<span class="token function">chmod</span> +x clash-linux-amd64 <span class="token function">cat</span> my_paid_config.yaml <span class="token operator">&gt;</span> ~/.config/clash/config.yaml<span class="token function">sudo</span> <span class="token function">mv</span> clash-linux-amd64 /usr/local/bin/clashclash <span class="token parameter variable">-d</span> ~/.config/clash<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/69931823d9f94936adb5ba782301da2f.jpeg" alt="配置网关"></p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> qemu </tag>
            
            <tag> gdb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>watermark</title>
      <link href="/2023/10/27/watermark/"/>
      <url>/2023/10/27/watermark/</url>
      
        <content type="html"><![CDATA[<p>内存回收机制主要依赖于三个字段LOW MIN HIGH</p><p>watermark有以下两种主要实现方式:<br>地址watermark - 在物理内存的某个地址处写入一个特殊的值,这个地址就是watermark。当内存使用量达到watermark地址时,就表示内存已使用到一定程度,可能需要采取相应措施(如结束某些进程释放内存等)。<br>值watermark - 内存中有一个变量用于存储一个数值,这个数值就是watermark。它可以表示当前内存已使用的百分比或者剩余内存的大小。当该值达到事先设置的阈值时,就触发相应的内存调节机制。</p><p>在x86 Linux上,内存watermark主要采用预先设置好的物理地址来反映内存使用状态,根据地址是否被占用来决定是否需要内存回收或进程终止。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> watermark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>通过PTE统计页面访问次数</title>
      <link href="/2023/10/26/%E9%80%9A%E8%BF%87PTE%E7%BB%9F%E8%AE%A1%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AE%E6%AC%A1%E6%95%B0/"/>
      <url>/2023/10/26/%E9%80%9A%E8%BF%87PTE%E7%BB%9F%E8%AE%A1%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AE%E6%AC%A1%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>分析系统内存访问的方法之一：页面表条目(PTE)位跟踪。操作系统可以清空PTE中的访问(A)位和脏(D)位,硬件页表遍历器会设置这些位。通过定期重置这些位并检查A位的状态,操作系统可以判断页面是否在重置间隔内被访问过。</p><h3 id="A位被置位的时刻？"><a href="#A位被置位的时刻？" class="headerlink" title=" A位被置位的时刻？"></a><span class="label primary"> A位被置位的时刻？</span></h3><p>每当访问一个页面时，<strong>硬件</strong>会<strong>自动</strong>将A位设置为1。这意味着，当CPU访问一个页面时，硬件会检测到这个访问并将A位从0设置为1，表示页面已经被访问。</p><h3 id="A位置位被清除的时刻？"><a href="#A位置位被清除的时刻？" class="headerlink" title=" A位置位被清除的时刻？"></a><span class="label primary"> A位置位被清除的时刻？</span></h3><ul><li>操作系统可以定期检查A位的状态，通常是通过一种叫做”页面表条目位跟踪”（Page Table Entry (PTE) Tracking）的方法来实现。</li><li>将一个页面从物理内存中置换到磁盘上时，通常会清零该页面的A位。</li></ul><h3 id="会不会存在A位还没有置0的情况下，页面又被访问了？"><a href="#会不会存在A位还没有置0的情况下，页面又被访问了？" class="headerlink" title=" 会不会存在A位还没有置0的情况下，页面又被访问了？"></a><span class="label primary"> 会不会存在A位还没有置0的情况下，页面又被访问了？</span></h3><p>是的，如果在上一次访问后,PTE的A bit还没有被操作系统重置为0,然后页面又被访问了,那么硬件仍会将A bit置1。</p><h3 id="struct-page里有-count和-mapcount，这个和PTE的A位有什么关系吗？"><a href="#struct-page里有-count和-mapcount，这个和PTE的A位有什么关系吗？" class="headerlink" title="struct page里有_count和_mapcount，这个和PTE的A位有什么关系吗？"></a><span class="label primary">struct page里有_count和_mapcount，这个和PTE的A位有什么关系吗？</span></h3><p>_count字段表示这个物理页框被多少个虚拟页映射和共享。它反映了一个页框的引用数量。<br>_mapcount字段表示这个页框被多少个PTE映射。<br>也就是说:_count和_mapcount表示页面的映射关系,而PTE中的A位表示页面的访问状态。struct page和PTE中的信息反映了不同的内存管理需要。<br>两者也有联系,比如在回收内存页时,只有_mapcount为0时,表示没有PTE指向该页,才可以安全回收。</p><p>基于4.15内核通过PTE统计页面访问次数代码流程图：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6660cc59f6994938a01bd9ff25569339.png"></p><p>为了保证每个进程里内存映射的独立进行，所以每个进程都会有独立的页表，而页表的起始地址就存放在 struct mm_struct 结构中的 pgd 属性中。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不同的总线和内存条组成</title>
      <link href="/2023/10/26/%E4%B8%8D%E5%90%8C%E7%9A%84%E6%80%BB%E7%BA%BF/"/>
      <url>/2023/10/26/%E4%B8%8D%E5%90%8C%E7%9A%84%E6%80%BB%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>一些关于DDR-T更多的信息在论文Characterizing the Performance of Intel Optane Persistent Memory——A Close Look at its On-DIMM Buffering有被讨论。</p></blockquote><h2 id="内存条组成"><a href="#内存条组成" class="headerlink" title="内存条组成"></a>内存条组成</h2><p>搬运和修改自<a href="https://www.cnblogs.com/yilang/p/11103061.html#:~:text=1%20%E6%8C%89%E5%AD%97%E7%BC%96%E5%9D%80%EF%BC%9A%20%E5%AF%B9%E4%BA%8E%E8%BF%99%E4%B8%AA256M%E5%86%85%E5%AD%98%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%AE%83%E7%9A%84%E5%AF%BB%E5%9D%80%E8%8C%83%E5%9B%B4%E6%98%AF64M%EF%BC%8C%E8%80%8C%E6%AF%8F%E4%B8%AA%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%E5%8F%AF%E4%BB%A5%E5%AD%98%E5%82%A832bit%E6%95%B0%E6%8D%AE%E3%80%82,2%20%E6%8C%89%E5%8D%8A%E5%AD%97%E7%BC%96%E5%9D%80%EF%BC%9A%E5%AF%B9%E4%BA%8E%E8%BF%99%E4%B8%AA256M%E5%86%85%E5%AD%98%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%AE%83%E7%9A%84%E5%AF%BB%E5%9D%80%E8%8C%83%E5%9B%B4%E6%98%AF128M%EF%BC%8C%E8%80%8C%E6%AF%8F%E4%B8%AA%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%E5%8F%AF%E4%BB%A5%E5%AD%98%E5%82%A816bit%E6%95%B0%E6%8D%AE%E3%80%82%203%20%E6%8C%89%E5%AD%97%E8%8A%82%E7%BC%96%E5%9D%80%EF%BC%9A%E5%AF%B9%E4%BA%8E%E8%BF%99%E4%B8%AA256M%E5%86%85%E5%AD%98%E6%9D%A5%E8%AF%B4%EF%BC%8C%E5%AE%83%E7%9A%84%E5%AF%BB%E5%9D%80%E8%8C%83%E5%9B%B4%E6%98%AF256M%EF%BC%8C%E8%80%8C%E6%AF%8F%E4%B8%AA%E5%86%85%E5%AD%98%E5%9C%B0%E5%9D%80%E5%8F%AF%E4%BB%A5%E5%AD%98%E5%82%A88bit%E6%95%B0%E6%8D%AE%E3%80%82">内存条的组成、编址、寻址和读写方式</a></p><ul><li>DIMM是英文Dual In-line Memory Module的缩写,意思是双列直插内存模块。DIMM模块有一个小的印刷电路板,上面焊接有一系列的内存芯片,并在电路板的两侧有一排金属接触针,可以直插在主板的内存插槽上。</li><li>CPU与内存之间的接口位宽是64bit，也就意味着CPU在一个时钟周期内会向内存发送或从内存读取64bit的数据。单个内存颗粒的位宽仅有4bit、8bit或16bit，个别也有32bit的。因此，必须把多个颗粒并联起来，组成一个位宽为64bit的数据集合，才可以和CPU互连。生产商把64bit集合称为一个RANK或Physical RANK。</li><li>Chip（Memory）存储芯片，又叫内存颗粒。真正提供存储的器件。</li><li>Bank:Chip里包含多个Bank。一个Bank就是一个存储矩阵库。由Chip的BA线的位宽决定个数。</li><li>Channel在计算机系统架构中指的是内存通道（Memory Channel）。内存通道的数量决定了CPU能够同时访问内存的带宽。双通道（Dual Channel）则有两个通道同时工作,可以同时读取两个内存模块的数据。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/13a2a5c123234fd0a99f23734c36edc0.png"></p><p>bank再往下分就是实际存储单位元的电路，一般来说横向选择排数的线路称为row（row enable, row select, word line），纵向负责传送信号的线路称为column（bitline），每组bank的下方还会有个row buffer（sense amplifer），负责将读出的row内容暂存。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f81976ab0534432a990e976c8a82df59.png"><br>物理地址要包含行地址、列地址，bank地址、rank信息等，直到定位到cell。</p><h2 id="CXL3-0"><a href="#CXL3-0" class="headerlink" title="CXL3.0"></a>CXL3.0</h2><p><a href="https://www.computeexpresslink.org/_files/ugd/0c1418_a8713008916044ae9604405d10a7773b.pdf">大部分内容来自这篇文章，作者是Debendra Das Sharma博士Intel院士兼I/O技术和标准总监；Ishwar Agarwal首席硬件工程师兼微软Azure硬件架构联合主席……</a><br>Compute Express Link™是一种<strong>开放式行业标准互连协议</strong>，可在主机处理器和加速器、内存缓冲区和智能I/O设备等设备之间提供高带宽、低延迟连接。它旨在通过支持异构处理和内存系统以及人工智能、机器学习、分析、云基础设施、网络和边缘云化、通信系统和高性能计算等领域的应用来解决不断增长的<strong>高性能计算工作负载</strong>。基于PCI Express®的I/O语义之上的一致性和内存语义，可在不断发展的使用模型中优化性能。这一点变得越来越重要，因为在这些新兴应用中处理数据需要在CPU、GPU、FPGA、智能NIC和其他加速器中部署标量、矢量、矩阵和空间架构的多样化组合。</p><p>CXL1.0于2019年3月首次推出，支持一组丰富的协议之间的动态复用，包括I/O（<strong>CXL.io</strong>基于PCIe）、缓存（<strong>CXL.cache</strong>）和内存（<strong>CXL.memory</strong>）语义。CXL在主机处理器和连接的CXL设备上的任何内存之间维护统一、一致的内存空间。这允许CPU和设备共享资源并在同一内存区域上运行，以提高性能、减少数据移动并降低软件堆栈复杂性（比如GPU和CPU用内存就挺麻烦的）。</p><p>基于CXL的行业成功和接受度（180多家成员公司的积极参与证明），CXL2.0于2020年11月发布，支持更多使用模型，同时保持与CXL1.1和CXL1.0的完全向后兼容性。CXL2.0在三大方面增强了CXL1.1的体验：支持单级交换机、支持持久化内存、内存池和安全性。这些功能使平台中的许多设备能够迁移到CXL，同时保持与PCIe5.0的兼容性以及CXL的低延迟特性。</p><p>CXL3.0基于PCIe6.0技术，将传输速率提高一倍至64GT/s，且与前代产品相比没有额外的延迟。这使得3.0版本产生了更多用例：</p><ul><li>内存池和共享：CXL3.0对CXL2.0中首次引入的内存池进行了重大增强。内存池能够将CXL附加内存视为可替代资源，可以根据需要灵活地分配和释放到不同的服务器（也称为节点或主机）。这使得系统设计人员无需过度配置机架中的每台服务器，同时获得最佳性能。图3显示了CXL2.0内存池的示例。在CXL3.0中，除了内存池之外，我们还引入了内存共享的概念。内存共享是使用硬件一致性在主机之间一致共享CXL连接内存的能力。因此，与内存池不同，内存共享允许给定的内存区域可由多个主机同时访问，并且仍然保证每个主机都能看到该位置的最新数据，而不需要软件管理的协调。这使得系统设计能够构建机器集群，通过共享内存结构来解决大型问题。图4显示了共享和池化内存拓扑的示例。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cb97d276823f47bab80034187b4c5df0.png" alt="图3"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/da32e99101d141769a8cf4a94cb9d77d.png" alt="图4"></p><div class="note info no-icon">在CXL协议中,定义了不同类型的设备:- CXL Type-1: Cache-Coherent (CC) Accelerator,如GPU- CXL Type-2: Memory Expander,如DRAM- CXL Type-3: IO and Memory Semantic (IOMS) Device,如SSD</div> <ul><li>Fabric：CXL3.0首次引入了Fabric功能，超越了PCIe和前几代CXL传统的基于树的架构结构。图5显示了CXL结构的非树形拓扑示例。CXL结构最多可支持4096个节点，这些节点可以使用称为基于端口的路由（PBR）的新的可扩展寻址机制相互通信。这里，节点可以是CPU主机、带或不带内存的CXL加速器、PCIe设备或Global Fabric Attached Memory（GFAM）设备。GFAM设备类似于传统的CXL Type-3设备，不同之处在于它可以使用基于端口的路由以灵活的方式被多个节点（最多4095个）访问。这种架构为构建强大的系统开辟了可能性，该系统由计算和内存元素组成，以满足特定工作负载的需求。图6显示了CXL Fabrics的几个示例用例。</li></ul><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/76fbf37cdd6849e299c4752a0db65012.png" alt="图5"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/50588c04fc3c467fb36f03d6ab091518.png" alt="图6 ML加速器和GFAM设备（左）、具有共享内存和NIC的HPC分析（右）"></p><div class="note primary no-icon">从2.0和3.0来看和内存设备的链接有直连和交换机链接。Direct Connect Mode允许CXL设备像本地PCIe设备一样直接连接在主机端口上,而不需要通过CXL Switch。避免了CXL Switch可能带来的额外跳数和延迟。对于一些对延迟较敏感的应用,Direct Connect Mode可以带来更好的性能。缺点是损失了CXL Switch的灵活性和端口虚拟化能力。</div> <p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d03c1f5fd8f84fd49eb3dfd02c7e85c3.png" alt="the Computer Architecture and Memory Systems Laboratory（CAMEL）at KAIST"></p><p>这个实验室还讲这个原型与只使用DRAM扩展内存做了一系列比较：<a href="https://camel.kaist.ac.kr/public/camel-cxl-memory-pooling.pdf">A Technology Brief by CAMEL KAIST</a>（但是个人觉得他对实验结果解释那部分不够详细，有的点不太懂）</p><div class="note warning no-icon">当前支持CXL的产品大多还停留在CXL1.0版本,CXL2.0的支持还比较有限</div> <h2 id="DDR（双数据速率）"><a href="#DDR（双数据速率）" class="headerlink" title="DDR（双数据速率）"></a>DDR（双数据速率）</h2><p>DDR是一种用于系统内存（RAM）的接口和技术。它允许CPU（中央处理器）与系统内存之间进行高速数据传输。DDR标准有多个版本，例如DDR3、DDR4和DDR5，每个版本都有不同的数据传输速率和技术规格。DDR主要用于存储和读取计算机内存中的数据，以供CPU使用。CPU通过DDR接口来访问内存中的程序和数据。DDR内存通常以多通道方式工作，其中每个通道可以连接一个或多个DRAM（Dynamic Random Access Memory）内存条。在这样的配置中，数据可以在不同的内存条之间分散存放，这被称为多通道内存架构，总带宽就是各通道带宽的总和。</p><h2 id="DDR-T"><a href="#DDR-T" class="headerlink" title="DDR-T"></a>DDR-T</h2><p>Intel Optane Persistent Memory使用DDR-T与CPU传输数据。特别设计用于大型数据中心和需要大量内存的高性能计算环境。DDR-T内存模块通常具有非常高的存储容量，通常以TB（Terabytes，千兆字节）为单位来衡量，因此得名为”Terabyte”。这使得DDR-T非常适合大规模数据分析、虚拟化、人工智能和其他内存密集型应用。</p><h2 id="PCIe（Peripheral-Component-Interconnect-Express）"><a href="#PCIe（Peripheral-Component-Interconnect-Express）" class="headerlink" title="PCIe（Peripheral Component Interconnect Express）"></a>PCIe（Peripheral Component Interconnect Express）</h2><p>PCIe是一种用于连接各种外部设备和扩展卡的高速总线标准。它通常用于连接图形卡、网络适配器、存储控制器、声卡等外部硬件设备。PCIe提供了高带宽和低延迟的数据传输通道，使外部设备能够与CPU和系统内存进行快速通信。PCIe通常不用于内存访问，而是用于连接外部设备，以扩展计算机的功能。</p><h2 id="UPI"><a href="#UPI" class="headerlink" title="UPI"></a>UPI</h2><p>跨NUMA节点间数据的传输。</p><h2 id="ISA"><a href="#ISA" class="headerlink" title="ISA"></a>ISA</h2><p>X86体系结构下有ISA总线.</p><p>ZONE_DMA：用于那些无法对全部物理内存进行寻址的硬件设备，进行DMA时的内存分配。例如前边介绍的ISA设备只能对物理内存的前16M进行寻址。该区域的长度依赖于具体的处理器类型。</p><p>内核态虚拟内存空间的前896M区域是直接映射到物理内存中的前896M区域中的，直接映射区中的映射关系是一比一映射。映射关系是固定的不会改变。<strong>直接映射区的前16M专门让内核用来为DMA分配内存，这块16M大小的内存区域我们称之为ZONE_DMA。</strong> X86体系结构下ISA总线的直接内存存取控制器叫DMA。16M到896M（不包含896M）这段区域，我们称之为 <strong>ZONE_NORMAL</strong>。</p><h2 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h2><p>这个和ZONE_DMA没有直接关系的！DMA（Direct Memory Access直接内存访问）技术</p><p>允许外围设备在不涉及中央处理器（CPU）的情况下将数据传输到系统内存或从系统内存传输数据。DMA是一种通用的机制，可以在各种数据传输场景中使用，包括文件拷贝。</p><p>如下图，一般文件读写在page cache使用buffer IO如果不命中有2次上下文切换和3次拷贝。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/57d7f3138355481fabd774aa88d22ba9.png"></p><p>部分应用使用direct IO可以减少拷贝。这取决于你的程序是否存在局部性原理可以充分利用page cache.</p><h2 id="DAX"><a href="#DAX" class="headerlink" title="DAX"></a>DAX</h2><p>允许应用程序直接将数据从内存传输到存储设备。绕过传统的文件系统栈，允许应用程序通过Direct Memory Access (DMA)直接与存储设备进行交互，特别是与非易失性内存（NVM）设备一起使用。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CXL </tag>
            
            <tag> bus </tag>
            
            <tag> DDR </tag>
            
            <tag> PCIe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>X86架构下64位的用户空间和内核空间</title>
      <link href="/2023/10/26/Linux%2064%E4%BD%8D%E7%9A%84%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/"/>
      <url>/2023/10/26/Linux%2064%E4%BD%8D%E7%9A%84%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%92%8C%E5%86%85%E6%A0%B8%E7%A9%BA%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么需要虚拟内存？"><a href="#为什么需要虚拟内存？" class="headerlink" title="为什么需要虚拟内存？"></a><span class="label primary">为什么需要虚拟内存？</span></h2><ol><li>内存隔离，使每个运行的进程都认为它拥有整个系统的内存。在多进程系统中直接操作物理内存地址的话，我们需要精确地知道每一个变量的位置都被安排在了哪里，而且还要注意当前进程在和多个进程同时运行的时候，不能共用同一个地址，否则就会造成地址冲突。</li><li>延迟内存分配,只有当页面真正被访问时,才需要从磁盘加载到物理内存。</li><li>页式内存管理技术。</li><li>通过与磁盘空间的交换实现扩展内存容量。</li></ol><h2 id="虚拟地址怎么认？"><a href="#虚拟地址怎么认？" class="headerlink" title="虚拟地址怎么认？"></a><span class="label primary">虚拟地址怎么认？</span></h2><p>在32位机器上，指针的寻址范围为 $2^{32}$，所能表达的虚拟内存空间为4GB（$2^{30}$是从B到GB）。在目前的64位系统下只使用了48位来描述虚拟内存空间，寻址范围为 $2^{48}$，所能表达的虚拟内存空间为256TB（字节可寻址，一个地址的大小是Byte）。</p><ul><li>低128T表示用户态虚拟内存空间，虚拟内存地址范围为：0x0000 0000 0000 0000 - 0x0000 7FFF FFFF F000 。</li></ul><p>1左移47位（二进制到16进制4位一组）得到的地址是 0x0000800000000000，然后减去一个 PAGE_SIZE（默认为 4K），就是0x00007FFFFFFFF000，共 128T。</p><ul><li>高128T表示内核态虚拟内存空间，虚拟内存地址范围为：0xFFFF 8000 0000 0000 - 0xFFFF FFFF FFFF FFFF 。</li></ul><p>0x[用户/内核态位][47位偏移量][保留位][12位页内偏移].用户空间的虚拟内存地址的高16位全部为0.内核空间的虚拟内存地址的高16位全部为1.</p><p>这样一来就在用户态虚拟内存空间与内核态虚拟内存空间之间形成了一段0x0000 7FFF FFFF F000 - 0xFFFF 8000 0000 0000 的地址空洞，我们把这个空洞叫做canonical address空洞。高16位不全为0也不全为1.</p><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/4956918c43e186d49df7b9802f080de8.png"></p><h2 id="64位Linux系统下，进程虚拟内存空间布局是怎样的？"><a href="#64位Linux系统下，进程虚拟内存空间布局是怎样的？" class="headerlink" title="64位Linux系统下，进程虚拟内存空间布局是怎样的？"></a><span class="label primary">64位Linux系统下，进程虚拟内存空间布局是怎样的？</span></h2><p>在进程的地址空间中，包括heap, stack和通过mmap匿名映射的区域，这些区域在建立的时候只有虚拟地址，当它们真正被访问到的时候，内核才会为其分配物理页面，这种方式被称作demand allocation（按需分配），它和page cache的demand paging的概念是对应的这部分物理页面由于没有对应外部存储介质上的文件，因此被统称为anonymous pages。</p><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/532e6cdf4899588f8b873b6435cba2d8.png"></p><ol><li>准备运行的程序编译成二进制文件存放在磁盘中，CPU会执行二进制文件中的机器码来驱动进程的运行。所以在进程运行之前，这些存放在二进制文件中的机器码需要被加载进内存中，而用于存放这些机器码的虚拟内存空间叫做<strong>代码段</strong>。</li><li>在代码段跟数据段的中间还有一段不可以读写的保护段，它的作用是防止程序在读写数据段的时候越界访问到代码段，这个保护段可以让越界访问行为直接崩溃，防止它继续往下运行。</li><li>程序代码中我们通常会定义大量的全局变量和静态变量，这些全局变量在程序编译之后也会存储在二进制文件中，在程序运行之前，这些全局变量也需要被加载进内存中供程序访问。<strong>指定了初始值的全局变量和静态变量在虚拟内存空间中的存储区域我们叫做数据段。没有指定初始值的全局变量和静态变量在虚拟内存空间中的存储区域叫做BSS段。</strong>因为内核知道这些数据是没有初值的，所以在二进制文件中只会记录BSS段的大小，在加载进内存时会生成一段0填充的内存空间。</li><li>程序在运行期间往往需要动态的申请内存，所以在虚拟内存空间中也需要一块区域来存放这些动态申请的内存，这块区域就叫做堆。堆空间中地址的增长方向是从低地址到高地址增长。内核中使用start_brk标识堆的起始位置，brk标识堆当前的结束位置。当堆申请新的内存空间时，只需要将brk指针增加对应的大小，回收地址时减少对应的大小即可。比如当我们通过malloc向内核申请很小的一块内存时（128K之内），就是通过改变brk位置实现的。</li><li>程序在运行过程中还需要依赖动态链接库，这些动态链接库以.so文件的形式存放在磁盘中，比如C程序中的glibc，里边对系统调用进行了封装。glibc库里提供的用于动态申请堆内存的malloc函数就是对系统调用sbrk和mmap的封装。这些动态链接库也有自己的对应的代码段，数据段，BSS段，也需要一起被加载进内存中。还有用于内存文件映射的系统调用mmap，会将文件与内存进行映射，那么映射的这块内存（虚拟内存）也需要在虚拟地址空间中有一块区域存储。这些都被放在<strong>文件映射与匿名映射区</strong>。</li><li>在程序运行的时候总该要调用各种函数吧，那么调用函数过程中使用到的局部变量和函数参数也需要一块内存区域来保存。这一块区域在虚拟内存空间中叫做<strong>栈</strong>。</li></ol><h2 id="虚拟内存是怎么被组织的？"><a href="#虚拟内存是怎么被组织的？" class="headerlink" title="虚拟内存是怎么被组织的？"></a><span class="label primary">虚拟内存是怎么被组织的？</span></h2><blockquote><p>可以通过<code>cat /proc/pid/maps</code>或者<code>pmap pid</code>来查看某个进程的实际虚拟内存布局。</p></blockquote><p>这涉及到具体的数据结构和红黑树，不是这篇blog重点，下面放一个概要图。</p><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/f28079c8b8fbf1853570302bee2a1929.png"></p><h2 id="ZONE类型与内核空间"><a href="#ZONE类型与内核空间" class="headerlink" title="ZONE类型与内核空间"></a>ZONE类型与内核空间</h2><ol><li>内核空间是所有进程共享的，不同进程进入内核态之后看到的虚拟内存空间全部是一样的。</li><li>内核态虚拟内存空间的前896M区域是直接映射到物理内存中的前896M区域中的，直接映射区中的映射关系是一比一映射。映射关系是固定的不会改变。<strong>直接映射区的前16M专门让内核用来为DMA分配内存，这块16M大小的内存区域我们称之为ZONE_DMA。</strong> X86体系结构下ISA总线的直接内存存取控制器叫DMA。16M到896M（不包含896M）这段区域，我们称之为 <strong>ZONE_NORMAL</strong>。</li><li>物理内存896M以上的区域被内核划分为<strong>ZONE_HIGHMEM</strong>区域，我们称之为高端内存。这块动态映射区内核是使用 vmalloc 进行内存分配,将不连续的物理内存映射到连续的虚拟内存上。<br><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/0bd4766b19d043bb4aebdd06bdf8e67c.png"></li></ol><p><img src="https://images.weserv.nl/?url=https://cdn.xiaolincoding.com//mysql/other/e1f2e689c2754b2af540c6d0b6ab327f.png"></p><p>内核页面属于逻辑zone的那个不可移动类的。</p><h2 id="与用户态内核态有什么联系？"><a href="#与用户态内核态有什么联系？" class="headerlink" title="与用户态内核态有什么联系？"></a>与用户态内核态有什么联系？</h2><ul><li>内核空间用于存放操作系统内核的代码和数据，具有高特权级别，只能由内核态的代码访问。</li><li>用户空间用于存放用户应用程序的代码和数据，具有较低的特权级别，只能由用户态的代码访问。</li></ul><p>这种分离允许操作系统保护其核心功能并确保系统的稳定性和安全性。用户应用程序需要通过系统调用来与内核进行通信和访问系统资源。</p><h2 id="页表是物理页和虚拟页的纽带"><a href="#页表是物理页和虚拟页的纽带" class="headerlink" title="页表是物理页和虚拟页的纽带"></a>页表是物理页和虚拟页的纽带</h2><p>页表也是放在物理页中的。因为页表也是可以被放在LLC之类的靠近CPU的地方缓存的，大内存使用后page walk的开销对DRAM5-6次的访问发生次数到底是不是瓶颈？加上页面被动态迁移，就算是再大的内存，热的部分在DRAM里了，LLC可以存那个区域的PDE，这样更减少了page walk。</p><p>page fault的发生是建立物理页和虚拟页对应的重要途径。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9e06fd08fdf24e6e8d72c2d4be13531a.jpeg"></p><h2 id="分配虚拟页就是vma是malloc和mmap在做，但是他们区别是什么呢？"><a href="#分配虚拟页就是vma是malloc和mmap在做，但是他们区别是什么呢？" class="headerlink" title="分配虚拟页就是vma是malloc和mmap在做，但是他们区别是什么呢？"></a><span class="label primary">分配虚拟页就是vma是malloc和mmap在做，但是他们区别是什么呢？</span></h2><p>PS. 文章根据<a href="https://mp.weixin.qq.com/s?__biz=MzUxODAzNDg4NQ==&amp;mid=2247522087&amp;idx=2&amp;sn=fe8f4cd34d68e0a10658dee88bd337df&amp;chksm=f98dd38dcefa5a9ba43a9d1ac96852532f53278915a6f6b9f5187b8c1c885c1e5848ebabbc86#rd">公众号@bin的技术小屋</a>做了一定增删。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kernelspace </tag>
            
            <tag> userspace </tag>
            
            <tag> namespace </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux出现文件很少但磁盘容量已满的情况</title>
      <link href="/2023/10/26/Linux%E5%87%BA%E7%8E%B0%E6%96%87%E4%BB%B6%E5%BE%88%E5%B0%91%E4%BD%86%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F%E5%B7%B2%E6%BB%A1%E7%9A%84%E6%83%85%E5%86%B5/"/>
      <url>/2023/10/26/Linux%E5%87%BA%E7%8E%B0%E6%96%87%E4%BB%B6%E5%BE%88%E5%B0%91%E4%BD%86%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F%E5%B7%B2%E6%BB%A1%E7%9A%84%E6%83%85%E5%86%B5/</url>
      
        <content type="html"><![CDATA[<p>虚拟机磁盘显示99%的容量，但是我的文件只有2.5G，虚拟机一共100G，其他容量来自哪里？<br>使用ncdu磁盘分析工具<code>sudo ncdu /</code>，这会显示磁盘上的文件和目录，按大小排序。可以导航到占用大量空间的目录，并查看哪些文件或子目录占用了大量的磁盘空间。</p><p><code>/var/log/syslog</code>有64.5G，清空syslog文件<code>sudo truncate -s 0 /var/log/syslog</code></p><p>为避免将来再次出现类似问题，配置日志轮换（log rotation）定期删除旧的日志数据并限制日志文件的大小。syslog使用logrotate工具来进行日志轮换。编辑logrotate配置文件，<code>/etc/logrotate.conf</code>或<code>/etc/logrotate.d/</code>目录中。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">/var/log/syslog <span class="token punctuation">{</span>    size 100M       rotate <span class="token number">10</span>    compress    delaycompress    missingok    notifempty    create <span class="token number">644</span> root root<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>限制syslog文件的大小为100MB，保留最多10个旧日志文件，并启用压缩以节省空间。<br>很多日志都有一个.conf文件都能用这个方式限制大小。</p>]]></content>
      
      
      <categories>
          
          <category> Basics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ncdu </tag>
            
            <tag> log rotation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transparent memory offloading in datacenters</title>
      <link href="/2023/10/15/Transparent-memory-offloading-in-datacenters/"/>
      <url>/2023/10/15/Transparent-memory-offloading-in-datacenters/</url>
      
        <content type="html"><![CDATA[<p><a href="https://facebookmicrosites.github.io/cgroup2/docs/pressure-metrics.html">这些策略已经在现在的内核中使用了</a></p><h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary no-icon"><ul><li>文章来自ASPLOS’22四篇Best Paper之一 （但是这个公司和密西根大学之后合作的一篇内容相关性还挺高的）</li><li>TMO: Transparent Memory Offloading in Datacenters</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li> Johannes Weiner, Niket Agarwal, Dan Schatzberg, etc. Mate Inc.</li><li> Dimitrios Skarlatos, 卡内基梅隆大学CMU</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>机器学习等新兴应用对内存需求的大幅增长，加上DRAM设备扩展速度的放缓[19,25] 以及DRAM成本的大幅波动，使得DRAM作为唯一的内存容量解决方案变得昂贵得令人望而却步。</p><p>近年来，大量非DRAM更便宜的内存技术，例如NVMe SSD[8,11]和NVM[14,17,24,30,34,37]已成功部署在数据中心，或正在使用它们方式。此外，新兴的非DDR内存总线技术，例如CXL[9]提供类似内存的访问语义和接近DDR的性能。这些趋势的融合为内存分层带来了过去不可能的新机会[12,16,21,31–33,39,40]。</p><p>使用压缩内存可以大幅降低成本，但这仍然不够，我们需要NVMe SSD等替代内存技术来进一步降低成本。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>性能不变的情况下，节约内存。TMO根据设备的性能特征和应用程序对内存访问速度减慢的敏感度，自动调整要卸载到异构设备（例如压缩内存或SSD）的内存量。 </p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><blockquote><p>zswap是一项Linux内核的虚拟内存压缩功能，可为将要交换的页面提供压缩回写缓存。当内存页将要交换出去时，zswap不将其移动到交换设备，而是对其执行压缩，然后存储到系统RAM内动态分配的内存池中。回写到实际交换设备的动作则会延迟，甚至能完全避免，从而显著减少Linux系统用于交换的I/O；副作用则是压缩所需的额外CPU周期。</p></blockquote><p>唯一已知的大规模采用内核驱动交换用于延迟敏感的数据中心应用程序的是Google的zswap部署[18][43].在本文的其余部分中将其称为g-swap。作为先驱，g-swap极大地推进了现有技术水平，但仍然存在几个主要局限性:仅支持单个慢速内存层；某些应用程序的数据难以压缩，例如具有量化字节编码值的机器学习模型。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>TMO需要回答两个问题：要卸载多少内存以及要卸载哪些内存。<br>（1）页面会冷多久：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4d3acd6320e244119f41bdab3ffe1a62.png"></p><blockquote><p>service-level agreement SLA 网站服务可用性的一个保证<br>为了简化数据中心中应用程序的操作，需要使用大量内存来启用微服务并提供基础设施级功能。我们将软件包、分析、日志记录以及与数据中心应用程序部署相关的其他支持功能所需的内存定义为<strong>数据中心内存税</strong>。这部分占20%,最先被卸载，因为内存税的SLA比程序直接消耗的SLA少。</p></blockquote><p>（2）不同程序文件页和匿名页所占比比例不同<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/08366eac5f724234bd566fd6c704ff40.png"></p><p>（3）考虑到卸载到的SSD耐久性不同SSD设备的延迟等，设计架构<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/35bf4ada06834da894f7c606466dbe8d.png"></p><p>图6显示了TMO架构概述（左）以及内存和存储布局（右）。</p><ol><li>未修改的工作负载在容器1内执行，并通过系统调用和分页与内核内存管理子系统交互。 </li><li>Senpai是一个用户空间组件，负责控制内存卸载过程并决定应从每个工作负载中卸载多少内存。Senpai以压力信息的形式监控应用程序性能下降。</li><li>压力信息由内核中的压力失速信息（PSI）组件报告。</li><li>基于PSI信息Senpai通过写入cgroup控制文件来驱动卸载过程，从而触发内核的内存回收逻辑。回收逻辑决定要卸载哪些内存。</li><li>内存管理子系统向PSI模块暴露内存压力信息。</li><li>5触发对卸载后端和常规文件系统的读写操作。<br>TMO支持卸载到基于zswap的压缩内存池或通过交换存储设备。</li></ol><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/35bf4ada06834da894f7c606466dbe8d.png" alt="总体设计图"></p><h2 id="5-1-PSI有效捕获内存卸载对不同应用程序的影响"><a href="#5-1-PSI有效捕获内存卸载对不同应用程序的影响" class="headerlink" title="5.1 PSI有效捕获内存卸载对不同应用程序的影响"></a>5.1 PSI有效捕获内存卸载对不同应用程序的影响</h2><p>每个非闲置进程，PSI会进一步区分进程可运行的时间段和因资源不足而停滞的时间段。将计算潜力定义为以CPU数量为上限的非闲置进程数量。PSI=停滞数/计算潜力*100%. 对于容器和全系统域，PSI为每种资源引入了两个压力指标，分别称为 “some”和 “full”。some指标跟踪域内至少有一个进程在等待资源时停滞的时间百分比。full指标跟踪的是所有进程同时延迟的时间百分比。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0bdbab3ec366466d9c89cbbb2c0d1d0d.png"><br>虚线框是执行时间和停顿时间。执行时间标准化为100%并分为四个部分。some停顿蓝色箭头，full停顿绿色箭头。第一阶段A和B停止其中一个即可都有资源用some指标12.5%；第二阶段同时停止6.25%，full指标6.25%，some指标18.75%。“some”旨在捕获由于缺乏资源而导致各个进程增加的延迟，而“full”则表示容器或系统中完全非生产性的时间量。</p><div class="note danger">这个示例里执行时间归一化后为啥那个方块不是满的，就是25%没有拉满，难道是为了画图好看，听他解释应该是满的才对啊？黑色斜条纹块很weird</div> <p>PSI只出现在以下3个场景：<br>为了跟踪内存压力，PSI记录专门在内存不足时发生的事件所花费的时间。目前，这包括三种情况。第一种情况是当内存已满时进程触发回收页面并且进程尝试分配新页面。第二种情况是进程需要等待IO发生故障，即最近从文件缓存中逐出的页面出现重大故障。第三种情况是进程在从交换设备读取页面时发生阻塞。</p><div class="note primary">一种现有机制是驻留集大小（RSS），它反映了一个进程当前分配的物理内存量,包括该进程使用的代码、数据和堆栈等所占用的内存。RSS的主要限制是并不能捕捉到内存或缺乏内存对应用程序性能的影响。其他指标（例如升级率）代表每秒的swap-ins。提升率的一个缺点是它没有考虑卸载后端的性能特征。此外，由于卸载导致更多内存变得可用，它无法捕获应用程序性能的改进。<p>在PSI之前，操作员依赖于关联内核时间、应用程序吞吐量变化、回收活动事件计数器、文件重读和换入等间接指标来估计工作负载的资源健康状况。这需要对存储硬件设备特性和内核行为有直观的了解。例如，文件超前读取算法可能会在不同程度上使应用程序免受已记录的缓存重读的影响。另一方面，PSI可直接在进程级别测量合格停滞事件造成的生产力损失。它考虑了底层硬件的差异、内核内存管理算法的有效性，甚至工作负载的内部并发性（即部分停滞与完全停滞）。</p><p>所以这个指标反应资源总数变化的情况下，对每个进程性能的影响。仍然是对某个进程/程序，我按量分配DRAM，看他性能提高了，再给一点，<br>而且从热力图来看，热度的提升是有一定时间的。</p></div> <h2 id="5-2-Senpai"><a href="#5-2-Senpai" class="headerlink" title="5.2 Senpai"></a>5.2 Senpai</h2><p>每隔几秒，Senpai就会为每个cgroup计算一次要回收的内存量，如下所示：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bb712a3ad3724964a1443b7d3dcd8c05.png" alt="PSI确定内存压力，阈值根据实验动态调节，Senpai从LRU中执行卸载"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/59595c4f3cfc49699351c75a066e57ca.png"></p><p>𝑃𝑆𝐼𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑和 reclaim_ratio是可配置参数。 𝑐𝑢𝑟𝑟𝑒𝑛𝑡_𝑚𝑒𝑚 是cgroup的当前内存占用量。</p><p>动态工作负载在某些情况下可能会导致问题。例如，如果容器内存快速增长且其大小正在主动扩展，则它可能会被阻塞，直到 Senpai进一步提高其限制。为了解决这个问题，我们在内核中添加了一个无状态内存回收cgroup控制文件。通过这个控制钮，Senpai可以要求内核精确回收计算出的内存量，而不施加任何限制，从而避免了阻塞扩展工作负载的风险。（看不懂这个解决方案哩）</p><p>Senpai在导出内核之上的用户空间中以PSI和cgroup内存控制接口实现。与假设的内核内实现相比，这有几个优点。首先，用户空间可以完全访问浮点单元，因此可以更有效地执行计算。其次，用户空间组件的发布周期往往比内核快得多。计划在具有不同性能SLO阈值的工作负载中利用不同的Senpai配置。</p><h2 id="5-3-回收操作"><a href="#5-3-回收操作" class="headerlink" title="5.3 回收操作"></a>5.3 回收操作</h2><p>Senpai依靠内核从cgroups中回收冷内存页。Senpai让内核的回收算法选择要卸载的页面，而不是使用昂贵的全页表扫描来确定哪些内存页面是冷页面。该算法通过为文件和交换支持页面维护一对活动/非活动页面LRU列表，并首先回收较冷的页面来运行。这种机制经过生产测试，能以相对较低的CPU成本和较高的准确性识别较少使用的内存页面。在Senpai中使用这种机制大大简化了我们的实现过程。在生产中，Senpai驱动的回收只消耗所有CPU周期的0.05%，可以忽略不计。</p><p>但是有改进。由于很多文件只会被读一次，当文件缓存缺页时，换进来的Page会优先放在inactive list中。当再次读到这个页面时，按照soft fault的惯例，页框会从inactive 挪到active。Linux中使用radix tree来追踪页框，radix tree通过一种特殊的元素来追踪匿名内存被换到swap中的情况。作者扩充了这个特殊的元素，称之为shadow entry。当inactive list中的内容被换出时，有个计数器（workingset_time）会自增。每个页框被移除的时候，shadow entry中都会记录当前的workingset_time。也即当第一个被reclaim的页面对应的shadow entry记录0，第二个被reclaim的页面记录1，等等单调自增。</p><p>当某个被reclaim的页面发生PF被换回时，用当前的workingset_time减去shadow entry中记录的值可以得到页面被换出期间清理了多少页面，称之为refault distance。如果refault distance小于active list，则当前这次PF就被认为是refault。当发生refault时，内核就会开始倾向于回收匿名页。同时发生refault的页面会直接加入到active list并使active list 长度加一。当refault distance小于active list时，其实就说明这个页面使用的比较频繁，这时候我们就应该扩大一点active list把这个页面加到acitve list中，并且转向回收匿名内存。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>这篇文章主要是为了卸载冷数据，提出的指标很新颖也有效。而且这是关注内存和硬盘之间交互的，也挺有参考价值。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>迁移过程可以由应用程序本身、用户空间库[6,12,26,29]、内核或虚拟机管理程序驱动。</p><p>我们将内存卸载后端定义为保存卸载内存的慢速内存层。在我们当前的生产队列中，这由NVMe SSD和压缩内存池组成。未来我们预计这将包括NVM和CXL设备。</p><p>在跨卸载后端具有多样性的异构存储器系统中，给定的主要故障率（那些硬件计数器）可能在慢速存储设备上构成问题，而在较快存储设备上则无关紧要。一般来说，很难理解特定的内核事件是否是工作负载的功能问题。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring Memory Access Similarity to Improve Irregular Application Performance for Distributed</title>
      <link href="/2023/10/15/Exploring-Memory-Access-Similarity-to-Improve-Irregular-Application-Performance-for-Distributed/"/>
      <url>/2023/10/15/Exploring-Memory-Access-Similarity-to-Improve-Irregular-Application-Performance-for-Distributed/</url>
      
        <content type="html"><![CDATA[<h2 id="0-论文信息"><a href="#0-论文信息" class="headerlink" title="0.论文信息"></a>0.论文信息</h2><div class="note primary"><ul><li>文章来自IEEE Transactions on Parallel and Distributed Systems，TPDS，2023</li><li>Exploring Memory Access Similarity to Improve Irregular Application Performance for Distributed</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Wenjie Liu， Xubin He，美国宾夕法尼亚州费城天普大学计算机与信息科学系</li><li>Qing Liu，美国新泽西州纽瓦克市新泽西理工学院电气与计算机工程系</li></ul><h1 id="1-Background"><a href="#1-Background" class="headerlink" title="1.Background"></a>1.Background</h1><p>stacked DRAM and off-chip DRAM的异构。<br>并行工作模式使得在高性能集群上产生跨节点的不规则内存访问行为（memory access behaviors，<strong>MAB</strong>）。<br>但是执行共享功能或者数据结构时，节点间会产生相似的MAB。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/59b533c0f549441a95aade87ee7d645f.png"><br>工作节点之间内存访问相似性的示例。每个工作节点执行共享二进制代码的不同部分, 创建MAB的各种组合。如图所示,两个节点之间的相似性是通过重叠的MAB以及每个MAB覆盖的页面数量来估计的,例如,节点K和节点S共享相同的MAB并产生高相似性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0ea19701091942909a12b353a43e0b5f.png"><br>最常用的6个MAB所覆盖的内存页的细分。可以得出三个观察结果。首先, MAB,例如MAB 0,可以被所有节点共享为执行相同的二进制代码。<br>第二，在节点3.4.6.7.8.11.12.13上观察到相同的MAB组合;因为这样的节点可以执行相同的任务。第三, <del>MAB 5仅在节点14.上显示</del> ，并且大量页面被MAB 5覆盖,这意味着相似性不仅存在于节点之间，而且存在于节点内的多个页面之间。</p><h1 id="2-解决的问题（Motivation）"><a href="#2-解决的问题（Motivation）" class="headerlink" title="2.解决的问题（Motivation）"></a>2.解决的问题（Motivation）</h1><p>为了利用堆叠DRAM实现的高性能，设计高效混合存储器系统的核心是两个挑战。</p><ul><li>第一个挑战是确定运行时的最佳数据位置。</li><li>第二个挑战是减少元数据造成的开销。下图比较了现有技术的堆叠DRAM命中率和元数据导致的相应性能下降。如图所示，最先进的方法不断提高堆叠DRAM命中率。同时,对于不规则的HPC应用程序,元数据导致的性能开销显著增加，这降低了堆叠DRAM带来的性能增益。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/7c1d020bbf2d4c08b35440e4307a2647.png"></li></ul><h1 id="3-其他学者解决这个问题的思路和缺陷"><a href="#3-其他学者解决这个问题的思路和缺陷" class="headerlink" title="3.其他学者解决这个问题的思路和缺陷"></a>3.其他学者解决这个问题的思路和缺陷</h1><p>现有的方法通过利用每个<strong>数据块的访问历史</strong>[7] [8] [9]或操作系统[10] [11]提供的提示来识别热数据，这些提示忽略了由集群的并行工作模式启用的共享MAB,并且可能在集群环境中表现不佳。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span> C. C. Chou, A. Jaleel, and M. K. Qureshi, “CAMEO: A two-level memory organization with capacity of main memory and flexibility of hardware-managed cache,” <span class="token keyword">in</span> Proc. IEEE/ACM 47th Annu. Int. Symp. Microarchitecture, <span class="token number">2014</span>, pp. <span class="token number">1</span>–12.<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span> J. B. Kotra, H. Zhang, A. R. Alameldeen, C. Wilkerson, and M. T. Kandemir, “Chameleon: A dynamically reconfigurable heterogeneous memory system,” <span class="token keyword">in</span> Proc. IEEE/ACM 47th Annu. Int. Symp. Microarchitecture, <span class="token number">2018</span>, Art. no. <span class="token number">533</span>.<span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span> E. Vasilakis, V. Papaefstathiou, P. Trancoso, and I. Sourdis, “Hybrid2: Combining caching and migration <span class="token keyword">in</span> hybrid memory systems,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2020</span>, pp. <span class="token number">649</span>–662.<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span> A. Kokolis, D. Skarlatos, and J. Torrellas, “PageSeer: Using page walks to trigger page swaps <span class="token keyword">in</span> hybrid memory systems,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2019</span>, pp. <span class="token number">596</span>–608.<span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span> A. Prodromou, M. Meswani, N. Jayasena, G. Loh, and D. M. Tullsen, “MemPod: A clustered architecture <span class="token keyword">for</span> efficient and scalable migration <span class="token keyword">in</span> flat address space multi-level memories,” <span class="token keyword">in</span> Proc. IEEE Int. Symp. High Perform. Comput. Architecture, <span class="token number">2017</span>, pp. <span class="token number">433</span>–444.<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>针对不规则应用的优化。现有的研究针对不规则应用, 要么将不规则性暴露给作业调度器进行自适应调度[31] ,[32] ,要么利用不规则性在异构集群中进行更好的资源分配[33] [34]。Nozal等人提出在使用OneAPI框架执行不规则应用程序时，平衡分配给不同硬件的负载[31]。 此外, Dai等人在运行时检测不均匀分布的不规则工作负载, 并将不堪重负的节点的工作负载迁移到分配较少工作负载的节点[32]。另一方面,利用不规则性来细化集群中的资源分配。杨等人观察了集群中运行的不规则应用程序导致的随机和不规则网络流量,并提出利用数据压缩技术来最大限度地减少网络带宽消耗[34]。此外, Shin等人分析了GPU上运行的不规则应用程序导致的性能下降,并提出加快地址转换过程，以最大限度地减少SIMD指令的数据准备时间[33]。</p><p>系统中的相似性。相似之处存在于许多方面各方面的计算系统, 以及先前的工作探讨了相似性以提高系统性能。Koller等人提出了一种基于观察到的存储和访问数据的高度相似的I/O内容的I/O重复数据消除方法[35]。同时,Xiao等人利用高级程序中丰富的自相似性来加速芯片之间的数据通信[36]。刘等人利用集群的并行工作模式,提出了提高掉队性能的方法。</p><p>混合存储器系统。现有工程确定热点通过监视每个数据块的存储器访问行为或依赖于操作系统发现的信息来获取数据。通过监测每个数据块的内存访问行为, 可以使用获得的访问行为来识别下面访问的缓存行，并且可以执行准确的数据迁移/缓存[7]、[8]、[9]。另一方面,Prodromou等人提出利用操作系统观察到的内存访问行为，并使用多数元素算法来预测热页[1]。此外,研究人员将计算能力集成到堆叠的DRAM中，这进一步减少了内存流量,并为内存密集型应用提供了更高的效率[37] ,[38]。</p><h1 id="4-围绕该问题作者如何构建解决思路（Design）"><a href="#4-围绕该问题作者如何构建解决思路（Design）" class="headerlink" title="4.围绕该问题作者如何构建解决思路（Design）"></a>4.围绕该问题作者如何构建解决思路（Design）</h1><h2 id="4-1-Similarity-Monitor"><a href="#4-1-Similarity-Monitor" class="headerlink" title="4.1 Similarity Monitor"></a>4.1 Similarity Monitor</h2><p>提出了一种量化方法来测量节点之间的内存访问相似度，<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/97d94f0e504f4a95936dca253e9c8429.png">将来自所有节点的MAB聚合为全局内存访问行为向量(<strong>GMABV</strong>) ,生成每节点内存访问行为矢量(<strong>PMABV</strong>) ，并计算PMABV之间的距离作为相似性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f907559fea214cc1b9b62e61ce11e4ef.png"><br>比较两种共享方案，全局共享无法为不规则应用提供高的共享精度，因为并非每个MAB都对其他节点有帮助。相反,选择性共享按需共享MAB,这会给部署的节点带来性能开销。由于MAB共享不准确,这两种方法都存在性能损失，GS和SS的性能损失分别高达13.1%和15.4%， 这表明在不规则应用的情况下需要一种有效的MAB共享方案。</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># Similarity-Based Sharing</span>Input: Receiving access behavior AccBhv from <span class="token function">node</span> k<span class="token keyword">if</span> AccBhv <span class="token keyword">in</span> GMABV <span class="token keyword">then</span><span class="token keyword">if</span> <span class="token function">node</span> k belongs to a Similarity Group <span class="token keyword">then</span>Forward AccBhv to nodes within the group with global sharing<span class="token keyword">else</span>Perform selective sharing with all Low Similarity nodesend <span class="token keyword">if</span>Update metadata<span class="token keyword">else</span>Update PMABV , GMABV and Similarity Matrixend <span class="token keyword">if</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-2-Access-Behavior-Buffer"><a href="#4-2-Access-Behavior-Buffer" class="headerlink" title="4.2 Access Behavior Buffer"></a>4.2 Access Behavior Buffer</h2><p>根据SMABS的设计，节点之间只共享当前工作集的MAB，这减轻了每个节点上MAB监控带来的开销。此外，由于每个节点处理的大量结构化数据和HPC应用程序中丰富的循环，跨节点的多个内存页共享相同的MAB。因此，即使是单个共享MAB也可以描述相应节点内多个页面的访问行为。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c8a23de4e70141a0a79a68b7c4769a7d.png"><br>如图访问行为缓冲区的一个条目。状态和覆盖比特都确定新观察到的MAB是否将被发送到相似性监视器以进行相似性更新。而起始地址和覆盖页是用来快速识别内存请求的滑动窗口部分。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e6b261c2909546e593d09f8c527e1daf.png"></p><h2 id="4-3-Swap-Executor"><a href="#4-3-Swap-Executor" class="headerlink" title="4.3 Swap Executor"></a>4.3 Swap Executor</h2><p>为了利用访问行为缓冲区中缓冲的M4B,交换执行器用于通过使用共享M4B作为“配方”来执行底层混合存储器系统的数据放置。</p><h2 id="4-4-SM-HMS-Overview"><a href="#4-4-SM-HMS-Overview" class="headerlink" title="4.4 SM-HMS Overview"></a>4.4 SM-HMS Overview</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d1f03539af6148d385ff12d71c165eaf.png"></p><h1 id="5-从结果看，作者如何有力证明他解决了问题"><a href="#5-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="5.从结果看，作者如何有力证明他解决了问题"></a>5.从结果看，作者如何有力证明他解决了问题</h1><p>在模拟器上运行的，首先是作业完成时间比较：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/02a690c97d864595a981b18d199b8106.png"><br>之后是IPC比较：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/963bb9358e614c849013917e811a2781.png"><br>还讨论了相应的参数敏感性、堆叠命中率（不过这都是针对片内DRAM的了）延迟和带宽的收益应该是来自于共享吧。之后容量占用之类的数据都很好，不放图片了。</p><p>我们对由多达256个节点组成的各种集群配置上的一组不规则应用程序的评估结果表明， SM-HMS在完成时间减少方面优于最先进的方法Cameo、Chameleon和Hyrbid2 ， 分别减少了58.6%、56.7%和31.3%， 平均减少了46.1%、41 .6%和19.3%。与理想的混合存储系统相比， SM-HMS的效率高达98.6%(平均91.9% )。此外,在常规应用.上的实验表明，所提出的<br>SM-HMS有利于更广泛的HPC应用。</p><h1 id="6-缺陷和改进思路"><a href="#6-缺陷和改进思路" class="headerlink" title="6.缺陷和改进思路"></a>6.缺陷和改进思路</h1><p>这些最开始设置的5类MAB分别是啥？到底是怎么将访问行为表示为向量的，<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a85a47828a004423ad5a501188b6a2b2.png"></p><h1 id="7-创新点"><a href="#7-创新点" class="headerlink" title="7.创新点"></a>7.创新点</h1><p>应该就是利用相似性去做，减少了一些重复操作？从根本上讲, SMHMS区别于存在通过根据量化的内存访问相似性在工作节点之间共享内存访问行为, 并将共享的内存访问行为作为“配方”来执行准确和知情的缓存替换,从而提高了堆叠DRAM的利用率,并相应地降低了元数据开销。</p><h1 id="8-该论文的相关资料"><a href="#8-该论文的相关资料" class="headerlink" title="8.该论文的相关资料"></a>8.该论文的相关资料</h1><p>srds，中文名作者一般都不会公布什么资料。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> 堆叠DRAM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Exploring the Design Space of Page Management for Multi-Tiered Memory Systems</title>
      <link href="/2023/10/13/Exploring-the-Design-Space-of-Page-Management-for-Multi-Tiered-Memory-Systems/"/>
      <url>/2023/10/13/Exploring-the-Design-Space-of-Page-Management-for-Multi-Tiered-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h2><div class="note primary"><ul><li>文章来自USENIX Annual Technical Conference, (ATC), 2021</li><li>Exploring the Design Space of Page Management for Multi-Tiered Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn, （韩国）亚洲大学</li></ul><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>AutoNUMA用于平衡Numa节点间的内存访问，基本思想是：定期统计各进程的内存访问情况， 并unmapping pages，然后触发NUMA hinting fault，在page fault中重新均衡内存访问，目的是使运行 进程的CPU尽量访问本地节点上的内存，提升性能。</p><h1 id="Abstract摘要"><a href="#Abstract摘要" class="headerlink" title="Abstract摘要"></a>Abstract摘要</h1><p>随着由各种类型的内存组成的分层内存系统的到来，如DRAM和SCM(storage class memory)，操作系统对内存管理的支持正变得越来越重要。然而，目前操作系统管理页面的方式是在假设所有的内存都具有基于DRAM的相同能力的情况下设计的。这种过度简化的做法导致了分层内存系统中非最佳的内存被使用。本研究深入地分析了在目前的Linux设计中页面管理方案，将NUMA扩展到支持同时配备DRAM和SCM的系统（英特尔的DCPMM）。在这样的多层内存系统中，我们发现影响性能的关键因素不仅是访问位置，而且还有内存的访问层。当考虑到这两个特性时，有几种替代页放置的方法。然而，目前的运行系统只优先考虑了访问位置。本文探讨了页面管理方案的设计空间， 称为AutoTiering，以有效使用多层内存系统。我们的评估结果表明，与现有的Linux内核相比，我们提出的技术可以通过释放多层内存层次结构的潜力，显著提高各种工作负载的性能。</p><h1 id="1-Introduction简介"><a href="#1-Introduction简介" class="headerlink" title="1 Introduction简介"></a>1 Introduction简介</h1><p>随着内存计算的出现，如数据分析、键值存储和图形处理，对高密度DRAM的需求在最近几年一直在稳步增长[27]。然而，由于扩大DRAM密度的挑战，一种新的内存类别已经受到关注，以弥补DRAM和SSD之间的性能差距。例如，英特尔最近发布了其基于3DXpoint技术的非易失性存储器，称为Optane DC Persistent Memory Module（DCPMM），它提供了比DRAM更高的密度，同时性能超过了基于闪存的SSD[23]。谷歌、甲骨文、微软和百度等云厂商已经在其云服务中采用了这种存储类内存（SCM）[4, 14, 17,25]。</p><p>由于现代服务器系统是用非统一内存访问（NUMA）架构构建的，未来的大内存系统将采用传统NUMA架构上的分层内存的形式，称为多层内存。图1展示了本研究中使用的一个真实世界的多层内存系统。每个计算芯片有两种类型的内存：DRAM（上层）和英特尔的DCPMM（下层）。我们将DRAM和DCPMM配置为完全暴露给软件的内存。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/080d36e6d55a472597e75b673d907ee2.png"></p><p>本文提出，最近在Linux[15]和分层内存研究[16,20,35]中的进展并没有导致多层内存系统中的最佳页面放置。随着新的内存类别成为主内存的一部分，影响性能的关键因素不仅是访问位置，而且是内存的访问层。然而，目前的页面放置方案是为纯DRAM的NUMA架构建立的，只考虑线程和内存之间的位置[2,8,12,13,21,38]。因此，目前的设计远远没有发挥出多层内存系统的潜在优势。例如，假设从低层（DCPMM）向高层（DRAM）内存升级页面时，本地DRAM变得满了。在这种情况下，目前的技术状况是将页面留在下层内存上，而不管远程DRAM（上层的）是否可用。对于纯DRAM的NUMA系统来说，这样的决定是合理的， 因为替代方案之间没有区别。然而，在多层内存系统中我们不能考虑与访问层相当的所有可能的选择。在放置页面时，应该先考虑内存的访问层，再考虑访问位置，因为访问层对性能有更大的影响。</p><p>这个限制促使我们重新审视商用操作系统的页面管理方案，并探索多层内存系统的页面管理的设计空间。在这项研究中，我们引入了一组新的页面管理方案。我们的第一个方案，称为AutoTiering-CPM，在找不到最佳内存节点（如本地DRAM）时，使用访问层和定位度量（locality metric）保守地寻找升级或迁移的替代方案。</p><p>虽然这种保守的方法可以通过考虑替代方案来实现更好的性能，但这样的设计并没有释放出软件管理的分层内存的全部潜力。为了有效地利用上层内存的有限容量我们设计了一个为多层内存系统量身定做的页面回收方案。我们的第二项技术是有机会的页面提升或迁移，称为AutoTiering-OPM，它可以明智地将页面从上层内存中剔除。为了有效地回收，我们通过估计页面的访问频率来预测上层内存中访问次数最少的页面作为受害者。当决定提升哪一页时，我们的OPM将该页与受害者进行比较，以确定哪一页的访问量相对较大。通过OPM，我们可以在减少对低层内存的访问的同时，实现上层内存的更好的有效性。</p><p>除非上层内存中有空闲空间，否则升级操作要等到降级操作完成后才进行。为了从关键路径上隐藏降级页的延迟，我们在上层内存中保留了一组空闲页，以便立即满足升级请求。当保留页的数量超过阈值时，我们的<code>kdemoted</code>会唤醒并在后台将访问量最小的页面回收到空闲页池中。<code>kdemoted</code>与传统的回收方式不同， 因为它只负责将页面降级到下层内存而不是存储。</p><p>在这项研究中， 我们在Linux内核v5.3的基础上实现了我们提出的方案。我们利用AutoNUMA设施，它定期扫描内存页面并标记为不可访问，以捕获非本地DRAM访问。一旦这些页面被重新访问，它就会产生一个缺页，称为NUMA暗示缺页。我们把NUMA缺页作为来自DCPMM节点的页面升级或来自远程DRAM节点的迁移的需求信号。我们用基于故障的设施建立每个页面的访问历史，并在降级页面时使用这些信息。</p><p>实验结果表明，我们的AutoTiering可以显著提高各种应用的性能。与基线Linux内核[15]相比，GraphMat和graph500的性能分别提高了2.3倍和6.9倍左右。大多数SPECAccel工作负载显示出2倍的速度提升。与英特尔最近的方法[36]相比，我们的性能改进显示出高达3.5倍的速度提升。</p><h1 id="2-Background-and-Motivation背景和动机"><a href="#2-Background-and-Motivation背景和动机" class="headerlink" title="2 Background and Motivation背景和动机"></a>2 Background and Motivation背景和动机</h1><h2 id="2-1-Large-Memory-Systems大型内存系统"><a href="#2-1-Large-Memory-Systems大型内存系统" class="headerlink" title="2.1 Large Memory Systems大型内存系统"></a>2.1 Large Memory Systems大型内存系统</h2><p>数据中心通常采用多芯片NUMA架构来提高具有高内核数和内存容量的商用服务器的性能。虽然这可以增加每台服务器的DIMM插槽的数量，但扩展DRAM密度仍然是一个重大障碍。它对经济有效地构建大型内存系统提出了挑战。同时，由于SCM提供了字节寻址和非易失性的特性，它在弥合DRAM和SSD之间的性能差距方面正获得越来越多的关注。英特尔最近发布了3DXPoint非易失性内存（DCPMM） ，可以不经修改安装在DIMM上[23]。许多云计算供应商，如谷歌、微软、甲骨文和百度已经在其云计算服务中采用了英特尔的DCPMM[4,14,17,25]。最近，</p><div class="note info">三星公司透露了一种基于CXL（Compute Express Link）的DRAM模块连接到系统中，形成分层的内存系统[1]。由于这种新型内存的速度不如DRAM，它们不能完全取代DRAM</div> 。相反，未来的计算机系统将提供一种带有DRAM和SCM的分层内存架构形式。<p></p><p>在这项研究中，我们利用DCPMM作为DRAM和SSD之间的一个新层级。英特尔DCPMM提供了两种类型的分层内存系统，可分为硬件辅助型和软件管理型。在硬件辅助模式下，DCPMM作为主内存暴露给软件而DRAM作为硬件管理的缓存，对软件来说是不可见的。内存控制器自动地将经常访问的数据放在DRAM缓存中，而其余的数据则保存在容量大但速度慢的DCPMM上。另一方面，在操作系统的支持下，DRAM和DCPMM都可以作为正常的内存暴露出来，对软件可见，将内存分为快慢两类[15]。我们把这称为软件管理的分层内存系统。在这种环境下，操作系统的支持可以有效地使用DRAM和DCPMM，因为完全由软件来控制。本文通过了解硬件的组织方式，重点讨论分层内存系统的系统软件方面。</p><h2 id="2-2-Performance-Characteristics性能特点"><a href="#2-2-Performance-Characteristics性能特点" class="headerlink" title="2.2  Performance Characteristics性能特点"></a>2.2  Performance Characteristics性能特点</h2><p>我们描述了一个与Linux操作系统一起运行的 、由软件管理的分层内存系统的明显性能特征。图1展示了本研究中使用的系统结构。该系统有两个CPU socket。对于每个CPU socket，都有一个DRAM节点和一个DCPMM节点整个物理地址空间是由DRAM和DCPMM节点组成的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4a382593ca9b41e8bd9675935b8b5456.png"></p><p>在多层内存系统中，影响性能的关键因素不仅是访问位置，而且还有内存的访问层数。图2a 显示了从MLC[18]测得的四个内存节点中每个节点的读访问延迟和带宽。访问本地DRAM的性能优于其他三个内存节点，这在传统的NUMA架构中是公认的。另一方面 ，我们观察到由于设备的特性，本地DCPMM（I-DCPMM）比远程DRAM（R-DRAM）要慢。这与本地内存总是比远程内存快的传统观点形成了鲜明的对比。请注意，我们在带宽测量中也观察到类似的模式。</p><p>同样，图2b显示了对四个CPU（Intel Xeon Gold 6242）socket和只使用DRAM的系统进行的同类型评估。在访问任何远程DRAM节点的延迟和带宽方面没有明显的差异。因此，在纯DRAM系统中，在远程DRAM节点上放置页面是一个相对简单的任务。</p><p>这些明显的特征促使我们去探索操作系统中页面管理的设计空间。操作系统需要有能力通过了解多层内存系统的性能特征来有效地、动态地（重新）定位内存。与纯DRAM系统不同，由于访问层的原因，不是所有的远程内存节点都可以被认为是平等的。</p><h2 id="2-3-OS-Support-of-Multi-Tiered-Memory操作系统对多层次内存的支持"><a href="#2-3-OS-Support-of-Multi-Tiered-Memory操作系统对多层次内存的支持" class="headerlink" title="2.3 OS Support of Multi-Tiered Memory操作系统对多层次内存的支持"></a>2.3 OS Support of Multi-Tiered Memory操作系统对多层次内存的支持</h2><p>本文所关注的多层内存层次结构与传统的两层内存不同。尽管如此，目前的Linux仍然依赖于现有的NUMA框架来支持多层内存系统。这种有限的操作系统支持使得页面放置在多层内存架构中成为次优的。尽管我们可以根据访问延迟重新定义NUMA距离表，但它并没有充分挖掘出多层内存系统的潜力。首先，Linux以二进制的方式将内存节点分类为本地或远程。当升级或迁移页面时，完全不考虑远程节点之间的几种选择。其次，Linux不支持从上层内存向下层内存迁移（或回收）页面。在这项研究中，我们通过考虑跨访问层以及访问位置的性能特征，重新审视了页面放置策略。</p><h1 id="3-Analysis-of-Page-Management-to-Multi-Tiered-Memory-Systems对多层次内存系统的页面管理分析"><a href="#3-Analysis-of-Page-Management-to-Multi-Tiered-Memory-Systems对多层次内存系统的页面管理分析" class="headerlink" title="3 Analysis of Page Management to Multi-Tiered Memory Systems对多层次内存系统的页面管理分析"></a>3 Analysis of Page Management to Multi-Tiered Memory Systems对多层次内存系统的页面管理分析</h1><p>在这一节中，我们研究了现有的Linux页面管理技术，这些技术是为纯DRAM的NUMA系统设计的。然后，我们发现对于分层内存系统缺乏足够的支持。尽管AutoNUMA[33]可以用于这种多层内存，但我们观察到它未能充分利用多层内存的优势。</p><h2 id="3-1-Initial-Page-Placement最初的页面放置"><a href="#3-1-Initial-Page-Placement最初的页面放置" class="headerlink" title="3.1 Initial Page Placement最初的页面放置"></a>3.1 Initial Page Placement最初的页面放置</h2><p>随着存储类内存（如Optane DCPMM）在主内存中的引入，传统的仅基于访问位置的页面放置对性能产生了负面影响， 因为性能的最关键因素不仅是位置，而且还有内存层。同时，目前操作系统中的页面放置方案已经在基于DRAM的NUMA架构中得到了很好的确立，只考虑线程和内存之间的访问位置[8,13,21]。在Linux中， 默认的页面分配策略试图尽可能地使用本地内存，以减少访问远程内存所带来的性能损失。只有在本地内存没有空闲空间的情况下，内存分配器才会在被称为fallback路径的远程内存节点上寻找空闲空间[9]。</p><p>因此，在多层内存系统中，默认的（本地优先）分配策略被认为是有害的。图1a中的数字介绍了当线程在CPU-0上运行时，仅考虑物理距离，默认回落路径中使用的顺序。如果本地DRAM节点没有足够的空闲空间，内存分配器就会检查回退路径，以确定分配请求应该被发送到哪个内存节点我们预计分配器应该要求远程DRAM节点获得一个空闲页，因为这个节点比本地DCPMM节点提供更好的性能。然而，令人惊讶的是，最先进的Linux内核中的fallback路径显示了本地DCPMM（低层）。它没有考虑到一个明显的特点，即在多层内存系统中，内存类型对性能比访问位置更敏感。</p><p>Problem: If the local DRAM is full, the fallback prioritizes allocating from the local DCPMM (lower-tier), even though the remote DRAM (upper-tier) performs better.</p><h2 id="3-2-Dynamic-Placement动态放置"><a href="#3-2-Dynamic-Placement动态放置" class="headerlink" title="3.2 Dynamic Placement动态放置"></a>3.2 Dynamic Placement动态放置</h2><p>尽管最初的页面放置在给定的内存空间中起着至关重要的作用，但这个决定可能不代表最佳性能， 因为它取决于运行时的内存访问流量。为了根据访问模式调整页面的位置，AutoNUMA 设施已被纳入Linux的一部分，自动将页面迁移到在运行时离运行的线程更近的内存节点[33]。操作系统检查访问位置以发现被访问的页面是放在本地内存还是远程内存上。如果是在远程内存上，该页就会被迁移到本地内存上，以避免次要请求的远程访问。这种方法提高了运行在纯DRAM的NUMA系统上的应用程序的性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/25e5b24a332942eda1d936b3f8790096.png"></p><p>然而，我们发现，目前的设计并没有利用多层内存层次的优势。图3显示了随着时间的推移，Graph500的内存节点对128GB数据集的内存使用率。第5节对实验设置进行了细节解释。首先，我们注意到上层内存没有得到有效利用，因为更频繁访问的页面（深红色）主要驻留在下层内存（节点2）。相比之下，访问频率较低的页面被放置在上层内存中。其主要原因是，当前的内存管理不允许在没有空闲空间的情况下将页面升级或迁移到上层内存。尽管这样的设计决定对于纯DRAM系统来说是合理的，但是对于多层内存系统，我们需要重新考虑这个假设。图4描述了三种情况，即AutoNUMA由于本地DRAM缺乏空闲空间而导致页面升级或迁移失败。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3eaa55b57d7d44b7925221a4b1cf7dcc.png"></p><p>即使不能将页面升级或迁移到满足访问层和访问位置的最佳内存节点上，在多层内存系统中也有有效的替代放置方法。例如，当从远程DCPMM到本地DRAM的页面升级失败时， 我们有两种可能的解决方法:将页面放在远程DRAM上或本地DCPMM上。</p><p>Problem: Pages in the lower-tier are not promoted whenthe upper-tier is fully utilized</p><p>第二，我们观察到在低层内存的内存节点上，页的分布是倾斜的。这是因为在目前的Linux操作系统中没有考虑到向CPU-less节点（DCPMM）的页面移动。因为传统的操作系统是在假设内存访问性能受CPU和内存节点之间的访问定位影响很大的情况下设计的，所以将页面移动到CPU-less节点的情况不会发生。只有当目标上层内存有空闲空间时，驻留在CPU-less节点（下层内存）的页面才能通过AutoNUMA进行升级。图1a显示了目前Linux内核支持的所有可能的页升级和迁移路径的箭头。即使上层内存已经满了，所以操作系统不能把页放在首选访问层上，我们也需要通过自由地允许页在任何CPU-less的节点上移动来保持下层内存的访问位置性。</p><p>Problem: Pages are never migrated to the CPU-less (lower-tier) nodes due to a NUMA policy that does not apply to multi-tiered memory systems.</p><h2 id="3-3-Page-Reclamation页回收"><a href="#3-3-Page-Reclamation页回收" class="headerlink" title="3.3 Page Reclamation页回收"></a>3.3 Page Reclamation页回收</h2><p>目前的页面回收也是为基于存储的交换设备支持的纯DRAM系统设计的，而不是分层的内存系统。传统上，当内存节点耗尽时，<code>kswapd</code>会将内存中不活动的页面直接回收到存储中，而不考虑内存层。将低层内存中的页面回收到存储设备上是合理的。然而，当下层内存有足够的空间时， 这对上层内存页来说并不是一个理想的解决方案。这个限制与第3.2节中解释的两个问题交织在一起。</p><p>Problem: Frequently accessed pages from the lower-tier cannot be promoted without demoting less frequently accessed pages from the upper-tier.</p><p>在Linux操作系统中，每个进程的虚拟地址空间的一部分可以被映射，即文件支持区或匿名区。在文件备份区域的页包含了内存中现有文件的内容这样以后对同一文件的I/O操作就可以用内存访问代替。另一方面，属于匿名区域的页面不代表任何文件内容。这用于在内存中保留任意数据（例如，malloc）。对于每个内存区域，Linux操作系统将内存页分为活跃和不活跃内核有非活动列表，包含可能不使用的页面，同时将最近访问的页面保留在活动列表中。 然而，目前的页面分类过于保守，无法从活动和非活动列表中，精确区分哪些页面是经常被访问的，哪些是不经常被访问的。</p><p>Problem: Binary page classification (either active or inactive) is too coarse-grained to be used for tiering.</p><p>最后，当前的页面回收是在后台小心执行的，以隐藏从关键路径访问外存设备的花销。在分层内存系统中，将页面降级到低层内存的成本比将页面换出到存储设备的成本更低。我们需要将降级到低层内存与传统的回收到存储设备的做法脱钩。</p><h1 id="4-Automatic-Multi-Tiered-Memory自动多层内存"><a href="#4-Automatic-Multi-Tiered-Memory自动多层内存" class="headerlink" title="4 Automatic Multi-Tiered Memory自动多层内存"></a>4 Automatic Multi-Tiered Memory自动多层内存</h1><p>本节探讨了页面管理对分层内存系统的设计空间。我们的页面管理的目标是是充分利用多层内存的优势，以提高证明大内存应用程序的性能。为了保持我们的设计简单我们的设计以AutoNUMA设施为基础。表1总结了每个设计空间的支持机制。在下面的小节中，我们解释每个方案的设计和实现。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4fedc098d9c04b1b8a7b139292ecea75.png"></p><h2 id="4-1-Exploiting-Multi-Tiered-Memory多层次内存开发"><a href="#4-1-Exploiting-Multi-Tiered-Memory多层次内存开发" class="headerlink" title="4.1 Exploiting Multi-Tiered Memory多层次内存开发"></a>4.1 Exploiting Multi-Tiered Memory多层次内存开发</h2><p>利用多层次的内存如图4所示，我们将NUMA故障作为DCPMM节点或远程DRAM节点的页面升级或迁移的需求信号。请注意，目前的AutoNUMA只在上层（本地DRAM）内存有空闲空间的情况下才处理升级和迁移请求。否则，该请求将被丢弃，而故障页将重新出现在原始内存中。当本地DRAM被完全占用时，我们提出的设计允许页面被提升或迁移到多层内存层次结构中的下一个最佳内存节点。这种方法可以利用多层内存层次结构的优势， 提供比现有Linux内核更高的性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5642f9ffa89d45269910ea0e188cfbbc.png"></p><p>图5描述了当本地DRAM满的时候，我们如何反应。在多层内存系统中，有三个需求来源，即页面迁移或升级到本地DRAM。多层的层次结构为设计内存布局提供了新的机会。首先，（5a）当故障页驻留在本地DCPMM中时①，我们将该页升级远程DRAM中，作为第二最佳位置②。由于远程DRAM提供了比本地DCPMM更低的延迟和更高的带宽，我们可以提高那些最初在低层内存中分配内存的应用程序的性能。第二， （5b）如果出错的页面位于在远程DCPMM①，我们有两个选择， 即开发利用多层内存层次结构的优势。我们试图将该页提升到远程DRAM②。如果如果远程DRAM也没有空闲空间，我们就尝试将该页迁移到本地DCPMM③。与现有的Linux内核不同，我们修改后的内核支持将页面迁移到CPU-less的节点（本地DCPMM）。我们称这种AutoTiering 为保守的升级和迁移（CPM）。最后，（5c） 发生故障的页面在远程DRAM中①。这意味着该页已经在排第二的最佳位置了。现有的AutoNUMA无法完成上层内存节点之间的页面迁移操作。因此，它导致了次优的性能。在这种情况下，我们考虑采用页面交换的方式来满足对内存亲和性的需求②。之前的研究提出了分层内存的页面交换机制[35]。我们重新利用这个机制来解决同层内存节点之间的迁移故障。在内部，对于每个内存节点，我们跟踪哪些页面不能被迁移。然后我们利用这些信息来确定可以通过交换操作解决的迁移需求。我们将此称为AutoTiering CPM with Exchange（CPMX）。</p><p>由于这种设计不需要对在现有的Linux操作系统的基础上，它很容易集成到AutoNUMA设施之上。我们预计，我们的构架式设计可以成为这种软件管理的分层内存系统的实用解决方案。</p><h2 id="4-2-Opportunistic-Promotion-and-Migration机会性的升级和迁移"><a href="#4-2-Opportunistic-Promotion-and-Migration机会性的升级和迁移" class="headerlink" title="4.2  Opportunistic Promotion and Migration机会性的升级和迁移"></a>4.2  Opportunistic Promotion and Migration机会性的升级和迁移</h2><p>由于我们设计了一个保守的方法来寻找最佳的替代方案，这仅限于提取软件管理的分层内存的全部性能优势。在我们的保守设计中，经常使用的页面可以驻留在低层（DCPMM）内存中，而上层（DRAM）内存则存放不经常访问的数据。为了缓解这种不理想的内存位置，我们探索了一种渐进式的策略，即从上层内存中机会主义地降级一个页面，以创造自由空间。这是保守设计和渐进性设计之间的主要区别。通过降级页面，页面升级的请求可以成功。对于页面降级的效率，我们需要有能力选择一个在短时间内极不可能被重用的页面。否则，错误的选择会对性能产生负面影响。我们在下面的小节（4.2.1） 中解释我们如何选择降级的页面。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1234890e7c7c4e7a8e08f26aa480b00b.png"></p><p>图6描述了我们的渐进式方法是如何进行页面降级的。当NUMA页面故障发生时①， 我们从上层（本地DRAM）内存中找到访问次数最少的页面，并比较访问次数最少的页面和故障页面的访问频率。如果所选页面的访问频率相对低于故障页面，我们就把所选页面降级，把故障页面放在更高层次的内存节点上。否则，我们会阻止页面升级或迁移请求，以保持上层内存有更频繁的访问页面。</p><p>为了从本地DCPMM（6a）或远程DCPMM（6b）中提升一个页面， 我们将访问次数最少的页面降级到低层内存②a或者②b。页面降级目的地取决于该页之前被访问的地方，以保持其位置性。之后，③我们可以最终将该页提升到本地DRAM节点。此外，图6c显示了如何从远程DRAM进行页面迁移的请求。我们把这种称为AutoTiering的机会性推广和迁移（OPM） 。</p><p>我们进一步优化我们的渐进式设计 ，将降级和升级（或迁移）融合到一个交换操作中，称为AutoTiering OPM with Exchange（OPMX）。当降级的目的地与晋升或迁移的来源相同时，我们利用交换操作，而不是单独的晋升和迁移。例如（6a），如果我们需要将一个选定的页面降级到本地DCPMM②a， 并将该页面提升到本地DRAM③，两个单独的操作被融合。该交换操作消除了不必要的页面分配和释放操作。</p><p>万一我们找不到要从上层内存中降级的页面，我们会尝试将该页面升级到下一个最佳位置——远程DRAM——就像通常的情况一样。由于我们是以机会主义的方式进行页面升级和迁移，我们可以减少过度的页面升级和降级导致的相关性能开销——页表操作和TLB shootdown。</p><h3 id="4-2-1-Predicting-the-Least-Accessed-Page预测访问量最少的页面"><a href="#4-2-1-Predicting-the-Least-Accessed-Page预测访问量最少的页面" class="headerlink" title="4.2.1 Predicting the Least Accessed Page预测访问量最少的页面"></a>4.2.1 Predicting the Least Accessed Page预测访问量最少的页面</h3><p>为了使渐进式设计有效，我们的目标是从上层内存中找到访问最少的页面。正如第3.3节所解释的，Linux操作系统将内存分为文件支持的和匿名的页面，作为LRU列表。当页面升级或迁移由于上层内存缺乏可用空间而失败时，我们会优先调查文件支持区域的页面，如果我们无法从文件支持区域找到访问量最小的页面，则会转移到匿名区域。</p><p>①文件支持的页面。我们研究是否可以通过降低属于文件支持区域的页面的等级来腾出空间。由于文件支持的页面被维护在两个LRU列表中，活动和非活动， 我们将非活动列表中最老的页面视为访问量最小的页面。每当文件支持的页面被重新访问时（例如，<code>sys_read</code>或<code>sys_write</code>），操作系统会将该页面标记为被访问，并将其移至活动列表。由于操作系统可以跟踪文件支持的页面的访问情况，我们通过查看非活动列表来估计访问最少的页面。如果不是，我们就转到活动列表。注意，我们保留在内核参数中配置的页面缓存的部分（例如，<code>vfs_cache_pressure</code>）。如果我们不能在文件支持区域找到可重用的空间，那么我们就在匿名区域寻找一个页面作为后备路径。</p><p>②匿名页面。另一方面，我们保留每个页面的访问（故障）信息，以明智地选择匿名区域中访问最少的页面。为了尽量减少监控开销，我们利用AutoNUMA使用的页面扫描工具，跟踪页面在给定的时间窗口内是否被访问。然后，我们用N比特向量为每个页面建立访问历史。这意味着我们要保持到最后N次的访问历史。我们将N设置为8。基于访问历史，我们将页面分为N个级别（最少访问的页面列表），其中N是被设置的比特数，如图7所示。一旦需要进行页面降级，我们在LAP[0]列表中找到其中一个页面，因为这些页面至少有N次没有被访问了。如果LAP[0]列表是空的，我们尝试从LAP[1]列表找一个，等等。这之后，我们就能选择在高层最少被访问的页面，并且实施页面降级去底层内存。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c4a1f0a968e442a9880a2d9da44d6095.png"></p><h2 id="4-3-Hiding-Latency-of-Page-Demotion隐藏页面降级的延时"><a href="#4-3-Hiding-Latency-of-Page-Demotion隐藏页面降级的延时" class="headerlink" title="4.3  Hiding Latency of Page Demotion隐藏页面降级的延时"></a>4.3  Hiding Latency of Page Demotion隐藏页面降级的延时</h2><p>为了执行页面升级，我们应该将一个页面从目标节点上降级。在完成页面降级之前，由于缺乏空间，我们不能继续进行升级请求。故障处理时间是关键路径中两个操作的总和：页面降级和升级。为了从关键路径中移除页面降级，我们探索了一种软件优化技术。</p><p>我们保持一个由几个保留页组成的页面池。我们把4KB和2MB的保留页分别确定为16和4。保留页允许我们在不需要降级过程的情况下立即为升级请求提供服务，即使上层内存已满。这种方法比页面交换方案[35]更具成本效益，因为它将页面降级的延迟隐藏在关键路径中。页面降级到下层内存比页面晋升到上层内存需要更长的时间，因为用于下层外存提供了更好的读取性能而不是写性能。为了在后台有效地降级页面，我们维护了一个新的内核线程，称为<code>kdemoted</code>，在一个批次中降级最少接受的页面。一旦保留页的数量低于某个阈值，我们就唤醒内核线程，开始降级过程。通过敏感性研究，该阈值被设定为4。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4f2ba49324264133bedc6c4d123f4641.png"></p><p>图8描述了简单的优化如何隐藏了页面降级的延迟。对于每一个晋升请求①黑，即使上层内存已满②黑 ，该页也可以被晋升。在完成升级后，NUMA故障处理程序被返回，没有降级过程，而未来对该页面的访问③黑將在上层内存发生。同时<code>kdemoted</code>将降级访问量最小的页面作为一个批次①回收到内存池②的需求。我们将此称为OPM-BD（背景降级）。</p><h1 id="5-Evaluation评估"><a href="#5-Evaluation评估" class="headerlink" title="5 Evaluation评估"></a>5 Evaluation评估</h1><h2 id="5-1-Experimental-Setup实验设置"><a href="#5-1-Experimental-Setup实验设置" class="headerlink" title="5.1 Experimental Setup实验设置"></a>5.1 Experimental Setup实验设置</h2><p>为了评估我们提出的方案，我们使用了一台配备了两个英特尔至强黄金5218处理器的NUMA服务器，并为每个CPU socket配置了一个16GBDDR4-2666 DIMM和一个128GB英特尔Optane DC持久内存（DCPMM） 的多层内存层次结构。服务器系统总共有32GB DRAM和256GB DCPMM作为主内存。<strong>为了尽量减少测量变化，我们关闭了硬件功能，包括超线程、 DVFS、Turbo-Boost和预取</strong>。我们使用Linux内核5.3和Ubuntu18.04服务器作为我们的基线，并在内核之上实现我们提出的方案。我们的代码可在<a href="https://github.com/csl-ajou/AutoTiering%E3%80%82%E6%88%91%E4%BB%AC%E8%BF%90%E8%A1%8C%E6%9D%A5%E8%87%AAgraph500%E3%80%81SpecACCEL%EF%BC%88OpenMP%EF%BC%89%E3%80%81">https://github.com/csl-ajou/AutoTiering。我们运行来自graph500、SpecACCEL（OpenMP）、</a> Graph-Mat[32]和Liblinear[22]的基准，这些基准在最近的大内存系统中使用[2,35]。我们为所有的基准测试配置了它们，在两个socket上使用所有的32个内核和超过64GB的内存，以充分强调多层内存系统。由于页面大小会以各种方式影响性能我们评估了大页面（2MB）和基本页面（4KB）的性能。</p><h2 id="5-2-Experimental-Results实验结果"><a href="#5-2-Experimental-Results实验结果" class="headerlink" title="5.2 Experimental Results实验结果"></a>5.2 Experimental Results实验结果</h2><p><strong>使用我们的保守方法(CPM)的性能</strong>。图9显示了比原版Linux内核（第一条）的加速结果。请注意，在默认的Linux内核中，AutoNUMA已经启用。图9中的第二条显示了我们保守的升级和迁移（CPM）的速度，第三条显示了应用保守的交换（CPMX）时的性能变化。对于我们评估的大多数工作负载，我们可以看到我们的CPM有明显的性能改善。在503.postencil，553.pclvrleaf和560.pilbdc中，与基线相比，速度提高了2倍以上。另外，559.pmniGhost显示了1.6倍的性能提升。我们的保守设计（CPM）可以将页面从低层（DCPMM）内存节点升级到远程DRAM节点，尽管由于远程DRAM比本地DCPMM更快，所以局部性没有得到保留。我们还可以在两个DCPMM节点之间迁移页面，以便在访问低层内存时有更好的访问局部性。因此，我们可以更有效地利用多层内存系统，提高性能。对于graph500和GraphMat，我们看一 下，其速度分别为17%和19%左右。GraphMat和Liblinear将大型数据集从文件中读取到其内存数据结构中。文件支持的页面占据了DRAM节点的很大一部分，而关键的数据结构位于DCPMM节点中。之后，它执行Pagerank算法进行分析。不幸的是，我们无法观察到调用kswapd来释放不活动的文件备份页。因此，我们的CPM和CPMX没有机会有效利用DRAM节点。</p><p>另一方面，我们观察到555.pseismic的性能完全没有提高。这表明，在基线中，内存放置在上层和下层内存之间是很平衡的。因此，我们无法利用升级和迁移的机会。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d3a9355909c84c6c906d8538b4fc4f30.png"></p><p><strong>使用我们的渐进式方法(OPM)的性能</strong>。我们在第4.2节中描述了我们的渐进式设计，它从上层（本地DRAM）节点中找到访问量最小的页面，并将其降级到下层内存中。如图9所示，OPMX在大多数工作负载中都比CPMX有更好的性能。特别是graph500，GraphMat，Liblinear和559.pmniGhost显示出比CPMX明显的速度提升。503.postencil，553.pclvrleaf和560.pilbdc的性能略有下降，大约下降了7%到8%。560.pilbdc未能摊销页面升级和降级的开销，而553.pclvrleaf显示页面交换的机会减少了。</p><p>我们分析了graph500的运行行为，以了解性能的提高。在执行核心图算法之前，它产生了大量的中间数据，占据了DRAM空间。因此，基线和我们的CPMX在运行BFS（广度优先搜索）时花了大部分时间访问低层内存。有趣的是，它在地址空间的一小部分上表现出了内存访问位置性，尽管整个地址空间是巨大的。有了OPMX，我们可以将访问频率较低的数据降级。到低层内存，并将高层内存空间用于更频繁访问的数据。因此，我们可以大幅度地提高性能。</p><p>对于GraphMat来说，我们看到显著的性能改善来自于文件支持的页面的降级。通过默认，文件支持的页面最初被放在非活动列表中。在该页面被重新访问后，它将从非活动列表移至活动列表。通过将访问量最小的页面从文件支持的页面中降级，我们可以提高上层内存的利用率。</p><p>通过OPM（BD），我们可以进一步提高大多数工作负载的性能。与OPMX相比，graph500和GraphMat的改进是相当大的。555.pseismic和559.pmniGhost显示出明显的改善，其他SPEC工作负载也恢复了交换版本的退化性能。同时，Liblinear显示出轻微的性能下降。</p><p><strong>内存使用的分布</strong>。我们分析了随着时间的推移，多层次的内存是如何被利用的。图10比较了基线和我们对选定工作负载的渐进式设计的内存使用情况，这从我们的CPM中受益。对于三个SPEC工作负载，低层内存与CPM的平衡性更好。这是因为我们允许页面被迁移到CPU-less的节点上。尽管CPM无法以满足所需的访问层，它可以保留低层内存的访问局部性。对于GraphMat来说，与SPEC工作负载相比，性能改善相对较小，555.pseismic也没有改善。原因是基线已经在较低级别的DIMM上保持内存平衡。在下一段中，我们看一下我们基于OPM的方案对这两个工作负载的有效性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bf26e3ef51dd4cc3b632f01088b2e185.png"></p><p><strong>LAP分类的有效性</strong>。为了评估我们的LAP方案的有效性，我们将所有的页面分解到每个LAP级别，并随着时间的推移建立一个直方图。图11显示了选定的三个工作负载，与CPM相比，OPMX显示了明显的改善。对于graph500，GraphMat和559.pmniGhost，相对频繁访问的页面（深红色）对应于第7级或第8级被放置在DRAM节点上，而DCPMM节点服务于相对较少访问的页面。这个结果表明，我们的OPMX对于工作集适合于上层内存的应用是有效的。</p><p>为了实现我们的LAP方案，每个页面都需要额外的空间来保存访问历史（8B）、两个列表指针（16B）、页面帧号（8B）和最后故障的CPU编号（1B）。与基线相比，由于8字节的对齐方式，我们需要每页32字节的元数据，它导致系统内存32GB的288MB DRAM与256GB DCPMM。它稍微减少了0.91%的有效DRAM内存空间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/41fc75455ce841e8a5d9b6049aba02c1.png"></p><p><strong>使用后台降级的延迟隐藏</strong>。我们测量了晋升和交换延迟在页面上的分布我们用ftrace以及内核和用户时间来测量页面故障路径中的推广和交换延迟分布。图12显示了为OPM（BD）的页面推广和OPMX的页面交换服务的延迟CDFOPM（BD）和OPMX的页面交换的延迟CDF。实线是分布，包括延迟隐藏方案（OPM（BD）），而虚线是OPMX方案。这两种方案的故障延迟都不同。我们观察到，OPM（BD）在所有工作负载中都显示出比交换版本更好的延迟分布，因为降级是在关键路径之外。特别是graph500和555.pseismic对后台降级有利。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/177121b8fad946a08ce966a4d8c602c3.png"></p><p>另一方面，Liblinear的最终表现在图9中显示，Liblinear的最终性能没有得到改善。即使延迟减少了，内核的执行时间也没有明显改变。时间并没有显著改变。这是由于有可能引起应用程序线程和后台内核线程之间的内存访问争夺。我们计划在我们未来的工作中用严格的kdemoted调度来解决不希望出现的后台降级情况。</p><p><strong>多程序工作负载的性能</strong>。我们评估了我们提出的方案在每个套接字和跨套接字两种情况下对多程序工作负载的工作情况。图13显示了两个应用程序在同一台服务器上运行时，CPM和OPM（BD）的速度提升。我们用工作负载的组合模拟了四种多程序的场景（mix-1到4）。当基于套接字（每个套接字）隔离应用程序时，CPM没有提供任何预期的性能改进，因为它缺乏利用内存的多层次结构的机会。另一方面，我们可以观察到OPM（BD）在所有情况下都有明显的性能改进，除了553.pclvrleaf在mix-2中，因为更多经常访问的页面可以被放在上层内存上。当允许应用程序跨套接字运行时，它们可能无法有效地利用上层内存节点，因为应用程序的线程之间的内存使用并不是均匀分布的。在跨套接字设置中，我们观察到CPM可以通过利用多层内存层次结构而发挥作用。此外，与CPM相比，OPM（BD）可以进一步提高性能。与CPM相比，在所有情况下都能进一步提高性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3bfb0281431041f1b389b5061fd5f563.png"></p><p><strong>工作集的敏感性</strong>。图14显示了工作集大小从32GB到160GB变化时的性能。160GB的选定工作负载的性能。我们观察到，graph500和559.pmni,Ghost的性能被CPM和OPM（BD）显著提高。对于503.postencil和然而，对于503.postencil和553.pclvrleaf，OPM（BD）和CPM显示出类似的随着工作集大小的增加，OPM（BD）和CPM表现出类似的性能改进。正如在LAP分类段落中所解释的，在这些基准中，大部分的<br>在那些对OPM（BD）不太有利的基准中，大部分页面都是均匀访问的。对OPM（BD）有利。请注意，与原版Linux（Baseline）相比，我们的方法的提速是显著的，而且仍然有效。由于其他工作负载，如GraphMat和Liblinear。有固定的问题输入大小，我们无法评估工作集的敏感性。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b3f99c46e51c414c81c4860c205caa19.png"></p><p><strong>与先前研究的性能比较</strong>。图15展示了与英特尔最近提出的名为Tieringv0.6[36]的性能比较结果。请注意，Tieringv0.6是基于Linux内核5.9版本的，但没有并入主线。我们表明，我们的OPM（BD）在大多数工作负载中的性能超过了Tiering v0.6。对于匿名内存区域，Tieringv0.6通过使用AutoNUMA框架，将页面升级到上层定时器内存。通过监控设施，他们调查该页是否在最近两次连续扫描中被访问。如果是这样，他们认为该页是热的，并升级它。另一方面，我们的OPM（BD）保持了过去N（8）次的访问历史。这个决定比看前两次访问更准确此外，Tieringv0.6也继承了传统AutoNUMA的限制。一旦本地DRAM满了，它就不允许将页面推广到远程DRAM或本地DCPMM上。相反，<code>kswapd</code>会被触发，将页面回收到较低层的内存。相比之下，我们的LAP方案通过有选择地执行升级和降级，使上层内存得到更好的利用。对于graph500，我们观察到Tieringv0.6比OPM（BD）表现得更好。我们的方案在执行前构建图形的时间上表现出色，但在遍历图形时，与Tieringv0.6相比采用OPM（BD）的graph500访问了更多的低层内存中的页面。在Liblinear中，它表明Tieringv0.6更积极地将文件支持的页面降级，导致上层内存的更好利用。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5bfcb18fbfea4bcfb1cc5b9fc8a65689.png"></p><p><strong>大页的性能</strong>。为了尽量减少大内存应用的TLB缺失的开销，现代计算机系统提供了大页面选项。图16显示了当我们利用大页面（2MB）而不是基本页面大小（4KB）时的速度提升结果。对于GraphMat和553.pclvrleaf，在OPM （BD）方案中可以观察到大页面的性能改进。另一方面，大多数的工作负载并没有出现明显的差异。559.pmniGhost和560.pilbdc的性能有所下降。当分析具有大页面的SPEC工作负载的LAP直方图时，与基础页面相比，我们的方案可以快速将页面分离到LAP级别。然而，由于在移动大页面时，页面降级的开销会增加，所以会出现性能下降的情况。为了减少开销，我们利用了复制页面的多线程（MT）版本的优势[35]。559.pmniGhost只显示了改进的性能，但其他工作负载变得更差。正如前面一段提到的（延迟隐藏），我们将在未来的工作中进一步研究如何扩展我们的方案以减轻性能开销。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0193ff1662d8412b825f712b723451d3.png"></p><h1 id="6-相关工作"><a href="#6-相关工作" class="headerlink" title="6 相关工作"></a>6 相关工作</h1><p>在整个硬件和软件方面，为有效使用分层（或异质）内存系统做出了巨大努力。我们将我们的方案与之前的方法进行比较。首先，以前的研究大多集中在设计两层内存系统的分页机制和策略[3, 10, 11, 19, 20, 24, 26, 29, 35]与之前的工作不同，本研究将问题空间扩展到分层内存在传统的NUMA架构上增加了一个多层次的内存系统。由于在多层内存系统中， 有几种替代性的页面放置方式本文在放置、升级和删除页面时利用了这些机会。第二，AutoTiering 并没有用先前工作中使用的预定义阈值将页面区分为热和冷[3,19,20]。在这项研究中，晋升和降级的决定是根据整个内存层的相对访问频率和重复性做出的。为了估计页面访问活动，我们依靠AutoNUMA设施。最后，我们使用一个真实世界的基础设施来评估我们提出的基于英特尔的OptanePersistentMemory（DCPMM）的想法，它最近引起了关注。之前的工作模拟或仿真了基于DRAM的两级内存系统[3,11,1.9,20,35]。尽管现实世界中的存储类内存（SCM）在读写性能上显示出不对称性，但在用DRAM模拟分层内存<br>时，它并没有被正确建模。</p><p>下面，我们将介绍操作系统界以前在软件方面的努力为了了解Linux中内存系统的异质性，ACPI6.2规范引入了异质性内存属性表（HMAT），为用户提供各种内存类型的性能信息[39]。从Linux内核5.0-rc1开始，持久性内存（这里指英特尔的DCPMM）可以被用作易失性主内存，尽管它比DRAM慢[15]。它可以提供丰富的主内存空间， 但支持分层内存层次的策略和机制还处于起步阶段。最近，一个新的用于硬件管理的DRAM缓存的内存分配器被称为shuffle page allocator，在Linux内核中被引入[34]。 然而，它不足以提取分层内存的全部性能，因为它没有考虑内存节点之间的距离和NUMA类型。另外，已经有一些努力来有效地支持快内存和慢内存之间的页面迁移，但是这些仍然依赖于活动和非活动列表管理[5,16,30]，直到现在还没有被合并到Linux内核的主线中。在Windows操作系统中，他们通过Mi ComputeNumaCost测量系统初始化时各种页面操作的成本， 并建立-个类似于Linux的NUMAdis-tance的表格，但这反映的是访问成本[37]。 由于Windows操作系统的信息有限，我们无法找到它们对多层内存的作用。</p><p>架构界的研究人员介绍了有效利用两层内存系统的硬件技术，同时将估计访问频率的性能开销降到最低[7,28,29,31]。Choe等人提出了硬件辅助的多层内存系统的内存分配方案，其中DRAM节点对软件是不可见的[6]。除此以外，没有一项工作考虑到多层内存系统的情况。通过架构的支持，跟踪和迁移页面等开销可以大大减少，但是考虑到放置的替代方案，做出晋升和降级决定的灵活性是有限的。</p><h1 id="7-Conclusion-总结"><a href="#7-Conclusion-总结" class="headerlink" title="7 Conclusion 总结"></a>7 Conclusion 总结</h1><p>这项工作探索了一套名为AutoTiering的新的页面管理方案，它从多层内存系统中获益。我们发现，Linux操作系统关注的是NUMA所带来的访问局部性，而不是内存层。然而，在多层内存系统中，访问内存的成本并不只与局部性成正比。我们通过考虑两个因素，即访问层和局部性，全面解决了利用多层内存系统的不同方面。我们用一个真实世界的分层内存系统建立了一个概念验证。我们的评估显示，在各种基准测试中，与现有的Linux内核5.3版本和英特尔的Tieringv0.6之前的方法相比，性能有了明显的改善。</p><p>未来的分层存储系统预计会更加多样化和异质化。为了使我们的方法更加普遍，我们可以维护一个描述可能的替代安置的表格。在操作系统中初始化内存时，我们可以测量各内存节点的实际性能。有了这样一个新的表格， AutoTiering 可以调整页面需要晋升、降级或迁移的地方，正如所解释的那样，没有一个静态的决定。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> NUMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Micron’s Perspective on Impact of CXL on DRAM Bit Growth Rate</title>
      <link href="/2023/10/13/Micron%E2%80%99s-Perspective-on-Impact-of-CXL-on-DRAM-Bit-Growth-Rate/"/>
      <url>/2023/10/13/Micron%E2%80%99s-Perspective-on-Impact-of-CXL-on-DRAM-Bit-Growth-Rate/</url>
      
        <content type="html"><![CDATA[<p>CXL（Compute Express Link™）是一种高速互连、行业标准接口，用于处理器、加速器、内存、存储和其他IO设备之间的通信。CXL通过允许异构和分布式计算架构的可组合性、可扩展性和灵活性来提高效率。CXL的主要优势是计算节点的内存扩展，填补了需要高带宽、容量和低延迟的数据密集型应用程序的空白。</p><p>在本文中，我们将阐述美光科技的观点：内存市场提供强劲的增长前景，Compute Express Link™ （CXL） 将为DRAM bit需求增长和可寻址市场 （TAM,total addressable market）增长带来净积极影响。</p><p>我们将首先讨论当今IT系统中的两个挑战，然后讨论CXL如何解决这些问题。最后，我们将解释我们认为CXL将对内存市场产生的影响。</p><h2 id="内存墙问题"><a href="#内存墙问题" class="headerlink" title="内存墙问题"></a>内存墙问题</h2><p>现代并行计算机架构很容易出现系统级瓶颈，从而限制应用程序处理的性能。历史上，这种现象被称为“内存墙”，微处理器性能的提高速度远远超过DRAM内存速度的提高速度。在过去的十年中，CPU核心数量的增长速度导致CPU和内存性能之间的差距越来越大（图 1），从而阻碍了复杂的计算挑战。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/82284a0bb0484aba83155e934c0f2760.png"></p><p>添加处理器核心只是解决许多应用计算挑战的一部分。在大多数情况下，拥有足够的内存带宽来为这些处理器内核提供数据至关重要。CPU供应商试图通过增加更多内存通道并提高新一代CPU中这些通道的数据速率来逐步改进，从而缓解扩展差距问题。新一代DRAM技术为内存数据速率的发展提供了暂时的缓解。</p><p>表1显示了过去十年中CPU核心数量和DDR DRAM数据速率随着2011年、2017年、2021年和2023年更多内存通道的增加而增加的进展。然而，即使有了理论内存数据速率和更多内存通道，内存带宽要跟上CPU核心数量的增长并随着时间的推移保持每个核心4GB/s是一项挑战。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c7bb70cbebb94edd8bb4cc227a450588.png"></p><p>以CPU核心数量衡量的平台处理能力与可用内存容量扩展之间的关系也同样受到挑战。如图2中的历史趋势数据所示，处理器核心数量增长相当迅速，而每个核心的系统内存容量增长却稳步下降。将内存控制器集成到CPU中通常会导致处理器与内存容量比率更加直接和受限。可以通过为每个通道添加更多DIMM来增加容量。然而，由于通道负载增加，为每个通道添加更多DIMM通常需要降低内存时钟速度，从而减少内存带宽，从而加剧前面讨论的内存墙问题。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/85106b4286dd43de958c0f23847650cd.png"></p><h2 id="IT资源效率最大化问题"><a href="#IT资源效率最大化问题" class="headerlink" title="IT资源效率最大化问题"></a>IT资源效率最大化问题</h2><p>应用程序和服务被分解为微服务，使得可以随着工作负载需求的起伏而优化可用资源。IT效率的主要限制因素之一是没有一种基础设施资源的组合适合所有工作负载。工作负载对计算能力、内存、存储、延迟和IO带宽有动态需求。随着算法的性质和复杂性发生变化，工作负载和服务经过优化，可以通过已安装且不变的公共和私有云硬件基础设施来交付。</p><p>IT工作负载历来都是针对高峰需求进行配置的。架构师和服务规划人员预测在一段时间内提供给定服务级别所需的最大资源需求，然后确保提供适当的峰值级别（以及一些额外的缓冲区）计算、内存、存储和网络资源给定服务器或服务器机架上的工作负载，包括满足峰值水平需求的功率。然而，这通常意味着资源的严重过度配置，因为工作负载需求很少在峰值水平运行。在最近的大部分时间里，该行业的整体数据中心资源利用率都非常低（低于50%，并且通常远低于2）。</p><p>随着时间的推移，虚拟化和云基础设施提供了重要的功能，通过增强的自动化、工作负载迁移和放置以及其他技术，帮助节省甚至回收因过度配置和利用不足而损失的资源。许多人认为这将大大减少数据中心服务器基础设施的TAM。这产生了完全相反的效果，产生了对更高密度CPU计算平台的需求，因为提高效率可以节省其他方面的成本，如电源和运行管理。因此，强化了杰文斯悖论“资源效率的提高将导致资源消耗的增加而不是减少”。</p><p>人们对提高灵活性和效率的渴望从未如此强烈，业界一直在讨论如何实现可组合的“未来数据中心”。下一代数据中心对资源的使用进行更细粒度的控制，包括重新思考如何不仅在数据中心级别共享资源，而且还跨机架共享资源甚至在服务器内。</p><h2 id="CXL架构数据中心的演变"><a href="#CXL架构数据中心的演变" class="headerlink" title="CXL架构数据中心的演变"></a>CXL架构数据中心的演变</h2><p>CXL已成为一种经济高效、灵活且可扩展的架构解决方案，将塑造未来的数据中心。 CXL将改变服务器和光纤交换机的传统机架和堆栈架构在数据中心的部署方式。拥有由CPU、内存、网络和存储组件组成的专用固定资源的专用服务器将让位于更灵活和可扩展的架构。如果机架中的服务器与网络、存储和计算的固定资源互连，就能通过软件管理基础设施动态组合，以满足人工智能和深度学习等现代和新兴工作负载的需求。</p><p>业界一直关注可通过具有内存访问功能的CXL设备（如CXL连接内存设备）释放的潜力。内存附加节点提供高容量内存扩展，可用于密集型服务器工作负载，并具有增加的内存带宽、低延迟和内存一致性，以实现异构计算/处理，并实现内存基础设施的分层。内存分层的引入方式与过去几十年存储分层的引入方式大致相同，最终将包括直连内存扩展、内存池和内存共享。</p><p>数据中心将更加以内存为中心，能够动态组合具有高Terabyte字节（TB）以上内存池的服务器，从而使更多应用程序能够在内存中运行。存储级内存成为新的主要活动数据存储层，NAND和磁盘驱动器用于在多个主机之间共享热数据和非活动数据。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/65fa682280034b73a0741413825ee9e2.png"></p><p>最终，数据中心将升级到所有服务器元素完全分解（包括计算、内存、网络和存储）的状态。大规模部署的容器和微服务将推动动态配置所需的底层资源，以实现具有<strong>平衡计算和内存比率</strong>且无性能损失的优化解决方案。借助CXL，随着可组合性管理软件的出现，按需配置中使用的服务和底层硬件的部署将显得无缝且快速，从而在异构环境中的即服务模型中创造更高的效率。</p><h2 id="CXL-如何解决内存墙问题"><a href="#CXL-如何解决内存墙问题" class="headerlink" title="CXL 如何解决内存墙问题"></a>CXL 如何解决内存墙问题</h2><p>用于内存设备凝聚和一致性的CXL协议属性将通过支持将内存扩展到服务器DIMM插槽之外来解决“内存墙”问题。CXL内存扩展是一种双管齐下的方法，通过增加带宽来克服“内存墙”问题，并为支持CXL的服务器增加数据密集型工作负载的容量。</p><p>对于典型的工作负载，保持每个CPU核心的带宽以获得理想的效率非常重要。随着核心数量迅速增加，带宽会出现不足（参见表 1）。直接连接CXL内存扩展允许服务器平台进行扩展并缩小额外带宽的差距以保持平衡。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/aacf6242525244268488b272ffbfa75f.png"></p><p>另一个需要考虑的因素是，随着核心数量的增加，每个核心的容量会减少。应用程序的工作负载需求不断增长，需要快速分析收集的数据并将结果用于有用的业务洞察。这些高价值工作负载（即<strong>机器学习、NLP、计算机视觉、推荐系统、内存数据库</strong>等）可以通过每个系统中更高级别的内存来经济地运行。CXL内存模块可以直接插入服务器，为处理器提供超出直接连接内存通道的更多带宽和容量，并且<strong>延迟时间与双插槽服务器中处理器之间的NUMA链路相当</strong>。</p><h2 id="CXL如何应对I​​T效率和可持续发展挑战"><a href="#CXL如何应对I​​T效率和可持续发展挑战" class="headerlink" title="CXL如何应对I​​T效率和可持续发展挑战"></a>CXL如何应对I​​T效率和可持续发展挑战</h2><p>跨应用垂直领域的各种工作负载对计算操作、内存容量、带宽和延迟高度敏感。在云、企业或边缘数据中心的传统机架服务器上运行的应用程序必须满足服务级别协议（SLA）。一种常见的方法是将这些类型的应用程序工作负载分布到多个系统上。构建IT基础设施并不总是遵循简单的经验法则来实现计算和设备资源之间的系统平衡。平衡这些资源取决于工作负载，这些工作负载可以是计算限制、内存限制或IO限制。</p><p>基于CXL的系统的初始部署提供了性能和容量的扩展选项，以匹配基于工作负载需求的计算资源的扩展。内存、存储、网络和随着外形和连接的标准化，加速器成为可互换的模块，并且可以根据工作负载需求来组合服务器。这种方法允许服务器制造商（包括云提供商）减少他们需要开发和维护的服务器SKU的数量，以满足其客户群的无数应用程序。它还可以帮助IT管理员正确调整具有足够资源的服务器的大小，以减少单个工作负载必须分布的服务器数量，从而提高效率和性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a8ca37be3e8a473e81e8b4f6dffe46a3.png"></p><p>随着时间的推移，CXL架构的价值将扩展到机架，从而实现可组合性。可组合性是指在支持一个或多个工作负载的一台或多台服务器中更灵活地配置内存与计算资源比率的能力。资源平衡可以通过内存扩展、内存池或内存共享来实现。在机架上，横向扩展方法允许根据应用程序的要求动态分配资源池（计算、网络、内存、存储和IO）并进行无缝集成。当实例通过使用机架内本机设备级发现的组合管理软件上线时，计算、内存、网络和存储被分配给应用程序或微服务。在高峰需求期间，可以为应用程序动态分配额外的资源以满足SLA。当应用程序工作负载需求缓和时，可以释放额外的资源并将其重新分配给其他服务。资源共享或池提供了更高的利用率，而无需过度配置系统，这也意味着更高的性能、降低的软件堆栈复杂性和更低的总体系统成本。</p><p>当然，将多少共享（因此可组合）资源聚合到任何给定工作负载总是存在限制，因为客户需要考虑安全和容错因素以及工作负载效率和利用率问题。尽管内存池创建了满足峰值水平所需的资源，但85%的组织需要99.99%的正常运行时间才能满足SLA（service level agreements服务级协议），这必须在机架内的内存池中考虑到，即使在池化CXL连接内存时，也会导致一定程度的超额订阅被采纳。此外，虽然内存池可以缓解近期内存过度配置的问题，但必须仔细考虑内存池扩展故障，以避免整个机架的服务器故障，从而驱动冗余以避免停机。一种受到青睐的方法是创建资源区或Pod，在有效使用共享资源与最大限度地减少服务中断的影响之间取得平衡，并提供适当的安全性和合规性功能。</p><p>数据中心最大的举措之一是推动net-zero排放。效率是数据中心可持续发展方程中的一个关键变量。就像服务器虚拟化一样，扩大设备共享和资源池通常会减少数据中心的过度配置，但规模更大。将专用设备资源转换为共享池资源并将其进行动态分配可降低计算节点的功耗。不仅减少了每个计算节点的功耗，还改善了气流和热量，以实现机架内更高效的冷却，减少对HVAC系统的需求，从而进一步降低数据中心的功耗。</p><h2 id="CXL对DRAM-bit需求增长的影响"><a href="#CXL对DRAM-bit需求增长的影响" class="headerlink" title="CXL对DRAM bit需求增长的影响"></a>CXL对DRAM bit需求增长的影响</h2><p>现在让我们讨论一下CXL将如何影响DRAM bit需求增长。</p><p>CXL支持的池化和CXL支持的内存带宽扩展对比特需求增长的净影响将是积极的。总而言之，我们预计CXL将在中短期内帮助维持数据中心比特20%的高增长。</p><p>近期CXL内存市场取决于支持CXL的服务器平台向广泛行业推出的速度。由于CXL内存是一个新兴市场，CXL上的内存增长将非常快，但直到2026年才会对整个DRAM市场产生巨大影响。Yole Intelligence市场研究小组预测，到2028年，CXL上的DRAM bit需求将增长到接近100艾比特。Yole Intelligence 预测，到2028年，CXL位将占服务器DRAM位总数的31%。</p><p>卡内基梅隆大学/微软最近发表的一篇论文讨论了池化如何影响CXL TCO节省。该论文提出了一种基于CXL的池解决方案，通过将给定超大规模工作负载集的内存需求减少9-10%，可以节省4-5%的TCO。数据中心DRAM位增长复合年增长率（CAGR）仍保持在20%的范围内，其中包括CXL的影响。即使添加了内存池，这对整个数据中心DRAM位增长的影响也很小。通过粗略数学计算，将9-10%的减少量乘以20-30%的预期复合年增长率即可实现这一效果。理论上最坏情况的计算表明，由于合并，损失会减少2-3个百分点。当然，这种理论场景是不可行的，因为池化伴随着延迟权衡和软件优化要求，并且<strong>池化并不适用于所有工作负载</strong>。其次，池的适用性和扩展受到一定程度冗余的容错需求以及跨多个托管服务器级联内存池故障的风险的限制。最后，逐步采用CXL会减弱任何影响。支持CXL的池将无法解决当前非CXL数据中心安装基础的问题。</p><h2 id="CXL-对行业收入的影响-TAM-和美光的财务模型"><a href="#CXL-对行业收入的影响-TAM-和美光的财务模型" class="headerlink" title="CXL 对行业收入的影响 TAM 和美光的财务模型"></a>CXL 对行业收入的影响 TAM 和美光的财务模型</h2><p>内存的收入TAM增长取决于位数和价格，而价格取决于供需平衡。 CXL是一种互连解决方案，其技术采用本身并不会增加市场供应。CXL本身不应成为行业供需的破坏性因素，定价预计将促进TAM的增长。在某些配置中，连接到CXL接口的内存与标准内存插槽相比更具成本效益，使服务器系统的构建和部署规模超出预算目标。CXL的第一个用例围绕单主机配置的内存扩展。内存扩展可恢复内存限制工作负载的计算和内存之间的平衡，否则这些工作负载将分布在多个服务器之间，并将内存从这些服务器整合到CXL扩展插槽。能够支持CXL 1.1+ 的新型服务器将于2023年上市，但主要用作CXL新兴内存解决方案的概念验证。真正的部署将于2024年底开始，届时支持CXL 2.0的服务器将提供更多内存扩展选项，并标志着服务器中平均DRAM内容量开始增加。我们预计这将是CXL接口收入增长的开始，并预计到2025年该市场将达到20亿美元。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e31421c95c7f41d4893436c06e38d3f6.png"></p><p>资源扩展是CXL演进的第一步，然后再转向完全可组合性和内存池，我们目前预计这将在2026年开始增长。2026年，许多新服务器将支持CXL 3.0，服务器市场预计将增长到2100万个单位，为分解提供必要的支持。影响内存池采用率的因素包括CXL交换机以及可以处理分层内存池和跨多个主机分配以最大限度减少延迟的软件。超大规模企业将在短期内成为内存池扩展的早期采用者。它们很可能在单主机内存扩展和机架内内存池之间均匀分配增长。我们以及行业分析师Yole Intelligence预计，到2030年，CXL附加内存市场将超过200亿美元，数据中心内存市场预计将达到1000亿美元，其中大部分增长将在2025年之后。</p><p>我们对CXL影响的看法和预期已纳入我们的长期模型和跨周期财务模型中，因此对CXL技术采用的预期不应改变投资者对我们财务业绩的预期。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>CXL提供了平衡“内存墙”问题所需的架构，并为通过内存扩展实现经济的内存解决方案提供了新的向量。此外，CXL灵活且可扩展的架构可提高计算和内存资源的利用率和运行效率，以便根据工作负载需求纵向扩展或横向扩展资源。 CXL 附加内存为分层内存存储的新领域提供了巨大的增长机会，并实现了独立于CPU内核的内存扩展。 CXL将有助于维持比没有它时更高的DRAM bit增长率。换句话说，我们预计CXL不会导致DRAM bit增长加速，但它对DRAM增长具有净积极作用。</p><p>美光对 CXL 技术的承诺使客户和供应商能够推动内存创新解决方案的生态系统。要了解有关美光如何实现下一代数据中心创新的更多信息，请访问 micron.com/solutions/server。</p>]]></content>
      
      
      <categories>
          
          <category> Livre blanc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CXL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Characterizing the Performance of Intel Optane Persistent Memory</title>
      <link href="/2023/10/13/Characterizing-the-Performance-of-Intel-Optane-Persistent-Memory/"/>
      <url>/2023/10/13/Characterizing-the-Performance-of-Intel-Optane-Persistent-Memory/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/Tian_pp/article/details/124859309%E8%BF%99%E7%AF%87%E6%80%BB%E7%BB%93%E7%9A%84%E6%9B%B4%E5%8A%A0%E7%BB%86%E8%87%B4%EF%BC%8C%E5%9C%A8%E8%AE%BE%E8%AE%A1%E6%97%B6%E5%8F%AF%E4%BB%A5%E8%80%83%E8%99%91%E8%BF%99%E4%BA%9B%E5%BB%BA%E8%AE%AE">https://blog.csdn.net/Tian_pp/article/details/124859309这篇总结的更加细致，在设计时可以考虑这些建议</a></p></blockquote><h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自Proceedings of the Seventeenth European Conference on Computer Systems, (EuroSys), 2022</li><li>Characterizing the Performance of Intel Optane Persistent Memory————A Close Look at its On-DIMM Buffering</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Lingfeng Xiang, Xingsheng Zhao, Jia Rao, Song Jiang, Hong Jiang, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>最近的研究[8,13,23,26,30,32,35,36]发现Optane DCPMM不应简单地被视为速度较慢的持久DRAM。与DRAM相比，Optane DCPMM表现出复杂的行为，并且性能会根据访问大小、访问类型和模式而发生巨大变化。</p><p>连接Optane DCPMM和集成内存控制器（iMC）的新DDR-T协议支持异步存储，以隐藏长写入延迟；而DRAM使用的DDR4协议对于加载和存储是同步的。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>目标是了解DIMM缓冲如何影响应用程序感知的性能。我们使用微基准评估了现有的两代Optane DCPMM，并得出了以下以前未报道过的发现：</p><ul><li>在单独的DIMM读缓冲区和写缓冲区中对读和写的管理方式不同。读取缓冲区提供更高的并发性和有效的DIMM预取，从而实现高读取带宽和卓越的顺序性能，但无助于隐藏媒体访问延迟。写入缓冲区提供的并发性有限，但却是支持DDR-T协议中异步写入的管道中的关键阶段。</li><li>除了写入合并之外，写入缓冲区还提供低于读取的延迟和一致的写入延迟，无论工作集大小、写入类型、访问模式或持久性模型如何。</li><li>缓存行访问粒度和3D-Xpoint媒体访问粒度之间的不匹配会对 CPU缓存预取的有效性产生负面影响，并导致持久内存带宽的浪费。</li><li>由于异步DDR-T协议，缓存行刷新或普通写入在到达iMC中的写入挂起队列时返回，以隐藏较长的介质写入延迟。栅栏指令对读写操作进行排序以实现崩溃一致性，仅保证刷新全局可见，但不一定完成。因此，在栅栏指令返回后读取最近刷新的缓存行可能会经历几乎一个数量级的延迟，因为读取需要等待刷新完成。</li></ul><div class="note info">但是实际在使用的时候，延迟的差距带来的性能影响并不是很严重，主要还是带宽影响了TPS系统吞吐（每秒处理的数量），虽然延迟和带宽是有瓜葛不能完全分开的。</div> <h2 id="4-结论"><a href="#4-结论" class="headerlink" title="4. 结论"></a>4. 结论</h2><p>这里直接纪录一些个人比较感兴趣的这篇论文的观察结论：</p><p>测试台允许通过BIOS配置单独启用/禁用英特尔可扩展处理器中的三个CPU缓存预取器。我们首先禁用所有三个CPU预取器，以研究是否存在独立于CPU预取的DIMM预取机制。图6（a）和（e）显示，在G1和G2 Optane DCPMM中，Optane DCPMM和iMC的读取比率均接近1，这表明没有观察到明显的DIMM上预取活动。相反，当分别使能各个CPU预取器时，DCPMM的读取比率与iMC的读取比率不同。根据WSS，每个图中都有三个区域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/303687a65e5d422588e1d496884118e5.png"></p><ul><li><p>WSS（working set sizes）小于读缓冲区16KB。 WSS完全适合读缓冲区，并且CPU预取器预取到读缓冲区的所有数据都会导致后续访问中的缓冲区命中。因此，除了程序要求的数据之外，不从介质加载任何附加数据。</p></li><li><p>WSS大于读缓冲区但小于L3缓存。工作集不再适合读缓冲区，但仍适合最后一级缓存LLC（G1服务器上为27.5MB，G2服务器上为 36MB）。虽然iMC的读取比率保持为 1，因为所有CPU预取数据都会导致LLC命中，但Optane DCPMM的读取比率显着增加。由于WSS大于读取缓冲区大小，因此预取数据在命中缓冲区之前就会被逐出，从而导致介质的浪费和重复加载。</p></li><li><p>WSS大于L3缓存。由于大型WSS会调用频繁的CPU预取，导致LLC未命中，并导致Optane DCPMM和iMC中的读取比率增长。一个值得注意的观察结果是DCPMM读取率明显高于iMC。对于256B的访问块（即4个高速缓存行），当DCPMM从介质加载整个XPLine时，iMC最多会在访问块的边界误预取一个额外的高速缓存行。</p></li></ul><p>Optane DIMM中的预取活动是由CPU预取决定的，DCPMM中的误预取损失比DRAM中的预取损失特别高。 CPU预取中的高速缓存行粒度与媒体访问粒度之间的不匹配需要在预取错误预测时从媒体加载4个高速缓存行或一个XPLine。对于针对XPLine大小和对齐数据访问进行优化的工作负载，CPU预取可能会占DCPMM带宽的一半。建议程序员仔细权衡在此类工作负载中预取的好处和成本。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Optane NVDIMMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An empirical guide to the behavior and use of scalable persistent memory</title>
      <link href="/2023/10/12/An-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/"/>
      <url>/2023/10/12/An-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自18th USENIX Conference on File and Storage Technologies, (FAST), 2020</li><li>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jian Yang, Juno Kim, Morteza Hoseinzadeh, Steven Swanson. 加州大学圣迭戈分校UCSD</li><li>Joseph Izraelevitz. 科罗拉多大学博尔德分校</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>Optane DIMM这种新型非易失性DIMM支持字节粒度访问，访问时间与DRAM相当，同时还提供断电后仍可保存的数据存储。在过去的十年中，研究人员撰写了大量论文，提出了新的编程模型、文件系统、库和应用程序，旨在利用NVDIMM承诺提供的性能和灵活性。这些论文得出结论并做出设计决策，但没有详细了解真正的NVDIMM的行为方式或行业如何将它们集成到计算机架构中。现在我们可以为这些系统的程序员提供详细的性能数据和具体指导，重新评估现有技术的性能，并为真正的Optane DIMM重新优化持久内存软件。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>从微观和宏观层面探讨了英特尔Optane DIMM的性能特性。特别注意其性能相对于传统DRAM或过去用于模拟NVM的其他方法的特殊性。</p><p>我们发现Optane DIMM的实际行为比“较慢、持久 DRAM”标签所暗示的更为复杂和细致。与DRAM性能相比，Optane DIMM性能更依赖于访问大小、访问类型（读与写）、模式和并发程度。此外，Optane DIMM的持久性与英特尔最新处理器提供的架构支持相结合，为软件设计人员带来了更广泛的设计选择</p><p>根据这些观察结果，我们推荐了一组最佳实践，以最大限度地提高设备的性能。随着我们加深理解，我们随后探索并重新优化持久内存应用级软件中现有技术的性能。</p><h2 id="4-围绕该问题作者如何构建解决思路"><a href="#4-围绕该问题作者如何构建解决思路" class="headerlink" title="4. 围绕该问题作者如何构建解决思路"></a>4. 围绕该问题作者如何构建解决思路</h2><h3 id="4-1-Optane-DIMM架构详细信息"><a href="#4-1-Optane-DIMM架构详细信息" class="headerlink" title="4.1 Optane DIMM架构详细信息"></a>4.1 Optane DIMM架构详细信息</h3><blockquote><p>当PM做内存：直接访问（Direct Access，DAX） 机制是一种支持用户态软件直接访问存储于持久内存（Persistent Memory，PM） 的文件的机制，用户态软件无需先将文件数据拷贝到页高速缓存（Page Cache）</p></blockquote><p> 与传统DRAM DIMM一样，Optane DIMM位于内存总线上，并连接到处理器的集成内存控制器（iMC）。英特尔的Cascade Lake处理器是第一个（也是唯一一个）支持 Optane DIMM的CPU。在此平台上，每个处理器包含一个或两个处理器芯片，其中包含单独的NUMA节点。每个处理器芯片有两个iMC，每个iMC支持三个通道。因此，处理器芯片总共可以在其两个iMC上支持六个Optane DIMM。</p><p> 为了确保持久性，iMC位于异步DRAM刷新（ADR）域内Intel的ADR功能可确保到达ADR域的CPU stores能够在电源故障中幸存下来（即，将在保持时间内刷新到NVDIMM，&lt;100 µs）。 iMC为每个 Optane DIMM维护读取和写入挂起队列（RPQ和WPQ）（图1(b)），并且ADR域包括WPQ。一旦数据到达WPQ，ADR将确保iMC在电源故障时将更新刷新到3D-XPoint介质。ADR域不包括处理器缓存，因此存储仅在到达WPQ后才会持久。</p><p> <strong>iMC使用高速缓存线（64字节）粒度的DDR-T接口与Optane DIMM进行通信，该接口与DDR4共享机械和电气接口，但使用不同的协议来实现异步命令和数据计时。</strong></p><p> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/adac8e025a634145a7ffef40338f51f9.png"></p><p> 对NVDIMM（图1(b)）的内存访问首先到达DIMM控制器（本文中称为XPController）该控制器协调对Optane介质的访问。与SSD类似，Optane DIMM执行内部地址转换以进行磨损均衡和坏块管理，并为此转换维护一个地址间接表（AIT）。地址转换之后，就发生对存储介质的实际访问。由于3D-XPoint物理介质访问粒度为256字节（本文中称为 XPLine），XPController将较小的请求转换为较大的256 字节访问，导致写入放大，因为小型存储变成了读-修改-写操作。 XPController有一个小的写合并缓冲区（本文称为XPBuffer），用于合并相邻的写操作。</p><p> Optane内存可以（可选）跨通道和DIMMs交错使用，如下图：<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d0857fcda1134f34a2ec95d566c57664.png"></p><h3 id="4-2-Optane-DIMM与DRAM的不同之处"><a href="#4-2-Optane-DIMM与DRAM的不同之处" class="headerlink" title="4.2 Optane DIMM与DRAM的不同之处"></a>4.2 Optane DIMM与DRAM的不同之处</h3><p> 这些测量反映了软件看到的加载和存储延迟，而不是这些底层内存设备的加载和存储延迟：<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/451bbc5df2304383a25a93ef1a2fe1b4.png" alt="和硬盘间的读写IO延迟"></p><blockquote><p>尾延迟是指在读取或写入数据时，一些操作所需的时间明显超过了大多数操作的平均时间。这种情况可能导致一些请求的响应时间明显延长，从而影响系统的整体性能。</p></blockquote><blockquote><p>99.9百分位延迟：这是在一组操作中，有99.9%的操作所经历的延迟时间的阈值。换句话说，只有0.1%的操作会在这个延迟时间之上。通常，这个度量标志着绝大多数操作的延迟情况，但允许一小部分极端情况的出现。</p></blockquote><blockquote><p>99.99百分位延迟：这是更严格的延迟度量，表示在一组操作中，有99.99%的操作所经历的延迟时间的阈值。只有0.01%的操作会在这个延迟时间之上。这个度量用于更强调延迟的可靠性，确保绝大多数操作都在非常短的时间内完成。</p></blockquote><blockquote><p>最大延迟：这是一组操作中的最长延迟时间，表示最慢的操作需要多长时间才能完成。最大延迟是最极端的情况，通常用于评估系统的最坏情况行为。在一些实时应用和服务中，尤其是需要低延迟的情境下，最大延迟非常重要，因为即使只有极少数操作的延迟超出预期，也可能对系统的性能和用户体验产生重大影响。</p></blockquote><p> 尾部延迟一项显示顺序写入一小块内存区域（热点）的尾部延迟的实验。Optane内存具有罕见的“异常值”，其中少量写入需要长达50µs才能完成（比通常的延迟增加了100倍）</p><p>  <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/521c5119e9214b84ad2290110aa03cc0.png"></p><p>  DRAM带宽不仅高于Optane，而且随线程数可预测（且单调）扩展，直至DRAM带宽饱和，而这在很大程度上与访问大小无关。首先，对于单个DIMM，最大读取带宽是最大写带宽的2.9倍，而DRAM只有1.3倍的差距。除了交错读取之外，Optane性能随着线程数的增加而呈现非单调性。对于非交错（即单DIMM）情况，性能在1到4个线程之间达到峰值，然后逐渐下降。</p><p>   <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/74c2552a0c1c429ca5d497c61f411c81.png" alt="带宽与线程数 该实验显示了本地DRAM、非交错式和交错式 Optane内存上线程数增加（从左到右）时的最大带宽。所有线程都使用256B访问大小。（注意垂直刻度的差异）"></p><p>   <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b2704fb5d70b40a6bfe34bf494accd34.png" alt="访问大小的带宽 实验显示（从左到右）本地DRAM、交错和非交错Optane内存上不同访问大小的最大带宽。图表标题包括每个实验中使用的线程数（读/写（ntstore）/写（clwb））"></p><h3 id="4-3-Optane-DIMM与其他仿真技术的不同之处"><a href="#4-3-Optane-DIMM与其他仿真技术的不同之处" class="headerlink" title="4.3 Optane DIMM与其他仿真技术的不同之处"></a>4.3 Optane DIMM与其他仿真技术的不同之处</h3><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3773889806b14aa9bba6d6952daa2a06.png"></p><p> 这些图中的数据表明，没有一种模拟机制能够捕获 Optane 行为的细节——所有方法都与真实的 Optane 内存有很大的偏差。他们无法捕捉傲腾内存对顺序访问和读/写不对称性的偏好，并对设备延迟和带宽给出非常不准确的猜测。</p><h3 id="4-4-Optane-DIMM最佳实践"><a href="#4-4-Optane-DIMM最佳实践" class="headerlink" title="4.4 Optane DIMM最佳实践"></a>4.4 Optane DIMM最佳实践</h3><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9c439f09a06947798a73788207f85a40.png" alt="单个 DIMM 上的 EWR 和吞吐量之间的关系 每个点代表不同访问大小、线程数和功率预算配置的实验。注意指标之间的相关性"></p><p> 图8绘制了我们系统性地扫描Optane性能的所有测量结果中单个DIMM的EWR和有效设备带宽（dram和cpu之间的！）之间的强相关性。基于这种关系，我们得出结论，努力最大化EWR（Effective Write Ratio）是最大化带宽的好方法。</p><p> 而且由于最小粒度是256B，但是iMC发出64B的话，就会在缓冲区被合并为256B，所以下图探究了每次写入更新多少大小合适。<span class="label primary">避免小型的stores超过16KB也不合适。</span><br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b2618ae65fc840ee9e2bfca3cca67d6b.png"></p><p> 绘制iMC争用图。在固定数量6个线程的情况下，<span class="label primary">随着每个线程访问的DIMM数量的增加，带宽会下降</span>。为了获得最大带宽，线程应固定到DIMM。图15说明了当多个核心针对单个DIMM时，iMC中有限的队列容量如何影响性能。该图显示了使用固定数量的线程（24个用于读取，6个用于 ntstore）向6个交错 Optane DIMM 读取/写入数据的实验。我们让每个线程随机访问N个 DIMM（跨线程均匀分布）。随着N的增加，针对每个DIMM的写入器数量会增加，但每个DIMM的带宽会下降。可能的罪魁祸首是XP Buffer的容量有限，但EWR仍然非常接近1，因此性能问题肯定出在iMC上。</p><p> <span class="label primary">避免对远程 NUMA 节点进行混合或多线程访问</span>PM性能下降率与远程DRAM与本地 DRAM的性能下降率类似。然而，当线程数量增加或读/写混合工作负载时，Optane内存的带宽会急剧下降。根据我们的系统扫描结果，相同工作负载下，本地和远程Optane内存之间的带宽差距可能超过30倍，而本地和远程DRAM之间的差距最大仅为3.3 倍。<br> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/125f5e3edb7949c0811862d4532ac6cd.png"></p><h2 id="5-如何扩展到未来几代-NVM"><a href="#5-如何扩展到未来几代-NVM" class="headerlink" title="5.如何扩展到未来几代 NVM"></a>5.如何扩展到未来几代 NVM</h2><p> 增加或减少 256 B 内部写入大小可能会很昂贵。人们普遍认为Optane是相变存储器，由于功率限制，较小的内部页面尺寸长期以来一直是相变存储器的标志[2]。较小的内部页面大小不太可能，因为由此产生的存储器密度较低。不同的底层存储单元技术（例如自旋扭矩 MRAM）将带来更彻底的改变。事实上，电池供电的 DRAM 是一种众所周知且广泛部署的持久内存技术（尽管可扩展性或成本效益不高）。对于它，我们的大多数指南都是不必要的，尽管由于缓存一致性模型的限制，非临时存储对于大型传输仍然更有效。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Optane NVDIMMs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Summary of hybrid main memory systems</title>
      <link href="/2023/10/09/A-Summary-of-hybrid-main-memory-systems/"/>
      <url>/2023/10/09/A-Summary-of-hybrid-main-memory-systems/</url>
      
        <content type="html"><![CDATA[<h2 id="异构内存设备"><a href="#异构内存设备" class="headerlink" title="异构内存设备"></a>异构内存设备</h2><p>下面收集了目前用于做异构内存的设备以及他们的特点：<br>缩写 名称 访问方式<br>RDMA 通过内存总线和处理器直接相连 DDR4<br>PCM 变相存储器 通过内存总线和处理器直接相连<br>ReRAM 电阻式存储器 通过内存总线和处理器直接相连<br>Optane PM(3D-XPoint)  通过内存总线和处理器直接相连 DDR-T</p><p>DCPMM的一些特性只与它的内部结构和存储技术有关。因此，近期DCPMM研究得出的指南并不广泛适用于其他类型的PMEM。遵循这些指导方针可能会得到一个高度专门化的系统，该系统可以很好地与DCPMM一起工作，但与其他PMEM不兼容。为了防止这种过度专业化，认为系统设计者应该正确地理解这些特性的根本原因，以及它们如何适用于未来不同类别的PMEM。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在过去的几十年里，DRAM(动态随机存取存储器)的性能和容量一直遵循摩尔定律，从而跟上了CPU技术的进步。然而，一个基于DRAM的系统，会因为容量、成本和电力消耗的特性受限于现代的工作量。新型非易失性存储器（Non-volatile Memory，NVM）具有传统外存系统持久性和传统内存系统字节可寻址的特点，有望实现高密度、接近零的静态能源消耗和更低的每字节成本。这篇文章对当前混合内存系统的设计进行了总结。The survey covers different aspects of the Hybrid Main Memory Design，包括系统架构、内存管理粒度、监控方法、和迁移算法。此外在抽象混合内存硬件设备的基础上，本文还总结了应用程序感知的混合内存系统，并且指出现有工作中存在的不足。</p><h2 id="Keywords"><a href="#Keywords" class="headerlink" title="Keywords"></a>Keywords</h2><p>Main Memory，Hybrid Memory，Heterogeneous Memory，Tiered Memory</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>The unrelenting growth of the memory needs of emerging datacenter applications.但是随着DRAM容量增速放缓，单个DRAM容量有限，扩大容量规模需要依赖主板插槽数量的增加。而这又带来了延迟和带宽的损失。与此同时DRAM的成本也逐年上升[X 图1]，这使得内存设备的TCO占到了37.1。目前新兴的非易失性主存储器技术和non-DDR memory bus technologies的出现，有效减缓了DRAM与慢速持久存储(即磁盘和SSD)间延迟与容量的差异，有希望从根本上改变存储系统的格局。</p><p>尽管新型计算有很多优势，但由于其相对较高的写入延迟、写入能耗和有限的写入持久性，它目前还不能直接替代DRAM。为了充分发挥两者在混合内存系统中的优势，在性能提升、节能降耗、磨损均衡、数据持久性等方面存在许多开放的研究问题。为了解决这些问题，近年来已经有许多关于内存层次结构的设计[xxx]和内存管理[xxx]方案的研究。这些研究成果带来了混合内存架构、操作系统和编程模型的创新，提供了丰富的思路和工具。</p><p>由于硬件技术的迅速发展，以往的综述已经不能完整的反应整个领域的现状。随着intel 傲腾持久内存从上市到停产和CXL1.0到CXL3.0功能的扩充，对混合内存研究的测重也有所不同。本文对现有工作的回顾可以帮助更清晰的了解领域的发展以及现状。读者可以将一些现有发方法和工具应用到实际生产环境中。对目前工作优点和局限性的讨论也将帮助研究人员更深入地思考。</p><p>我将从以下5个角度对现有工作进行介绍：</p><ul><li>第3段介绍混合内存的系统架构，作为混合内存系统设计的基石。</li><li>第4段介绍了混合内存的管理粒度，大致包括以对象、数据结构、物理页、Hugepage等大小。内存管理粒度的选择影响着第5段和第6段监控方法和算法的设计。不同管理粒度各有优劣。</li><li>   第5段介绍对混合内存中对内存对象或者页面的监控，通过监控信息辅助算法决定内存对象或者页面的调度（应该在何时位于何种内存介质中）。</li><li>   第6段介绍内存迁移算法。</li><li>   第7段介绍应用程序感知的混合内存系统并讨论目前此类系统存在的挑战和对今后工作的展望。</li></ul><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>对系统架构的讨论大致可分为两个角度和4个模式。一个角度是只考虑单个socket的情况，在这个情况下又分为分层内存、缓存、以及前两者混合的架构模式。另一种角度是讨论新加入的非CPU绑定的NUMA节点对现有NUMA架构造成的影响，一些以前设计的优化策略可能反而变得对系统有害。</p><h3 id="分层架构"><a href="#分层架构" class="headerlink" title="分层架构"></a>分层架构</h3><p>分层架构是指NVM通常作为DRAM的后备存储使用,两者构成内存的分层系统。这样做可以提供大容量的存储,同时保持DRAM的高速访问性能。但是管理复杂,为了使得分层系统的性能更接近DRAM，需要精心设计替换算法。</p><h3 id="缓存架构设计"><a href="#缓存架构设计" class="headerlink" title="缓存架构设计"></a>缓存架构设计</h3><p>DRAM被用作NVM的缓存，映射部分NVM内容以提高性能。这样的管理相对简单,不必像分层系统那样做页面的迁移，利用程序局部性掩盖NVM设备延迟和带宽的劣势。但当程序有极差的局部性时，命中率不高，效果不佳。</p><p>两种架构各有利弊,为获取互补优势,一个思路是结合分层缓存的概念。即部分DRAM用作NVM缓存，同时剩余的部分DRAM作为NVM上一层的快速内存层。这样的动态设计可以更灵活使用内存以达到更高的效率，但设计实现复杂度将远远大于前面两种并且不合适的设计甚至带来更差的效果。</p><h3 id="NUMA架构下的异构内存"><a href="#NUMA架构下的异构内存" class="headerlink" title="NUMA架构下的异构内存"></a>NUMA架构下的异构内存</h3><p>在NUMA系统中,根据页的物理地址映射决定将页放置在哪个节点上,以优化访问延迟。但混合内存系统中,由于NVM和DRAM的访问延迟差异很大,页放置位置的选择会更加复杂,需要考虑区分页的访问特性来决定放置在NVM还是DRAM上。混合内存的加入使得NUMA系统的页管理面临更大的挑战。需要更聪明地区分页的特征,动态地在内存层级之间迁移,以发挥多层内存体系结构应有的优势。</p><p>但是目前对于NUMA架构的研究和单个socket的研究是相隔离的，单个socket策略必然会对整个NUMA策略产生影响。</p><h2 id="管理粒度"><a href="#管理粒度" class="headerlink" title="管理粒度"></a>管理粒度</h2><p>在x86指令集架构下的系统默认的管理粒度是4KB也就是Page结构体的大小。然而一些研究者们发现采用更大或者更小的管理粒度对于混合内存系统而言可以取得更好的性能收益。</p><p>当内存变得很大之后，TLB很小缓存的地址转换的条目相对覆盖率就低了，地址转换的开销增大。转换开销主要是由于TLB未命中时缓慢的page table walk造成的。TLB的命中速度很快，但page table walk可能需要四次内存访问才能查询分层页表。较新的处理器由于采用了更深的页表结构，需要多达五次内存访问。大页面通过两种方式减少转换开销。1.由于大页面的单个条目映射了更大的地址范围，因此可以通过增加TLB覆盖率来降低TLB未命中的频率。2.通过减少需要访问的页表级数，加快单个walk速度。</p><p>但是使用大页面往往容易造成内存膨胀和内存碎片，同时不恰当的迁移大页面带来的开销也容易使得系统开销超过地址转换的开销节省。比如每个页面中的热数据只占应用程序总内存足迹的一小部分，但却导致了应用程序总内存引用的很大比例。所以也有些工作提出了如何在适当的时候拆分大页面以及只在较慢的内存层使用大页面。</p><p>一些研究者发现一部分应用对象在不同的执行阶段表现出高度突变的内存访问频率。很大一部分的应用程序的对象都比页小得多同时显示出较长的生命周期。因此他们在编译阶段对程序代码进行分析，提前预估访问模式，决定内存应该从哪种内存介质被分配。在程序动态运行时，依据前一阶段的访问模式对后一阶段进行对象粒度的迁移，节约迁移成本。但是这样的做法依赖于源码，也就是说没有源码的情况下很难通过修改的编译器去提前捕获程序的特征。</p><h2 id="内存监控"><a href="#内存监控" class="headerlink" title="内存监控"></a>内存监控</h2><p>对对象粒度的访问频繁程度统计主要由运行时被指向的指针做分析。而对于物理页面大致分成以下3种方法来监控热页面。</p><p>缺页异常时将页面进行迁移或者收集数据，但是缺页异常在程序执行的关键路径上，此时再去执行其他的操作很容易产生更高的延迟。</p><p>基于页表的页面扫描，在虚拟地址和物理地址转换的过程中会使用页表，页表上有一个A位的访问位，每次页面被访问后都会置位。通过定期扫描页表，扫描后再清空置位来搜集页面访问信息。但是对页表的扫描间隔、扫描的采样粒度会影响CPU开销以及准确度。</p><p>基于硬件的采样，不同设备都会有硬件计数器PMU以及PEBS，这些采样时可以记录物理地址和pid等信息，也会随着采样粒度更小而开销更大。除此之外，如果硬件设备更换,它的性能监控计数器（PMC）的寄存器地址可能会有变化。这时需要重新查看新硬件的编程手册,确定PMC寄存器的新地址,然后修改软件程序中的寄存器访问代码,将其映射到新的地址上。</p><h2 id="迁移算法"><a href="#迁移算法" class="headerlink" title="迁移算法"></a>迁移算法</h2><p>页面迁移算法各种各样，很难将他们分类为不同类别。比较常见的是根据上次是否被访问或者某段时间被访问的次数进行迁移。目前Linux内核二次机会LRU链表就是依据此进行的页面回收。基于默认的单层内存，人们很容易想到在两层内存层种都使用LRU链表。确实有大部分工作基于LRU链表做出改进，并且也取得了显著的效果，比如图4通过在底层增加一个链表使得局部性更好的被保留，系统吞吐量相比于不做迁移最高提升接近2.5倍。但是如果缺少时间局部性，或者被记录的时间不够就不能了解这个页面很长一段时间的访问历史。想要捕获页面的访问历史，目前的方法还比较局限。</p><p>还有的方法通过设置一个阈值来表示页面到底是冷的或者是热的。固定阈值可能的缺陷是在不同时刻定义的热页面多于DRAM（即高性能但有限的快速内存）或者热页面远远小于DRAM，这使得很多不同程度冷的页面也混合到了快速内存层，这就产生了无效的页面分类。为了避免出现以上情况，最近的研究采用了动态阈值的方式，将页面依照实际系统DRAM量分为不同程度的冷热页面，由于提高了DRAM层的利用效率，整个性能也获得不错提升。</p><p>但其实使用的方法真没法分类，太多了都是依据各自要解决的问题为出发点设计的。</p><h2 id="应用程序感知"><a href="#应用程序感知" class="headerlink" title="应用程序感知"></a>应用程序感知</h2><p>设计应用程序感知的混合系统一个前提在于服务器运行同一种或者某几种特定的工作负载。这些工作都尝试从一些关键视角入手,比如工作负载和应用特征、数据本地化、内存访问模式等,在混合内存系统上进行数据放置、任务 Mapping 和内存访问优化,以期获得更好的性能。</p><p>考虑特定工作负载特征,进行负载均衡感知的数据放置。如Merchandiser提出了面向任务并行HPC应用的混合内存数据放置方法,可以提高负载均衡。Liu分析内存访问相似性,改进不规则应用在分布式系统上的性能。Wang根据应用适合的线程数以及内存层优化了HPC程序的数据放置。 HNgraph在NUMA系统中进行图处理任务的数据和任务放置优化。<br>在云数据中心规模下,提供透明的内存卸载机制,以充分利用混合内存系统。Google和Meta分别从系统架构和数据中心视角探讨了这一问题。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>:tear: 可以在笔者的英文综述里对照，不想放了30+篇</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Operating System Level Data Migration Scheme</title>
      <link href="/2023/10/05/An-Operating-System-Level-Data-Migration-Scheme/"/>
      <url>/2023/10/05/An-Operating-System-Level-Data-Migration-Scheme/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">- 文章来自Design, Automation &amp; Test in Europe (DATE), 2016- An Operating System Level Data Migration Scheme in Hybrid DRAM-NVM Memory Architecture</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Reza Salkhordeh and Hossein Asadi, Data Storage Systems &amp; Networks (DSN) Lab, 伊朗谢里夫理工计算工程系</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>本文提出了一种在主存储器中同时采用DRAM和NVM的混合存储器架构中的数据迁移方案。所提出方案的主要目的是减少DRAM和NVM存储器之间无益数据迁移的数量，以提高性能和功效。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>与Clock-DWF不同的是，每次写入命中都会导致将页面移动到DRAM主存储器，在所提出的方案中，NVM LRU中的每次命中都将被与LRU算法类似地处理，但有一个区别。如果某个页面停留在LRU顶部页面的时间超过阈值访问次数，则该页面将被视为热页面，并将被移至DRAM。由于在两个存储器之间移动数据页的成本很高，因此使用此阈值将防止在以前的研究（例如Clock-DWF）中很可能发生的无益迁移。</p><p> CLOCK-DWF 使用两种时钟算法，每个内存模块使用一种时钟算法。当发生页面错误时，如果导致页面错误的请求是写入，则该页面将被移动到DRAM，否则它将被移动到NVM。时钟算法的修改使 CLOCK-DWF 能够找到活跃的并且写入为主的数据页并将它们移动到 DRAM 内存。如果针对驻留在 NVM 存储器中的数据页的写入请求到达，则该数据页将被移动到 DRAM。在两个存储器之间迁移页面需要对两个存储器进行多次访问。但CLOCK-DWF中没有考虑这种影响，这将导致其模型不准确</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>使用两个最近最少使用（LRU）队列（一个用于DRAM，一个用于NVM），并优化NVM的LRU队列，以防止无益的迁移到DRAM。LRU队列中的优化是最小的，因此所提出的方案将具有与未修改的LRU几乎相同的命中率。</p><h3 id="1）建立性能能量模型"><a href="#1）建立性能能量模型" class="headerlink" title="1）建立性能能量模型"></a>1）建立性能能量模型</h3><p>18年华科的对象级迁移的性能能量建模就和这个差不多。但是这里的性能能量建模只适用于说明之前方案的缺陷，和后面方案设计没太大关系。</p><p>性能模型取决于DRAM和NVM的延迟、驱逐的粒度以及存储器之间迁移的延迟。为了衡量性能，我们使用平均内存访问时间(AMAT)。迁移的开销将在对内存的所有访问之间按比例分配。公式1显示了AMAT 的公式。表I中提供了参数的描述。在该等式中，前两项计算DRAM或NVM中所有命中访问的 AMAT。第三项考虑页面错误。由于将数据页从磁盘传输到内存将通过DMA 完成，因此将数据块写入内存的延迟将与从磁盘读取下一个数据块的延迟重叠。因此，操作系统只看到磁盘延迟，在这个术语中我们只考虑磁盘延迟。最后两项计算两个存储器之间的迁移成本。当发生迁移时，将从一个内存读取数据页并将其写入另一个内存。由于数据页的粒度比对内存的实际访问要大得多（通常为4到16B），因此我们使用PageFactor，它是一个系数，可将数据页的移动转换为要被访问的内存数量。</p><div class="note info no-icon">PageFactor这个转换多余了吧，按页迁移也没法只移动这几B的，属于附带消耗。这位伙计定义的表格里的能耗什么的怎么收集到的，还有那些概率</div> <p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b5b2cad055864692a88d2b9ecbb5485d.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/add1e59ff2e8473f8454f2bf31f34f89.png"></p><p>无论到达存储器的请求数量如何，都会消耗静态功耗，而每个发送到存储器的请求都会消耗动态功耗。我们的电源模型考虑了两个内存之间的迁移以及将页面从磁盘移动到任一内存模块以及服务请求的静态和动态电源。每次访问内存时都会计算动态功耗。这将导致功耗模型独立于应用程序运行时和内存大小。因此，我们引入每个请求的平均功率(APPR)作为测量功率的指标，如公式2所示。与性能模型类似，前两项计算对存储器的所有命中访问的功率。第三项和第四项考虑将数据页从磁盘移动到内存模块的写入功率。最后两项考虑了两个存储器之间迁移的功率效应。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/78719c197cd24a28942ca867cf81a3a1.png"></p><p>由于静态功耗与请求无关，因此我们引入了一个名为 AvgStaticPower 的新参数，该参数按比例分配给定时间间隔内到达内存的所有请求之间的静态功耗。对所有请求按比例分配静态功耗的原因是，从操作系统的角度来看，主内存会消耗功率（包括静态和动态）来服务请求，并且这两个功耗来源都应被视为服务成本的请求。对于特定工作负载，AvgStaticPower根据公式3计算。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1103cf68e870402d91ee88f71097d61c.png"></p><h3 id="2）迁移方案"><a href="#2）迁移方案" class="headerlink" title="2）迁移方案"></a>2）迁移方案</h3><p>为了找到在迁移时能够改善功耗和性能的数据页（相对于迁移成本），所提出的方案存储了一些关于数据页的附加信息，例如NVM LRU队列中的读和写计数器。请注意，此附加信息不会干扰LRU，并且不需要了解此内务信息。对于NVM队列中的每个数据页，将存储两个计数器，用于计数从数据页进入队列时起对相应数据页的读写访问的次数。</p><p>图 3 显示了所提出的由两个 LRU 队列组成的数据迁移方案的架构。虚线表示所提出的技术执行的操作，实线表示传统的 LRU 管理算法。暗数据页访问更频繁，被视为热数据页。与 CLOCK DWF 将读取请求发出的页面错误放置在 NVM 上相反，所提出的方案将所有页面从磁盘移动到 DRAM 区域。这是因为移动到 NVM 或 DRAM 将导致 NVM 中的页面写入，因为 DRAM 始终已满，并且将数据页面移动到 DRAM 会向 NVM 发出驱逐。因此，就 NVM 写入而言，迁移到 NVM 或 DRAM 的成本是相同的。与旧数据页相比，新访问的数据页具有更高的访问概率，并且将此新页移动到 DRAM 将导致 DRAM 命中率而不是 NVM 命中率的增加。这将有助于提高性能和电源效率，因为 DRAM 在动态功耗和延迟方面具有优越性。存储内务信息的开销并不大，对于 4KB 数据页来说约为 0.04%。然而，在 NVM 中保留所有页面的计数器有一些缺点。首先，它需要一个排序方案，以便识别冷但会在很长一段时间内访问一次的数据页。这些数据页将在 NVM 中驻留足够长的时间以具有较高的计数器值，因此将被移动到 DRAM，在那里它们无法与热数据页竞争，并且将返回到 NVM，这使得它们迁移到 DRAM 没有任何好处。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/26073db180a64a9c888e27b6f08665c5.png"></p><p>算法 1 显示了在收到请求的情况下所提出方案的流程。由于 DRAM 包含最热的数据页，因此所提出的方案首先搜索 DRAM，如果没有找到，则转到 NVM。在 DRAM 中查找数据页将导致正常的 LRU 内务处理。否则，NVM 中的额外内务信息将根据请求类型进行更新。读取和写入计数器将分别存储在 NVM 中的 readperc 和 writeperc 顶部数据页。因此，在命中的情况下，从顶部数据页掉落的数据页的读和写计数器将被清除。第 10 行到第 22 行初始化相应数据页的计数器。如果 NVM 中数据页的计数器值超过读取阈值或写入阈值（取决于请求类型），则会将其迁移到 DRAM。将新数据页插入内存和逐出策略与 LRU 相比没有变化，因此，为了简洁起见，算法中省略了这些细节。读取阈值和写入阈值的值决定了我们计划如何积极地阻止有用概率较低的迁移。它与 DRAM 和 NVM 之间的迁移成本密切相关，而迁移成本又与所采用的 NVM 的性能和功耗特性有关。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c39c07a13b894e9ba202c0ea626bdc5b.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>发现了先前研究的缺点，并提出了一种新颖的混合内存数据迁移方案。在LRU上加了一些小改动。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>在过去几年提供的各种NVM中，相变存储器(PCM)、自旋转移扭矩(STT-RAM) 和电阻RAM(PRAM)被认为是主存储器中最有前途的NVM[2] 。</p><p>本文中我们使用COTSon全系统模拟器[13]。</p><p>CLOCK-DWF 维护 DRAM 和 NVM 的两种时钟算法。 NVM中的时钟算法与传统的时钟算法有一点不同。如果对 NVM 中的数据页进行写访问，则相应的数据页将被移动到 DRAM。因此，NVM 不会响应任何写访问。该方法的主要目的是减少 NVM 中的写入次数。虽然这会阻止任何写入到达 NVM，但 NVM 中数据页的每次写入访问都会导致两个存储器之间的数据页迁移。然而，DRAM 的时钟算法有所不同，它尝试将写主导的数据页保留在 DRAM 内存中，并逐出主要读主导的数据页。这是因为与 NVM中的写入请求相比，只读页面具有更好的性能与功耗权衡。发生页面错误时，如果请求是读，则相应的数据页将被移动到NVM，如果是写，则数据页将被移动到DRAM。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Characterizing and Optimizing Hybrid DRAMPM Main Memory System with Application Awareness</title>
      <link href="/2023/10/04/Characterizing-and-Optimizing-Hybrid-DRAMPM-Main-Memory-System-with-Application-Awareness/"/>
      <url>/2023/10/04/Characterizing-and-Optimizing-Hybrid-DRAMPM-Main-Memory-System-with-Application-Awareness/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自Design, Automation &amp; Test in Europe(DATE), 2022</li><li>Characterizing and Optimizing Hybrid DRAM-PM Main Memory System with Application Awareness</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Yongfeng Wang, Yinjin Fu, Yubo Liu, Zhiguang Chen, Nong Xiao中山大学计算机科学与工程学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>PM大容量之后，<span class="label primary">多个应用程序工作负载并发执行的数据放置、内存并发和工作负载调度方面存在关键的管理挑战</span>作者总结在混合内存系统场景下讨论的3个问题：数据放置、优化线程分配、并发程序执行顺序如表1</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c2fd2e2f7b514331a1be9c6f6fe7fcb0.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>提出了一系列应用感知的操作策略，包括：应用感知的数据放置、自适应线程分配和避免应用间干扰，以提高混合存储器上多个应用的并发性能。</p><p><span class="label primary">1)如何将应用工作负载分配到合适的内存设备上，以提高并发应用的整体系统性能?</span><br>2)如何通过为每个应用程序分配最佳的线程数来保证不同工作负载的并发执行的公平性?<br>3)什么样的应用工作负载调度方案可以避免或减轻两个或多个同时运行的应用之间的干扰，以实现更高的性能?</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>但是这些方案没有去具体考虑并发应用的个别需求(应用感知)。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="1）将应用感知解释为带宽敏感或者延迟敏感。"><a href="#1）将应用感知解释为带宽敏感或者延迟敏感。" class="headerlink" title="1）将应用感知解释为带宽敏感或者延迟敏感。"></a>1）将应用感知解释为带宽敏感或者延迟敏感。</h3><p>根据系统资源需求的不同，我们可以将计算服务器的应用工作负载分为CPU绑定和内存绑定。为了将我们的工作重点放在内存资源管理上，绑定内存的应用被进一步分为对带宽敏感的应用和对延时敏感的应用。我们的分类使我们能够清楚地区分工作负载类别，因为每个工作负载类别都形成了自己独特的集群。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cddfdfc49de643f0b30cf713269f15c8.png"></p><p>图1显示了所有工作负载的延迟和内存带宽需求的敏感性。当一个应用程序在x轴和y轴上的数值都很低时，它<br>就是一个受CPU约束的应用程序。对性能要求高的数据库工作负载对延迟最敏感，而对带宽的敏感度低。而大数据<br>分析的工作负载，如OLAP、Lu cb和facesim,对带宽和延迟的敏感性处于中间水平。HPCG、Graph500和一些数据密<br>集型工作负载对带宽的敏感性最高，对延迟的敏感性最低绑定CPU的工作负载，如HPL、swaptions和freqmine, 对<br>内存延迟或带宽的敏感性不高。</p><div class="note info no-icon">不过，这个图是怎么得到的？</div> <p>下图表示应用感知优化后达到的效果：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b4675b4852e44b7cba5a23362663c62f.png"><br>大致看起来是想说，第三节数据放置达到缩短运行时间的效果、第四节线程分配、第五节接口冲突。</p><h3 id="2）数据放置策略"><a href="#2）数据放置策略" class="headerlink" title="2）数据放置策略"></a>2）数据放置策略</h3><p>在混合内存中，两种内存介质上的应用性能是完全不同的，而且这种影响的大小在很大程度上取决于应用的特性为了评估这一点，我们从PARSEC[16]、splash2x[17]和NPB[18]选择了几个测试。</p><div class="note info no-icon">这里选择都是并发的程序，而且两种介质可以影响性能的差别就是延迟和带宽（还有其他什么吗？），程序又有不同的延迟带宽敏感。在纯PM上运行，这只说了物理机配置，所以这是怎么跑起来的？</div> <p>图3显示了在PM上运行的8线程应用程序工作负载与DRAM上运行的应用程序工作负载的性能下降情况以及完成时间的比率。每个应用程序将分别在PM和DRAM上运行10次，我们记录它们的平均完成时间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8b9d7567cae24be79b422c66b5289d60.png"></p><p><span class="label primary">持久性存储器的带宽比DRAM低，这使得应用程序在PM上运行的时间更长。以ocean_cp为例，它在计算的不同阶段流经其分割的许多不同的网格，随着问题大小的增加，会产生大量的容量和冲突失误[17]。当工作集超过高速缓存的<br>大小时，内存带宽就会成为这个应用的瓶颈。在这个实验中，ocean_cp在DRAM上的读和写分别消耗了40.9GB/s和<br>15.3GB/s的带宽，而在PM上由于其带宽较低，只消耗了2.45GB/s和0.8GB/s。这个17倍的带宽差距可以解释ocean_cp在PM上的减速。PM的有限带宽也是对带宽敏感的应用减速的主要原因，如canneal、radix、NPB/FT、NPB/MG和<br>NPB/SP。此外，其他应用工作负载在PM上的运行时间不到DRAM上的1.5倍，如freqmine和swaption。 它们都是与CPU绑定的应用，可以充分利用内存的局部性来减少高速缓存的miss。因此，内存不是这类应用的瓶颈。因此,在不同的内存介质上运行的应用程序对这些工作负载的执行时间有着难以察觉的影响。</span></p><div class="note info no-icon">这里的意思是带宽敏感和数据放置关系更紧密，延迟敏感和之后要说的会比较相关。后面的分配策略完全也不考虑热页面迁移了，直接不是带宽敏感丢给PM。硬件性能计数器很多啊，带宽敏感是用啥表示？而且你在DRAM不够时怎么在分配之前知道这是不是带宽敏感的，那就是用户手动指定嘛，运行时硬件计数肯定不能提前知道；另一方面正在运行时的内存分配要按照不同workload做标记咯，要不你现在要分配的page你也不知道是哪个程序申请的，他自己是不是带宽敏感</div> <p>当应用程序工作负载启动时，如果有足够的可用空间，它首先尝试将其分配到DRAM上。否则，我们必须判断应用程序是否对带宽敏感。对于带宽敏感的应用程序，它可以将CPU-Bound应用程序的一些占用的DRAM页面迁移到PM，然后为新应用程序分配这些DRAM页面。如果DRAM中的所有应用程序工作负载都是带宽敏感的，我们只需将新应用程序分配给PM即可。此外，我们还将对带宽不敏感的应用程序分配给性能下降较低的PM。为了使我们的应用程序感知策略切实可行，应用程序类型可以由用户定义或根据硬件性能计数器的信息自动分类。</p><p>下面这个实验表示通过这种方法程序放置对了，而且节约了总的执行时间。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/23f0d575dd7547d4902c544be540cd3a.png"></p><h3 id="3）线程分配（内存并发）"><a href="#3）线程分配（内存并发）" class="headerlink" title="3）线程分配（内存并发）"></a>3）线程分配（内存并发）</h3><p>在不同介质上，线程可扩展性不同，在PM上运行的对带宽敏感的应用程序的性能不能随着线程数的增加而同步提高。因此，有必要为运行在不同内存介质上的应用程序找到一个合适的并行参数。</p><div class="note info no-icon">这个怎么还能放先验知识，而且the remain thread resources can be allocated to other applications这个资源是可以被量化的吗？</div> <p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f9dbccacad674d3b8eb4008a6a6113cc.png"><br>在第1组中，左侧结果显示每个应用程序传统均匀分布的8个线程的持续时间。根据先验知识，我们发现PM上的最佳线程NPB/FT数量为4。</p><h3 id="4）带宽争用"><a href="#4）带宽争用" class="headerlink" title="4）带宽争用"></a>4）带宽争用</h3><p>当两个或多个应用程序同时访问混合内存时，会出现带宽争用，这将导致严重的干扰和并发性能下降。而且混合内存中的这种干扰比纯DRAM内存系统中的干扰更为复杂。为了证明这一点，我们首先尝试遍历所有内存访问模式。我们的测试包括4种类型的内存访问模式（1）SR：顺序读取，（2）RR：随机读取，（3）SW：顺序写入，（4）RW：随机写入1、2、4、8和10DRAM和PM上运行的线程，将形成40种内存访问模式。然后，我们运行带宽基准测试并获得所有内存访问模式的带宽，没有任何干扰。最后，分别运行40种内存访问模式构建40种干扰，对每种内存访问模式重新进行基准测试，得到每种干扰下的带宽缩减率。结果如图7所示，概括为以下两个方面：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/a4f8361713864c139a2c9e25be08ccb5.png" alt="X轴代表干涉类型，Y轴代表基准类型。颜色越深意味着与没有干扰时相比，带宽减少得越多"></p><p><span class="label primary">访问DRAM对PM的带宽影响很小，而PM的读或写会显着减少DRAM的带宽。</span> PM上10线程顺序读取的带宽为13GB/s，相对于10线程DRAM读写的干扰，最多可以降低15%（11GB/s）。相比之下，PM上有10个干扰线程进行顺序读取，DRAM上顺序读取的带宽可减少80%（94GB/s至18GB/s）。而且，对于DRAM的带宽来说，PM读写造成的干扰比DRAM上的干扰更为显着。</p><p>其次，线程数量和内存访问模式对并发应用的干扰影响很大。随着线程数量的增加，干扰会变得更加严重。在具有1、2、4、8或10个线程的PM上随机写入将使具有10个线程的DRAM上的顺序写入的带宽减少33.8%、68.2%、89.9%、96.7%、97.6%（68GB/s 至 1.7GB/s）。对于内存访问模式来说，随机写入会对DRAM的写入性能产生最严重的干扰。考虑到PM上的密集写入会减少带宽95%以上在 DRAM 上运行的带宽敏感应用程序仍然会受到严重影响。<span class="label primary">PM上的密集访问将显着减少DRAM的带宽。对PM的随机写入和顺序读取分别对DRAM的写入和读取带宽有更严重的干扰</span></p><p>解决方式就是通过上面的分析动态调整这些应用程序工作负载的执行顺序,也是通过先验知识去查找。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b406960826234fc8a2610dc1b7a2befd.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>这个在上一节已经说了。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>也是在5部分吐槽了。解决方案很多靠先验经验或者用户直接指定，而且细节也不咋提。虽然这个是针对并发程序设计的，也可以和其他混合内存系统相比较一下啊。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>从带宽敏感考虑了内存分配，而且性能提升还不错。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>GPU内存系统中，应用感知的内存调度方案[13], [14]可以通过减少地址转换和数据请求之间的干扰来提高<br>公平性和整体系统性能。</p><p>应用感知的存储器通道划分算法[15]可以为不同的应用分配首选的存储器通道，以减少应用间的存储器干扰。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Going Vertical in Memory Management</title>
      <link href="/2023/09/15/Going-Vertical-in-Memory-Management/"/>
      <url>/2023/09/15/Going-Vertical-in-Memory-Management/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自International Symposium on Computer Architecture，ISCA，2014</li><li>Going Vertical in Memory Management: Handling Multiplicity by Multi-policy</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Lei Liu，Zehan Cui，Yungang Bao，Mingyu Chen，Chengyong Wu，中国科学院计算机研究所计算机体系结构国家重点实验室</li><li>Yong Li匹兹堡大学ECE系</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>来自不同领域的许多新兴应用程序通常表现出异构内存特征。<span class="label primary">当在并行平台上组合运行时，这些应用程序会呈现出令人畏惧的各种工作负载行为</span>，这对任何<span class="label primary">内存分配策略的有效性</span>提出了挑战。先前的基于分区或随机内存分配方案通常仅管理内存层次结构的一级，并且通常针对特定工作负载。</p><p><strong>共享内存资源</strong>的有效管理对于应用程序性能和系统吞吐量非常重要。然而，商业化并行机中使用的大多数现有内存和缓存管理机制都采用通用地址交错或调度/分区方法，这些方法忽视了当今异构环境中不同的内存利用率特征和不同的资源需求。这通常会导致程序间扰动、资源颠簸、内存/缓存利用率低下，从而导致性能下降。</p><p>当时架构允许所有应用程序共享LLC（末级缓存）和DRAM组，从而在许多情况下导致严重的争用。一般用页面着色解决，存在两种基于页面着色的分区技术，即高速缓存分区和DRAM存储体分区。如图 1 所示，可以通过使用操作系统物理页地址中表示LLC集索引（LLC颜色位）的位作为颜色位来实现缓存分区。当为应用程序分配页面时，操作系统可以为物理页面分配特定的颜色，以便应用程序只能访问指定颜色的缓存集。</p><div class="note info no-icon">说人话：运行的特定某个程序只能使用颜色A的DRAM分区，或者只能缓存到颜色A'的LLC缓存分区中</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/510d5e5c024448c78ccbc62c9e047875.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>处理各种动态变化的内存和缓存分配需求。需要设计一种能够通过区分内存特性来选择合适的分配策略的内存管理系统。</p><p>（为了实现这一目标，简单地集成最佳性能的机制是不切实际的，因为几乎所有最先进的方案都需要对内存控制器/分配器或缓存层次结构进行昂贵的更改，更不用说检测和预测应用程序需求和冲突方面的挑战了。）</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>最近的几种解决方案尝试通过将主内存（DRAM 组）[10,16,17,29] 或缓存 [15,24,30,31,32] 水平分区为独占片来隔离具有不同内存资源需求的应用程序。这些方法避免了对内存占用较小的程序的干扰，但可能会通过有效减少容量来影响较大工作负载的性能。操作系统级别的分区和其他内存分配优化更加灵活，并且在许多案例方面表现良好。</p><p>先前的研究工作[12,29]表明，LLC和DRAM争用会显着降低整体系统性能，并且已经提出了许多解决方案来缓解争用问题。最有效的优化之一是基于页面着色的软件分区，它允许操作系统内核利用底层架构信息，例如 LLC 和 DRAM 的物理地址映射。通过页面着色，可以通过修改内核伙伴系统来缓解争用问题 [4,10,15,17,21,22,24,26]，同时避免对内存控制器或缓存层次结构进行昂贵的硬件更改。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>为了处理多样化且动态变化的内存和缓存分配需求，我们通过垂直分区增强现有的“水平”缓存/DRAM 存储体分区，并探索由此产生的多策略空间。着色位分为三类：bank-only、仅高速缓存位（C位）和重叠位（O位在图 1 中同时索引bank和高速缓存）。特别是，O位启用垂直分区（VP），通过内存层次结构垂直地对高速缓存和bank组进行分区。结合水平和垂直分区形成了以前未研究过的分区策略空间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d82182ffc00c4216b76bef9a55d7bd4a.png" alt="从操作系统和典型多核机器上三类颜色位的角度来看的地址映射"></p><p>水平内存和缓存分区的好处是否可以累积（即，我们应该进行垂直分区吗？）所以作者测了测如下表所示的几种搭配<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/26bbd7193c5f467c9dbd3405d6959b7c.png"></p><p>把214个workload根据性能提升的原因做了可视化<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/93ea1c4e302d44109902c0078a399e3f.png"></p><p><strong>从上述定量研究中可以得出一个明显的结论：内存分配策略的有效性取决于特定的应用程序特性，特别是缓存需求。实际上，工作负载可能包含多个同时运行的应用程序，这些应用程序具有不同特征的任意组合，这使得确定适当的内存分配任务具有挑战性。</strong>然后总结了一下其他因素影响不大，缓存分区性能表现出的性能差异更大。为了验证缓存利用率特征对缓存分区策略的潜在影响，我们收集了当缓存配额从8/8（使用整个缓存）减少到1/8时各种应用程序的性能下降情况。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d6f56fdc5eca4311851b27a93bc66713.png"></p><p>每个应用程序都会执行八次，每次都会通过基于页面着色的缓存分区分配不同数量的 LLC。根据结果​​，我们将应用程序的缓存行为分为四类：Core Cache Fitting（CCF）、LLC High （LLCH）、LLC Middle（LLCM） 和LLC Thrashing（LLCT）。图 4 报告了SPEC2006基准测试套件中各种基准测试的分类 [1]。CCF应用程序（表示为绿色曲线），例如hmmer和namd，在使用较少的LLC资源时不会显着降低性能，因为它们的工作集大小足够小，可以容纳L1和L2每核专用缓存。LLCT应用程序（黑色曲线），例如libquantum，也对缓存配额不敏感，但这是由于缓存抖动行为而不是较小的工作集大小。LLCH应用程序（红色曲线）（例如mcf）由于其资源匮乏的特性，因缓存配额减少而遭受最严重的性能下降。与LLCH相比，LLCM（蓝色曲线）应用程序使用更少的缓存资源，因此速度减慢没有LLCH应用程序那么多。例如，gcc和bzip2是LLCM，因为当缓存从8/8减少到4/8时，它们不会遭受明显的降级。然而，当缓存配额降至3/8以下时，性能会急剧下降。</p><p>但是要动态分类：做图4时的静态分析发现热页面的数量在很多情况下可以反映应用程序的LLC需求。图5显示了多个基准测试的热门页数量和缓存需求之间的相关性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e10451bd578c4c58bbc3ceb565996949.png"></p><p>研究了这些策略针对<strong>2000</strong>多个工作负载的性能，并通过<strong>数据挖掘方法将结果与应用程序特征相关联</strong>。基于这种相关性，我们得出了几种实用的内存分配规则，并将其集成到统一的多策略框架中，以指导动态和多样化的多编程/线程工作负载的资源分区和合并。（生成了一套实用的分区和聚合规则以及一棵策略决策树，帮助HVR自动选择策略、动态资源分区和聚合。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c95f6f8820ec410285d3dadd9678cb5c.png"></p><p>（置信度和支持度是数据挖掘中的术语。在我们的工作中，支持被定义为规则中包含特定类型应用程序的工作负载的比例；置信度表明该规则的准确性。）</p><div class="note info no-icon">其实就是先采样热页，制定一些阈值，得到分类算法。然后将workload名称和分类绑定成一个键值对，再将分类和着色位的性能（就是最初的散点图）建立相关规则（通过观察得到的，比如：“包含 LLCT 和其他应用程序（LLCH、LLCM、CCF）的工作负载应使用 C-VP 或 A-VP（37.1% 支持，94.4% 置信度）”）。！！总是一开始使用bank-only然后迁移！！</div><p>在 Linux 内核 2.6.32 中将我们的方法实现为重构的页面索引系统加上一系列内核模块。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>大量实验表明，在实践中，我们的框架可以选择适当的内存分配策略，并始终优于未修改的Linux内核，与现有技术相比，性能提升高达11%。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>运行时采样得到分类，再和着色位绑定太晚了吧。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> A </tag>
            
            <tag> Chip on </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dancing in the Dark Profiling for Tiered Memory</title>
      <link href="/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/"/>
      <url>/2023/09/15/Dancing-in-the-Dark-Profiling-for-Tiered-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Parallel and Distributed Processing Symposium, (IPDPS), 2021</li><li>Dancing in the Dark: Profiling for Tiered Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Jinyoung Choi, 加州大学河滨分校（UC Riverside）</li><li>Sergey Blagodurov, 美国超威半导体公司（AMD）</li><li>Hung-Wei Tseng, UC Riverside</li></ul><h2 id="2-Introduction-amp-Background"><a href="#2-Introduction-amp-Background" class="headerlink" title="2. Introduction &amp; Background"></a>2. Introduction &amp; Background</h2><ul><li>究竟如何向软件公开每种内存类型是一个悬而未决的问题</li><li>多种内存技术协同-&gt;更多内存、更少延迟、更大带宽</li><li>tiered memory architectures （TMAs）</li><li>缓存会给更多设计给硬件，平面寻址会有更多软件策略</li></ul><p><strong>软件监控方法，通过TLB条目和内存页的现状：</strong></p><ol><li>页面访问不可见只有缺页时才知道，识别被多次访问的代码段很难。</li><li>硬件监控的多样性，是由供应商提供，没有标准。</li><li>创建一个新的分析工具，组合硬件监控器的信息，抽象出一些二级指标，再通过一些策略，做出页面热度排名之类的。</li></ol><p><strong>与硬件缓存相比，软件控制的分层内存4个优点：</strong></p><ol><li>首先，分层内存允许直接从数据所在的层进行就地内存访问。使用硬件缓存时，请求的内存块将从第2层可寻址内存引入第1层缓存，从而导致流量增加。而且缺页导致的迁移开销大。</li><li>其次，在TMA中，内存页面可以在任一层中找到；缓存会在内存中创建重复的、可能不一致的页面副本，并且需要机制来保持数据一致。RDMA做缓存带宽效率会比较低[2][10]</li><li>第三，分层内存解决方案允许缓存策略微调（通过工作负载混合、服务级别协议等）以适应高层策略决策并消除过度迁移。（不懂）</li><li>第四，由于与根深蒂固的NUMA架构相似，分层内存可以利用NUMA和异构内存管理（HMM）[5]系统基础设施。 Linux社区中关于如何将NVM暴露给操作系统一直存在争论。当前的提案围绕将NVM配置为无CPU的NUMA节点以及使用AutoNUMA或其他现有的管理NUMA方法[11]、[12]管理TMA平衡。这里再次强调的是内存如何分配和移动，而不是如何分析热度（例如，AutoNUMA 中的定期取消映射和页错误处理如何产生开销 [13]）。我们的工作重点是比较各种监控方法，以最小的开销获得最大的热度可见性，这对 NUMA 和分层内存都有好处。</li></ol><p><strong>内存分析方法：</strong></p><ol><li><span class="label info">页表项PTE位跟踪。PTE包括已访问A位和脏D位。操作系统可以清除这些PTE位，并且硬件页表遍历器PTW将设置它们。</span>PTW在TLB未命中时设置A位。A位不区分在分析间隔期间访问一次的页面和多次访问的页面。更频繁的A位检查可以提高分析的信息量，但也会增加开销。</li><li><span class="label info">TBP能够从加载或存储指令收集地址跟踪</span>。在AMD系统上，基于指令的采样IBS[18]、[19]使CPU指令能够在穿过管道时被标记，允许在指令执行时收集数据，并在指令退出时引发中断。</li><li>轻量级分析（LWP） [20] 是AMD64系列15h AMD处理器的硬件扩展，与IBS的不同之处在于LWP在生成中断之前收集大量数据。</li><li>英特尔的处理器基于事件的采样（PEBS）是一种基于跟踪的功能，类似于IBS/LWP，其中处理器在指定的内存区域中记录标记的样本。PEBS样本可以根据许多事件（例如缓存未命中）来选择，每个PEBS记录包含<span class="label info">时间戳、线性地址和物理地址</span>等[21]</li><li>硬件性能计数器HWPC是大多数现代CPU和GPU上可用的特殊硬件寄存器，作为性能监控单元PMU的一部分。通过事件复用，perf和pfmon等软件工具可以监控比物理寄存器更多的事件。HWPC是粗粒度的（所有进程页面的一个指标），并且不能用于获取内存访问跟踪 [25]</li><li>BadgerTrap[6]拦截TLB Miss，并且对TLB做相应修改，用采样页预估总体未命中次数。</li></ol><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>这篇文章评估了很多内存监视器，提出一种统一的方法，为分层内存提供详细、低开销的可见性。</p><p>(1) 提出了一种低开销、高精度的分析机制，可以缓解TMA中的性能问题。<br>(2) 本文产生的见解可以指导TMA的高效内存管理策略。<br>(3) 本文使用所提出的分析机制来实现和评估内存管理策略，以在不进行任何硬件修改的情况下实现加速。<br>(4) 介绍了一种分析工具作为可升级的解决方案，以提高分层内存系统的性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b3e4837a025a42dbaec15d17e0663308.png"></p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>figure1系统的架构图，说明了每个部分的功能，其实就是将以上提到的组合了一下，取长补短。TMP 使用多种互补的监测方法，最大限度地提高信息量并最大限度地减少开销（表 I）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d1e3a9c929af46c796a7d2a5061b421c.png"><br>实施方案，非常细节可以对着代码看。</p><p>TMP在分层内存中的使用还是识别页面冷热，并且排序，然后确定哪些页面是可以迁移的，迁移时虚拟地址不变，页面物理地移动（这一段介绍也很细节，可以对着代码看）作者figure6的分析表明，是想要去提高第一层的命中率的。</p><div class="note danger no-icon"> 迁移会涉及TLB shootdown这个开销大？</div><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>选用了云工作负载和HPC工作负载，但是这位怎么只给了表格没有引用emmm，而且云工作负载看起来像自己取的名字。</p><h2 id="7-缺陷和wuwu改进思路"><a href="#7-缺陷和wuwu改进思路" class="headerlink" title="7. 缺陷和wuwu改进思路"></a>7. 缺陷和wuwu改进思路</h2><p>但是整个组合会变得复杂，到底在收集信息时内核负担会不会变大反而效率更低。文章说：保持工作负载开销低于应用程序开销的 5%。</p><p>有的对硬件的采样是不允许的，那么这个可能有的地方收集不了数据。</p><p>有的采样率还是得手动调。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>数据密集型服务器：内存中键值存储、数据库、整合在各个云服务器上的虚拟机以及需要大量内存的高性能计算HPC应用程序。</p><p>对延迟的优化，可以通过命中更多的热页面，常用的优化内存延迟的方式。</p><p>内存技术及其相关字节可寻址接口（例如 NVDIMM P、CXL、3D XPoint DIMM、CCIX和Gen-Z）具有不同的延迟、带宽、功耗、持久性和每GB成本特征，成功的分层内存架构必须依赖系统来最小化与内存内容访问和每个内存请求的地址转换相关的延迟。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> 内存监控工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Center Ethernet and RDMA Issues at Hyperscale</title>
      <link href="/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/"/>
      <url>/2023/08/22/Data-Center-Ethernet-and-RDMA-Issues-at-Hyperscale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE Computer Volume: 56, Issue: 7, July 2023</li><li>Data Center Ethernet and Remote Direct Memory Access: Issues at Hyperscale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Torsten Hoefler , ETH Zürich</li><li>Duncan Roweth, Keith Underwood, and Robert Alverson, Hewlett Packard Enterprise</li><li>Mark Griswold, Vahid Tabatabaee, Mohan Kalkunte, and Surendra Anubolu, Broadcom</li><li>Siyuan Shen, ETH Zürich</li><li>Moray McLaren, Google</li><li>Abdul Kabbani and Steve Scott, Microsoft</li></ul><p>使服务器应用程序可以直接操作远程服务器的内存,不需要经过操作系统和CPU。</p><p>Fabric，就是支持RDMA的局域网(LAN)。</p><p>怎样才能对内存进行传输，注册。 因为RDMA硬件对用来做数据传输的内存是有特殊要求的。</p><p>在数据传输过程中，应用程序不能修改数据所在的内存。<br>操作系统不能对数据所在的内存进行page out操作————物理地址和虚拟地址的映射必须是固定不变的。<br>注意无论是DMA或者RDMA都要求物理地址连续，这是由DMA引擎所决定的。</p><p>几十年来，以太网一直在有线局域网领域占据主导地位，范围从私人住宅的部署到最大的数据中心。数据中心经历了巨大的增长在过去的十年中，联网机器的数量超过了当今最大的超级计算机的规模。尽管仍存在一些差异，但此类超大规模数据中心和超级计算机的网络要求非常相似。[1]然而，超级计算机传统上使用专用互连进行连接，而数据中心则建立在以太网上。由于类似需求和规模经济，随着每一代新技术的产生，两者的结合不断紧密。我们认为现在是重新思考融合互连的基本假设和架构的最佳时机。</p><h3 id="数据中心以太网的新环境"><a href="#数据中心以太网的新环境" class="headerlink" title="数据中心以太网的新环境"></a>数据中心以太网的新环境</h3><p>多种技术趋势正在加速高性能互连的融合。首先，不断增长的网络性能要求推动了可支持TB级带宽的更高效的主机堆栈的发展，每秒数亿美元的交易以及新兴数据密集型应用程序（例如人工智能（AI））所需的单位微秒延迟。[2]这些极端要求迫使所有协议和硬件都尽可能高效，以至于尽可能有效排除许多传统上驱动数据中心网络的TCP/ IP状堆栈远程直接内存访问（RDMA）是大约三十年前用于高性能计算（HPC）工作负载的远程内存访问（RDMA），后来又将其扩展到InfiniBand(IB) verbs RDMA的目标存储。RDMA可以通过网络启用CPU释放的硬件加速DMA。在过去的10年中，它事实上已成为低开销和高速网络的标准。</p><blockquote><p>InfiniBand verbs 是InfiniBand架构中定义的一套编程接口(verbs),用于应用程序访问InfiniBand网络资源并利用RDMA(远程直接内存访问)技术进行通信。</p></blockquote><blockquote><p>在数据中心网络中排除传统TCP/IP栈的主要原因有:</p></blockquote><ol><li>TCP/IP栈开销大。TCP/IP栈在主机内核中运行,每次网络IO都需要经过完整的网络协议处理,包括缓冲拷贝、上下文切换、校验计算等,增加延迟和CPU使用。</li><li>不必要的内存复制。数据在应用、内核、NIC间多次缓冲拷贝,影响吞吐量和延迟。</li><li>CPU利用率高。网络栈处理占用大量CPU资源,对计算密集型应用影响很大。</li><li>通信堆栈长。TCP/IP通信堆栈包含过多层,每个层在主机内核和NIC都要处理,增大延迟。</li><li>难以实现kernel bypass。 TCP/IP难以实现应用直接控制网络硬件,无法实现kernel bypass架构。</li><li>可扩展性差。TCP难以实现应用级的可扩展性和负载均衡。 </li><li>缺乏数据中心网络特性。TCP/IP缺乏RDMA、RoCE、sr-iov、overlay网络等数据中心特性。）所以数据中心采用RDMA、userspace network stack、overlay network等技术,可以获得更高性能、弹性和效率）。</li></ol><p>如今，几乎所有超级计算机架构以及领先的数据中心提供商都在生产中使用 RDMA。几十年前对负载平衡、拥塞控制和错误处理的简单假设并不适用于当今带宽提高100倍、消息速率提高10倍以上的网络。此外，简单的RDMA网络接口卡（NIC）通常会通过附加功能进行增强。由此产生的“智能网卡”通常会减轻大量负载。服务并实施专门的网络协议。现代网络交换机还具有改进的功能，从先进的网内遥测和网内计算功能到网内负载平衡和拥塞控制。3我们认为，当前现有的标准和部署的基础设施存在根本差距必须在不久的将来解决这个问题，以支持高效的高性能网络。</p><blockquote><p>遥测数据(Telemetry Data)是指从网络设备中采集、汇报的与网络运行状态相关的数据。它通过实时反映网络的运行情况来帮助网络管理和运维。常见的网络遥测数据包括:</p></blockquote><ul><li>基础信息:设备型号、配置、软件版本等静态信息。</li><li>流量数据:接口流量速率、总流量、流量方向等。</li><li>QoS数据:接口队列长度、延迟、丢包、拥塞情况等。 </li><li>路由信息:路由表、下一跳等动态路由信息。</li><li>会话信息:活动的会话连接数、来源/目的地等。</li><li>资源利用率:CPU使用率、内存使用率、链接利用率等。</li><li>环境数据:设备温度、风扇速度等环境信息。</li><li>事件和警报:设备故障、链路中断等事件信息。<br>遥测数据可以通过SNMP、NETCONF等管理协议获取,也可以通过流式遥测技术像gRPC Streaming、 Kafka等机制订阅获取。收集到的遥测数据可以用于网络状态分析、事件检测、容量规划、流量工程等。</li></ul><blockquote><p>网内遥测(In-band Telemetry, INT)是一种网络遥测技术,它可以在网络数据包中携带遥测数据,并随着数据包传递通过网络。网内遥测的主要特征和优势包括:</p></blockquote><ul><li>遥测数据直接嵌入数据包中,不需要额外的控制消息,更加高效。</li><li>可以提供数据包在网络中的实时状态,如延迟、丢包等信息。</li><li>可以细粒度地反映网络状态,每一个数据包都是探针。</li><li>可以快速发现网络热点,进行负载均衡和故障定位。</li><li>无需专用监控网络,不会占用额外带宽。</li><li>可以配合软件定义网络(SDN)实现可编程的遥测控制。<blockquote><p>网内遥测通常需要数据平面支持,在交换机或网卡中实现遥测头插入和解析。通过网内遥测技术,可以极大地增强网络的可观测性, 一些主流的网内遥测技术包括: </p></blockquote></li><li>INT: IETF正在推进的INT标准。</li><li>IOAM: In-situ OAM,由Cisco推出。</li><li>P4INT: 基于P4语言的可编程INT实现。</li><li>iOAM: IOAM的衍生协议。</li></ul><h3 id="RoCE：融合还是胶带"><a href="#RoCE：融合还是胶带" class="headerlink" title="RoCE：融合还是胶带"></a>RoCE：融合还是胶带</h3><p>RoCE（RDMA over Converged Ethernet）是可以在Ethernet网络上运行RDMA的网络协议。其主要特点如下:</p><ul><li>RoCE在Ethernet上实现了RDMA功能,使RDMA不再只局限在专用的InfiniBand网络上。</li><li>在IP/Ethernet网络基础设施上,通过对数据平面进行改进来实现RDMA。</li><li>支持两种传输方式:RoCEv1使用UDP封装;RoCEv2使用一种特殊的以太网帧格式。</li><li>RoCEv1依赖数据中心级别的Lossless Ethernet技术来实现可靠传输。RoCEv2新增了自身的流控机制。</li><li>RoCE可以获得与InfiniBand接近的低延迟和高吞吐性能。</li><li>与iWARP相比,RoCE更加依赖硬件卸载,实现CPU利用率更低。<br>RoCE的优势在于兼容现有的以太网网络,使RDMA应用更易于部署,不再需要专门的IB交换机网络。RoCE已得到广泛支持,是数据中心采用RDMA的主流选择之一。</li></ul><p>传统上，当交换机缓冲区已满时，以太网会丢弃数据包，并依赖于端到端重传。为了支持RoCE，CE引入优先流控制（PFC）来实现链路级无损操作。PFC重新利用以太网中存在的以太网暂停帧来支持具有不同链路传输速率的网络。PFC增强暂停帧以停止（或限制）特定优先级上的流量，以避免数据包丢失。不幸的是，这套复杂的协议会干扰网络中的不同层，并降低当今一些最重要工作负载的效率。</p><p>RoCE的语义、负载平衡和拥塞控制机制继承自InfiniBand。这意味着所有消息都应该按顺序出现在目的地，就好像它们是通过静态路由传输一样，本质上不允许许多数据包级负载平衡机制。对于人工智能训练工作负载（长期存在的流），多路径机制可以大大缩短作业完成时间。此外，RoCE v2使用基于IP显式拥塞通知（ECN）的简单拥塞控制机制。当检测到拥塞时，兼容 ECN 的交换机会对数据包进行标记，接收方会将该信息转发回发送方，从而在单个参数的引导下降低其注入率。无拥塞期后，使用第二个配置参数再次自动提高速率。</p><h3 id="下一代高性能网络"><a href="#下一代高性能网络" class="headerlink" title="下一代高性能网络"></a>下一代高性能网络</h3><p>对于某些工作负载，消息延迟（有时是消息速率）起着核心作用。其中一些属于 OBS 类别，但其他一些具有复杂的数据相关消息链，形成应用程序中的关键性能路径。这些通常是强大的扩展工作负载，解决问题的时间很重要，并且必须容忍低效的执行。具有严格期限的大规模模拟（例如天气预报和石油勘探）属于这一类，但一些事务处理和搜索/推理工作负载也属于这一类。在这里，通常具有严格的（个位数微秒）延迟要求。</p><p>除了流量类型之外，部署环境也在发生变化。新出现的机密计算理念要求所有流量在线路上进行加密。理想情况下，流量在安全飞地中进行端到端加密和解密，并且没有网络设备（NIC 或交换机）值得信任。此外，相关的新兴多租户场景需要管理来自单个主机的数万个连接。这些通常由智能 NIC 提供支持，通过管理资源（例如带宽和安全性）虽然有速率限制和过滤。此外，需要更先进的负载平衡和路由的新的经济高效、小直径和专用拓扑成为极端带宽部署的必要条件。2,8 这些要求的许多组合对下一代高性能网络。</p><h3 id="RoCE存在的问题"><a href="#RoCE存在的问题" class="headerlink" title="RoCE存在的问题"></a>RoCE存在的问题</h3><p> 关于论文中提出RoCE需要改进的8个方面的问题,我总结如下:</p><ol><li> congestion control (拥塞控制)。RoCEv1没有拥塞控制机制,需要依赖DCQCN。RoCEv2虽有拥塞控制但需要进一步完善。</li><li>physical layer (物理层) 。RoCE对PHY层时钟同步和链路断开检测还需改进。</li><li>path MTU (路径MTU)。RoCE需要更好处理不同MTU路径的情况。</li><li> flow steering (流导向)。需要更好的QoS和流量工程能力来导向不同优先级的RoCE流量。</li><li> resilience (弹性) 。如何改善RoCE的故障恢复能力需要进一步研究。</li><li> labeling (标签)。RoCE当前还不支持MPLS等标签交换技术。</li><li> standards (标准)。需要更多针对RoCE在数据中心使用的标准化工作。</li><li> debugging (调试) 。RoCE网络故障定位和性能诊断工具需要加强。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDMA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Towards an Adaptable Systems Architecture for Memory Tiering at WarehouseScale</title>
      <link href="/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/"/>
      <url>/2023/08/20/Towards-an-Adaptable-Systems-Architecture-for-Memory-Tiering-at-WarehouseScale/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS, 2023</li><li>Towards an Adaptable Systems Architecture for Memory Tiering at Warehouse-Scale</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>all Google. Padmapriya Duraisamy, Wei Xu, Scott Hare etc.</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>DRAM主导大规模计算环境中的基础设施支出，如果不进行架构转变，这种趋势可能会恶化（给不起钱了）。使用PM在高度多租户的仓库规模计算环境中提出了许多挑战。其<span class="label primary">应用程序的多样性</span>和规模激发了一般情况下应用程序透明的解决方案，可适应特定的工作负载需求。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><div class="note info">将25%的DRAM换为PM，性能损失不到5%。该设计点在性能下降的限制下最大限度地降低成本。</div> <ul><li>定义内存分层的机器级优化指标，用于自适应平衡复杂的高层次全机群应用性能和利用率目标（第2节）。</li><li>引入了用于大规模实时复杂系统评估的健壮A/B测试方法（第4节）。</li><li>首次对生产仓库规模环境中的可直接访问分层内存系统进行全面分析，该系统成功服务于不同的应用程序类别（第5节和第7节）。</li><li>评估一系列策略并证明硬件辅助事件分析满足性能要求的有效性（第6节），确定主动降级和快速检测升级的重要性。（还是免不了做这个工作，hh）</li><li>揭示了一旦布局得到很好的优化，地址转换开销、干扰效应和页面大小问题如何成为关键挑战，需要新的malloc级技术来减少“访问碎片”，特别是对于大页面。</li><li>利用实时A/B测试机制，以事实为依据开发自适应分层感知集群调度，在最大限度提高利用率的同时减少性能影响tails，并利用应用指导的分层感知巨页管理，减少访问碎片，提高第二层利用率（第8节）。</li></ul><p>我们的经验凸显了大规模管理内存层的复杂性，其中工作负载行为可能每天都会发生变化，或者稳定数周后突然发生变化。我们相信，这种系统设计和用于捕获大规模实时多样化工作负载的复杂交互影响的方法将开辟新的、日益重要的研究途径。</p><h2 id="4-围绕该问题作者如何构建解决思路"><a href="#4-围绕该问题作者如何构建解决思路" class="headerlink" title="4. 围绕该问题作者如何构建解决思路"></a>4. 围绕该问题作者如何构建解决思路</h2><blockquote><p>SLI是请求得到正常响应的百分比。Service Level Object 服务水平目标，是围绕SLI构建的目标。通常是一个百分比，并与一个时间范围挂钩。比如，月度、季度、年度等。99%（2个9的正常运行时间）：意味着在过去30天中有1%，或者说7.2小时的停机时间。</p></blockquote><p>与内存分层相关的应用服务可分为两类——高重要性延迟敏感型（HILS）和其他（非HILS）。HILS包括对响应时间有严格要求的面向用户的应用、处于其他HILS应用关键路径上的缓存应用以及数据处理任务中的生产层（Production Tier，见[48]）。非HILS包括面向吞吐量的应用、批处理、ML训练管道和其他SLO[48]要求较弱的应用。这些类通常位于同一台机器上。Borg调度器[50]会根据观察到的性能主动管理集群中的作业，内存分层也会产生性能影响。</p><h3 id="4-1-指标"><a href="#4-1-指标" class="headerlink" title="4.1 指标"></a>4.1 指标</h3><p>为了更好地分析分层堆栈，我们定义了两个直接连接到分层架构本身的代理指标：<br>• 次要层驻留率(STRR)是<strong>驻留在第2层的已分配内存的比例</strong>。它提供了有关层使用情况的标准化视角。<br>• 次要层访问率(STAR)是针对<strong>驻留在第2层的页面的应用程序的所有内存访问的比例</strong>。较低的STAR意味着较低的性能影响。<br>未充分利用Tier1 DRAM并没有任何好处。 STAR增加反映了性能下降。使用tier2越多，风险就越大，但通过保持STAR较低可以降低风险。目标：最小化STAR，最大化STRR。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/15f7395f6de14157835ea50861ebf19a.png" alt="STAR的累积分布函数CDF显示，在超过99%的实例中socket级别Tier2访问率保持在1%以下。我们实现了将性能下降总体限制在 5%以下的运营目标，特别是当STAR低于0.5%时"></p><h3 id="4-2-架构"><a href="#4-2-架构" class="headerlink" title="4.2 架构"></a>4.2 架构</h3><p>TMTS以页面粒度动态管理分层内存放置。其系统架构包括四层，如图2所示。底层对硬件进行抽象，呈现跨两种或多种类型存储设备分段的物理地址空间。从顶部开始的第二层将页面管理策略与候选检测和页面迁移机制分开，这些机制由较低层内核组件执行。针对工作负载的特定方面进行优化，用户空间策略层提供了这种灵活性和速度。顶层集群层由Borg调度程序组成，它与节点代理 Borglet一起工作，管理连续的多机器作业请求流，观察负载和性能指标，在每台机器上并将任务分派到各个服务器[48]。在TMTS 中​​，它采用分层感知调度策略来实现更好的工作放置，如第8.2节所述。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/046b396e4d8b45c7926231cf250b74da.png"></p><h3 id="4-3-冷页驱逐与降级策略"><a href="#4-3-冷页驱逐与降级策略" class="headerlink" title="4.3 冷页驱逐与降级策略"></a>4.3 冷页驱逐与降级策略</h3><p>当一个页面在之前几秒内没有被访问过时，我们就将其归类为阈值为t的冷页面[32]。降级策略选择根据应用程序类别、每个应用程序或随时间变化确定𝑡的值及其粒度。<br><span class="label primary">浅浅提了一下他们的工作负载98%都是匿名页，所以策略也有所偏好</span></p><h3 id="4-4-热页升级策略"><a href="#4-4-热页升级策略" class="headerlink" title="4.4 热页升级策略"></a>4.4 热页升级策略</h3><p>对最后一级缓存LLC未命中事件进行采样来分析最近访问的第 2 层内存中的地址。由于第2层是可缓存的，因此只需考虑LLC未命中事件。对所有LLC未命中事件进行采样是不切实际且没有用的，绝大多数未达到第一级的流量。不幸的是，硬件不支持对内存存储精确事件过滤。We configure sampling to collect 1% of memory loads from tier2 and promote all the pages identified by this sampling. 我们在第6.2节中检查了这种实用但不完美的检测器的有效性。</p><div class="note warning no-icon">就是指把第二层内存分为100份，迁移其中的一整份去第一层内存，但是这个采样怎么采？是相邻的还是指定步长，选中1%是要排序还是其他的怎么着？</div> <p>为了检测采样可能遗漏的热点页面，我们还执行主动、定期的基于扫描的升级。我们将页面热龄定义为最近访问页面的扫描周期数。我们扩展了内核中的页面位扫描器来跟踪页面热年龄，这使得它能够区分活跃页面和轻度访问页面。为了提高效率，我们不在A位扫描中进行TLB失效。尽管这种优化可能会牺牲一些页面年龄的准确性，但我们并没有看到它在实践中影响降级和升级的有效性。</p><div class="note info no-icon">页面年龄这个20年发在TC的APM做过，但是搞两个不会运行时有啥冲突吗？</div> <h3 id="4-5-页面迁移"><a href="#4-5-页面迁移" class="headerlink" title="4.5 页面迁移"></a>4.5 页面迁移</h3><p>ufard后台线程使用Linux中的标准perf接口（例如 perf_event_open()）来设置采样和处理访问事件。它还在内核中安装了一个小型BPF[3]程序，以优化从内核内页面A位扫描器到每个NUMA节点BPF环形缓冲区的tier2热页面年龄及其页面地址的收集。</p><h3 id="4-6-硬件限制导致的策略约束"><a href="#4-6-硬件限制导致的策略约束" class="headerlink" title="4.6 硬件限制导致的策略约束"></a>4.6 硬件限制导致的策略约束</h3><p>我们的部署使用英特尔傲腾持久内存的变体作为第2层。此类第2层DIMM的带宽受到高度限制，支持的内存带宽仅为英特尔®至强®可扩展处理器上典型DDR4通道的内存带宽的1/10。</p><p>在当前的硬件实现中，第2层DIMM带宽饱和也会影响常规DRAM延迟，因为第2层DIMM与DRAM DIMM共享内存通道。这限制了升级和降级的积极性。表1列出了我们的配置中不同访问模式下测得的Tier2 DIMM带宽。测得的空闲读访问延迟约为325ns，详情请参见[28,55]。根据冷页配置文件（图3）、可用DIMM容量和硬件带宽限制，我们的目标是将25%的系统内存容量用于第2层。这些硬件限制导致我们当前的部署受到以下策略决策限制：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8add2e9fbff5488da1074b431149e3de.png"></p><p>没有直接分配到第2层：此约束避免了第2层中新分配的页面直接就很热。系统仅将任务内存分配到第1层，并依靠降级来填充第2层。仅分配到tier1可能会增加OOM情况。QoS在限制对连接到远程socket的第2层DIMM的访问方面效果较差，因为反馈信号必须在插槽之间传输。所以只将任务的内存降级到任务运行的socket上的第2层DIMM。</p><p><span class="label info">这些选择并不是TMTS的基础，而是底层硬件实现的产物。未来的硬件实现可能不会受到这些限制，从而放宽上面讨论的限制。当前限制的另一个好处是展示系统设计的弹性。</span></p><h3 id="4-7-TLB-Misses-and-Huge-Pages"><a href="#4-7-TLB-Misses-and-Huge-Pages" class="headerlink" title="4.7 TLB Misses and Huge Pages"></a>4.7 TLB Misses and Huge Pages</h3><p>大页面（例如2MB）可以减轻4KB页面带来的虚拟到物理地址转换延迟和TLB覆盖问题。但这些大小给基于页面迁移的分层内存系统带来了新的挑战。由于地址映射的更改，迁移会导致TLB失效。此外，大页面中的小热区域会导致整个页面显得很热。</p><p>在当前的实现中，对象最初分配在tier1中。如果TMTS将大页识别为降级候选，则该页首先被拆分为4KB页面，然后降级。这允许未来的访问提升单独的4KB页面，并减少迁移成本和不必要的tier1占用。并非原始大页面的所有降级4KB页面都可以升级，从而阻止它们成功地重新组合成大页面。由于系统的DRAM容量较低，我们看到DRAM压力增加，这会导致tier1碎片化。</p><p>我们进行了两项实验，这两项实验都以最小的副作用提高了TLB 命中率。<br>首先，我们尝试完整迁移大页面，而不是在降级时将其分解为4KB页面。这使TLB未命中平均减少了4.7%，并将平均性能提高了0.5%。升级带宽按预期增加，但增幅&lt;1%，并且带宽拥塞事件没有增加。平均STRR降低 &lt;1%。<br>其次，我们提高了内存压缩的积极性，目标是提高tier1中4KB页面的重组率。我们发现大页覆盖率提高了25%，平均性能提高了0.5%。额外的CPU成本微不足道，&lt; 0.1%。</p><div class="note warning no-icon">但是，兄弟你说拆就拆吗？页表怎么转换，页表转了TLB的缓存还要一致性。两种页面混合你要怎么做地址转换？笔者感觉可能是因为大家默认了现在也有大页TLB了？最近有看见一篇文章说，反正大页TLB不用也就空在那里。如果使用透明大页的话，还是很好实现混合页面的。但是如果使用静态的大页面（就是开机前就分配好），从运行方式来看，不做点改变一个程序运行用两种页面还是有困难的。</div> <h2 id="5-评估"><a href="#5-评估" class="headerlink" title="5. 评估"></a>5. 评估</h2><p>在Optane PM上做的。这里只记录感兴趣的。<br>作者们首先说加入PM后的情况和只使用DRAM差不多；然后说明了冷页面控制的很好：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/37b3a657a7fa48d6a39d0c8b3f687177.png"><br>图5（a）是STRR，（b）是将冷内存覆盖率定义为存储在tier2中的2分钟冷页的比例。一个重要的参考点是地址空间中可用冷页的相对数量，在图3（a）的实验中观察到在28%到42%之间变化。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/bbc62466bd56426ba61a32a89ba0a1d2.png"></p><p><span class="label warning">但是有一点，PM不可能刚刚好就给25%，肯定会多给吧？你又不放在远端，被定义的冷的超出25%，那dram降级有装不下，只能OOM？</span></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/761b677057cc4885830e9522fee1f0fa.png" alt="升级、降级和应用程序访问tier2的带宽消耗"></p><h2 id="6-应对仓库级规模问题的适应性政策"><a href="#6-应对仓库级规模问题的适应性政策" class="headerlink" title="6. 应对仓库级规模问题的适应性政策"></a>6. 应对仓库级规模问题的适应性政策</h2><p>虽然部署的TMTS满足大多数应用程序的性能目标，但一些异常值会出现显着的性能下降，如图6c所示。上述案例研究展示了我们计算环境中典型的冷内存访问模式的多样化组合。为了在这样的环境中扩展分层，我们必须实现跨节点、集群和应用程序层的垂直集成，并制定策略，共同实现更高的冷内存识别，同时将性能下降降至最低。本节讨论支持扩展TMTS的三种自适应策略。</p><p>Lagar-Cavilla等人[32]描述了一种动态的、按应用程序冷龄阈值策略，旨在维持目标升级率。<br>然后以前调度的优化Borglet在集群间、机器间选择策略的完善，考虑每个机器的第一层使用率之类的。</p><p><span class="label info">我们相信，使用软件提示来帮助调整页面内的内存分配和对象放置的类似方法将变得越来越重要。例如，除了开发人员将应用程序内的分配注释为可能的热或冷之外，编译器还可以使用自动化技术（例如配置文件引导优化）来提供类似的提示。</span></p><h2 id="7-积累"><a href="#7-积累" class="headerlink" title="7. 积累"></a>7. 积累</h2><p>冷页面驱逐还挺不一样，不直接分配PM也有理由也挺新奇；作者在设计和描述时感觉也尽可能在表示我知道CXL，我知道硬件会变，但是我们这个策略仍然可以参考。</p><p>图6是不同应用程序IPC的情况，说明不同应用程序差异，用这个方式分类应用程序第一次见。</p><p>使用TensorFlow框架[6]构建的面向吞吐量的机器学习训练管道，具有高内存带宽使用率和不可预测的访问模式。用这个workload也是头一次。机器学习应用程序是一个面向吞吐量的训练管道，在训练阶段频繁更新内存中的ML模型。该应用程序存储的内存数据可以是密集的，也可以是稀疏的，具体取决于更新当前正在训练的ML模型所需的数据量。这些ML模型更新需要大量内存带宽，并且它们的内存访问模式是不可预测的，因为应用程序经过优化，可以有效地将训练数据从磁盘读取到内存中，而不是针对内存中访问的局部性进行优化。训练工作线程的单个实例的任何减慢都可能减慢应用程序的整个训练阶段，从而导致CPU和加速器周期的浪费。</p><p>奇怪，他不是直接降级去PM吗，为什么要在分配时隔离冷热的虚拟地址空间。“为了避免并置冷热对象，我们扩展了new运算符以接受一个提示参数，该参数指示预期分配的对象的访问频率。C++内存分配器的开源TCMalloc实现[4]使用此参数来分隔虚拟地址空间中的“冷”和“热”分配。由于不经常访问的对象聚集在页面上，因此可以建议内核不要使用透明大页面映射冷区域。这些策略导致频繁访问和不经常访问的对象的分离，并为每个对象提供不同的策略。”</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> Warehouse-Scale Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Merchandiser Data Placement on Heterogeneous Memory for TaskParallel HPC Applications with LoadBalance Awareness</title>
      <link href="/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/"/>
      <url>/2023/08/19/Merchandiser-Data-Placement-on-Heterogeneous-Memory-for-TaskParallel-HPC-Applications-with-LoadBalance-Awareness/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ACM SIGPLAN Symposium on Principles &amp; Practice of Parallel Programming, PPoPP, 2023</li><li>Merchandiser: Data Placement on Heterogeneous Memory for Task-Parallel HPC Applications with Load-Balance Awareness </li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Zhen Xie，University of California, Merced Argonne National Laboratory</li><li>Jie Liu，University of California, Merced</li><li>Jiajia Li，North Carolina State University</li><li>Dong Li，University of California, Merced</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>图1.a给出了一个基于MPI的任务并行应用的例子。在DMRG中，一个哈密顿矩阵首先被分割成多个块，每个块被分配给一个MPI 进程(1-3行) 。然后每个MPI进程运行一个计算循环，作为输入(第5-7行)循环的一次迭代被认为是1个任务实例。因此，MPI进程中的任务是重复执行的。在每个迭代结束时，有一个全局的MPI进程之间的同步。</p><p>图1.b给出了1个基于OpenMP的任务并行应用的例子，一个主循环运行了许多SpGEMM (C=A*B) 。在主循环的每一次迭代中，A首先被分割成若干个bins，部分 A*B 得到部分C</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2d6f15b49475435ba52e45722d3e969f.png"></p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>异构内存让这种并行的，有同步点的workload负载均衡，这个策略算是对以前迁移策略的补充。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ea9867b9f36043af9a699eb42ec35696.png"><br>当大家都将访问频率高的页面放在DRAM时，忽略了并行计算有同步点的实际情况，这会使得高性能计算不同任务间负载不均。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>Input-Aware Memory Access Quantification<br>既然是从任务的角度来解决问题，那么如何确定什么样的数据放入快速内存中？<br>一种是程序员自己去设置。<br>另一种分类是流式的、一定步长的，需要考虑左右两边数据的、随机的。一个程序还可以有多种模式同时存在。（这在16年eoursys有比较相似的工作）这些模式的识别是现有的开源工具改造的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519a7d4c0e2149bf9fd8ddbdd870afcc.png"></p><p>阿尔法的值，根据步长和数据类型来确定，由作者提前枚举好了的。综上给了两种方法来判断输入数据的将来被访问量，PM用的是一篇论文的方法，DRAM用的是另一篇论文（先大页面采样，热的再去采样那一块的小页面），以这次的访问预估下次访问频率。如果没有指定的情况就是随机那种模式。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b690c63eb3a3414991abdf491177a583.png"></p><p>迁移的决策模型应该是，作者的想法就是怎么把数据放置在不同PM的敏感性展现出来，区分出数据在不同异构放置的差别。<br>时间上的预测完全参考13年的一篇论文。输入的是各种硬件采样值和当前DRAM的访问量，然后有个相关性函数去做判断。</p><p>相关函数用的GRB（梯度提升回归）：每个学习算法准确率都不高。但是它们集成起来可以获得很好的准确率。这些学习算法依次应用。也就是说每个学习算法都是在前一个学习算法的错误中学习。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/89e360308c2b400b8d7318f8e4b2efc6.png"></p><p><strong>性能模型的设计概述</strong><br>上一个同步点前的任务被输入，在编译时判断访问模式和访问预测做静态分类。运行时根据以前大家做的采样以及机器学习的运行时间预测，达到迁移和负载均衡的目的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/03757318787c480091970cfc3d2290a2.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>首先是性能提升。然后可以看到改进后执行时间方差减少了。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1807463f60ca4d25a648d88c1e9a5a63.png"></p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>解决方案太黑盒了吧。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>关注的问题的角度。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>Spindle是清华写的，开源的。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transparent Page Placement for CXL Enabled Tiered Memory</title>
      <link href="/2023/08/19/TPP/"/>
      <url>/2023/08/19/TPP/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ASPLOS，2023，CCFA</li><li>TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hasan Al Maruf，Mosharaf Chowdhury – University of Michigan密歇根大学</li><li>Hao Wang，Niket Agarwal，Pallab Bhattacharya – NVIDIA</li><li>Abhishek Dhanotia，Johannes Weiner，Chris Petersen，Shobhit Kanaujia，Prakash Chauhan – Meta Inc.</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><ul><li>CXL [7] 是一种基于 PCI Express（PCIe）接口的开放式、业界支持的互连。<br>它支持主机处理器和设备（例如加速器、内存缓冲区、智能 I/O 设备等）之间的高速、低延迟通信。</li><li>CXL 在同一物理地址空间中提供字节可寻址内存，并允许使用标准内存分配 API 进行透明内存分配。</li><li>CXL-Memory 访问延迟也与 NUMA 访问延迟类似。</li></ul><p>这篇文章将直接附加到 CPU 的内存称为本地内存，将 CXL 附加内存称为CXL-内存。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5aac1cd90eef40299199fc6d08e7b1f0.png"></p><blockquote><p><strong>补充：</strong>现在CXL同时支持多种内存的应用案例都是DRAM和HBM（是一种高带宽内存 High Bandwidth Memory）。HBM更适用于对带宽要求极高的场景,如高端GPU、AI加速卡等。HBM单片容量较小(4-16GB),但带宽巨大(超过500GB/s)。</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>现实实践中面对的问题：<br><strong>数据中心应用程序的内存需求增加</strong>在META公司中每一代硬件内存消耗的能源和总花费占比都在不停增长。</p><p><strong>同类服务器设计中的扩展挑战</strong>内存控制器仅支持单代内存技术，这限制了不同技术的混合搭配，具有不同的每 GB 成本和带宽与延迟配置。大内存容量都是2的几次方。限制了细粒度；每一代服务器的带宽和容量都有限制。图4</p><p><strong>数据中心应用程序的轻量级表征</strong>现有的工具会导致较高的 CPU 开销（每个核心超过 15%），并且通常会减慢应用程序的速度。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/149d075453ee4bfb9e5e3e7214a92947.png"></p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><h3 id="5-1采样数据工具"><a href="#5-1采样数据工具" class="headerlink" title="5.1采样数据工具"></a>5.1采样数据工具</h3><p>Chameleon 的主要用例是了解应用程序的内存访问行为，即应用程序内存的哪一部分保持热-温-冷状态、页面在特定温度层上存活多长时间、访问它们的频率等等。长期内存。</p><p>收集器做两类采样，放入hash表，定时唤醒。<br>收集器唤醒工作线程以处理当前哈希表中的数据，并移动到另一个哈希表以存储下一个间隔的采样数据<br>工作器处理哈希表里的数据，统计每个页面的位图大小是64位，生成报告后休眠。</p><blockquote><p>load是从内存读取数据到处理器的寄存器中的指令,而store则是将寄存器中的数据写入内存的指令。两者都是与内存打交道的指令,但方向不同,load是内存到寄存器,而store是寄存器到内存。</p></blockquote><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/5c874a20c38e4191b939a5ef0ca59536.png"></p><p>我们使用 Chameleon 来分析我们生产中运行的跨不同服务域的各种大型内存绑定应用程序，并进行以下观察。 </p><ol><li>访问的内存的很大一部分在几分钟内保持冷状态。我们可以将其卸载到慢速层内存，而不会对性能产生重大影响。 </li><li>大部分匿名内存（为程序的堆栈、堆和/或 mmap 调用创建）往往更热，而大部分文件支持的内存往往相对更冷。</li><li>页面访问模式在有意义的持续时间（分钟到小时）内保持相对稳定。这足以观察应用程序行为并在内核空间中做出页面放置决策。 </li><li>工作负载对不同页面类型（文件和匿名页面）具有不同程度的敏感度，并且随着时间的推移而变化。</li><li>冷页重新访问时间因工作负载而异。分层内存系统上的页面放置应该意识到这一点，并主动将热页移动到较低的内存节点，以避免高内存访问延迟。于Web而言，几乎80%的页面会在 10 分钟内被重新访问。</li></ol><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ec7f0938b1844787b423dbe944b29744.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/927aa0606137423b80bc79092b10fdca.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f7c7b158461d442ea37c3eb4d5b9d7f9.png"></p><h3 id="5-2架构设计"><a href="#5-2架构设计" class="headerlink" title="5.2架构设计"></a>5.2架构设计</h3><p>TPP 将“较热”页面放置在本地内存中，并将“较冷”页面移动到 CXL 内存中。<br>TPP 的设计空间可分为四个主要区域：</p><ul><li>(a) 轻量级降级到 CXL-Memory，</li><li>(b) 解耦分配和回收路径，</li><li>(c) 热页升级到本地节点，</li><li>(d) 页面类型感知内存分配。</li></ul><p> 一般来说NUMA系统的回收顺序是：首先回收本地节点上不活跃的页面；然后回收远程节点上不活跃的页面；如果还不够,才考虑回收本地节点上最近最少使用的页面。所以在图的上面①会将本地回收候选页面迁移到CXL节点的降级列表中，除非本地节点的容量小于工作集大小的热部分，否则在回收期间热页面迁移到 CXL 节点的机会非常低。如果降级期间的迁移失败（例如，由于 CXL 节点上的内存不足），我们将回退到该失败页面的默认回收机制。</p><p> Linux为节点内的每个内存区域维护三个watermakes（最小、低、高）。如果节点的空闲页面总数低于low_watermark，Linux会认为该节点面临内存压力并启动该节点的页面回收。在我们的例子中，TPP将它们降级为CXL节点。对本地节点的新分配将停止，直到回收器释放足够的内存以满足high_watermark。<span class="label info">由于分配率较高，回收可能无法跟上</span>，本地内存分配频繁停止，<span class="label info">更多页面最终出现在CXL节点中</span>，最终降低应用程序性能。所以现在将回收和分配化为不同的门槛②，回收和驱逐会提早发生，而禁止页面分配会在内存饱和度更高一步的时候。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d51196b572b849d48550379551aa0801.png"></p><p>NUMA系统中CPU访问另一节点页面时,生成缺页异常提醒内核进行页面迁移。不常访问的页面产生的升级流量很容易填满本地节点的空闲空间，并为CXL节点产生更高的降级流量。于是通过页面在LRU列表中的位置来检查页面的年龄。如果故障页面处于非活动LRU状态，我们不会立即考虑该页面进行升级，因为它可能是不经常访问的页面。仅当在活动 LRU 中找到故障页面（图13中的①）时，我们才将其视为升级候选页面。这大大减少了升级流量。</p><p>然而，操作系统使用LRU列表进行回收。如果内存节点没有压力并且回收没有启动，则非活动LRU列表中的页面不会自动移动到活动LRU列表。由于CXL节点可能并不总是处于压力之下，因此经常可以在非活动LRU列表中找到缺页的页面，又会没法向上迁移。为了解决这个问题，每当我们在非活动LRU列表中发现缺页异常时，我们都会将该页面标记为已访问，并立即将其移至活动LRU列表（图13中的②）。如果在下一次NUMA提示缺页异常，则它将处于活动LRU中，并提升到本地节点（图13中的③）。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>因为工作负载就这些特性，那就直接对症下药，效果都挺明显的。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>但是换了workload就没这个效果了。按照这种方式，每家都得自己去设计啰。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>是根据工作负载来设计架构的，关注的问题点比较小，也就容易做到很细致出色吧。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><p>一个奇怪的现象，虽然大家可能观察值不同，但是最后系统设计都比较像，好像这两部分可以分裂一样。<br>那个Chameleon工具，说是开源但是并没有，之后可以再去看看。<br>TMO: Transparent memory offloading in datacenters. In ASPLOS, 2022.可以看看。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</title>
      <link href="/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/"/>
      <url>/2023/08/18/Aware-Data-Structure-Refinement-and-Placement-for-Heterogeneous-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><p><strong>文章来自23年7月TechRxiv</strong><br>Performance, Energy and NVM Lifetime-Aware Data Structure Refinement and Placement for Heterogeneous Memory Systems</p></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Manolis Katsaragakis,微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院,鲁汶大学(KU Leuven)比利时</li><li>Christos Baloukas, Lazaros Papadopoulos, 微处理器和数字系统实验室国立雅典理工大学(NTUA)电气与计算机工程学院</li><li>Francky Catthoor,鲁汶大学微电子研究中心</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>为了有效利用 DRAM/NVM 异构内存系统，多年来已经提出了几种数据放置算法 [15]、[16]。尽管数据放置算法通常很复杂，并且能够有效地利用正在执行的应用程序的复杂内存层次结构，但它们的结果通常受到以下事实的限制：<span class="label info">原始应用程序通常是为 DRAM 而不是为异构内存设计的</span></p><p>作者通过一些实验展示了数据组织改进的应用程序级方法对异构 DRAM/NVM 系统上数据放置算法结果的影响（作为整个论文的动机）。其中有两个新的概念（这两个概念也对后续设计有很大影响，但是这两个概念并不常见）：</p><ul><li><span class="label danger">动态数据类型细化方法 Dynamic Data Type Refinement methodology（DDTR）</span>（D. A. Alonso, S. Mamagkakis, C. Poucet etc. Dynamic memory management for embedded systems. Springer, 2015）</li><li><span class="label danger">帕累托最优 Pareto Optimal</span></li></ul><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>异构内存中可以从数据组织的角度改进数据放置，所提出的方法旨在满足三个主要目标：<br>(i) 展示通过应用程序级优化实现的数据放置算法的改进结果。<br>(ii) 基于多个目标（性能、能耗、对 NVM 寿命的影响）评估布局解决方案。<br>(iii) 提供可用性、可扩展性和可扩展性特征（例如，即使对于相对较大的设计空间，也有合理的探索时间，并支持各种应用领域、数据放置算法和真实或模拟的 NVM 技术）。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><ul><li>几种数据放置算法 [15]、[16]</li><li>应用级数据优化方法[18]、[19]</li><li>库支持 [13]、[17] </li><li>根据布局粒度级别进行分类：数据结构、内存对象或内存页 [23] [15] [24] [25][23] 的作者提出了一种在线配置文件引导的数据分层解决方案，涉及异构内存系统的页面粒度放置，旨在提高 HPC 应用程序的性能。其他指示性方法包括静态代码检测工具，用于自动执行内存对象放置以实现性能和能源优化[15]。在数据结构放置粒度上，[24]中提出了缓存和非缓存 NVM 的写感知数据结构放置。 [25] 中提出了基于人工智能的页面粒度数据放置。</li><li>异构 DRAM/NVM 系统上的应用程序域特定布局：最近还研究了应用程序域特定布局方法，目标是与领域无关的布局算法相比获得更好的结果。这些领域包括与数据库相关的工作负载[26]、大数据应用程序[27]、[28]、基于图的[29]、[30]、[31]和基于深度神经网络的应用程序[32]、[33]。</li></ul><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/85597fd358cb4e0882741b4808f25738.png"></p><p>图2显示了所提出的方法的概述。该方法的输入是目标应用程序的源代码。它由三个步骤组成：</p><ol><li><p><strong>数据组织的优化</strong>，此步骤的主要目标是有效选择应用程序的数据结构实现，以最大限度地减少数据访问次数和内存占用。实现方式是源代码的原始数据结构被库的数据结构替换。（仅限于基于列表的数据结构实现的 C++ 标准模板库 (STL) 兼容变体。）<span class="label default">这一步生成一组帕累托最优解</span>,每一个都对应于正在优化的应用程序的数据结构实现的不同组合。</p></li><li><p><strong>内存对象分析</strong>，基于内存跟踪和分析工具，<span class="label default">分析上一步提供的应用程序的帕累托最优版本</span>。通过平台感知采样和分析 (2b)，我们收集每个对象的内存跟踪，包括：加载和存储操作、对象大小和 LLC 未命中。还可以根据所选数据放置算法的输入要求来监视其他指标。</p></li><li><p><strong>异构内存系统上的放置和评估</strong>，模型接收（i）每个内存页、对象或数据结构的分析信息和（ii）内存规格作为输入，例如读/写延迟、读/写能耗和内存容量（3b）。每个应用程序版本都根据所选的数据放置算法部署在异构内存系统上，并执行。为了监控执行时间、能耗和NVM写入次数，需要实时监控（3e）。应用程序版本在真实或模拟的异构存储器系统上放置和执行。开发人员可以在性能、能耗和 NVM 写入访问次数之间进行权衡，并选择满足设计约束的帕累托最优解决方案。</p></li></ol><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>所提出的方法根据以下标准进行评估：<br>● DDTR 等高级数据组织优化方法对数据放置结果的影响，包括性能、能耗和对 NVM 寿命的影响。<br>● 该方法可以在多大程度上有效地集成具有不同优化目标的各种数据放置算法以及各种内存技术（模拟的或真实的硬件）。<br>● 当应用于具有相对大量数据结构的应用程序时，该方法在探索时间方面的可扩展性。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ul><li>相当于需要人工干预去做这个优化。</li><li>而且优化过程依赖帕累托（来自财经领域）最优和DDTR（15年一篇论文），这是以前没有过的应该，那么到底有多可靠呢？审稿方对这些态度咋样呢？</li><li>评估方面，没有和其他的工作比较，也看不出什么优势emmm。</li></ul><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>没有任何相关工作研究应用级数据优化和异构内存系统上的数据放置之间的相互作用，以提高性能、能耗和对 NVM 寿命结果的影响。提出了一种内存管理方法，该方法结合了动态数据结构细化步骤以及异构内存系统上的放置算法。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><ul><li>最开始对程序数据组织优化应该是离线完成的，<a href="https://github.com/mkatsa/DDTR-DRAM-NVM">可能可以作为参考</a>。</li><li>可以考虑的workload：从 Shark ML 库 [42] 和 Chrono 物理引擎 [43] 中选择了五个代表性应用程序。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> Rxiv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>P2Cache An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</title>
      <link href="/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/"/>
      <url>/2023/08/17/P2Cache-An-Application-Directed-Page-Cache-for-Improving-Performance-of-Data-Intensive-Applications/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li><strong>文章来自HotStorage 23</strong> </li><li>P2Cache:An Application-Directed Page Cache for Improving Performance of Data-Intensive Applications</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Dusol Lee, Inhyuk Choi, Chanyoung Lee, Jihong Kim, 首尔大学Seoul National University</li><li>Sungjin Lee, 大邱庆北科学技术院DGIST</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p><span class="label info">大多数操作系统在主机DRAM内存中使用页面缓存，以利用I/O访问的局部性。</span>但页面缓存的高速缓存管理策略可能与应用程序的特定I/O特征不太匹配。如果应用<span class="label info">程序随机访问I/O地址空间，则页面缓存使用的标准读取策略可能无效。</span></p><p>现有缓存策略的局限性——评估三种常见技术：</p><h3 id="2-1操作系统级缓存"><a href="#2-1操作系统级缓存" class="headerlink" title="2.1操作系统级缓存"></a>2.1操作系统级缓存</h3><p>采用 LRU 替换策略与预读算法相结合，如果应用程序具有中等局部性，通常可以实现较高的命中率 [11]。现有的数据密集型应用程序通过<span class="label info">高度定制的算法处理大量数据</span>，从而导致复杂的 I/O 模式。不幸的是，由于其通用设计，操作系统级页面缓存通常<span class="label info">无法捕获各个应用程序的独特行为</span>，从而即使可以实现更高的命中率，也无法提供次优的性能。</p><p>下图是Lumos [2]，执行图形处理算法 - Pagerank [15]的IO模式和性能趋势（它使用专门的数据结构和优化技术来优化图形处理引擎，因此生成的 I/O 访问模式非常复杂。）</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2cc466ecd6cb409bb417d9e68a446c3b.png" alt="数据集大小超过系统内存后，应用程序开始遭受缓存抖动，导致性能严重下降（参见图 1(b) 中的基线）"></p><ul><li>性能低下是由于操作系统级页面缓存的内存管理无效，该缓存使用 LRU 和预读策略，而不考虑输入工作负载模式。操作系统级页面缓存优先驱逐最近最少引用的页面，<span class="label danger">但这些页面实际上很快就会再次被引用，特别是在循环 I/O 模式下。</span></li><li>可以观察到，使用具有小数据固定的 MRU 策略（而不是 LRU）会带来更高的性能，如图 1 中的 MRU+PIN 所示，性能提高了 25%。然而，很难改变内部的情况。适应输入工作负载的内核缓存替换策略。</li></ul><h3 id="2-2基于提示的操作系统级缓存"><a href="#2-2基于提示的操作系统级缓存" class="headerlink" title="2.2基于提示的操作系统级缓存"></a>2.2基于提示的操作系统级缓存</h3><p>作为一种替代方案，一些应用程序（例如 GridGraph [1] 或 SQLite [7]）尝试通过 fadvise [16] 和 madvise [17] 向内核提供应用程序级提示来更好地管理缓存数据。在保留内核级缓存管理相同优点（强大的数据保护和高效的数据共享）的同时，它能够<span class="label info">通过在应用程序代码中嵌入重要的缓存管理决策</span>（例如，WILLNEED、SEQUENTIAL、DONTNEED）来实现更高的缓存命中率。</p><p>然而，它也有缺点。首先，它需要在修改现有应用程序代码方面付出巨大的努力。其次，仅通过注入提示很难精细控制内核级页面缓存。我们在代码中精心添加了提示信息，以MRU方式管理内核缓存。修改后的版本表现出更高的命中率，但性能仍然比使用 MRU 慢得多（参见图 1 中的 FADV）。这是因为缓存抖动。 </p><h3 id="2-3用户级缓存"><a href="#2-3用户级缓存" class="headerlink" title="2.3用户级缓存"></a>2.3用户级缓存</h3><p>为了减轻I/O特征（应用程序）和（内核级页缓存）策略之间的不匹配问题，数据密集型应用程序经常在应用程序级别[3-7]上实现自己的页面缓存。</p><ul><li>受内核缓存策略干扰，驱逐了有用的页面，违反了应用程序的意图。</li><li>无法利用内核的保护和共享功能。<br>下图使用 Simrank [19] 进行了实验，这是一个具有自己的用户级缓存的图形应用程序。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2439afe15dd740a6917ea14962b24687.png"><br>如图 2（a）所示，Simrank 的 I/O 参考模式大多是随机的，因为它将流行数据缓存在用户级缓存中。由于这种特定于应用程序的管理，对于相同的数据集，Graph-Walker 表现出比 Lumos 更高的性能（见图 2(b)）。然而，当内存不足时，GraphWalker 的性能下降幅度比 Lumos 更高。</li></ul><h2 id="3-解决的问题"><a href="#3-解决的问题" class="headerlink" title="3. 解决的问题"></a>3. 解决的问题</h2><p>内存密集型程序的I/O更加具有特性，内核级页面缓存由于无法考虑特定于应用程序的 I/O 模式而无法提供高性能。应用程序级提示可以缓解该问题，但与最佳效果相比，效果有限。虽然用户级自定义缓存可以高效工作，但它无法利用内核的基础设施，并且会因内核干预而导致性能下降。于是做了一个<strong>允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE</strong>。 </p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>如果直接实现了用户级页面缓存（例如，如Jaydio [8]或RockSDB的Direct-io [9]），则可能无法对某些内核功能进行介入，例如用于确保数据保护和数据一致性的功能。更重要的是，如果SSD或主机存储系统发生重大变化，则需要重新实现用户级缓存。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1392a78bce7340b886bd89b864b1e283.png"><br>图 3 显示了 P2Cache 的操作概览。要为应用程序创建自定义页面缓存，可能需要应用程序的特定于应用程序的数据来开发新的缓存策略。如果需要，数据会被移动到内核的受保护内存(1)。为了避免其他应用程序对数据进行未经授权的访问，为每个自定义页面缓存分配了一个密码(passwd)。使用P2C API函数以及应用程序的内核数据，为P2Cache的每个探测点实现一个eBPF程序 (2)。将eBPF程序加载到各自的探测点(3)后，只要内核的执行流到达这些点，就会执行eBPF程序，应用程序的自定义页面缓存就会生效。在执行eBPF程序之前，扩展的eBPF VM会验证程序是否有权访问应用程序拥有的内核数据以及特定于应用程序的数据。这是通过将eBPF程序的passwdprobe与从相应应用程序传递来的passwd进行比较来实现的。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，使用我们的P2Cache实施的Cusmom Page缓存可在数据密集型图应用中提高32％的性能，内存容量低于数据集容量时也是相比其他方案性能下降更缓慢。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>做了一个允许应用程序开发人员构建与目标应用程序的I/O特征匹配的自定义内核级别的CACHE，别人的都是应用层级的。</p><h2 id="9-积累"><a href="#9-积累" class="headerlink" title="9. 积累"></a>9. 积累</h2><blockquote><p>数据密集型工作负载：Lumos [2] and GraphWalker [4]。目前使用的工作负载，容量如果内存不能容下，则会杀死进程，这里使用这两个工作过负载还可以使得Memory Size / Dataset Size成比例。Lumos 维护多个文件并同时扫描它们，将混合 I/O 模式发送到磁盘。 Lumos 还使用多个元数据文件并重复读取它们，从而产生高度本地化的 I/O 模式。</p></blockquote><blockquote><p>2.2节的实现手段可以模仿。eBPF程序也能去影响内核。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> O </tag>
            
            <tag> Cache </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Coalescing for Multi-Grained Page Migration</title>
      <link href="/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/"/>
      <url>/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>Coalescing for Multi-Grained Page Migration</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>华中科技大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>观察结果1，一些热页在虚拟地址和物理地址上都有连续性。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>Tamp使用DRAM作为数据缓冲器来缓存NVM中多种尺寸的热页(所谓的多粒度页)。相应地使用分裂的超级页TLB和多粒度TLB来分别加速NVM和DRAM的地址转换。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>每一个idea都有对应的问题要解决，最后嵌套太多了，所以投的刊物可能会不好。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> O </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自ACM Transactions on Architecture and Code Optimization, (TACO), 2019</li><li>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>XIAOYUAN WANG, HAIKUN LIU, XIAOFEI LIAO, JI CHEN, HAI JIN, YU ZHANG, and LONG ZHENG, 华中科技大学</li><li>BINGSHENG HE, 新加坡国立大学</li><li>SONG JIANG, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>在大内存系统中，<strong>超级页一直被用来减轻地址转换开销</strong>。 然而，在由DRAM和NVM组成的混合存储系统中，<strong>超页面往往会阻碍轻量级页面迁移，而轻量级页面迁移对性能和能量效率至关重要</strong>。 </p><blockquote><p>Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and NVM.</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><ol><li>如果大多数内存引用<strong>分布在超级页的一个小区域中</strong>，那么以超级页粒度（例如2MB）进行的页迁移会导致DRAM容量和带宽的巨大浪费，从而导致无法承受的性能开销。 成本可能比超级页面迁移的好处还要大。 这给超级页面的使用带来了一个困境，因为<strong>轻量级页面迁移可能会超过扩展TLB覆盖的好处</strong>。 </li></ol><blockquote><p>However, page migration at the superpage granularity (e.g., 2MB) can incur unbearable performance overhead due to a vast waste of DRAM capacity and bandwidth if most memory references are distributed in a small region ofthe superpage (see Section 2.2). The cost may be even larger than the benefit of superpage migration. This presents a dilemma for the use of superpages,<br>since the lightweight page migration can outweigh the benefits of extended TLB coverage.</p></blockquote><p><a href="https://www.jianshu.com/p/bea989a85a31">累积分布函数图怎么看</a><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d3b808800554ed3b2d0a153353c4e26.png" alt="2MB超级页的累积分布函数与给定区间(108个周期)内一个超级页中被触及的4KB小页的数量"><br>从图中可以看到，很大一部分工作负载有80%以上的概率：2MB的页面中被访问的4kb页面只有12.5%。<br>还有一张表格来说明问题：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/abbbcb642622478cb35fd087610a7f69.png" alt="4kb热页访问统计"><br>使用的工作负载：</p><ul><li><a href="https://www.spec.org/cpu2006">SPEC CPU2006</a></li><li><a href="http://parsec.cs.princeton.edu/index.htm">Parsec</a></li><li><a href="http://www.cs.cmu.edu/%E2%88%BCpbbs/">Problem Based Benchmarks Suit（PBBS）</a></li><li><a href="http://whitedb.org/">WhiteDB</a></li><li><a href="https://redis.io/">Redis</a></li><li><a href="http://graph500.org/">Graph500</a></li><li><a href="http://www.netlib.org/benchmark/">Linpack</a></li><li><a href="https://www.nas.nasa.gov/publications/npb.html">NPB-CG</a></li><li><a href="http://icl.cs.utk.edu/hpcc/">HPC Challenge Benchmark GUPS</a></li></ul><ol start="2"><li><strong>轻量级热页的标识</strong>：为了支持轻量级页迁移，大量工作提倡通过内存控制器监视内存访问。 然而，当<strong>主存容量变大时</strong>，以每页粒度（即4KB）使用<strong>访问计数器会导致高得令人望而却步的存储开销</strong>。</li></ol><blockquote><p>Identification oflightweight hot pages: to support lightweight page migration, a large body of work advocates monitoring memory accesses through the memory controller [55, 63]. However, using access counters at per-page granularity (i.e., 4KB) leads to prohibitively high storage overhead when the capacity of main memory becomes large.</p></blockquote><ol start="3"><li>轻量级页面迁移对TLB覆盖率的影响：页面迁移通常会<strong>分割超级页面</strong>，从而<strong>破坏物理地址的连续性</strong>。 </li></ol><blockquote><p>Impact oflightweight page migration on TLB coverage: page migrations often fragment superpages and thus break the physical address continuity.</p></blockquote><ol start="4"><li>热页寻址效率：由于热页占应用程序内存引用的主要部分，因此必须进一步<strong>减少DRAM中那些热页的地址转换开销</strong>。 </li></ol><blockquote><p>Efficiency of hot pages addressing:ashot pages<br>contribute to a major portion of applications’ memory references, it is essential to further reduce<br>the overhead of address translation for those hot pages in the DRAM.</p></blockquote><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的工作主张分割超级页面以实现轻量级内存管理，如页面迁移和共享，同时牺牲地址转换的性能[37,58]当<strong>超级页面中的热小页面迁移到DRAM时，保持改进的TLB覆盖率</strong>仍然是一个挑战。</p><blockquote><p>Previous work has advocated splintering superpages to enable lightweight memory management such as page migration and sharing, while sacrificing the performance of address translation [37,58]. It is still a challenge to retain the improved TLB coverage when the hot small pages within superpages are migrated to the DRAM.</p></blockquote><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>针对上述问题，提出了一种新的内存管理机制Rainbow, Rainbow在Superpage粒度上管理NVM，并使用DRAM在每个Superpage内缓存频繁访问（热）的小页面。相应地，Rainbow利用拆分TLB[2,7,30,52]的可用硬件特性来支持不同的页面大小，其中一个TLB用于寻址超级页面，另一个TLB用于寻址小页面。 Rainbow将SuperPage中的热小页迁移到DRAM中，而不会损害SuperPage TLB的完整性。 因此，Rainbow实际上将DRAM架构为NVM的缓存。</p><ul><li>为了减少细粒度页面访问计数的存储开销，分两个阶段进行计数。在<strong>给定的时间间隔内</strong>，Rainbow<strong>首先计算Superpage粒度下的NVM内存访问</strong>，然后选择<strong>前N个热门Superpage作为目标</strong>。 在第二阶段，我们<strong>只监视那些小页面</strong>(4KB)粒度的热点超页面，以识别热点小页面。 这种基于历史的策略避免了监视大量冷超页中的子块（4KB页），从而显著降低了热页识别的开销。 </li><li>我们采用<strong>拆分TLB</strong>来加速DRAM和NVM引用的地址转换性能。当一些小页迁移到DRAM时，为了保持SuperPages TLB的完整性，我们在内存控制器中使用位图来识别迁移的热页，而<strong>不会分裂SuperPages</strong>。</li><li>我们提出了一种<strong>物理地址重映射机制来访问DRAM中迁移的热页</strong>，而不必为寻址DRAM页而遭受昂贵的页表遍历。为了实现这一目标，我们将迁移的热点页面的目的地址存储在其原始住所（超级页面）中。 一旦热页对应的TLB未命中，DRAM页寻址应求助于对超级页的间接访问。这种设计在逻辑上利用了SuperPage TLBS作为4KB页面TLB的下一级缓存。 因为Superpage TLB命中率通常很高，所以Rainbow可以显著加快DRAM页面寻址的速度。 </li></ul><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，与现有的内存迁移策略相比，在没有Superpage支持的情况下，Rainbow可以将应用程序的TLB丢失率降低99.9%，并将应用程序的性能（以OFIPC为标准）平均提高2.9×(45.3%)。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA, 2021</li><li>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Mazen Alwadi1, Vamsee Reddy Kommareddy1, Clayton Hughes2, Simon David Hammond2, Amro Awad3<br>University of Central Florida1, Sandia National Laboratories2, North Carolina State University3</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>它们存在着高写延迟和有限的写持久性。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。对于大容量的NVM快速和持久的缓存能力是有限的。利用DRAM作为NVM的一个快速持久的缓存，受到能源支持的限制。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新DRAM缓存内容到NVM</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>实现NVM的快速持久的DRAM缓存，我们利用选择性的NVM镜像对缓存在DRAM中的持久页面进行了新的内存控制器。</p><p>在DRAM中缓存时将持久区域的更新镜像到NVM。 Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>写次数不变，对于写耐久性的破坏</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>虽然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作。仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变</p><blockquote><p>crash-consistent applications 崩溃一致性程序;crash宕机，或主机、程序停止工作等情况。</p></blockquote><p><code>这篇翻译的不好，还有很多问题，会持续修改</code></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>非易失性存储器（NVM）具有传统外存系统（storage systems）持久性和传统内存系统（memory systems）字节可寻址的特点。然而，它们存在着<strong>高写延迟和有限的写持久性</strong>。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。然而，这可能会使缓存页面的持久性失效，从而导致在性能和可靠性方面的权衡问题。在本文中，我们提出了Stealth-Persist，这是一个新的架构支持功能，允许需要持久性的应用程序在DRAM中运行，同时保持NVM提供的持久性功能。Stealth-Persist创造了一个持久性内存的假象，供应用程序使用，同时利用DRAM进行性能优化。我们的实验结果表明，Stealth-Persist将持久性应用的性能提高了42.02%。</p><h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p><strong>新兴的非易失性存储器（NVMs）正逐渐成熟，达到了接近大规模生产阶段和广泛采用的水平[11],[15],[16],[59],[65]<strong>。 例如，最近，英特尔发布了OptaneDC产品，它是一种使用3DXPoint技术的内存模块[21]。这些NVM-based内存模块以超低的空闲功率运行，但仍然具有非常高的密度。例如，每一个Optane DC模块可以有512GB的容量[3]。因此，在需要高内存容量但可能受到电源限制的服务器中，它们是一个非常引人注目的补充。虽然DRAM模块必须频繁地进行刷新操作，它占了很大比例的电力消耗[38]，但NVM并不这样做。传统上，</strong>NVM在断电或关机后仍能保留数据，这对崩溃一致性应用程序非常有用</strong>[9],[17]，并能承载快速访问文件[52]。然而，新兴的NVM的读写延迟比DRAM的延迟要低很多。例如，在英特尔的OptaneDC上，读取延迟为300ns，而DRAM的读取延迟接近70ns[46]4.3倍慢的读取访问。虽然NVM的<strong>写延迟可以通过外部缓冲或利用电池支持的内部写缓冲器来隐藏</strong>(例如， 英特尔的Optane DC)写入延迟队列)，NVM的设备写入延迟可以比DRAM的延迟慢十倍[13],[33]。</p><p>新兴NVM持久特征的未来对于许多<strong>数据恢复和崩溃一致性至关重要的应用</strong>是有吸引力的[40]。此外，新兴的NVM允许直接访问和更新持久性文件，而不会产生昂贵的缺页[52]。例如，数据库的在NVM上可以有几百个千兆字节，并且可以直接在应用程序中进行读写操作，类似于DRAM[52],[59]。此外，英特尔的持久性内存开发工具包( PMDK)允许开发应用程序，<strong>利用NVM的持久性对关键数据结构进行持久更新[9]。理想情况下，在崩溃之后，持久性应用程序应该能够通过从NVM读取数据结构来恢复</strong>[14]。然而，NVM的大的读/写延迟会大大减慢对持久性数据的访问。在其他方面，每个应用程序需要在使用低(相对于DRAM)的NVM和使用DRAM之间做出选择，前者可以实现崩溃一致性 ，后者则会失去内存子系统中的持久性能力。随着新兴的NVMs的不断采用，以及持久性编程库( 如PMDK)的日益普及，我们预计越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。</p><p>新兴的NVM可以作为存储设备( 例如，在SSD内)如英特尔的Optane硬盘[1] ，或作为部分或系统的内存层次。为了将NVM集成到内存层次中，有很多标准和选项[2],[3],[6]。 最值得注意的是，英特尔的类似DIMM的NVM模块( 称为OptaneDC[3])可以作为主存储器或作为主存储器的一部分与其他存储器选项(如DRAM和HBM)一起集成。在作为主内存的一部分使用时，它可以作为一个单独的物理内存添加区，扩大DRAM的物理地址范围，<strong>或者将DRAM作为OptaneDC的物理范围的硬件管理缓存使用[3]。前者被称为App Direct 模式，这类似于将不同的内存区域暴露于非统一存储器体系结构（NUMA）中的系统</strong>。我们把后者称为内存模式。<strong>内存模式放弃了持久性特性</strong>，因为当应用程序从内部缓存刷新其更新时，可以在易失性DRAM中更新内存块。然而，<strong>由于DRAM缓存大量的NVM页面，可以大大改善访问延迟</strong>，特别是对于频繁使用的页面。另一方面，<strong>AD模式确保了映射到NVM地址范围的页面的持久性，但由于它依赖于容量有限的内部处理器缓存(而不是外部DRAM)，因此会产生巨大的成本</strong>。   因此，目前Optane内存模式的集成操作，忽略了持久性应用对持久和高性能的需求。</p><p>JEDEC还为含有NVM的内存模块提供了几种标准。特别的，JEDEC为含有NVM的DIMM (称为NVDIMM)定义了三种不同的标准，即NVDIMM-N、NVDIMM-P和NVDIMM-F[5], [6]。这三种不同的选择提供了不同的暴露容量、持久性保证和管理复杂性的折中。特别的，NVDIMM-N只将DRAM暴露给软件，并在崩溃期间利用超级电容器为DIMM供电，提供将DRAM数据复制到NVM的能力( 目前是 flash-based基于闪存的)。因此，NVDIMM-N具有类似于DRAM的<strong>暴露延迟</strong>，但将内存容量限制在DRAM大小。NVDIMM-F將NVM (目前是基于闪存)暴露给软件，并作为一个块设备直接访问。平均而言，NVDIMM-P对不同的NVM技术有更广泛的定义，并允许内部DRAM缓存与NVDIMM-P的几个持久性选项，如<strong>深冲命令</strong>，因为NVDIMM-P没有能量支持。NVDIMM-P的一个主要优是它利用了<strong>一个交易协议，允许它使用非确定性的时间</strong>，与NVDIMM-N和NVDIMM-F相比，它依赖于确定性的时间。显然，在这三种选择中，NVDIMM-P是最适合新兴的NVM (不是为闪存定制的)和高容量系统。此外，由于DIMM上有高效的能源支持，内部DRAM可以被认为是NVDIMM模块内的NVM的一个高速缓存。NVDIMM-P没有能量支持（energy-backing），除了DRAM缓存在NVDIMM-P中而不是像内存模式下的独立模块，其他与内存模式下的Intel’s Optane DC类似。</p><p>虽然NVDIMM-P作为一个概念很有趣，但由于以下原因，对于大容量的NVM快速和持久的缓存能力是有限的。首先，如果需要几十或几百GB的DRAM来有效地缓存大型NVM，那么就需要笨重和潜在的昂贵的电池来提供能量支持。此外，由于能源支持只支持内部DRAM，客户被限制在相同的供应商和NVDIMM-P中可用的DRAM缓存的特定大小容量。另外，具有独立的NVM和DRAM模块的杠杆老化内存模式解决了NVDIMM-P的灵活性限制，但需要昂贵和不环保的( 和笨重的)电池支持[43]。换句话说，利用DRAM作为NVM的一个快速持久的缓存，受到能源支持(或独立能源)能力的限制，然而这种能力需要在需要大型DRAM模块时得到更多的提升。因此，我们在<strong>本文中的目标是允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新（flush）DRAM缓存内容到NVM</strong>。因此,<strong>我们能够在有NVM的系统中集成首选的DRAM模块，同时也能在DRAM中实现持久性数据的缓存，而不牺牲持久性或需要额外的电池支持能力。</strong></p><p>为了实现NVM的快速持久的DRAM缓存，我们利用<strong>选择性的NVM镜像</strong>对缓存在DRAM中的持久页面进行了新的内存控制器。<strong>我们的方法支持两种内存模式和AD模式</strong>，并能确保对缓存在DRAM中的持久页面的更新的持久性。此外，我们的内存控制器通过放松对DRAM缓存页更新的镜像， <strong>将写入NVM的次数降到最低</strong>，如果他们在NVM中的源页是在NVM的逻辑非持久性部分(即，用于支持不需要持久化的页面)。类似于当前处理器中的内存模式支持我们的内存控制器透明地在NVM和DRAM之间迁移页面。然而，我们通过从NVM中的原始地址推断出其语义来确保DRAM缓存页的持久性。如果页面被缓存在DRAM中，我们的方案只会产生额外的写入，此外，NVM的写入也会发生。然而，未来的读取将从DRAM中进行，这使得快速和持久的NVM页面的缓存成为可能。此外，通过允许持久性页面位于DRAM中，我们的方案利用额外的银行级别的并行性来访问持久性页面，而不是强迫所有的访问到NVM。我们的方案在精神上类似于内部处理器缓存中通常使用的write-through方案，但是由于写的性质和DRAM如何暴露给系统(内存模式或应用直接模式)，涉及到新的优化和设计考虑。虽<strong>然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作</strong>。</p><p>为了评估我们的设计，我们使用了Whisper基准[41]中的持久性应用程序。为了研究我们方法的可靠性，我们还开发了6个内存密集型基准，类似于Janus[37]以前的工作。 一个开放源码的架构模拟器，结构模拟工具包(SST) [50]用来被模仿我们的方法。平均来说，与只使用NVM持久性应用相比，我们观察到<strong>42.02%性能的提高和88.28%NVM的读取减少</strong>。请注意，对持久性数据只使用NVM是唯一允许在没有任何备用电池的情况下进行数据持久化的选择，因此被作为我们的基线使用。<br>综上所述，我们的工作的贡献如下。</p><ul><li>我们提出了Stealth-Persist，这是一种新的硬件支持， 通过在DRAM中实现对热持久页的缓存来提高持久性应用的性能，同时确保数据的持久性，不需要外部电源支持且软件透明。</li><li>我们讨论了将Stealth-Persist与混合DRAM-NVM主存储器系统的实现垂直和水平的相结合。</li><li>我们讨论了Stealth-Persist的设计方案，在方案性能和镜像页面的数量之间进行了权衡。</li><li>我们广泛地分析了Stealth-Persist在不同的区域大小、不同的替换策略和不同的镜像阈值下的开销。</li></ul><h1 id="二、背景"><a href="#二、背景" class="headerlink" title="二、背景"></a>二、背景</h1><p>在本节中，我们将介绍与我们的工作相关的主题，以帮助读者了解我们的工作，然后是工作动机。</p><h2 id="A-新兴的非易失性存储器"><a href="#A-新兴的非易失性存储器" class="headerlink" title="A.新兴的非易失性存储器"></a>A.新兴的非易失性存储器</h2><p>新兴的NVM，如3DXPoint和英特尔的OptaneDC，具有更高的密度、字节寻址能力、更低的每比特成本、比DRAM更低的空闲功耗和非易失性，但具有<strong>更高的访问延迟和有限的写入耐久性</strong>[15],[32],[33],[35]。例如，NVM-based DIMM可以被用来存储文件和内存页，它可以使用常规的加载/存储操作来访问。为了实现这种类型的访问，最近的操作系统(OS)开始支持通过DAX文件系统[52]将内存配置为持久性或非持久性。在DAX文件系统中，一个文件可以直接被内存映射和使用常规的加载/存储操作访问，而不需要将其内容复制到页面缓存中[15]。然而，<strong>NVM的访问延迟比DRAM的访问延迟慢3-4倍</strong>。因此，研究者主张建立同时具有NVM和DRAM部分的内存系统[48]，[62]。</p><h2 id="B-混合主存储器-HMM"><a href="#B-混合主存储器-HMM" class="headerlink" title="B.混合主存储器(HMM)"></a>B.混合主存储器(HMM)</h2><p>混合主存储器( HMM)系统由于其密度和超低的空闲功率，预计将有很大的NVM部分，而由于其快速的读/写操作DRAM部分將很小。HMM可以以两种不同的方式部署，水平或垂直。在垂直方向上，NVM作为一个新的内存层被连接起来，DRAM被用来缓存NVM的数据[3],[6]。这种方案允许更快地访问大型内存池，并且需要特殊的硬件将数据从NVM迁移到DRAM，例如，在英特尔的OptaneDC内存模式下，英特尔的Xeon可扩展处理器的内存控制器对缓存线进行缓存。然而，因为DRAM的波动性，这样的方案并不能提供持久性。在第二种方法中，HMM系统的水平实现将NVM和DRAM暴露在物理地址空间中，就像在NVDTMM-P和OptaneDC的AD模式中那样，如果需要的话依靠OS来处理数据访问和页面迁移[27],[47],[48],[58]。在这两种情况下， 需要有混合内存管理方案，管理不同的持久性和性能要求。</p><p>文献中提出了基于内存层次的不同混合内存管理方案。诸如HetroOs[27]、RTHMS[44]和Nimble[58]提出了软件解决方案，以检测哪些页面需要迁移到最快的内存(例如，DRAM)。当DRAM和NVM都被内存映射并暴露给操作系统时，这些方案适用于<strong>混合内存系统的水平实施</strong>。另一方面，混合内存系统的垂直实施将DRAM作为一个缓存。因此，DRAM没有暴露给操作系统，其中缓存页使用专用硬件处理，通常是内存控制器的扩展，如英特尔的OptaneDC内存模式[3]。像Ramos等人[48]提出的方案，根据每个页面被访问的频率，使用多队列(MQ)结构对页面进行排名，然后使用页面的排名来决定哪些页面需要迁移到DRAM中，哪些页面需要保留在NVM中。然而，跟踪所有的页面并检查MQ结构来提升和降低页面的等级会产生很高的开销，因此在每个时间段只检查队列的头部。在讨论了混合内存系统的管理方案之后，我们讨论了一些用于HMM中的页面缓存的方案。</p><h2 id="C-页面缓存策略"><a href="#C-页面缓存策略" class="headerlink" title="C.页面缓存策略"></a>C.页面缓存策略</h2><p>页面缓存策略是用来决定哪些页面应该被缓存在DRAM中，如果你想缓存NVM的页面。在这一节中，我们将讨论两个策略，这些策略将在我们的设计中使用。</p><p><strong>首次触摸策略</strong>。该策略在第一次访问时缓存页面，并根据LRU算法选择一个页面进行驱逐。<br><strong>多队列(MQ)<strong>。多队列(MQ)。MQ最初被设计用来对磁盘块进行排序，后来被Ramos等人[48]用于混合内存系统的页面放置。MQ的工作方式如下。MQ定义了M个块描述符的LRU队列。队列的编号从0到M-1， 队列M-1的块是访问量最大的块。每个描述符都包含区块的编号、一个引用计数器和一个逻辑到期时间。在第一次访问一个区块时，它的描述符被放在队列0的尾部，其过期时间被更新为<code>CurrentTime+LifeTime</code>。这</strong>两个时间都是以访问次数来衡量</strong>的，<code>LifeTime</code>代表在区块过期前对不同区块的连续访问次数。每次区块被访问时，其过期时间被重置为<code>CurrentTime +LifeTime</code>，其引用计数器被递增，其描述符被推到其当前队列的尾部。在队列i中的块的描述符被访问了一定数量后，它被提升到队列i+1，在队列M-1中达到饱和。另一方面，最近没有被访问的区块会被降级。在每次访问时，所有队列头部的描述符都被检查是否过期。如果描述符过期了，它就被放在下面队列的尾部，其生命时间被重置，其降级标志也被设置[48]。如果一个描述符连续收到两次降级，该描述符将被从MQ结构删除。为了减少升级/迁移的开销，这些操作只在每个周期结束时进行。</p><p>由于已经证明MQ在选择要替换的页面方面优于其他算法[48], [64], 它与我们的目标一致。因为它有助于检测出性能关键的页。因此，在我们的实验中，我们使用了Ramos等人提出的MQ设计[48]。在讨论了缓存策略之后，我们现在提到了目前可用的工业混合内存系统的实现。</p><h2 id="D-目前的工业HMM系统"><a href="#D-目前的工业HMM系统" class="headerlink" title="D. 目前的工业HMM系统"></a>D. 目前的工业HMM系统</h2><p>目前，市场上有不同类型的HMM系统。例如，JEDEC定义了三种不同的被称为NVDIMM的HMM标准。NVDIMM类型有不同的特点、持久性和性能特征。此外，英特尔最近透露了关于Optane DC的内存模式和AD模式的细节。<br><strong>NVDIMM-N</strong>包含一个DRAM部分，一个NVM部分，以及一个超级电容。系统在正常执行时使用DRAM，而NVM只在崩溃时使用超级电容供电用于复制DRAM数据。[6]<br><strong>NVDIMM-F</strong>模块是连接到DDR总线上的NVM，它的访问延迟相对高于DRAM。因此，可以在系统中安装一个DRAM，并用于缓存NVDIMM-F的数据，代价是数据的持久性|6]。<br><strong>NVDIMM-P</strong>仍然是一个DIMM的建议，它有内存映射的DRAM和NVM，其中软件根据数据的大小和持久性要求将数据放在NVM或NVDIMM中。<br><strong>Optane DC内存模式</strong>是英特尔持久性内存的一种操作模式。它类似于NVDIMM-P。</p><h2 id="E-持久性内存编程模式"><a href="#E-持久性内存编程模式" class="headerlink" title="E. 持久性内存编程模式"></a>E. 持久性内存编程模式</h2><p>由于NVM的持久性特点，访问一个NVM内存对象就像访问一个存储文件。因此。应用程序需要一种方法来重新连接到先前分配的存储器对象。因此，持久性内存区域需要名称和访问控制来进行访问。存储网络工业协会（SNIA）建议操作系统提供命名、权限和内存映射的标准文件语义。因此，一些操作系统增加了对文件系统的直接访问（DAX）支持[4]。DAX允许应用程序直接使用持久性内存而不使用系统的页缓存。图1显示了持久性内存感知文件系统的工作原理[52]。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d916d5cd134a4909a11c4901a00fe76b.png"></p><p>使用持久性内存（PM）对象需要程序员考虑多个问题以确保数据的持久性和一致性。其中一个问题是原子性；什么样的支持由硬件提供，而什么是留给软件处理的[52]。英特尔的硬件确保了8字节写入的原子性，因此如果一个对象大于8字节，那么软件就有责任确保更新对象的原子性[52]。此外，<strong>确保数据的持久性需要将数据一直推送到持久性领域</strong>，因为大部分的数据更新是在易失性处理器的缓存中完成的。持久域从内存控制器中的一个小的缓冲区，即<strong>待处理队列（WPQ）开始。WPQ由异步DRAM (ADR)刷新功能支持。ADR提供的电源可以确保在断电的情况下将WPQ的内容刷新到NVM上</strong>[15], [55], [59], [65]。图2显示了具有持久性存储器的系统中的持久性域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/444a25e88ec046da99d0953429fe4d21.png"></p><p><strong>为了将数据一直刷新到持久化区域。确保原子性和排序，需要遵循一组特定的指令</strong>。清单1显示了一个代码例子，它取自来自SNIA的NVM编程模型V1.2[8]。该代码显示了持久化对象<code>a</code>和<code>a_end</code>。为了确保这些持久化对象的更新的一致性、原子性和顺序性，每次更新这些持久化对象之一时都会调用<code>msync</code>操作。请注意，第7行的更新没有使用<code>msync</code>操作，因为它不是在更新一个持久化对象。<code>msync</code>操作是用来<strong>强制更新一个内存范围到持久化领域的</strong>。此外，它创建了一个屏障，以保证在继续进行之前执行先前的存储，<code>fsync</code>操作做的是对文件具有相同的功能[52]。</p><h2 id="F-动机"><a href="#F-动机" class="headerlink" title="F.动机"></a>F.动机</h2><p><strong>在主存储器中拥有一个持久性的部分可以使应用具有不同的持久性要求。然而，为了确保数据的持久性，应用程序的持久性数据应该被放置在内存的NVM部分，由于NVM的访问延迟较慢，这阻碍了这些应用程序的性能</strong>。另一方面，将应用程序的数据放在DRAM上，会导致更好的性能，但不能满足这些应用程序的数据持久性要求。为了确保应用程序的数据持久性，持久性应用程序应该遵循第二节E中提到的编程模型。如前所述，<strong>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域</strong>。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。虽然持久性应用的数据大小不太可能适合易失性缓存，但在大得多的DRAM中缓存这种持久性数据可以为持久性对象提供明显的读取速度提升。同时，期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择( 如供应商)， <strong>是现有解决方案的主要缺点</strong>。因此，仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b9c998558da04df3b8e44015a419ded8.png"></p><p>表一是现有技术之间的比较。从表一中，我们可以观察到，在支持高技术方面的差距。性能持久性内存，以及大容量持久性内存，因此<strong>Stealth-Persist旨在弥补这一差距</strong>。图3显示了在OptaneDC AD模式(所有持久性数据都在NVM中)上运行的持久性应用的性能开销，与在不提供数据持久性的DRAM系统上运行的性能开销相比。从图3中，我们可以看到，在OptaneDC的AD模式上运行的应用平均会产生2.04倍的速度下降。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/003104b458bd4b08824b99b3b496a649.png"></p><h1 id="三、设计"><a href="#三、设计" class="headerlink" title="三、设计"></a>三、设计</h1><p>在本节中，我们根据可能的设计方案及其权衡来讨论Stealth-Persist的设计。首先，我们开始讨论设计要求，以及潜在的设计方案。</p><h2 id="A-设计要求"><a href="#A-设计要求" class="headerlink" title="A.设计要求"></a>A.设计要求</h2><p>我们的设计应该满足必要的要求，以允许广泛采用和高性能，同时保留持久性对象的语义。总的来说，这些要求如下。</p><ul><li>灵活性:我们的设计应允许将任何DRAM模块，无论其容量如何，都集成到配备有NVM的系统中，而不需要任何特殊的电池备份或特定的DIMM修改。</li><li>持久性:任何被认为是持久的内存页或对象(即，可从崩溃中恢复)应该是可恢复的，而不需要任何额外的电池备份支持，无论该页位于何处(NVM或DRAM)。</li><li>高性能:对持久性页面和对象的访问应该和对DRAM的访问一样快。</li><li>透明度:利用持久性内存进行崩溃恢复的应用程序不应该需要明确地管理当前驻留在DRAM中的对象的缓存和持久化。</li></ul><p>为了把这些要求放在持久性应用的背景下，我们可以想象一个访问几十GB持久性对象的持久性应用。 理想情况下，除了NVM模块外，系统还应该能够集成DRAM模块。系统的所有者应该可以灵活地选择什么样的容量和供应商来选择这样的DRAM和NVM模块，这就提供了灵活性。然而，对持久性对象的更新应该是持久的并且在崩溃时持久存在，无论它们存在于何处(DRAM或NVM)。虽然通过持久性模型和框架，即<code>clflushes</code>和内存栅栏使易失性缓存中的对象的更新变得持久，但如果持久性对象被缓存在片外DRAM中，目前还没有支持保证它们的持久<br>性，这给我们带来了持久性要求。最后，<strong>应用程序最好能将其持久性对象缓存在DRAM中</strong>，以尽量减少获取不适合在易失性处理器缓存中的持久性对象的成本。因为<strong>这些对象通常只有几兆字节的易失性处理器缓存</strong>。获取片外持久性对象的延迟比慢速NVM的延迟短（300ns的读取延迟与70ns的DRAM相比），这一要求使我们看到了设计要求的第三个要素，即高性能。因此,持久性应用应该能够将不适合内部易失性缓存的持久性对象缓存在快速的片外DRAM中，同时保留其持久性能力。最后，所有用于缓存和持久化持久性对象页面的操作都应该透明地发生在软件上，而不需要将这些细节暴露给应用程序，这就给我们带来了最后一个要求，即透明度。</p><h2 id="B-设计选项"><a href="#B-设计选项" class="headerlink" title="B.设计选项"></a>B.设计选项</h2><p>现在我们将讨论有可能满足我们要求的设计方案。</p><p>一种选择是支持新的指令，<strong>在缓存线被刷新之前不提交——不仅从易失性缓存，而且从片外DRAM到NVM</strong>。这种设计方案可以通过在内存控制器的支持下向指令集架构(ISA)引入新的指令来实现，或者通过修改当前指令的实现，<strong>使其从内部易失性高速缓存( 例如，clflush)以及从DRAM到NVM中刷新缓存线。假设DRAM是通过内存控制器作为NVM数据的硬件管理缓存来运行的，这样的指令就需要内存控制器首先检查要保存的缓存线是否在DRAM中，读取它，然后将它刷新到NVM中</strong>。这种方法的主要问题是。(1)它需要改变ISA、持久性编程库和处理器内核来支持这种新的指令。此外，(2)持久化数据的延迟将大大增加，特别是当刷新的块在DRAM中被标记为脏块时。请注意，即使DRAM缓存的是页而不是缓存行，它仍然需要类似的支持，但需要新的指令在页的粒度上操作，而不是<code>clflush</code>。</p><p>另一个选择是利用小型固定尺寸的备份能力(例如，超电容)， 为刷新DRAM的特定部分提供动力。例如，无论模块的总大小如何，都要有足够的电力来刷新8GB的DRAM。内存控制器或系统的软件可以潜在地迁移或在地址空间的这个子区域放置持久的页面，标记为持久的。当电源故障发生时，内存控制器( 或外部系统电路)有足够的电力来刷新DRAM的那一部分。虽然这样的解决方案在精神上类似于NVDIMM-N，它提供了选择任何DRAM模块和容量的灵活性。然而，具有持久性支持的部分的大小被限制在系统的备份能力范围内。另一方面，这样的解决方案需要外部系统的支持，并将DRAM的持久性部分的大小限制在电源备份能力上。同样，这样的备份能力通常成本很高， 需要高面积(笨重)， 而且可能对环境不友好。</p><p>虽然第一种方案提供了高性能、持久性和灵活性，但它缺乏透明度。同时，第二种方案具有部分灵活性(需要系统支持和可能的ISA变化)， 部分的高性能(只有一小部分DRAM可以作为持久性内存使用)， 透明度和持久性。因此，我们的设计应该提供完全的透明度、高性能、持久性和灵活性，而不需要任何额外的系统支持或超出现代系统所提供的备份能力。</p><h2 id="C-Stealth-Persist"><a href="#C-Stealth-Persist" class="headerlink" title="C.Stealth-Persist"></a>C.Stealth-Persist</h2><p>在满足上述设计要求的同时，我们的设计还应该与混合内存系统的不同集成方式兼容。特别是垂直内存模式(如Optane DC的内存模式)和水平内存模式(如OptaneDC的AD模式)在深入探讨不同集成模式下Stealth-Persist支持的细节之前，我们将讨论Stealth-Persist如何满足设计要求。</p><p>为了满足灵活性的要求，Stealth-Persist的实现是为了支持在DRAM中缓存时将持久区域的更新镜像到NVM。因此，它不需要系统的任何支持，并且可以在任何DRAM大小下工作。通过镜像缓存在DRAM中的持久性页面的更新，持久性的要求得到了满足。为了使我们的解决方案对软件透明， Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存。图4描述了Stealth-Persist中的读写操作，在高层次上。</p><p>如图4所示，内存控制器集线器MCH)处理对持久性页面的写入的镜像，如果在而直接从DRAM上提供读取请求。通过这样做，Stealth-Persist确保了对NVM的写入的持久性，同时允许对这种持久性对象进行快速的读取操作。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/69796fb223084662b46606e733932280.png"></p><p>虽然在高层次上，这种设计<strong>看起来类似于内部处理器缓存中通常使用的写通方案</strong>，但在考虑到混合内存系统的背景时，会出现许多挑战和潜在的分歧。第一个挑战是<strong>如何决定一个页面是否应该被镜像</strong>。第二个挑战是<strong>如何快速识别一个页面是否被缓存在DRAM中，它被缓存在哪里</strong>?以及<strong>在运行期间如何保证两个副本都是一致的</strong>。第三，<strong>由于不是所有NVM中的页面都需要被持久化，对存储在NVM中的页面的更新需要有选择地进行镜像</strong>。最后，Stealth-Persist需要进行调整，以适应无数种整合混合内存系统的方式。本节的以下部分将讨论这些挑战以及我们如何克服它们。</p><p>1)页面镜像。无论使用何种HMM管理方案，水平(例如应用程序直接模式)或垂直(内存模式)， Stealth-Persist都需要将部分(或整个) DRAM作为持久性页面的镜像区域。<strong>在垂直内存设置中， 整个DRAM将被用作NVM的缓存，因此，任何缓存在DRAM中的页面也可能被镜像到NVM中</strong>。同时，<strong>在水平设置中，由于DRAM和NVM的物理范围明确地暴露在系统中，我们让内存控制器保留一部分DRAM，仅作为镜像区域使用。DRAM的其余部分将直接暴露给系统</strong>，就像在AD模式中一样。位于NVM中的任何持久性页面都可以被缓存在DRAM的镜像区域中， 而不考虑设置，也就是这种区域的大小。<strong>在每个以NVM地址为目标的内存访问中，我们需要透明地检查该页是否当前驻留在DRAM中。读取和写入操作都需要这种检查;如果访问的页面被缓存在DRAM中，那么读取操作可以直接从DRAM中进行，而写入操作则需要更新NVM中的副本，以保证镜像页面副本之间的一致性并确保持久性。当一个页面不在DRAM中时我们需要从NVM中读取它(或写人它)。由于镜像区域可以被认为是NVM中持久性页面的缓冲区/缓存，我们需要为DRAM中的所述缓存/缓冲区定义插入和驱逐策略</strong>。</p><p>为了简单起见，我们使用了一个类似于垂直内存管理方案中使用的页面插入策略。通过这样做，如果使用内存模式，除了在DRAM中缓存持久性页面时向NVM进行额外的写入外，不需要对管理策略进行任何改变。同时，对于AD模式，在DRAM中定义的镜像区域将被管理，类似于内存模式中的DRAM缓存，此外还有对NVM的镜像写入。考虑到这一点，我们<strong>使用了两个简单的策略来放置DRAM缓冲区中的页面</strong>: ( 1)首次触摸策略(FTP)和(2) 多队列(MQ)策略，正如之前的工作中所提出的[48]。</p><p>2)DRAM镜像区域查询。为了确保Stealth-Persist可以快速<strong>检查一个页面是否在DRAM中(镜面区域的地址）， Stealth-Persist使用一个硬件管理表来跟踪镜面区域的页面</strong>。镜像的映射表包含了镜像的缓存页地址的转换，如图5所示。映射表的每个条目都包含一个组ID,它是用NVM中的镜像页地址与镜像区域的页数的模数函数来计算的。此外，每个条目包含六对转换，将36位NVM的页面地址映射到36位镜像DRAM的页面地址。此外，我们为每个翻译使用3位(共18位)作为替换策略的LRU位，这使得翻译总共有450位，其余512位用于组ID (32位) 和填充。因此，一个页面可以通过时钟替换策略或条目内的LRU驱逐从镜像区域中移除。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8130fea0f2ba4f3bbadb1dff9316d3c4.png"></p><p>请注意，镜像的映射的存储要求是在镜像区域中， 每6页的映射表是64字节。因此，我们<strong>在内存控制器中使用一个小的缓存来缓存镜像的映射表项，同时在DRAM中保持该表</strong>。每当收到对持久化区域的内存请求时，就会计算被请求页的组ID，并检查镜像的映射表缓存是否有被请求的组ID,这可能导致三个不同的情况。(1）该条目被缓存，页面被缓存，请求从相关的DRAM页面提供。(2)条目缓存，页面没缓存，条目没有镜像，请求要去NVM。(3)条目没有被缓存，必须检查DRAM中的映射表以获得条目和它的镜像页。由于映射表缓存缺失可能会导致从DRAM中提供请求，并进行两次访问，或者在检查DRAM后从NVM中提供请求，所以我们将请求发送到DRAM中，如果条目在表中，NVM再从DRAM中提供请求，如果不在表中，则从NVM中提供。</p><p>3)镜像页的连贯性更新。在Stealth-Persist中，镜像区域页和NVM页之间应该保持一致性 。由于持久性页面被认为是可恢复的，对持久性页面的写入应该是持久的。因此，对镜像区域的写入应该被推送到两个内存。Stealth-Persist将对镜像区域页面的写请求推送到DRAM的易失性写缓冲区和NVM的持久性WPQ。请注意，<strong>一个写请求只有在它被放入WPQ后才会退役，这保证了写的持久性。另一方面，属于非持久性区域的镜像页不需要数据一致性 ，也不需要可恢复性，这就是为什么Stealth-Persist实现了选择性镜像</strong>。Stealth-Persist对一致性没有任何影响。如果DRAM和NVM模块在同一个插座（socket）上，这是英特尔的DCPMM所支持的配置，NVM和DRAM拷贝之间的一致性由MC通过镜像来管理，而与内部处理器缓存的一致性是在传统系统中处理。然而，如果我们偏离了目前的标准，即NVM和DRAM在同一个插座上，即各自在不同的插座上，那么我们可以指定靠近NVM的内存控制器作为主控，因此它将负责处理镜像、重映射等，并相应地将镜像表缓存中的任何请求转发给拥有DRAM模块的插座的内存控制器。</p><p>4)选择性镜像。Stealth-persist实现了选择性镜像技术，以减少对NVM的写入次数,这可以通过将<strong>指向非持久性区域的写入只提交给其DRAM镜像版本来实现</strong>。Stealth-Persist在垂直HMM实现中实现了选择性镜像，就像在Optane DC的内存模式中一样，在水平HMM实现中实现了OptaneDC的AD模式。在这两种情况下，StealthPersist需要持久性内存区域的地址范围，这可以在系统启动时由内核传递给Stealth-Persist，例如，Linux 命令<code>memmap= 2G!8G</code>可以用来保留一个从地址8G开始的2GB持久性区域。请注意，将非持久化区域中的页的写操作只转发到其镜像版本，会违反这些页的一致性。 然而，由于这些页面是在非持久性区域中，而且这些应用预计是不可恢复的，所以写入可以只提交给镜像页面，而如果页面被驱逐，整个页面将被写回NVM。</p><h2 id="D-概述"><a href="#D-概述" class="headerlink" title="D.概述"></a>D.概述</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ec2628a98966478c8c30d234e670ce39.png"></p><p>图6显示了整个Stealth-Persist设计。对于每个最后一级缓存(LLC)的缺失，首先内存控制器检查该请求是否是对持久性区域的请求①如果是对持久性区域的请求，镜像表缓存被查询到NVM页面的当前状态②。如在三C2节中讨论过，镜像表缓存通过查看已经缓存的镜像表条目，或者从存储在DRAM中的镜像表中获取条目，并使用LRU策略替换组ID和相应的映射表条目，来验证NVM的页面镜像状态③如果页面被镜像，读取请求被转发到DRAM，而写入请求被转发到DRAM，以更新镜像区域，以及NVM，以保持数据④。在读的情况下，持久化内存访问被转发到多队列或FTP单元CT⑤这单元决定一个页面是否应该被镜像，如果是，则镜像表缓存被触发，使用LRU策略⑥替换其中一个映射。</p><h2 id="E-Stealth-Persist-NVM库的比较"><a href="#E-Stealth-Persist-NVM库的比较" class="headerlink" title="E. Stealth-Persist NVM库的比较"></a>E. Stealth-Persist NVM库的比较</h2><p>一些研究提出使用NVM库来解决NVM作为主存储器时的原子性、崩溃一致性和性能问题。NVM库的重点是将写操作移出关键路径以提高性能，但没有减少读延迟。相比之下，Stealth-Persist通过减少基本的内存读取操作的延迟来提高性能，这在NVM库中仍然是需要的。一些方案专注于容错(如Pangolin[61])、性能和强一致性(如NOVA[57]) 、减少编程工作量和性能(如Pronto[39]) 。虽然这些方案通过将写入开销移出应用程序的关键路径，或者通过在DRAM中缓冲一些更新来提高系统的性能，但是如果需要持久性，对NVM的写入是不可避免的。与此相反，Stealth-persist在写到NVM时，如果它们是指向NVM中的一个持久性区域，就会传播到NVM，并将写到非持久性区域的写入缓冲在DRAM的缓存页中。此外，Stealth-Persist在与所提出的NVM库不同的层中运行，这使得Stealth-Persist与这些方案正交。事实上，Stealth-Persist可以与上述方案同时使用，以进一步提高性能。</p><p>在一个不同的方向，Hagmann[24]提出了一个方案维护个日志以恢复磁盘中的文件系统。Petal[34]通过创建虚拟磁盘，使客户能够访问分布式磁盘，从而改善系统的性能，提高吞吐量。为了提供可恢复性，Petal使用写前记录。Condit等人[23]提出了一个方案，使用影子分页使持久性存储器的崩溃一致性， 在这个方案中，写是原地提交或使用局部的写时拷贝。Linux文件系统的BTRFS[49]使用B树数据结构，并使用写时拷贝作为更新方案。Rosenblum和Ousterhot[51]提出了一个日志结构的文件系统，该系统以顺序的方式执行所有写入磁盘的操作，并保持索引信息以加快数据检索。Seltzer等人。[53]提出了一个日志结构的文件系统，它具有更好的写入性能，更少的恢复时间，并能实现嵌入式事务和版本管理。提出这样的方案是为了确保原子性、可恢复性，并提高性能在磁盘中。然而，在确保持久性的同时，这些方案并没有提<br>高读取操作的性能。因此，它们与Stealth- Persist是正交的。</p><h1 id="四、方法论"><a href="#四、方法论" class="headerlink" title="四、方法论"></a>四、方法论</h1><p>我们在结构模拟工具包( SST)模拟器中对Stealth-Perit进行建模[50]。SST是一个基于周期级事件的仿真器， 对不同的硬件组件进行模块化设计。SST在工业界和学术界被广泛使用[28],[30],[31]。我们实现了一个混合的内存控制器组件，以处理DRAM和NVM。表二显示了模拟系统的配置。模拟的系统包含4个失序的内核，每个内核在每个周期执行2条指令。核心的频率是2GHz。三个级别的缓存，L1，L2和L3(包括)被模拟为32KB，256KB和1MB。DRAM的容量为1GB，NVM的容量为4GB。（请注意，选择DRAM和NVM的大小是由于仿真速度的限制，然而，最重要的参数是镜像区域的大小( 32MB)和应用程序的平均足迹( 256MB)。， 由于持久性应用程序的所有数据将驻留在NVM中，并且可以在镜像区域中持久性地缓存，因此我们关注应用程序的足迹与镜像区域的比例(8:1的比例) ，我们在本文后面将对此进行修改。） NVM的读写延迟为150ns和500ns[14]。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/f75fb2353b384d6cb0f88987635402e5.png"></p><h2 id="A-负载工作"><a href="#A-负载工作" class="headerlink" title="A.负载工作"></a>A.负载工作</h2><p>为了评估我们提出的方案，我们运行了11个持久性应用程序。如表三所示，其中六个基准是内部开发的，所有这些都是为了强调内存的使用，并在以前的工作中使用过[37]。每个应用程序的功能描述如下</p><ol><li>ARSWP:该基准从数据库中随机选择两个密钥并进行交换。 </li><li>RANDWR:选择随机钥匙，用所选钥匙的数据库条目更新一个随机值。</li><li>SEQWR:这与RANDWR类似，但钥匙是按顺序选择的，从数据库的第一个元素开始。 </li><li>AVL:数据库被映射到AVL树上，并在映射的数据库中搜索一个随机生成的密钥。如果没有找到密钥，则触发插入操作。 </li><li>BTREE:该基准将数据库映射为B树，与AVL类似，搜索一个随机密钥， 如果没有找到，则用假数据插入密钥。 </li><li>RBTREE:与AVL和BTREE基准类似，RBTREE基准将数据库映射为RB树， 并搜索一个随机密钥。</li></ol><p> <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/d02f12cdf7f04e548613eeaf042b87fb.png"><br>我们还运行了由威斯康星大学麦迪逊分校与惠普实验室合作开发的WHISPER基准测试套件[41]中的五项基准(表三中前面有W：)。TPCC基准测量的是基于复杂数据库和在其上执行的各种数据库事务的在线交易处理系统(OLTP)的性能。雅虎云服务基准( YCSB)是一个用于评估数据库管理系统的编程套件。W:TPCC和W:YCSB基准是Whisper基准套件的变种，它是以N-Store[12]为模型的，N-Store是一个持久性内存的远程数据库管理系统。W:CTREE和W:HASHMAP基准是使用NVML[56]库开发的，该库对持久性存储器区域执行插入、删除和获取操作。W:ECHO是一个用于持久性内存区域的可扩展键值存储。对W: CTREE和W:HASHMAP基准的地图获取功能进行了评估。</p><p>所有这些基准的密钥大小为512比特，数据库大小为1GB。在评估这些基准之前，首先用随机密钥填充数据库。这些基准的每千条指令的失误率( MPKI)见表III。每个基准对500M指令进行了评估。</p><h2 id="B-DRAM-Mirror-Configuration镜像配置"><a href="#B-DRAM-Mirror-Configuration镜像配置" class="headerlink" title="B. DRAM Mirror Configuration镜像配置"></a>B. DRAM Mirror Configuration镜像配置</h2><p>为了镜像虚拟机的数据，我们使用了32MB的DRAM。然而，正如第五章E节中所讨论的那样， 我们把镜像区域的大小从2MB到1GB不等(整 个DRAM被用作镜像区域)。最小化是以页为单位进行的。在MQ机制中，只有当页面达到MQ级别4时才会被镜像，也就是说，当一个页面被读取16次时。纪元的时间间隔被设置为10000次读取操作。尽管我们用上述配置评估了StealthPersist方法，但我们通过改变DRAM镜像、大小和阈值水平来进行敏感性分析。由内存控制器维护的镜像表缓存的大小为128组，每组有 6个映射。镜像表缓存的查找延迟为1ns。</p><h1 id="五、评估"><a href="#五、评估" class="headerlink" title="五、评估"></a>五、评估</h1><p>在本节中， 我们讨论了Stealth-Persist与直接使用NVM进行持久化的系统的对比结果。我们进一步展示了通过改变影响性能的不同参数而进行的敏感性分析。</p><h2 id="A-Stealth-Persist对性能的影响"><a href="#A-Stealth-Persist对性能的影响" class="headerlink" title="A.Stealth-Persist对性能的影响"></a>A.Stealth-Persist对性能的影响</h2><p>图7显示了使用Stealth-Persist方法的性能改进。基准方案是OptaneDC AD模式方案，其中所有的持久性内存请求都只存储到持久性内存(NVM)。这是实现此类系统的持久性应用的数据持久性的典型方式。平均而言，使用Stealth-PersistMQ和FTP方法，性能提高了30.9%和42.02%。应用程序的性能改善是镜像区域命中率的一个函数，这一点在第V-B节中讨论。Stealth-Persist FTP的改进高于Stealth-Persist MQ方法，因为每一个被读取的页面都在DRAM中被镜像这导致了大量的页面从NVM复制到DRAM中。平均来说，我们观察到隐身传输FTP比隐身传输MQ方法多镜像了542.96倍的页面，这大大增加了内存总线的流量和能源使用。对于像SEQWR和W:ECHO这样的顺序内存访问基准，Stealth-Persist FTP的改进是巨大的–2.34倍和2.5倍。分别为2.2倍。由于这些基准是按顺序访问内存的，所以这些基准的空间定位性很高。因此，当一个页面被读取时它立即被镜像到Stealth-persist FTP中，并被访问为连续的内存访问。另一方面，Stealth-Persist MQ方法，首先，页面应该达到一个阈值才能被镜像。对于AVL和RBTREE工作负载来说，隐身主义MQ方法优于隐身主义FTP，因为隐身主，义FTP非常频繁地替换镜像区域的页面，这导致了从镜像区域驱逐热页面。另一方面，Stealth-persist MQ方法倾向于将热页保留在镜像区域。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/fd4aaaa8276a419c8b119a5ad7190607.png"><br>对于ARSWP工作负载，Stealth-Persist方案的性能与Optane DC应用二维模式相比几乎没有变化，从图3来看，与使用DRAM作为主内存的系统相比，它受到了很大的影响它的速度要慢4.39倍。然而，ARSWP应用程序的内存访问非常稀疏，因此页面的重用距离很高，这导致在Stealth-persist FTP方法中，在这些页面被重用之前就被驱逐了。此外，ARSWP应用程序的页面没有达到Stealth-Persist MQ方法的镜像限制。因此，性能下降了3%，在由于检查镜像区域而只有0.02%的命中率，MQ方法。另一方面，由于有3%的命中率，Stealth-persist FTP的性能在ARSWP基准中提高了1.6%。然而，如第V-E1节所示，当镜像区域的大小增加时，ARSWP的性能得到改善。</p><h2 id="B-DRAM镜像命中率"><a href="#B-DRAM镜像命中率" class="headerlink" title="B.DRAM镜像命中率"></a>B.DRAM镜像命中率</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/55e20b926dcb4b9ababf12230c83b888.png"><br>图8显示了由DRAM镜像区域提供服务的读取百分比。我们注意到，有顺序内存访问的应用显示出最好的性能改进FTP在这些应用中显示出非常高的命中率。另一方面，在Stealth-Persist MQ方法中，具有随机跨度访问的应用和具有热页的应用显示出最高的命中率。如图8所示，在Stealth- Persist FTP方法中，镜像页平均占整个内存读取的57.81%。对于Stealth-Persist MQ，与Stealth-Persist FTP相比，它以合理的页面镜像数量服务于平均24.78%的整体读取。如图8所示，具有最高命中率的内存约束应用程序显示出最高的性能改进。在Stealth-Perist FTP中，WHISPER基准的镜像命中率，如CTREE和HASHMAP很高，但性能的提高没有SEQWR和ECHO基准那么多。这是因为CTREE和HASHMAP应用不像EPOCH和SEQWR那样内存密集，这与CTREE和HASHMAP的MPKI有关，如表三所示CTREE的MPKI为1.75，HASHMAP的MPKI为0.84。</p><h2 id="C-Stealth-Persist对NVM读取的影响"><a href="#C-Stealth-Persist对NVM读取的影响" class="headerlink" title="C.Stealth-Persist对NVM读取的影响"></a>C.Stealth-Persist对NVM读取的影响</h2><p>在这一节中，我们展示了使用Stealth-Persist方法减少发送到NVM的读取次数。当镜像区域的命中率很高， 大部分的读取都由镜像区域提供<br>这就减少了发送到NVM的读取数量。图9显示，与Optane DC AD模式( 100%)相比,使用Stealth-Persist FTP和MQ方法，NVM的读取次数平均减少了 88.28%和73.28%。对于SEQWR和W:ECHO基准来说，使用Stealth-Perist FTP显示出最高的性能改进，NVM读数分别明显减少了98.42%和98.02%。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/0491934cb8024507bcb52dedff4c7835.png"></p><p>如图10所示，Stealth-Persist万案对写入NVM的次数没有任何影响。然而,Stealth-Persist也会将镜像页的写入发送到DRAM中。因此，Stealth-Persist不影响NVM的写入持久性，也不增加能耗，这可能是由于增加NVM的写入量造成的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cf110ec94f8d4ebeace793e8ad7a01e1.png"></p><h2 id="E-敏感度分析"><a href="#E-敏感度分析" class="headerlink" title="E.敏感度分析"></a>E.敏感度分析</h2><p>尽管Stealth-Persist FTP和MQ与基线( OptaneDC AD模式)相比，平均提高了42.02%和30.9%的性能,但仍有改进的余地，因为镜像区域的命中率平均为57.81%和24.78%。错过的原因有很多，但主要是受Stealth-Persist设计中的镜像区域大小和镜像阈值的影响然而增加镜像区域的大小将增加硬件的复杂性( 镜像表的大小),而减少镜像阈值可能会导致提前替换所需的页面，这可能会导致会降低整体性能。为了充分分析镜像区域大小和镜像阈值的影响，我们在本节中改变了镜像区域大小和镜像阈值。此外，我们还展示了快速和慢速NVM.上的性能改进。所<br>有工作负载的平均值显示在敏感性结果中。</p><p>1)镜像区域对性能的影响。在DRAM中可以被镜像的持久性页面的数量取决于为镜像保留的DRAM内存的百分比。为了避免大量的内存开销， Stealth-Persist只保留了32MB的DRAM，也就是模拟系统中DRAM的3.125%，用于镜像持久性内存页。然而，正如前面所讨论的， 在使用Stealth-Persist时，可以镜像的页面越多，系统性能的上限就越大。因此，我们将镜像区域的大小从2MB改为1GB，以评估Stealth-Persist的性能改进。请注意，当镜像区域的大小为1GB时整个DRAM都被保留用于缓存镜像页面。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/43bef6e28284442c84f9a398b2558af4.png"></p><p>图11显示，增加镜像区域的大小可以提高FTP和MQ的性能。当镜像区域大小从2MB增加到1GB时，Stealth-Perist FTP的性能改善从1.28倍增加到1.83倍，Stealth-Perist MQ从1.14倍增加到1.38倍。由于MQ是一种基于确认的方法，只有当NVM页面被访问的次数超过阈值(4)时才会被镜像，所以在使用Stealth-Persist MQ的64MB镜像区域大小后，改进已经饱和。因此，即使镜像区域的大小增加了， 要镜像的页面数量也受到阈值的限制，因此性能的提高是饱和的。当镜像区域大小为64MB时，使用Stealth-Perist FTP的性能提升为1.48倍，使用Stealth-Perist MQ为1.35倍。另外，正如所断言的那样，ARSWP基准在32MB的镜像大小下没有显示出性能的改善， 在镜像区域大小为64MB、128MB、 256MB、512MB和1GB时，Stealth-Persist FTP分别实现了1.06倍、 1.22倍、 1.75倍、 2.65倍和3.22倍的改善。然而，使用Stealth-Persist MQ，我们观察到没有任何改善，因为ARSWP应用程序的页面没有达到镜像阈值。</p><p>2)镜像阈值水平对性能的影响。在图12中，我们显示了改变镜像的结果。阈值队列水平。当阈值水平降低时，使用Stealth-Perist<br>MQ方法的性能改善会增加。我们观察到，当阈值水平设置为1时，性能提高了1.46倍;当阈值水平为4时，性能提高了1.3倍。当阈值水平降低时，Stealth-Persist表现得很积极，因为有更多的页面被识别为镜像的候选。也就是说，当阈值水平为1时，如果应用程序至少读了2次页面，该页面就被确定为镜像候选。但是，当阈值水平为4时，只有当一个页面被读取至少16次时才会被镜像。因此，通过降低阈值水平实现的性能改善是以增加镜像的页面数量为代价的。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6e837cd8fd5146e4bf6af4d90366c21e.png"></p><p>另一方面，由于两个原因，增加阈值水平会损害性能的提高。1) 一个页面在达到阈值水平后被镜像，随着队列水平的增加，应用程序必须更频繁地访问该页面才能被确定为镜像候选。一般来说，这些页面的比例很小，而且它们经常被缓存在处理器中。2)在达到阈值水平后，页面的热度会消失。例如，如果阈值水平被设置为6，一个页面必须被访问至少64次才能被镜像。然而，在访问该页64次后，应用程序可能不再需要访问该页，从而否定了镜像的影响。</p><ol start="3"><li>NVM读/写延迟对性能的影响。尽管NVM的读取延迟与DRAM的读取延迟相当， 但它仍然比DRAM的读取延迟慢。与DRAM相比，NVM的写延时明显受到影响。在将页面从NVM镜像到DRAM的过程中，NVM的读/写延迟至失重要。因此，我们研究了Stealth-Persist对慢速和快速NVM的读/写延迟的影响。我们改变了NVM的读和写延迟，如图13所示。图13将NVM分为4种类型–中等:读写延迟为150ns和500ns，慢:读写延迟为300ns和700ns，非常慢:读写延迟为500ns和900ns，超慢:读写延迟为750和1000ns。随着NVM的读/写延迟的增加，使用Stealth-Persist的性能改善也会增加。对于超慢的NVM，Stealth-Persist在使用FTP和MQ时，性能分别提高了1 .87倍和1.54倍。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/32440afd47dd4fa4a7d9a7960e24f83c.png"></li></ol><h1 id="六、相关工作"><a href="#六、相关工作" class="headerlink" title="六、相关工作"></a>六、相关工作</h1><p>混合内存。最近，很多工作都在探索如何提高混合存储器系统的性能。例如，Ramos等 人[48]提出了一个在混合内存系统中放置<br>页面的方案。该方案使用多队列对页面进行排序，并且只将性能关键的页面迁移到DRAM中。然而，该方案并没有确保数据的持久性，只是专注于将性能矣键的页面放在DRAM中。HetroOs[27]的作者 提出了一个应用程序透明的万案，利用操作系统提供的应用程序的内存使用信息来决定在异构内存系统中放置数据。然而，HetroOs的动机纯粹是为了系统性能，并没有提供持久性的保证。因此，有持久性要求的应用仍将不得不承受高的NVM延迟。Nimble[58]的作者提出了一个方案，重用操作系统的页面跟踪结构，在存储器之间进行页面分层。此外，Nimble提供了一些优化，如透明的巨大页面迁移和多线程页面迁移，与本地Linux系统相比，这导致了40%的性能提升。然而，Nimble改善了内存之间的页面迁移，但并没有确保数据的持久性。Agarwal等 人[10]为混合内存系统中的GPU提出了一个页面放置方案。然而，所提出的方案是根据应用带宽要求在存储器之间迁移页面，没有考虑数据的持久性。Yoon等人[60]设计了一个策略，使DRAM能够在NVM内存中<br>缓存具有高频率的行缓冲区失误的页面。CAMEO[20]、PoM[54]、Mempod[45]和BATMAN[21]讨论 了可能的放松措施，以最大化整体内存带宽。所提出的技术依赖于编译器支持或Linux内核来检测感兴趣的页面。Lim等人[36]和Kommareddy等人[30]探讨了在分解内存系统中把远程页面迁移到本地内存的问题。</p><p>NVM数据的持久性。确保NVM驻留数据的持久性、性能和崩溃的一-致性最近一直是 人们矣注的焦点。例如，Janus[37]通过将后端内存操作分解为较小的超操作， 然后将超操作重叠起来，改善了持久性应用的写入延迟。除了前面提到的NVM库，英特尔的PMDK[9]、REWIND[18]、 NV-Heaps[22]和LSNVMM[25]为程序员提供了基于软件的高级接口，以确保数据的持久性并提供崩溃一致性支持 。基于硬件的</p><h1 id="七、结论"><a href="#七、结论" class="headerlink" title="七、结论"></a>七、结论</h1><p>要提高Hyblid内存系统中的持久性应用的性能，需要在DRAM中缓存NVM常驻数据。然而，將持久性应用的数据缓存在DRAM中会使这些缓存页的持久性失效。确保DRAM缓存页的持久性可以通过DRAM的电源备份来实现。然而，使用电池给DRAM供电是昂贵的，不可靠的，与传统的系统不兼容，而且不环保。因此，我们提出了Stealth-Pert，一种新型的内存控制器，允许在DRAM中 缓存NVM的识别页面，同时保证页面的持久性。通过从DRAM为NVM请求提供服务，Stealth-Perist利用了银行级别的并行性，减少了内存的消耗并带来了额外的性能提升。Steallh-Persit使 用StealthPersiFfP，在混合内存系统中， 每项应用的系统性能平均提高42.02%。然而，Stealth-PersiFfP需要将大量的页面从虚拟机复制到DRAM中。通过Steallb-PersitMQ方法，我们在合理的页面min-or下，性能提高了30.09%。Tealthisl通过商场硬件管理表、U1e内存控制器中的商场缓存以及利用WPQ来实现这一改进。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自HPCA 2022属于CCF-A</li><li>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Haikun Liu, Renshan Liu, Xiaofei Liao, Hai Jin, and Yu Zhang 华中科技大学大数据技术与系统Lab、服务计算技术与系统Lab、集群与网格计算Lab</li><li>Bingsheng He 新加坡国立大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>充分利用NVM和DRAM混合系统的优势，主要目标是将应用程序数据正确放置在混合存储器上。</p><p>观察发现1：每个页面中的热数据只占应用程序总内存足迹的一小部分，但却导致了应用程序总内存引用的很大比例。X轴显示了应用程序所有内存页中频繁访问的数据流量的百分比(按访问频率降序排列)，Y轴显示了应用程序的总内存引用的累积分布。对于soplex来说，近35%的热数据占了总内存引用的98%。这意味着迁移soplex中的部分热数据(变量和对象)比迁移整个页面更有好处。然而，对于一些应用程序，如dict，内存访问均匀地分布在应用程序的地址空间中，因此从数据迁移中获益较少。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/4168f8514f9242c3981f5314d9399b78.png"></p><p>观察发现2：很大一部分的应用程序的对象都比页小得多。这些应用程序的对象大小的分布函数。我们可以发现对于dict、isort、gcc和maxMatch来说，95%以上的对象都小于50字节。对于soplex，小对象的比例也超过85%。由于大多数应用层面的内存引用是对象和变量。程序员可以充分利用应用语义来优化对象粒度的数据放置/迁移。相反，一个页面可以包含许多对象，这些对象可能有不同的访问行为，合成的页面级访问统计可能变得复杂和不可预测。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/91c7474aaf8b47afb3816ffc63ed1b4c.png"></p><p>观察发现3：很大一部分应用程序的对象显示出较长的寿命。虽然以前的研究表明，许多程序的对象往往有一个相对较短的寿命[15]，[22], [23], 我们发现，在某些情况下，仍然有很大一部分长寿命的对象。图3显示了一些应用程序的对象寿命的累积分布函数。我们发现在gcc、isort和soplex中几乎所有的对象都有超过1000秒的寿命。特别是，gcc中所有对象的寿命都等于应用程序的总执行时间。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9618ef5f5ec84e2c83d4519155af6982.png"></p><p>观察发现4：一部分应用对象在不同的执行阶段表现出高度突变的内存访问频率。 尽管许多对象在其整个生命周期中表现出相对一致的访问行为（即所谓的不可改变的对象）， 但仍有很大一部分对象在不同的执行阶段表现出高度突变的访问频率（即所谓的易变对象），如表1所示。这意味着仍有很大的空间可以通过运行时的对象迁移来优化数据放置。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ce50f76d94ed4edc96bb02efd8325a12.png"></p><p>总之，观察结果1和2清楚地表明，数据访问监控和内存分配/迁移应该是在对象上，而不是在页面上。观察结果3和4表明，在混合内存系统中需要对对象迁移进行仔细的设计。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>提出混合内存系统编程接口，<strong>对象粒度迁移，减少迁移开销</strong>。减少系统耗能，同时提高应用性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的研究主要集中在页面迁移方案上，以实现更高的性能和能源效率。但是，这些方案都依赖于在线页面访问监控成本高。并且还会由于多核时维护缓存/TLB一致性和dram带宽争用产生更多页粒度迁移的开销。</p><p>依赖于页面访问的recency and frequency来决定数据在DRAM或NVM上的位置。1)一些用硬件辅助页迁移的方案由于目前硬件不支持页面访问技术，需要对硬件架构进行大量修改[5][6][11]。2)用操作系统监控内存访问只能引用1个访问位不足以表达页面冷热度[12][Thermostat13]。3)页面迁移通常<span class="label primary">需要一段时间来检测热页</span>。4)<span class="label primary">预测的页面访问模式可能与未来的访问行为不一致</span>，导致不必要的页面迁移。5)大页（huge page）已被越来越多地用于大数据应用和虚拟化平台[13][14]，由于对DRAM容量和带宽的低效利用，粗粒度的页面迁移甚至会降低系统性能。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/257cf52418304fce863269f59094a936.png"></p><p>1）首先静态分析对象被引用的次数。使用LLVM从源代码中生成中间代表（IR） 。为了计算对象级的内存引用，我们利用LLVM的PASS来遍历所有的加载/存储/分配/调用/映射指令，并在IR中插入探针。运行由修改后的IR生成的可执行文件，并通过LLVM PASS自动收集所有对象的访问信息到一个跟踪文件中。内存跟踪包含5个项目，包括”内存访问类型、虚拟地址、对象名称、对象类型和访问时间”。通过跟踪，我们很容易计算出对象的总内存引用和寿命。</p><p>2）<span class="label primary">然而，只有一部分指令导致了对主内存的实际数据流量。原因是片上高速缓存可以过滤大量的内存访问热数据。</span> 于是设计了缓存模拟器，将LLVM生成的跟踪文件作为输入，并过滤程序中所有在模拟缓存中被击中的虚拟地址。剩余的虚拟地址反映了高速缓存的缺失或高速缓存的驱逐操作，并导致对主内存的实际内存访问。图5显示了不同应用程序对高速缓存和主内存的所有数据访问的分布。我们发现，片上高速缓存平均能够过滤68%的总内存引用。对于gcc，甚至87%的数据访问都被过滤掉了。因此，当考虑到缓存过滤的影响时，应用程序的内存访问模式可能会非常不同。一些非常热的数据可能总是在高速缓存中被命中，而只导致对主内存的极少量的数据访问。因此，没有必要将这些数据从NVM迁移到DRAM上。通过我们的缓存过滤器，我们可以在NVM上获得真实的内存访问统计，并避免在运行时进行不必要的对象迁移。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ff01b08a245141909a3c4b67a028325b.png"></p><div class="note info no-icon">但问题是片上缓存也不是缓存的对象啊？</div> <p>3）建模决定数据效用（就是一个该不该放DRAM的权重吧）。提出了一个效用函数（延迟和能耗）来计算在给定的时间段 $T_i$内将对象放在DRAM或NVM上的好处<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/3c7f6307e86e457ca5419d132398b9a5.png"></p><p>4）程序执行期间的热对象迁移工作。因为之前说了有易变对象和不易变对象（指热度）。将对象的内存访问模式分为不同的阶段，设定阈值E，在每个时隙去计算当前这个对象效用值，并且判断是不是不可变对象。对于不可变的对象，如果其平均效用大于E，这些对象将被放置在DRAM上。对于突变的对象，其第一阶段的平均效用决定了该对象应该被放置的位置，而对象的迁移应该在一个阶段发生变化时进行。在图6中，对象A、B和D首先应该被分配到DRAM上。对象C一开始应该被分配在NVM上，然后应该被迁移到DRAM上。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c40ea2c19e95465ea086d09c0534c9d7.png"></p><p>5）初始对象分配的实现。根据效用值的阈值、当前DRAM容量这些情况来决定分配的层级的。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/8b3058d3eb274687b9c3d8c01ea24884.png"></p><p>扩展了Glibc库，为混合内存系统提供DRAM分配和NVM分配API。因此我们修改了Linux内核，在虚拟地址空间（VMA）中将NVM页与DRAM页区分开来。因此，我们在逻辑上将主内存分为两个区域。我们在VMA中标记NVM页，并为NVM分配提供一个新的分支NVM mmap。</p><p>6）运行时对象迁移的实现。采用静态代码工具，在应用程序的源代码中添加对象迁移指令。在运行时，应用程序本身执行对象迁移，没有任何操作系统的干预。我们观察到，<span class="label primary">大量密集的内存访问主要归因于循环状态[25]</span>。我们用循环作为断点，将源代码分成几个代码片段。首先，我们用LLVM跟踪对象的访问次数以及循环开始和结束时的时间戳。对于每个对象，我们在循环的执行过程中计算其效用值。循环开始和结束时效用值的突然变化意味着内存访问模式发生了变化，该对象应该在循环之前被迁移到另一种内存。我们用时间戳来定位这个特定的循环状态。最后，我们使用LLVM在循环的开头插入对象迁移语句。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1c528b5e931d43f2a379af33b9eb4d8a.png"></p><p>图7显示了一个混合内存分配和对象迁移的代码工具化的例子。我们根据LLVM中的抽象语法树（AST），以静态单一赋值（SSA）的形式遍历所有对象，并用DyMalloc改变源代码中的所有malloc。我们使用shared_ptr创建对象，这是一个智能指针，通过指针保留对象的共享所有关系。这允许几个shared_ptr对象共享同一个对象的内存。shared_ ptr使用一个控制块来记录所有指向目标对象的指针和总引用的数量。例如，在图8中，指向对象3的两个指针被记录在控制块中。只有当引用计数器变为零时，被管理对象才会被销毁。对于可改变的对象，我们使用MigrateToX API将对象复制到另一种内存中，然后所有存储的指向被管理对象的指针现在应该指向新的内存地址，如图8所示。对象迁移指令被插入到一个适当的地方（很可能是在循环语句之前）， 在那里对象的访问模式的变化。通过这种方式，我们在迁移后改变对象的虚拟地址，并在运行时更新对对象的所有引用。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d25170131ab4c1b87f596593e283a87.png"></p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>“有效的静态内存分配：在于2pp执行时间和性能差不多的情况下，与只使用DRAM的系统相比，””OAM w/o migration “”能够平均减少51%的内存能量消耗。这些结果表明，我们最初的OAM数据放置策略对提高混合内存系统的性能和能源效率是有效的。</p><p>在线内存迁移的有效性：与静态内存分配方案相比，对象迁移可以进一步提高应用性能，平均提高11%。没有迁移的OAM平均可以实现51%的EDP减少，而有迁移的OAM平均可以进一步减少10%的EDP</p><p>与一些页面迁移算法相比较：与CLOCK-DWF和2PP相比，OAM可以显著减少迁移流量，平均分别为42%和22%。开销也比他们都小。</p><p>适应不同数据和规模：执行时间都比不使用该方案要节省时间。</p><p>对不同NVM性能的敏感性：测试了目前的产品，对于Optance是由读密集引起的迁移，因为写延迟是差不多的。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>只为用C++编写的应用程序提供对象级迁移接口。<br>动态迁移都集中在循环。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>提供了一个离线剖析工具来详细描述应用程序的内存访问模式，并提出了一个<strong>性能/能源模型</strong>来指导应用程序对象的初始内存分配和动态迁移<strong>从能源消耗上解决了读写不均衡</strong>，而不需要任何硬件修改和操作系统干预的在线内存监控。</p><p>一个静态代码工具，用于自动转换应用程序源代码中的对象级内存分配和迁移，而不会给应用程序的程序员带来负担。</p><p>运行时监控和以往的想法都不同，也因此监控开销更小。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
            <tag> 细粒度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/05/01/hello-world/"/>
      <url>/2023/05/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-Create-a-new-post"><a href="#1-Create-a-new-post" class="headerlink" title="1.Create a new post"></a>1.Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行命令会在/source/_posts下创建新文章，之后需要使用MarkDown语法编写该文章。  </p><p><code>---</code>包括起来的内容称之为<code>Front-matter</code>有很多配置选项可以添加。<br>更多的简单语法可以参考<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程</a><br>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="2-清除旧数据"><a href="#2-清除旧数据" class="headerlink" title="2.清除旧数据"></a>2.清除旧数据</h3><p>文章写好之后，首先清除掉旧的数据</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo clean <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会清除掉之前生成的网页，即站点根目录下的public文件夹</p><h3 id="3-Generate-static-files"><a href="#3-Generate-static-files" class="headerlink" title="3.Generate static files"></a>3.Generate static files</h3><p>然后使用如下命令生成新的页面：More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate 或者简写 hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会将source文件夹下所有的md文件进行渲染，生成HTML页面，存放在public文件夹下</p><h3 id="4-Run-server"><a href="#4-Run-server" class="headerlink" title="4.Run server"></a>4.Run server</h3><p>在本地开启服务器，预览一下文章是否满意</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server  <span class="token string">'hexo s'</span> <span class="token keyword">for</span> short<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="5-Deploy-to-remote-sites"><a href="#5-Deploy-to-remote-sites" class="headerlink" title="5.Deploy to remote sites"></a>5.Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy <span class="token string">'hexo d'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="Blog-Template"><a href="#Blog-Template" class="headerlink" title="Blog Template"></a>Blog Template</h2><p>更高阶更详尽的Hexo Markdown教程参考<a href="https://blog.17lai.site/posts/cf0f47fd/#%E5%B8%B8%E7%94%A8%E6%A0%87%E8%AE%B0">夜法之书的博客</a><br>一些可以用到的LeTax数学公式编辑方式<a href="http://t.csdn.cn/VivVj">超详细 LaTex数学公式</a> || <a href="http://t.csdn.cn/iPVFt">LaTeX数学公式-详细教程</a></p><p>图床就是用csdn了 <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 还有好多文章慢慢搬运过来，不急。<br>但是需要在每个csdn的图片的链接上加上<a href="https://images.weserv.nl/?url=%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5LRU%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%89%8D%E7%BC%80%E3%80%82">https://images.weserv.nl/?url=真正的图片链接LRU，这个前缀。</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span>https://images.weserv.nl/?url<span class="token operator">=</span><span class="token punctuation">)</span><span class="token punctuation">{</span>% raw %<span class="token punctuation">}</span>在这之间写LaTex或者其他造成的符号转义冲突之类的报错<span class="token punctuation">{</span>% endraw %<span class="token punctuation">}</span>toc: <span class="token boolean">true</span>mathjax: <span class="token boolean">true</span>hide: <span class="token boolean">false</span>categories: Papertags:  - A - Hybrid Memory Systemsimg: https://images.weserv.nl/?url<span class="token operator">=</span>summary: 。---<span class="token comment">## 1. 论文信息</span><span class="token operator">&lt;</span>div <span class="token assign-left variable">class</span><span class="token operator">=</span><span class="token string">"note primary"</span><span class="token operator">&gt;</span>- 文章来自IEEE International Symposium on High-Performance Computer Architecture, <span class="token punctuation">(</span>HPCA<span class="token punctuation">)</span>, <span class="token number">2022</span>- 名字<span class="token operator">&lt;</span>/div<span class="token operator">&gt;</span> <span class="token comment">### 所有作者及单位</span> - A, 佛罗里达国际大学<span class="token punctuation">(</span>FIU<span class="token punctuation">)</span><span class="token comment">## 2. Background</span><span class="token comment">## 3. 解决了什么问题</span><span class="token comment">## 4. 其他学者解决这个问题的思路和缺陷</span><span class="token comment">## 5. 围绕该问题作者如何构建解决思路</span><span class="token comment">## 6. 从结果看，作者如何有力证明他解决了问题</span><span class="token comment">## 7. 缺陷和改进思路</span><span class="token comment">## 8. 创新点</span><span class="token comment">## 9. 积累</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个标签结束后必须空一行:</p><div class="note info">这里是 info 标签样式</div> <div class="note info no-icon">这里是不带符号的 info 标签样式</div> <div class="note primary">这里是 primary 标签样式</div> <div class="note primary no-icon">这里是不带符号的 primary 标签样式</div> <div class="note warning">这里是 warning 标签样式</div> <div class="note warning no-icon">这里是不带符号的 warning 标签样式</div> <div class="note danger">这里是 danger 标签样式</div> <div class="note danger no-icon">这里是不带符号的 danger 标签样式</div><p>然后是行内标签，比加粗更能显示重点，Fulid移植的。<br><span class="label primary">Label primary</span></p><p><span class="label default">Label default</span></p><p><span class="label info">Label info</span></p><p><span class="label success">Label success</span></p><p><span class="label warning">Label warning</span></p><p><span class="label danger">Label danger</span></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>this is a test for my first blog</title>
      <link href="/2023/04/30/this-is-a-test-for-my-first-blog/"/>
      <url>/2023/04/30/this-is-a-test-for-my-first-blog/</url>
      
        <content type="html"><![CDATA[<p><strong>詹青云2018年华语辩论世界杯决赛结辩</strong></p><p>大家好，我们今天和对方有三个根本的分歧。一是成功路径不同。我方承认，聚焦没有问题。如果一个年轻人在年轻的时代完全知道一生要什么，一生走下去从不后悔。没问题，挺幸福的。可是现实是，这个决定对于大多数人来说不应该在青年时代做。这个时候你的大脑没有发育完全、你的人生还在不停地变动、你的智识还有限，而这个世界在飞快的变化。很有可能你想要聚焦的东西有一天是你不适应、不喜欢或者被时代淘汰的东西。</p><p>这时候您方跳到了第二点告诉我说没关系，我只要坚定自己的内心我就没有问题了。这就是我们双方第二点分歧：幸福观的不同。您方的幸福观是一种妥协的幸福观，而我放的幸福观是进取的幸福观。您方的意思是不管我人生发挥得怎样，社会如何对待我，不用在乎！我妥协、我看开、我豁达，就可以幸福。对方辩友，那些历史上真正收获了豁达心态的人，杨慎“是非成败转头空”，王维“行到水穷处，坐看云起时”的时候，他们是在什么时候收获这种豁达，是在遍历人生的沧桑，经历了繁华，经过了奋斗，见识了人世中更深刻的道理，他可以领悟到繁华。就算我退一步，俗一点讲，我多读一点书，多看一点世界，对这个世界的理解和思考方式丰富一点，这种做加法的方法您才能收获真的豁达。</p><p>最后我们双方最根本的分歧是对这个时代理解不同。您告诉我说这个世界纷繁复杂，已经把太多选择推到年轻人的面前，所以我选择加就是在随大流。不是。我们仔细想一想，这个时代给我们多的选择不过是您方说的商品、营销课、成功学。可是人生加减法上，那些人生重大关头的选择是什么，这个社会真的在逼我们做加法吗？不是。我到了这个年纪就应该结婚生子，成家立业。在人生重大关头的选择上，这个社会是要求青年人割舍那些不切实际的幻想，割去那些错误的观念，回归一套社会范式，一套人生范式，是要求你做减法的。</p><p>这个时候真正追随自己内心是应该不顾这套范式的束缚，冲破束缚去追寻自己心中所爱，活出一个真正多元的世界。更重要的是，我们今天不是在替一个年轻人的幸福说话，是一代青年人。青年人拓宽人生边界的可能是在拓宽这个社会价值判断的可能。</p><p>既然这个世界号称它是多元而包容的，我们就应该去试，去让他实现这个诺言。</p><p>既然这个社会多元而包容，既然这个世界告诉我们“人不轻狂枉少年”，就没有人应该天然地觉得“轻狂”是一个贬义词。对方辩友一直在劝我们：人生选到自己最幸福的东西才是快乐的。对方辩友，各位，我们都是年轻人。在我们人生的这个阶段，有什么东西是唯一珍贵的？什么叫“欲买桂花同载酒，终不似，少年游”，什么叫“旧游无处不堪寻。无寻处，惟有少年心”。那个唯一带不走的东西，不就是青春本身吗？这一份机会你不珍惜，这一份可能你不珍惜，您跟我谈的是什么？是那一份安顿了的幸福，是那一份成熟了的幸福。可是这不是年轻人的幸福。因为年轻人不是在替你一个人谋幸福，不是你一个人看开了就可以。他是要为这个世界拓宽边界，是让所有的人都有机会把道路越走越宽，是</p><p>趁着年轻，我偏要勉强。</p><p>谢谢大家。</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perspective </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts, Practices, and Future Direction</title>
      <link href="/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/"/>
      <url>/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>虽然这篇的有些引用也是十多年前的数据，但是作为学习一个阶段的总结，和大佬对比一下在知识结构上的完整度，还有什么是不清楚的。还是挺有用的。</p></blockquote><div class="note primary"><ul><li>文章来自Journal of Computer Science and Technology, (JCST), 2021</li><li>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts,Practices, and Future Direction</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hai-Kun Liu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Di Chen, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Hai Jin, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Xiao-Fei Liao, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Binsheng He, 新加坡国立大学</li><li>Kan Hu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Yu Zhang, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li></ul><hr><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>非易失性主存储器（NVMM）最近已成为未来存储系统的一种有前途的技术。通常，NVMM具有许多理想的属性，例如高密度、字节寻址、非易失性、低成本和能效，但代价是高写入延迟、高写入功耗和有限的写入耐用性（写寿命短）。NVMM已经成为动态随机存取存储器（DRAM）的强有力的替代品，并将从根本上改变内存系统的格局。它们在系统架构设计、操作系统内存管理以及混合内存系统的编程模型方面带来了许多研究机会和挑战。在本文中，我们首先回顾了新兴NVMM技术的概况，然后对NVMM技术的最新研究进展进行了综述。我们根据不同的维度（如内存架构、数据持久性、性能改进、节能和磨损均衡）对这些研究进行分类。其次，为了展示构建NVMM系统的最佳实践，我们从架构、系统和应用的维度介绍了我们最近的混合存储系统设计工作。最后，我们对NVMM的未来研究方向提出了展望，并对设计挑战和机遇进行了阐述。</p><p><strong>关键词</strong>：非易失性存储器、持久性存储器、混合存储器系统、存储器层次结构<br>non-volatile memory, persistent memory, hybrid memory systems, memory hierarchy</p><hr><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在大数据时代，内存计算越来越受到数据密集型应用的青睐。内存子系统对现代计算系统的功能和性能具有越来越大的影响。使用DRAM (动态随机存取存储器)的传统大内存系统[1 ,2]在功率和密度方面面临严峻的可扩展性挑战[3]。尽管DRAM规模从2013年的28nm持续到2016年的10+ nm[4，5] ，扩展已经放缓，变得越来越困难。此外,最近的研究[6-10]表明，基于DRAM的主存储器约占物理服务器总能耗的30%-40%。新兴的非易失性主存储器（NVMM）技术，如相变存储器（PCM）、自旋转移扭矩存储器（STT-RAM）和3D XPoint[11]通常提供比DRAM更高的内存密度、更低的每的比特成本和待机功耗。NVMM技术的出现有可能弥合慢速持久存储（即磁盘和SSD）与DRAM之间的差距，并将从根本上改变存储系统的格局。</p><p>表1显示了闪存SSD、DRAM、PCM、STT-RAM、ReRAM和Intel Optane DC持久内存模块（DCPMM）的不同内存特点，包括读/写延迟、写耐久性和待机功耗[7，12，13]。尽管NVMM在密度和能耗方面具有各种优势，但其写入延迟比DRAM高约6倍-30倍， 写入功耗比DRAM高约5倍- 10倍。此外，NVMM的写入耐久性非常有限(约 $10^8$ 倍) ，而DRAM能够承受约 $10^{16}$ 次的写入操作。这些缺点使得很难直接替代DRAM。使用NVMM的一种更实用的方法是混合内存架构，由DRAM和NVMM组成[15,16]。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>为了充分发挥两者在混合内存系统中的优势，在性能提升、节能降耗、磨损均衡、数据持久性等方面存在许多开放的研究问题。为了解决这些问题，已经有许多关于内存层次结构的设计[15-18]、内存管理[19-21]和内存分配方案[22-24]的研究。这些研究成果导致了混合内存架构、操作系统和编程模型的创新。尽管学术界和工业界已经做出了大量工作来将新兴NVMM集成到存储器层次结构中，但仍有许多挑战需要解决。</p><p>另一方面，先前对NVMM技术的研究大多基于模拟/仿真NVMM器件，与真正非易失性（双列直插式内存模块Dual In-line Memory Modules）DIMM相比，NVMM设备承诺的性能可能存在各种偏差。最近宣布推出的Intel Optane DCPMM终于将NVMM DIMM商业化。真正的Intel Optane DCPMM与之前的研究预期承诺的功能相比，表现明显不同[18，20，26，28]。例如，如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3GB/s，而DRAM的读写带宽之间的差距要小得多（1.3x）。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。由于Optane DCPMM DIMM的这些关键特性，以前关于持久性内存系统的研究应该重新审视和优化，以适应真正的NVMM DIMM。</p><p><strong>贡献</strong>。本文首先回顾关于混合内存架构、操作系统级混合内存管理和混合内存编程模型的最新研究现状。表2显示了NVMM技术的最新研究分类。我们根据不同维度对这些研究进行分类，包括内存架构、持久内存(PM)管理、性能改进、节能、磨损均衡、编程模型和应用程序。我们还讨论了它们的相似性和差异，以突出设计挑战和机遇。其次，为了展示构建NVMM系统的最佳实践，我们<strong>从架构、系统和应用的维度展示了我们在混合存储系统设计方面的努力</strong>。最后，我们提出了在实际应用场景中使用NVMM的未来研究方向，并对研究领域的设计挑战和机遇进行了一些说明。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9239e045ee11414cb228f4b752b13f86.png" alt="将相关工作分类"></p><p>尽管有其他关于NVMM的研究，但鉴于NVMM的快速发展，这篇综述从一个独特的角度对NVMM进行了回顾。在[97]中, 作者介绍了PCM技术的最新研究。以解决有限的写入耐久性、潜在的长延迟、高能量写入、功耗等问题以及一些对内存隐私的担忧。在[98]中，作者对PCM设备及其架构和软件进行了全面的调查和回顾。其他一些有趣的调查侧重于在架构上将四种NVM技术(PCM、MRAM、FeRAM和ReRAM )集成到现有存储层次结构[99]中，或将NVM用于存储和主存储器系统的软件优化[100]。我们的调查与那些调查有三个不同之处。首先，先前的研究[97 ,98]从计算机架构的角度关注PCM设计。相比之下，我们的论文主要从存储器层次、系统软件和应用的维度来研究使用混合存储器的系统。其次，我们的论文包含了更多新发表的期刊、会议论文的评论。特别是，我们对新发布的Intel Optane DCPMM设备进行了更多研究。第三，我们介绍了最近关于存储器系统的近期经验，以阐明未来混合存储器系统的挑战和机遇。</p><p>本文的其余部分组织如下。第2节描述了现有的由DRAM和NVMM组成的混合内存架构。第3节介绍了NVMM中数据持久性保证的挑战和当前解决方案。4节介绍了混合存储器系统中性能优化和节能的最新研究。第5节介绍NVMM写耐久性的研究。第6节介绍了研究NVMM技术所做的努力和实践。在第7节中，我们讨论了NVMM的未来研究方向。我们在第8节结束本文。</p><h1 id="2-Hybrid-Memory-Architectures混合内存架构"><a href="#2-Hybrid-Memory-Architectures混合内存架构" class="headerlink" title="2. Hybrid Memory Architectures混合内存架构"></a>2. Hybrid Memory Architectures混合内存架构</h1><p>已经有很多关于混合存储器架构的研究。通常，主要有两种混合存储器架构，即水平和分层[18]，如图1所示。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e565e90afda244f6a8dcfbe3081068c1.png" alt="常见的架构，最近华科的工作也有混合的架构出现"></p><h2 id="2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构"><a href="#2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构" class="headerlink" title="2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构"></a>2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构</h2><p>许多DRAM-NVMM混合存储器系统[14 ,15 ,31]通过OS在平面(单个)存储器地址空间中管理DRAM和NVMM，并将它们两者用作主存储器[31 ,32]。为了提高数据访问性能，这些混合内存系统需要通过将频繁访问的热NVMM页面迁移到DRAM来克服NVMM的缺点，如图1（a）所示。需要开发内存访问监控机制来指导页面迁移。</p><p><strong>内存访问监控</strong>。Zhang和Li[31]使用多队列算法对页面的热度进行分类，并将热页面和冷页面分别放置在DRAM和NVMM中。Park等人[32]也主张用于管理DRAM和NVMM的水平混合存储架构。此外，他们还提出了三种优化策略来降低混合存储系统的能耗。他们以非常细的DRAM行粒度监视内存数据，并定期检查每个DRAM行中的访问计数器。根据计数器将数据写回NVMM，以减少DRAM刷新的能耗。在再次访问数据之前，数据不会从NVMM缓存到DRAM。脏数据尽可能长时间地保存在DRAM中，以减少DRAM和NVMM之间的数据交换开销以及NVMM的昂贵写操作。</p><div class="note info no-icon"> 听起来监控成本比较高 </div><p><strong>页面迁移</strong>。针对不同的优化目标，已经提出了许多页面迁移算法。Soyoon等人[33]认为，在识别热页时，NVMM写入的频率比数据访问的最近性更重要，并提出了一种称为CLOCK with Dirty bits and Write frequency（CLOCK-DWF）的页替换算法。对于每个NVMM写入操作, CLOCK-DWF需要首先将相应的页面提取到DRAM，然后在DRAM中执行写入。这种方法可能会导致许多不必要的页面迁移，从而给NVMM带来更多的能耗和写回操作。Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面，并使用两个最近最少使用的(LRU)队列分别选择DRAM和NVM中的要被驱逐的页面。Yoon[17]等人基于行缓冲区局部性进行了页面迁移，其中行缓冲区命中率低的页面被迁移到DRAM，而行缓冲区点击率高的页面仍保留在NVMM中。Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性。Khouzani[35]等人考虑了程序的内存布局和内存级并行性，以迁移混合内存系统中的页面。</p><p><strong>架构限制</strong>。在水平混合存储器架构中管理NVMM和DRAM有几个挑战。</p><p>首先,<span class="label primary">页面级内存监控成本高昂</span>。一方面，由于当今的商品x86系统不支持页面粒度的内存访问监控，因此硬件支持的页面迁移方案需要大量的硬件修改来监控内存访问统计[14,15,33]。另一方面, OS层的内存访问监控通常会导致显著的性能开销。许多操作系统在页面表条目(PTE)中为每个页面维护一个“已访问”位，以标识该页面是否被访问。然而，该位不能真实地反映页面访问的最近性和频率。因此，一些基于软件的方法将禁用Translation Lookaside Buffer（TLB）[102]来跟踪每个内存引用。这种页面访问监控机制通常会导致显著的性能开销，甚至抵消混合内存系统中页面迁移的好处。</p><p>第二，页面迁移成本也很高。<span class="label primary">一次页迁移可能导致多次页读/写操作（代价高昂）</span>。页面可能只包含一小部分热数据，因此由于内存带宽和DRAM容量的浪费，页面粒度的迁移成本相对较高。</p><p>第三，热页面检测机制可能需要很长时间来预热页面，从而降低页面迁移的收益。此外，<span class="label primary">对于某些不规则的内存访问模式，热页面预测可能不准确，从而导致不必要的页面迁移</span>。</p><h2 id="2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构"><a href="#2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构" class="headerlink" title="2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构"></a>2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构</h2><p>许多研究建议通过分层的缓存-内存架构来组织DRAM和NVMM[16,38,39]。他们使用DRAM作为NVMM的缓存，如图1（b）所示。DRAM缓存对操作系统和应用程序是不可见的，完全由硬件管理。</p><p>Qureshi等人[16]提出了一种由大尺寸PCM和小尺寸DRAM组成的分层混合存储系统。DRAM缓存包含最近访问的数据，以减少最昂贵的NVMM访问，而大容量NVMM存储大部分所需数据，以避免在应用程序执行期间进行昂贵的I/O操作。类似地，Mladenov[38]设计了一个具有小容量DRAM缓存和大容量NVMM的混合存储系统，并基于应用程序数据的空间局部性对其进行管理。DRAM作为按需缓存进行管理，并通过LRU算法进行替换。Loh和Hill[39]以缓存行的粒度管理DRAM，以提高DRAM缓存的效率，并使用组连接方式将NVMM数据映射到DRAM缓存。他们将元数据tag和数据放在同一个存储行中，以便可以快速访问缓存命中的数据，并减少标记查询的性能开销。</p><div class="note info no-icon"> 缓存是得把这个DRAM100%用起来的，水平就不好说了 </div><p>在这种内存结构中，由于DRAM被组织为N路集合关联缓存，因此需要额外的硬件来管理DRAM缓存。例如，需要SRAM存储器来存储DRAM高速缓存中数据块的元数据tag，并且需要硬件查找电路来查找DRAM高速缓冲存储器中所请求的数据。因此，为了访问DRAM缓存中的数据，需要两个内存引用，一个用于访问元数据，另一个用于实际数据。为了加速元数据访问，Qureshi[16]等人使用了高速SRAM来存储元数据。Meza等人[40]通过将元数据放在同一DRAM行中的数据块旁边，降低了标记存储的硬件成本。他们还建议使用片上元数据缓冲区将频繁访问的元数据缓存在小型SRAM中。</p><p><strong>架构限制</strong>。尽管分层混合存储器架构通常比单独访问NVMM中的数据的场景提供更好的性能，但在运行具有较差局部性的工作负载时，它可能会导致性能显著下降[103]。原因是大多数硬件管理的分层DRAM-NVMM系统为了简化而利用基于按需的数据预取策略，因此DRAM缓存位于存储器分层的关键数据路径中。如果数据块未命中DRAM缓存，则无论页面热度如何，都必须将其从NVMM提取到DRAM。这种缓存填充策略可能会导致DRAM和NVMM之间频繁的数据交换（类似于缓存抖动问题）。另一方面，硬件管理的缓存架构不能充分利用DRAM容量。由于DRAM缓存被设置为关联的，因此每个NVMM数据块被映射到一个固定的集合。当集合已满时,它必须在将新的NVMM数据块提取到DRAM之前驱逐数据块，即使其他缓存集合为空。</p><h2 id="2-3-Intel-Optane-DCPMM的体系结构"><a href="#2-3-Intel-Optane-DCPMM的体系结构" class="headerlink" title="2.3 Intel Optane DCPMM的体系结构"></a>2.3 Intel Optane DCPMM的体系结构</h2><p>最近发布的Intel Optane DCPMM与DRAM结合使用时支持水平和分层混合内存结构。OptaneDCPMM DIMM目前有两种操作模式:内存模式和应用程序直接模式[25]。这些模式中的每一种对于特定的用例都有其优点。</p><p><strong>内存模式</strong>。在这种模式下，DCPMM充当<strong>大容量的主存储器</strong>。操作系统将DCPMM识别为传统DRAM，<strong>并禁用DCPMM的持久性功能</strong>。如果将传统DRAM与DCPMM结合使用，它将隐藏在操作系统中，并充当DCPMM的缓存层。因此，DCPMM和DRAM实际上被组织在分层混合存储器架构中。内存模式的主要优点是提供在内存总线通道上提供优越内存容量。这种模式强烈强调在不修改上层系统的情况下围绕内存空间构建大容量存储环境以及应用程序。推荐的用例是扩展主内存容量，以实现更好的基础设施扩展，例如用于大数据应用程序的并行计算平台（MapReduce、图形计算）。</p><div class="note info no-icon"> 这种用例应该是指DRAM很少很少，PM特别多的，数据仓库那种级别的缓存吧？通常一般服务器的比例拿去做缓存真的很浪费容量咦 </div><p><strong>AD模式</strong>。在这种模式下，DCPMM为操作系统和应用程序提供了所有持久性特性。操作系统将DRAM和DCPMM分别作为主存储器和持久存储向应用程序公开。与DCPMM混合的传统DRAM仍然充当应用程序的标准DRAM，而DCPMM也被分配到存储器总线以实现更快的存储器访问。DCPMM用作两种namespace之一:直接访问存取（DAX）和块存储。前者的namespace是字节可寻址的持久存储，应用程序通过特殊的apis直接访问。因此，DCPMM和DRAM在这种模式下被逻辑地组织在一个水平混合内存架构中。后一个命名空间将DCPMM 作为区块存储设备提供给应用程序，类似于SSD，但是可以通过更快的内存总线访问。应用程序直接模式强调减少延迟和提高带宽的优势，比NVMe快2.7倍。推荐的用例适用于大型内存数据库，这些数据库需要满足数据持久性的要求。</p><p>还有一种结合了内存模式和应用程序直接模式的混合内存模式。DCPMM的一部分容量用于内存模式操作,DCPMM剩余的容量用于应用程序直接模式操作。这种混合内存模式为管理不同应用场景的混合内存系统提供了一种更灵活的方法。</p><div class="note info no-icon"> 这里没有说通过热插拔的dvdax模式，变成易失内存 </div><h2 id="2-4-Summary总结"><a href="#2-4-Summary总结" class="headerlink" title="2.4  Summary总结"></a>2.4  Summary总结</h2><p>上述两种混合存储器架构对于不同的场景有各自的优缺点。<span class="label primary">通常，分层架构更适合具有良好数据局部性的应用程序，而平面可寻址架构更适用于延迟不敏感或占空间较大的应用程序。</span>关于哪种架构比另一种架构更好，目前尚无定论。实际上，Intel Optane DCPMM支持分层和平面可寻址混合内存架构。当前DCPMM的一个限制是，在重新配置DCPMM模式后，系统需要重新启动。如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。</p><h1 id="3-Persistent-Memory-Management-持久内存管理"><a href="#3-Persistent-Memory-Management-持久内存管理" class="headerlink" title="3 Persistent Memory Management 持久内存管理"></a>3 Persistent Memory Management 持久内存管理</h1><p>数据持久性是NVMM的一个重要设计考虑因素。下面，我们首先介绍持久内存（PM）管理所面临的技术挑战，然后介绍有关持久内存管理的最新研究，包括持久内存的使用、持久内存访问模式、容错机制和持久对象。</p><h1 id="3-1-Technical-Challenges-技术挑战"><a href="#3-1-Technical-Challenges-技术挑战" class="headerlink" title="3.1 Technical Challenges 技术挑战"></a>3.1 Technical Challenges 技术挑战</h1><p>在混合内存系统中，NVMM可以在运行应用程序时充当主内存，并在应用程序完成时充当持久存储。NVMM的字节寻址能力和非易失性特性消除了内存和外部存储的区别。然而，当NVMM中的数据需要持久化时，就需要对NVMM中的数据进行重新组织和重新定位。</p><p>图2显示了持久内存（PM）的管理操作。NVMM区域是物理PM设备。NVMM区域可以像DRAM一样用作工作内存，也可以像磁盘一样用作持久存储。当程序完成时，工作内存中的数据应该刷新到持久存储中。此外，为了保证高可靠性，检查点机制被广泛利用来从电源故障或系统崩溃中恢复系统。Gao等人[104]开发了一种利用NVMM在混合内存系统中进行实时检查点的新颖方法。</p><div class="note info no-icon">Checkpointing（检查点）用于在运行过程中定期保存系统的状态和数据快照。这些检查点允许系统在发生故障或需要恢复时，从之前的状态继续执行，而不是从头开始重新计算。</div><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ba021f22f99b414582492752b9d4377a.png" alt="没看懂这个图"></p><p>有效管理PM面临多项挑战。首先，持久存储以文件系统的形式被广泛管理。作为字节可寻址NVMM提供比传统块设备更好的随机访问性能，基于PM的文件系统的性能瓶颈已经从硬件转移到系统软件堆栈。缩短软件堆栈中的数据路径至关重要。其次，由于许多CPU使用回写式缓存来实现写操作的高性能，因此末级缓存（LLC）可能会改变写回PM的数据顺序。如果出现断电或者系统崩溃的情况，可能会导致数据不一致的问题。因此，为了保证PM中数据的一致性，需要写操作的顺序和写原子性模型来保证PM中数据的一致性。第三，与基于PM的文件系统相比，持久对象和数据结构对于PM编程更有前景，因为它们消除了文件系统中的复杂数据结构，包括i节点、元数据和数据。然而，这些持久对象和数据结构仍然面临着保证数据一致性的挑战。接下来，我们将回顾试图解决这些挑战的研究<br>（这后面就跳过了，大部分在讨论文件系统，不是我目前考虑的，下次一定）</p><h1 id="4-Performance-Improvement-and-Energy-Saving性能改进和节能"><a href="#4-Performance-Improvement-and-Energy-Saving性能改进和节能" class="headerlink" title="4 Performance Improvement and Energy Saving性能改进和节能"></a>4 Performance Improvement and Energy Saving性能改进和节能</h1><p>由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。这些研究可分为三类:减少NVMM写入次数、减少NVM写入本身的能耗以及通过页面迁移减少DRAM的能耗。</p><h2 id="4-1-NVMM-Write-Reduction-NVMM写入减少"><a href="#4-1-NVMM-Write-Reduction-NVMM写入减少" class="headerlink" title="4.1 NVMM Write Reduction NVMM写入减少"></a>4.1 NVMM Write Reduction NVMM写入减少</h2><p>为了减少NVMM写入,分层结构显然更合适,因为DRAM缓存减少了大量NVMM写入。为此,开发了两种主要技术,即页面迁移和绕过NVMM写入。</p><p><strong>页面迁移</strong>。页面迁移[14 ,15 ,33 ,58 ,59]策略主要基于写入次数和每个页面的最近访问频率来选择要迁移的页面。它们的主要区别在于触发页面迁移的条件。</p><p>PDRAM[15]根据写入次数将PCM页迁移到DRAM。在PDRAM中,存储器控制器维护一个表以记录每个PCM页的访问计数。如果写入PCM页的次数超过给定阈值,则触发页故障，然后将该页从PCM页迁移到DRAM。</p><p>CLOCK-DWF[33]将页面的写入历史集成到CLOCK算法中。当发生页面错误时，虚拟页面将从磁盘提取到PCM。否则, 该页将在DRAM中分配,因为该页可能是写密集型页。</p><p>RaPP[14]根据页面的等级在DRAM和PCM之间迁移页面。在RaPP中, 页面按访问频率和最近度排序。排名靠前的页面从PCM迁移到DRAM。因此，频繁写入的页面被放置在DRAM中,而很少写入的页面则被放置在PCM中。此外, RaPP还将任务关键页面放置在DRAM中，以提高应用程序性能。通过监视LLC中每个页面的写回操作的数量,存储器控制器能够跟踪每个页面的访问频率和最近性。RaPP根据多队列（MQ）算法对页面进行排序[118]。传统MQ定义了多个最近最少使用（LRU）队列。每个LRU队列是一个页面描述符队列, 其中包括参考计数器和逻辑过期时间。当第1次访问页面时,页面将移动到队列0的尾部。如果页面的引用计数达到 $2^{i+1}$ ,则提示页面排队i+1。一旦PCM页面被移动到队列5,它就被迁移到DRAM。</p><p><strong>缓冲NVMM写入</strong>。在混合内存系统中, 缓存能够减少对NVMM的大量写入。适当的缓存替换策略不仅可以提高应用程序性能, 还可以降低NVMM的能耗。先前的研究[7,18]发现, 缓存中的许多块在被从缓存中逐出之前不会被再次使用。这些块称为死块，并消耗宝贵的缓存容量。DASCA[7]提出了一种死块预测方法,以减少STT-RAM缓存的能耗。驱逐这些死块将减少对STT-RAM缓存的写入,并且不会影响缓存命中率。WADE[62]进一步利用了NVMM读取和NVMM写入之间的能耗不对称性。由于NVMM写入操作比NVMM读取操作消耗更多的能量,因此频繁写入的块应保留在缓存中。WADE将缓存中的块分为两类:频繁回写的块和非频繁回写块。非频繁写回的块被替换,以提供更多机会将其他数据块保留在缓存中。</p><h2 id="4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低"><a href="#4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低" class="headerlink" title="4.2 NVMM Energy Consumption Reduction NVMM能耗降低"></a>4.2 NVMM Energy Consumption Reduction NVMM能耗降低</h2><p>由于NVMM写入显示的能耗是NVMM读取的能耗的几倍，因此在降低NVMM写入的能耗方面已经做出了许多努力。这些方法可以分为两类:差分写入（仅写入脏位而不是整行）和在单个写入期间并行多个写入。</p><p>如果要写入的位数超过缓存行中总位数的一半，则Flip-N-Write[64]尝试通过翻转位来减少PCM写入能耗。在一次写入期间, 如果行中超过一半的位被写入 ,则每个位被翻转,因此位翻转不超过总位的50%。同时,设置标记位以识别行中的位是否被翻转。当读取行时,标记位用于确定行中的位是否应该翻转。与Flip-N-Write类似, Andrew等人[73]提倡细粒度写入。它只监视脏位而不是一行中的所有位。一个叫做PCM的新术语引入功率令牌以指示单次写入期间的电源。假设为每个芯片分配Plimit Watts功率, 并且每个位写入需要Pbit Watts , Plimitpbit可以同时写入位。在芯片内,可以同时写入bank。在单个写入期间,如果多个写入请求位于不同的存储库中,并且总功耗不超过Plimit ,则可以同时执行这些写入。因此，细粒度写入不仅减少了NVMM写入，而且通过实现更高的存储体并行性来提高系统性能。</p><p>一些研究[65 ,66]通过分离SET和RESET操作来提高NVMM的能量效率。由于NVMM写入1比写入0消耗更多的能量和时间，如果以正确的方式执行这些写入,则可以减少写入延迟和能耗。三阶段写入[65]将写入操作分为比较阶段、写入零阶段和写入一阶段。在比较阶段,利用Flip-N-Write机制来减少写入次数。零位和一位分别在写零级和写一级中被分别写入。因为在大多数工作负载中,零写操作占了大部分写操作，所以Tetris write[66]进一步考虑了SET和RESET操作的不对称性, 并行调度代价高昂的写操作。在功率约束下, 写零操作被插入到写操作的剩余间隔中。</p><p>CompEx[67]提出了一种压缩扩展编码机制,以减少MLC/TLC NVMM的能耗。为了提高MLC/TLC单元的寿命,首先压缩数据以减少数据冗余。然后将扩展码应用于压缩数据并写入物理NVMM单元。对于具有8个状态的TLC单元，状态0、1、6和7称为终端能量状态，而状态2、3、4.和5称为中心能量状态。中心能量状态消耗更多的时间和能量，因为它们需要更多的编程和验证迭代。CompEx 利用扩展代码仅使用NVMM单元的终端能量状态。由于在编程MLC/TCL单元时，终端能量状态需要比中心能量状态更少的能量和时间，所以这一想法起效。混合片上缓存也被提出以减少CPU的功耗。RHC[68]构建了一个混合缓存，其中SRAM和NVMM中的每种方式都可以独立地打开或关闭。如果一行很长时间没有被访问，该行将被关闭，而其标签仍处于打开状态，以跟踪该行的访问。当对标签的访问超过阈值时，该行将通电。为了更好地利用高性能SRAM和低动态功耗NVMM，RHC对SRAM和NVMM采用不同的阈值。</p><h2 id="4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低"><a href="#4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低" class="headerlink" title="4.3 DRAM Energy Consumption Reduction DRAM能耗降低"></a>4.3 DRAM Energy Consumption Reduction DRAM能耗降低</h2><p>在只有DRAM的存储系统中, 静态能耗可以占存储系统总能耗的一半以上[69-71]。在混合存储器系统中,页面迁移技术被广泛用于减轻DRAM的能耗。非活动页面可以从DRAM迁移到NVMM，以便空闲的DRAM组可以断电。当页面稍后变为活动时, 它将再次迁移到DRAM。然而, 如果页面迁移没有正确执行,DRAM列组可能会频繁断电并重新激活。额外的能耗可能会抵消页面迁移带来的好处。</p><p>为了减少混合存储器系统的能耗，RAMZzz[8]揭示了高能耗的两个主要根源。一个是活动页面的稀疏分布,另一个是页面迁移可能不有效,因为DRAM的多能量状态之间的传输会引入额外的能量消耗。为了解决前一个问题, RAMZzz使用多个队列将具有类似活动的页面收集到同一个DRAM列中，从而避免频繁的能量状态转移。多个队列具有L个LRU队列来记录页面描述符。页面描述符包含一段时间内页面的ID和访问（读和写）计数。为了减少数据迁移的能量开销，将具有类似内存访问行为的页面重新组合在一起。这样，需要将页面分配给新的bank。RAMZzz在banks间并行迁移这些页面。</p><p>Refree[72]通过避免DRAM刷新，进一步降低了混合存储器系统中的DRAM能耗。当DRAM行需要刷新时,这意味着该行很长时间没有被访问。行中的数据已过时,不久以后不太可能再次访问。Refree将这些行逐出PCM，而不是在DRAM中刷新它们。在Refree中,所有行都会定期监视。此周期的间隔等于DRAM行自上次刷新以来的保留时间的一半。因此，行分为活动行和非活动行。激活行在访问。非活动行被逐出PCM，从而消除DRAM刷新。</p><h1 id="5-Write-Endurance-Improvement-写入耐久性改进"><a href="#5-Write-Endurance-Improvement-写入耐久性改进" class="headerlink" title="5. Write Endurance Improvement 写入耐久性改进"></a>5. Write Endurance Improvement 写入耐久性改进</h1><p>在混合存储器系统中,主要有两种策略来克服NVMM的有限写入耐久性。一个是减少NVMM写入，另一个是磨损均衡,它在所有NVMM单元之间均匀分布写入流量。</p><h2 id="5-1-Write-Reduction写入减少"><a href="#5-1-Write-Reduction写入减少" class="headerlink" title="5.1 Write Reduction写入减少"></a>5.1 Write Reduction写入减少</h2><p>已经提出了许多用于改善NVM寿命的写减少策略，包括数据迁移[8、14、15]、 缓存或缓冲[16]和内部NVM写减少[64、73、74]。</p><p>提出了一种延迟写入机制[16],以减少对PCM的写入。在分层混合存储器系统中,DRAM缓冲器用于隐藏高延迟PCM访问。当发生页面错误时, 数据将直接从磁盘提取到DRAM缓存中。在从DRAM高速缓存中逐出页面之前,页面不会写入PCM。行级写入还可以减轻NVMM上的写入操作, 从而减少NVMM的磨损[16]。对于内存密集型工作负载,写操作可能集中在几行中。通过跟踪DRAM中的缓存行，只有脏行被写回PCM,而不是页面的所有行。提出了内存压缩机制[67 ,75] ,以提高MLC/TLC NVMM的寿命。在写入NVMM单元之前, 首先压缩数据。因此,只有一小部分NVMM单元被写入。然而, 耐久性的提高是以性能适度下降为代价的。如果NVMM单元以较低的功耗写入, 则该单元可以以较高的写入延迟为代价维持更多的写入。具体地, 当写入NVMM单元的速度下降N倍时,单元的耐久性可以提高N到N3倍。Mellow Write[76]探索了这一功能，以提高NVMM的寿命。为了减轻性能下降, Mellow Write只采用只有一次写入操作的缓慢的存储体写入。</p><h2 id="5-2-Wear-Leveling磨损均衡"><a href="#5-2-Wear-Leveling磨损均衡" class="headerlink" title="5.2 Wear-Leveling磨损均衡"></a>5.2 Wear-Leveling磨损均衡</h2><p>与减少写入的方法不同, 磨损均衡在所有NVMM页面之间均匀分布写入。尽管写入总数没有减少,磨损均衡技术可以防止某些页面被高强度写入而快速磨损。</p><p>对于NVMM ,我们可以记录每行的写入计数,以指导磨损均衡策略。但是,不能忽略外部存储开销。Start Gap[77]提出了一种细粒度磨损均衡方案。PCM页的行以旋转方式存储。在0和15之间随机生成旋转值,以指示移位的位置。对于具有16行的PCM页面,旋转值的范围可以从0到15。当旋转值为0时,页面存储在其原始地址中。如果旋转值为1 ,则第0行存储在第1行的物理地址中,并且每一行的地址都被旋转值移位。</p><p>在PDRAM[15]中,磨损均衡由写入计数阈值触发。当页面的写入计数超过给定阈值时,将触发页面交换中断以将页面迁移到DRAM。交换的PCM页面被添加到列表中，这些页面将再次重新定位。</p><p>Zombie[78]为实现weal-leveling(这是不是多写了一个l)提供了另一个方向,并进一步延长了PCM的整体寿命。与在PCM单元之间均匀分配写入的Start-Gap之外, Zombie利用禁用页面中的空闲块为工作内存提供更多的纠错资源。当PCM单元磨损时,它变得不可用。由于从软件的角度来看,内存占用的空间是按照页来组织的,因此包含故障单元的整个页面将被禁用。但是，如果提供了一些备用单元格来替换出现故障的单元,则可以再次使用该页面。这些备用单元称为纠错资源。当所有备用单元耗尽时, 最终放弃包含失败单元的页面。通常，当页面被禁用时,大约有99%的位可用。Zombie利用禁用页面中的大量好比特作为备用纠错资源,其中好比特被组织在细粒度块中。通过将工作页面与纠错资源配对, Zombie可以延长NVMM的使用寿命。</p><p>DRM[79]在虚拟地址空间和物理NVMM地址空间之间添加了中间映射层。在中间地址空间中，一个页面可能映射到PCM中的一个好页面或两个有故障的兼容PCM页面。兼容页面意味着一对具有错误字节的页面,但这些错误字节都不位于两个页面的同一位置。因此，两个兼容的页面可以被组合成一个新的好页面通过这种方式, DRM将PCM寿命显著提高了40倍。</p><h1 id="6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践"><a href="#6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践" class="headerlink" title="6 Practices of Hybrid Memory System Designs混合存储系统设计实践"></a>6 Practices of Hybrid Memory System Designs混合存储系统设计实践</h1><p>在本节中，我们从内存架构、OS支持的混合内存管理和NVMM支持的应用程序的角度介绍了我们最近在NVMM系统设计和优化方面的努力和实践，如图5所示。在下文中，我们将简要介绍我们的实践。</p><h2 id="6-1-Memory-Architectural-Designs内存架构设计"><a href="#6-1-Memory-Architectural-Designs内存架构设计" class="headerlink" title="6.1 Memory Architectural Designs内存架构设计"></a>6.1 Memory Architectural Designs内存架构设计</h2><p>在本小节中，我们将介绍我们对混合内存模拟和仿真、硬件/软件协同混合内存架构、细粒度NVM压缩和磨损均衡以及混合内存感知片上缓存管理的研究。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/37d5195a01334fcb9a70359bfca5ba53.png"></p><h3 id="6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真"><a href="#6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真" class="headerlink" title="6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真"></a>6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真</h3><p>混合存储器体系结构仿真是研究混合存储器系统的先决条件。我们将zsim[27]与NVMain[20]集成, 以构建全系统架构模拟器。Zsim是用于x86-64多核架构的快速处理器模拟器。它能够对多核、片上缓存层次结构、缓存-致性协议(如MESI)、片上互连拓扑网络和物理内存接口进行建模。<strong>Zsim</strong>使用Intel Pin工具包收集进程的内存跟踪,然后回放内存跟踪以表征内存访问行为。<strong>NVMain</strong>是用于NVMM的架构级主存储器模拟器。它能够模拟不同的内存配置文件, 如读/写延迟、带宽、功耗等。它还支持子阵列级内存并行性和不同的内存地址编码方案。此外，NVMain还可以对混合存储器（如DRAM和存储器层次结构中的不同NVMM）进行建模。由于操作系统级内存管理不是由zsim模拟的,因此我们通过添加Translation Lookaside Buffer（TLB）和内存管理模块（如伙伴内存分配器和页表）来扩展zsim , 以支持全系统模拟。实施细节参考我们的开源软件。我们的工作为研究界提供了一个快速、完整的体系结构仿真框架。它可以帮助研究人员了解不同的NVMM特性,设计混合存储系统,并以简单高效的方式评估各种系统设计对应用程序性能的影响。</p><h3 id="6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器"><a href="#6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器" class="headerlink" title="6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器"></a>6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器</h3><p>当前基于仿真的NVMM技术研究方法太慢,或者无法运行复杂的工作负载,例如并行和分布式应用程序。我们提出HME[28]，一种轻量级NVMM使用非统一内存访问（NUMA）架构的性能仿真器。HME利用商品Intel CPU中可用的硬件性能计数器来模拟较慢NVMM的性能特性。为了模拟NVMM的访问延迟,HME定期向远程NUMA节点上的DRAM访问注入软件生成的延迟。为了模拟NVMM带宽,HME利用DRAM热控制接口在短时间内限制对DRAM通道的内存请求量。不同于另一个NVMM仿真器Quartz[29] ,它不模拟NVMM的写入延迟,HME识别写直通和写回缓存逐出操作, 以分别模拟它们的延迟。通过这种方式,与Quartz相比, HME能够显著减少NVMM访问延迟的平均仿真误差[29]。在真正的NVMM设备Intel Optane DCPMM问世之前, 这项工作可以帮助研究人员和程序员评估NVMM性能特性对应用程序的影响, 并指导混合内存系统的系统设计和优化。</p><h3 id="6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存"><a href="#6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存" class="headerlink" title="6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存"></a>6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存</h3><p>基于我们的混合存储器模拟器， 我们提出了一种称为HSCC[18]的硬件/软件协同混合存储器架构。在HSCC中,DRAM和NVMM在物理上组织在单个存储器地址空间中，并且都用作主存储器。然而,DRAM在逻辑上可以用作NVMM的缓存,也可以由OS管理。图6显示了HSCC的系统架构。我们扩展了页表和TLB，以维护NVMM到DRAM的物理地址映射, 从而以缓存/内存层次结构的形式管理DRAMNVMM。通过这种方式, HSCC能够像虚拟到NVMM地址转换一样高效地执行NVMM到DRAM地址转换。此外, 我们在每个TLB条目和页表条目中.添加一个访问计数器，以监视内存引用。与以前在内存控制器或操作系统中监视内存访问的方法不同,我们的设计可以精确地跟踪所有数据访问, 而无需额外的存储(SRAM)和性能开销。我们通过动态阈值调整策略识别频繁访问的(热)页面，以适应不同的应用程序,然后将NVMM中的热页面迁移到DRAM缓存，以获得更高的性能和能效。此外,我们开发了一种基于实用程序的DRAM缓存填充方案,以平衡DRAM缓存的效率和DRAM利用率。由于软件管理的DRAM顶面能够映射到任何NVMM页面,因此DRAM实际.上用作完全关联的缓存。这种方法可以显著提高DRAM缓存的利用率,并且还提供了根据应用程序的动态内存访问行为重新配置混合内存架构的机会。由于CPU可以绕过DRAM缓存直接访问NVMM中的冷数据，因此DRAM既可以用作平面可寻址混合存储器架构中的主存储器,也可以用作分层混合存储器架构。因此,与最先进的工作相比,HSCC可以将系统性能显著提高9.6倍，能耗降低34.3%[16]。我们的工作为实现可重构混合存储器系统提供了第一个架构解决方案,该系统可以在水平和分层存储器架构之间动态改变DRAMNVMM管理。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/c8216069dd1e4b42b6526c54ad69b35c.png"></p><p>我们进一步在HSCC上提出了以下技术,以提高缓存性能并改进磨损均衡机制。</p><p>由于NVMM块的缓存未命中惩罚是DRAM块的几倍,因此在平面可寻址混合存储器体系结构中，缓存命中率不是唯一需要改进的性能指标。为了最好地利用昂贵的LLC，我们提出了一种新的度量，即平均存储器访问时间（AMAT）,以评估混合存储器系统的总体性能。我们考虑了DRAM块和NVMM块的非对称缓存未命中惩罚，并提出了一种LLC未命中惩罚感知替换算法,称为MALRU[36,37] ,以改进混合存储器系统中的AMAT。MALRU动态地将LLC划分为保留区域和正常替换区域。MALRU优先替换LLC中的死DRAM块和冷DRAM块，使得NVMM块和热DRAM块保持在保留区域中。通过这种方式, 与LRU算法相比,MALRU实现了高达228%的应用程序性能改进。这项工作展示了混合存储器系统如何影响片上缓存的架构设计。</p><p>为了提高NVMM的写入耐久性，我们提出了一种新的NVMM架构,以支持空间无关数据压缩和磨损均衡[119]。由于许多应用程序的内存块通常包含大量零字节和频繁值, 我们提出了零重复数据消除和频繁值压缩机制（称为ZD-FVC[119]）, 以减少NVMM上的位写入。ZD-FVC可以集成到NVMM模块中，并完全由硬件实现, 无需任何操作系统的干预。我们在Gem5和NVMain模拟器中实现了ZD-FVC[119]，并使用SPEC CPU2006中的几个程序对其进行了评估。实验结果表明,ZD-FVC比几种最先进的方法要好得多。特别是, 与频繁值压缩相比, DZ-FVC可以将数据压缩比提高1.5倍。与数据比较写入相比，ZD-FVC能够将NVMM上的位写入减少30%，并将NVMM的寿命平均提高5.8倍。相应地, ZD-FVC还平均减少了43%的NVMM写入延迟和21%的能耗。我们的设计以简单高效的方式为NVMM提供了细粒度数据压缩和磨损均衡解决方案。它是其他磨损均衡方案的补充，以进一步提高NVMM寿命。</p><h1 id="6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件"><a href="#6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件" class="headerlink" title="6.2 System Software for Hybrid Memories混合存储器的系统软件"></a>6.2 System Software for Hybrid Memories混合存储器的系统软件</h1><p>在本小节中,我们介绍了软件层混合内存系统的实践，包括对象级混合内存分配和迁移、NUMA感知页面迁移、超级页面支持和NVMM虚拟化机制。</p><h3 id="6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移"><a href="#6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移" class="headerlink" title="6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移"></a>6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移</h3><p>页面迁移技术已被广泛用于改善混合存储器系统中的系统性能和能量效率。然而,以前的页面迁移方案都依赖于OS层中昂贵的在线页面访问监控方案来跟踪页面访问的最近性或频率。此外,由于额外的内存带宽消耗和缓存/TLB一致性保证机制,页面粒度上的数据迁移通常会导致非平凡的性能开销。</p><p>为了减轻混合内存系统中数据迁移的性能开销，我们提出了更轻量级的面向对象内存分配和迁移机制,称为OAM[120]。OAM的框架如图7所示。与之前的研究[44 ,121]不同, 我们进一步分析了细粒度时隙中的对象访问模式, 这些研究仅在静态对象放置的全局视图中描述了内存访问行为。OAM利用编译框架LLVM以对象粒度描述应用程序内存访问模式，然后将应用程序的执行分为不同阶段。OAM利用性能能量集成模型来指导不同执行阶段的初始内存分配和运行时对象迁移, 而无需对硬件和操作系统进行侵入性修改以进行在线页面访问监控。我们通过扩展Glibc库和Linux内核开发了新的内存分配和迁移API。基于这些API ,程序员能够将DRAM或NVMM显式分配给不同的对象, 然后迁移访问模式在DRAM和NVMM。我们开发了一个静态代码插入工具,可以自动修改遗留应用程序的源代码,而无需程序员重新设计应用程序。与最先进的页面迁移方法CLOCK-DWF[33]和2PP[44]相比, 实验结果表明,OAM可以分别显著降低83%和69%的数据迁移成本,并实现约22%和10%的应用程序性能改进。以前的持久内存管理方案通常依赖内存访问评测来指导静态数据放置,以及页面迁移（代价高昂）技术来适应运行时的动态内存访问模式。OAM提供了一种更轻量级的混合内存管理方案,支持细粒度对象级内存分配和迁移。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/257cf52418304fce863269f59094a936.png"></p><h3 id="6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理"><a href="#6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA感知混合内存管理" class="headerlink" title="6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理"></a>6.2.2 NUMA-Aware Hybrid Memory Management NUMA感知混合内存管理</h3><p>在非统一内存访问（NUMA）架构中，不同NUMA节点中应用程序观察到的内存访问延迟通常是不对称的。由于NVMM比DRAM慢几倍，混合存储器系统可以进一步扩大不同NUMA节点之间的性能差距。NUMA系统的传统内存管理机制在混合内存系统中不再有效,甚至可能降低应用程序性能。例如,自动NUMA平衡（ANB）策略总是将远程NUMA节点中的应用程序数据迁移到运行应用程序线程或进程的NUMA节点。然而，由于远程DRAM的访问性能可能甚至高于本地NVMM,ANB可能会错误地将应用数据移动到较慢的位置。为了解决这个问题，我们提出了HiNUMA[60]，这是一种用于混合内存管理的新NUMA抽象。当应用程序数据首次放置在混合存储器系统中时，HiNUMA将应用程序数据放置在NVMM和DRAM上，以分别平衡带宽敏感应用程序和延迟敏感应用程序的内存带宽利用率。总访问延迟。初始数据放置基于NUMA拓扑和混合内存访问性能。对于运行时混合内存管理,我们提出了一个新的NUMA平衡策略,名为HANB[60]，用于页面迁移。HANB能够通过考虑数据访问频率和内存带宽利用率来降低混合内存访问的总成本。我们在Linux内核中实现HiNUMA ,无需对硬件和应用程序进行任何修改。与NUMA架构中的传统内存管理策略和其他最先进的工作相比, HiNUMA可以通过有效利用混合内存来显著提高应用程序性能。从HiNUMA[60]中学到的经验教训也适用于配备真正IntelOptaneDCPMM设备的混合内存系统。</p><h3 id="6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储"><a href="#6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级页存储" class="headerlink" title="6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储"></a>6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级页存储</h3><p>随着应用程序占地面积和相应内存容量的快速增长,虚拟到物理地址转换已成为混合内存系统的新的性能瓶颈。在大内存系统中, 超页已被广泛用于减轻地址转换开销。然而,使用超级页面的副作用是，它们通常会阻碍轻量级内存管理,例如页面迁移,而页面迁移在混合内存系统中被广泛用于提高系统性能和能效。不幸的是,同时拥有超级页面和轻量级页面迁移是一个挑战。</p><p>为了解决这个问题,我们提出了一种新的混合内存管理系统Rainbow[41]以弥合超级页面和轻量级页面迁移之间的根本冲突。如图8所示，Rainbow以超页（2MB）的粒度管理NVMM,并将DRAM作为缓存管理以基本页的粒度（4KB）将热数据块存储在超级页中。为了加快地址转换, Rainbow使用了拆分TLB的现有硬件功能来支持超级页面和普通页面。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/13d8d4f2c76f4dc28be42a4424d5faee.png"></p><p>我们提出了一种两阶段页面访问监控机制来识别超级页面中的热基页面。在第一阶段，Rainbow记录所有超级页面的访问计数以识别前N个热超级页面。在第二阶段,我们逻辑上将这些热超页分割成基本页（4KB） , 并进一步监视它们以识别热基本页。这些方案显著减少了页访问计数器的SRAM存储开销和由于对热基页进行排序而导致的运行时性能开销。通过新的NVMM到DRAM地址重新映射机制, Rainbow能够将热基页迁移到DRAM,同时仍能保证超级页TLB的完整性。拆分的超页TLB和基本页TLB是并行查阅的。我们的地址重映射机制在逻辑上使用超页TLB作为基本页TLB的缓存。由于超级页TLB的命中率通常很高,Rainbow能够显著加快基本页地址转换。为了进一步提高TLB命中率, 我们还扩展Rainbow以支持多个页面大小,并一起迁移相邻的热基页面[42]。与不支持超级页面的最先进混合内存系统[18]相比,Rainbow通过同时使用超级页面和轻量级页面迁移的优势，可以将应用程序性能显著提高最多2.9倍。</p><p>这项工作提供了硬件/软件协同设计，以弥合超级页面和轻量级页面迁移技术之间的根本冲突。这可能是减轻大容量混合存储器系统中不断增加的虚拟到物理地址转换开销的一个有前途的解决方案。</p><h3 id="6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理"><a href="#6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理" class="headerlink" title="6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理"></a>6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理</h3><p>NVMM有望在云和数据中心环境中更受欢迎。然而,关于将NVMM用于虚拟机（VM）的研究很少。我们提出了HMvisor[61],一种管理程序/虚拟机协同混合内存管理系统, 以有效利用DRAM和NVMM。如图9所示, HMvisor利用伪NUMA机制来支持VM中的混合内存分配。由于VM中的虚拟NUMA节点可以映射到不同的物理NUMA节点, HMvisor可以将不同的内存区域映射到单个VM ,从而向VM暴露内存异质性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1402aec0f84e49268d98793dc3a37f4c.png"></p><p>为了支持VM中的轻量级页面迁移,HMvisor监控页面访问计数并进行虚拟机管理程序中的热页和冷页分类，然后VM通过域间通信机制周期性地收集热页面的信息。我们在VM中实现了一个可加载的驱动程序,以在DRAM和NVMM之间执行进程级页面迁移。由于HMvisor由VM本身执行页面迁移,因此HMvisor无需暂停VM进行页面迁移。HMvisor还提倡混合内存资源交易策略,以动态调整VM中NVMM和DRAM的大小。通过这种方式,HMvisor可以满足多样化应用程序的不同内存需求（容量或性能） ,同时保持VM的总货币成本不变。</p><p>HMvisor的原型在QEMU/KVM平台上实现。我们的评估表明,HMvisor能够以仅5%的性能开销为代价将NVMM写入流量减少50%。此外, 动态内存调整策略可以在VM承受高内存压力时显著减少VM中的主要页面错误,因此甚至可以将应用程序性能提高30倍。</p><p>这是一项在虚拟化环境中管理混合内存的早期系统工作。所提出的方案完全由软件实现, 因此也适用于新Intel Optane DCPMM设备的混合存储系统。</p><h2 id="6-3-NVMM-Supported-Applications-NVMM支持的应用程序"><a href="#6-3-NVMM-Supported-Applications-NVMM支持的应用程序" class="headerlink" title="6.3 NVMM-Supported Applications NVMM支持的应用程序"></a>6.3 NVMM-Supported Applications NVMM支持的应用程序</h2><p>由于混合存储器系统可以提供非常大容量的主存储器,因此它们已被广泛用于大数据应用,例如内存中的关键值KV存储和图形计算。在本小节中,我们介绍了NVMM支持的针对这些应用程序的系统优化实践。</p><p>具有大容量内存的内存KV存储可以在主内存中缓存更多的热数据,从而为应用程序提供更高的性能。然而,在混合内存系统中直接部署传统的KV存储（如memcached）存在若干挑战。例如，如何有效地识别热KV对象?如何重新设计NVMM友好的KV索引以减少NVMM写入?如何重新设计缓存替换算法以平衡混合内存系统中的对象访问频率和最近性?如何解决slab calcification问题[122] ,以在混合存储器系统中最佳地利用DRAM资源?</p><p>为了解决上述问题，我们提出HMCached[80] ,这是混合DRAM/NVMM系统的KV缓存（memcached）的扩展。图10显示了HMCached的系统架构。HM缓存跟踪KV对象访问并记录每个KV对的元数据结构中的进程计数, 因此HMCached可以轻松识别NVMM中频繁访问的对象,并将它们迁移到DRAM。这样, 我们逻辑上将DRAM用作NVMM的专用缓存,以避免更昂贵的NVMM访问。此外, 我们通过拆分基于哈希的KV索引来重新设计NVMM友好的KV数据结构, 以进一步减少NVMM访问。我们将KV对象的频繁更新元数据（例如,引用计数、时间戳和访问计数）放在DRAM中,其余部分（例如，键和值）放在NVMM中。我们利用多队列算法[118]来考虑DRAM缓存替换的对象访问频率和最近性。此外，我们建立了一个基于效用的性能模型来评估板类重新分配的效益。我们的动态slab重新分配策略能够有效解决slab calcification问题,并在数据访问模式发生变化时显著提高应用程序性能。与普通memcached相比HMCached可以显著减少70%的NVMM访问, 并实现大约50%的性能改进。此外，HMCached能够降低75%的DRAM成本,同时性能下降不到10%。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/76462968db584a9c8136d9c88d89346e.png"></p><p>据我们所知,我们是第一个探索混合存储系统中KV存储的对象级数据管理的。我们基于Memcached实现HMCached并开放源代码。我们发现，后来的研究（如flatstore[90]）也有类似的想法来解耦KV存储的数据结构。</p><p>今天,我们已经看到了许多内存中的图形处理系统,其中应用程序的性能与主内存的容量高度相关。高密度和低成本NVMM技术对于降低图形处理的I/O成本至关重要。如图11所示, 与基于SSD的存储系统相比,混合存储系统可以显著提高应用程序性能。图12显示了混合存储器系统和仅DRAM系统之间的应用程序性能差距。我们提出了NGraph，一种新的图形处理框架，专门设计用于更好地利用混合存储器。我们基于不同图形数据的访问模式开发混合内存感知数据放置策略,以减轻对NVMM的随机和频繁访问。通常，图形结构数据占总图形数据的大部分。NGraph根据目标顶点划分图形数据,并采用任务分解机制来避免多个处理器之间的数据争用。此外,NGraph采用了工作窃取机制，以最小化多核系统上并行图形数据处理的最大时间。我们称为ReRAM技术的。</p><p>基于图形处理框架Ligra[123]实现NGraph。与最先进的Ligra相比,NGraph可以将应用程序性能提高48%。从这项工作中获得的经验教训[91]可用于在配备真实PM设备的图形处理平台中进一步提高大规模图形分析的性能。</p><div class="note info no-icon"> 华科的工作关注的问题方向有的还挺小的，如果专注于某个问题的话，最后系统设计出来也是比较偏向于适合某个场景的。从后面第7章节来看，这个发展好像是走向越来越专业化的，不同的需求不同的架构和系统设计。 </div> <h1 id="7-Research-Directions研究方向"><a href="#7-Research-Directions研究方向" class="headerlink" title="7 Research Directions研究方向"></a>7 Research Directions研究方向</h1><p>NVMM技术的出现在材料、微电子、计算机架构、系统软件、编程模型和大数据应用领域引起了许多有趣的研究课题。随着IntelOptaneDCPMM等真正的NVMM设备越来越多地应用于数据中心环境,NVMM可能会改变数据中心的存储环境。我们的经<br>验和做法进行了一些初步和有趣的研究。在下文中,我们分享了NVMM未来研究方向的愿景,并分析了研究挑战和新机遇。图13说明了不同维度NVMM技术的未来趋势。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/21935e2720af4afeae966b3ce6e3724d.png"></p><ol><li><p>3D堆叠NVMM技术的发展仍在继续。NVMM有望提供更高的集成密度以降低成本。目前, 高端NVDIMM对于企业应用来说仍然过于昂贵。NVMM与传统DRAM和NAND闪存竞争的关键挑战是存储密度或每字节成本。NVMM技术主要有两种单片3D集成机制[124]。一种是将例如Intel/Micron 3DX点。另一种是垂直3D堆叠结构。然而, 3D集成技术尚未成熟。仍然存在许多挑战,如制造成本、柱电极电阻和潜路径问题。</p></li><li><p>NVMM越来越多地用于分布式共享存储器系统。随着NVMM的密度不断增加，单个服务器中的主内存容量可以达到数百TB。为了提高大容量NVMM的利用率,必须通过远程直接内存访问（RDMA）技术在多个服务器之间共享它们。使用NVMM的典型方法是将来自多个服务器的所有可共享内存聚合到混合共享内存资源池中, 例如Hotpot[49,125,126]。所有内存资源在全局内存空间中共享。有一些关于在数据中心和云环境中使用NVMM的初步研究[49,126,127]。使用PM的一个新趋势是将其作为分类内存进行管理,就像传统的分类存储系统一样。此型号与以前的共享PM系统不同, 在该系统中，PMDIMM分布在多个服务器中，由用户级应用程序共享。这些计算内存紧密耦合的体系结构在可管理性、可扩展性和资源利用率方面有几个缺点。相比之下，在少数存储器节点中配备有大量PM的分解PM系统可以由计算节点通过高速结构连接。这种计算/内存分类架构可以更容易地减轻数据中心环境中的上述挑战。然而,仍然存在许多挑战。例如,NVMM的持久性特性也应在分布式环境中得到保证。传统的PM管理指令（如clflush和mfence） 只能保证数据在单个服务器中持久化，但不能保证数据通过RDMA网络持久化到远程服务器。对于每个RDMA操作,一旦数据到达远程服务器中的网络接口卡（NIC），它就会向数据发送方发出确认。由于NIC中有数据缓冲区，数据不会立即存储到远程NVMM。如果此时发生电源故障,则无法保证数据持久性。因此,必须重新设计RDMA协议以支持flushing原语。此外,计算节点应支持对用户级应用程序透明的远程页面交换。为了支持这种机制应该重新设计传统的虚拟内存管理策略。另一方面,由于PM表现出类似内存的性能, 并且是字节可寻址的，因此需要对内存调度和管理进行新的设计，以适应分解的PM。</p></li></ol><p>3)基于NVMM的计算存储器集成计算机体系结构正在兴起。例如,新兴NVMM在存储器内处理（PIM）[95,96]和近数据处理（NDP）[128,129]架构中的应用正在兴起。PIM和NDP近年来已成为新的计算范式。NDP是指将处理器与存储器集成在单个芯片上,以便计算能够尽可能接近地访问存储器中的数据。NDP能够显著降低数据移动的成本。实现这一目标主要有两种方法。一种是将小型计算逻辑（如FPGA/ASIC）集成到存储器芯片中，以便在数据最终被提取到CPU之前对其进行预处理。另一种方法是将内存单元（HBM/HMC）集成到计算（CPU/GPGPU/FPGA）中。该模型通常用于许多处理器架构, 如IntelO Xeon PhiTM Knights Landing系列、NVIDIAO tesla V100和Google Tensor处理单元（TPU）。PIM指的是完全在计算机内存中处理数据。它通过在主存储器中执行计算,提供了高带宽、大规模并行性和高能量效率。使用NVMM（如ReRAM）的PIM通常可以并行计算两个或多个内存行的位逻辑, 并支持一步多行操作。该范例对于模拟计算方式中的矩阵向量乘法特别有效，并且可以实现极大程度的性能加速和节能。因此, PIM在加速机器学习算法（如卷积神经网络）中得到了广泛的研究（CNN）和深度神经网络（DNN）。尽管在PIM架构中.使用NVMM技术的兴趣越来越大[94-96 ,130]， 但目前的研究主要基于电模拟,没有一项研究可用于中型原型设计。</p><p>4)除了传统应用之外, 一些使用NVMM的新应用正在出现。尽管NVMM技术已初步应用于许多大数据应用, 如KV存储、图形计算和机器学习,但大多数编程框架/模型和运行时系统都是为磁盘设备和基于DRAM的主存储器而设计的, 它们在混合存储系统中并不有效。例如, 这些系统中广泛使用缓冲和延迟写入机制来隐藏I/O操作的高延迟。然而,混合存储器系统中可能不需要这些机制,甚至可能会损害应用程序性能。应重新设计Hadoop/Spark/GraphChi/Tensorflow等大数据处理平台,以适应NVMM技术的特点。除了这些传统应用之外,一些基于NVMM的新型应用正在出现。例如,有一些建议通过利用NVMM的切换过程的内在变化,将NVMM用作硬件安全原语, 如物理不可克隆函数（PUF）[131]。 PUF通常用于具有高安全性要求的应用,例如密码学。最近,已经提出了许多基于NVMM技术的逻辑电路并将其原型化[132-134]。例如，ReRAM技术被提议用作基于ReRAM的FPGA的可重构开关[133]。此外，STT-RAM技术被提出用于设计非易失性缓存或寄存器[135]。</p><h1 id="8-Conclusions结论"><a href="#8-Conclusions结论" class="headerlink" title="8 Conclusions结论"></a>8 Conclusions结论</h1><p>与传统DRAM技术相比, 新兴的NVMM技术具有许多良好的特性。它们有可能从根本上改变存储系统的面貌，甚至为计算机系统添加新的功能和特性。现在有很多机会重新思考当今计算机系统的设计,以实现系统性能和能耗的数量级改进。本文从内存体系结构、操作系统级内存管理和应用程序优化的角度全面介绍了最新的工作和我们的实践。我们还分享了我们对NVMM技术未来研究方向的展望。通过利用NVMM的独特特性, 有巨大的机会来创新未来的计算范式, 开发NVMM的多种新颖应用。</p><h2 id="积累"><a href="#积累" class="headerlink" title="积累"></a>积累</h2><p>一些可以进一步看一看的文献：</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png" alt="不同内存硬件性能比较"></p><p>如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3 GB/s，而DRAM的读写带宽之间的差距要小得多(1.3x)。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。</p><div class="note danger"> 但是这个可能不是我想要的CPU和内存的延迟带宽之类的，可能是针对做持久性时的数据 </div> <p>Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面。</p><p>Li[101]等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热度、内存级并行性和行缓冲区局部性.</p><p>“如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。”看看华科廖小飞团队在这个方面最近的工作。</p><p>“由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66]。”看一看这些有没有做内存的延迟和能耗的统计。</p><p>文中提到和写密集有关的（但是这个是store还是write还是要看看，有的需求write挺少的）CLOCK-DWF[33]，PDRAM[15]，RaPP[14]</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Review </tag>
            
            <tag> B </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</title>
      <link href="/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/"/>
      <url>/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary"><ul><li>文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</li><li>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</li></ul></div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Adnan Maruf, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Ashikee Ghosh, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Janki Bhimani, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Daniel Campello, Google</li><li>Andy Rudoff, 英特尔公司</li><li>Raju Rangaswami, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>将PM作为第二级内存直接暴露给CPU是目前比较有希望的一个做法：如何把数据在正确时间放入正确分层中去。于是面临一个是大家关注的问题。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>动机：通过四个工作负载，统计归类50个采样页面的访问模式，得出层级友好页面是需要迁移的对象（空间局部性）。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b9a6c5150a4c455c9eca0fbbcd1aaf99.png" alt="热力图"><br>同时一段时间访问过的页面在下一段时间被访问概率也很大（时间局部性）<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/cb7d1ef346b84032b6cae36568adf862.png" alt="相线图表示被访问的概率在不同时间窗口的变化"><br>然后说明了用frequency&amp;recency识别到的页面也具有层级友好的特征。相比于静态分层，说明了动态分层的必要。最后把整个比较模糊的大问题转化为：试图解决分层系统中：如何根据frequency&amp;recency来识别升级的热点页?如何在内核中设计一个简单、低开销而又高效的系统？</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>这些都是分层技术上的对比：<br>静态分层即一个内存页一旦被映射到一个分层，在其生命周期内就不会被重新分配到不同的分层。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/9b9a94f135544a11a3dfdb7aca8ed615.png" alt="现有内存分层技术的比较"><br>[11]Nimble：只根据recency来选择页面，这篇的作者为了解决frequency决定把工作负载执行期分为观察窗口和性能窗口。得出观察窗口频率高的，在性能窗口概率也高。专注于透明大页（THP）迁移。应用程序需要通过Nimble的启动器运行以利用其页面迁移技术。MC在内置内核实现的功能。Nimble需要一个额外的启动器来运行内核上的任何工作负载。</p><p>[12]AutoTieringhint page fault的缺页异常来跟踪页面访问，并使用recency来识别热点页进行升级。尽管缺页异常可以提供高准确度的页面访问跟踪，但跟踪所有页面的成本很高，因为每一个页面故障都必须在访问页面之前进行处理。这是因为基于缺页的软件页面访问跟踪成本很高，而且跟踪页面历史位以识别冷页面的开销也很大。所以在后面工作负载测试中表现很差。</p><p>[19]Thermostat源代码不可用没有评估，通过poisoning页表项（PTE）和触发缺页异常来跟踪巨大的页面，并将冷页面迁移到较低的内存层。</p><p>[22]AMP（非统一内存访问架构NUMA，主板会分成不同的插槽，每一个插槽一组cpu，以及和这组cpu离得近的内存。）这在两个插座的NUMA机器中是不现实的，因为每个节点通常有自己的DRAM、PM和CPU。AMP使用一个节点，只用于DRAM的分配，其他节点只用于PM的分配。AMP是在Linux内核4.15版本上实现的，它不支持所需的KMEM DAX驱动（从内核v5.1开始提供），以便PM作为主内存在分层系统中使用。AMP的核心设计原则要求它扫描和剖析来自DRAM和PM层的所有内存页，这在实际系统的内核中是不现实的，因为在我们评估的工作负载中，内存页的数量可以增长到数亿。<br>对比于PM当前这款硬件的两种用法。</p><p>[7]Memory-mode：数据不能持久化，dram作为缓存不透明。缓存是需要从高层获取数据的，而这里提出的分层是两层都能被直接访问的。</p><p>[44]对象级需要改变应用程序API，而内核级别的修改不需要应用程序有何变化。</p><p>[32]提出了一种有效使用持久性内存作为NUMA节点的设计。这个分层设计同时意识到了DRAM和PM节点 ,并且只通过NUMA平衡处理匿名页面的升级/移动。</p><p>[33-36]不需要硬件，而且都是主存没有缓冲。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>设计MULTI-CLOCK的主要假设是，最近被访问过一次以上的页面，在不久的将来更有可能被访问。</p><p>具体升级降级要求门槛就是那张图。（但是频繁程度不够吧？）及时更新页面引用状态的方式根据对内存页的访问模式不同而不同。无监督式：CPU在进程的页表入口中设置的页面引用位。 <img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/ca0046254e904f9991e9b0242696b89f.png" alt="文章的idea"></p><p>与CLOCK类似，升级是由每次系统守护程序kpromoted去完成的。它定期被唤醒，以扫描列表，更新它们，并将最近由无监督访问产生的升级列表中页迁移到更高层的页。</p><p>降级机制基于今天的虚拟内存系统中的页面驱逐技术。当内存达到内存压力时（该层与系统总内存量来计算）会去扫描每个列表。活动列表中的页面相对于非活动列表的比例超过了一个与该层可用内存量相关的阈值时，那么在活动列表中没有被标记为引用的页面将被移至非活动列表。最后，非活动列表被扫描，以寻找未被标记为引用的页面，并将其迁移到较低的层级。当没有更低层级列表可以迁移，就写回块存储。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>评估的目的是确定MULTI-CLOCK是否、何时以及如何能够提高应用工作负载的性能。</p><p>使用雅虎云服务基准（YCSB）[13]的六个不同的工作负载和GAP基准套件(GAPBS) [14] 的六个工作负载来讨论我们的结果。Memcached[3]，一个使用大量主内存来维护其数据的内存缓存服务，作为YCSB的键值存储后端。 配置时内存要被全部消耗，并且消耗一部分PM。</p><p>MULTI-CLOCk在所有YSCB工作负载上都优于静态分层、Nimble、AT-CPM和AT-OPM。对GAPBS的执行时间也比其他方案减少。</p><p>分析了MULT-CLOCK和Nimble所升级的页面数量。MULTI-CLOCK每次扫描平均升级758页，最多扫描1024。而N是把1024作为固定值。如果那些将来不会再被重新访问的页面被提升到DRAM中，那么提升这些页面的开销会损害系统性能。第二次再次被访问的百分比比N高15%</p><p>实现了目标，低开销，特别是密集型应用。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><ol><li>能耗方面摘要提了一下，后面也没说呀。</li><li>作者源码有一些问题，在运行高性能计算用MIP的时候会内核崩溃。</li><li>后面的性能评估对比了几种相关的工作，但是选择的workload也是比较局限的，GAPBS的论文都没有正式发出来。</li></ol><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><ol><li>在内核上修改代码，不需要其他程序有什么修改。</li><li>使用升级列表和频率可以很好的利用层级友好页面的局部性。</li></ol><h2 id="9-相关链接"><a href="#9-相关链接" class="headerlink" title="9. 相关链接"></a>9. 相关链接</h2><ul><li><a href="https://github.com/sylab/multi-clock">作者公布的源码</a></li><li><a href="https://docs.pmem.io/ndctl-user-guide/">ndctl用户手册</a></li><li><a href="https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/">PM的AD模式下的kmem模式设置</a></li><li><a href="https://github.com/brianfrankcooper/YCSB">YCSB工作负载源码</a></li><li><a href="https://cloud.tencent.com/developer/article/1004637">YCSB介绍与相关运行配置介绍</a></li><li><a href="https://github.com/pmem/ndctl/tree/main/Documentation/daxctl">ndctl下的daxctl安装源码下载</a></li><li><a href="https://memark.io/index.php/2021/04/09/pmem_intro/">持久内存开发资料汇总</a></li><li><a href="https://blog.csdn.net/qq_37858386/article/details/78444168">Linux驱动编程中EXPORT_SYMBOL介绍（因为作者公布的源码里有这个报错）</a></li><li><a href="https://github.com/pmem/ndctl/issues/108">daxctl fails to reconfigure to system-ram when DAX modules built-in</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/guide/qsg-intro-to-provisioning-pmem.html">持久内存配置简介，快速入门</a></li><li><a href="https://github.com/sbeamer/gapbs">GAPBS工作负载下载和使用</a></li></ul><h2 id="10-积累"><a href="#10-积累" class="headerlink" title="10. 积累"></a>10. 积累</h2><p> 当使用PM作为主存储器时，其持久性能力变得无关紧要（这里作者遵循原先的主存的设计，即易失性，抛弃了NVM数据持久的特点，后面提到的该产品Memory Mode，并且文献[10]说PMEP想要持久性靠的是<code>clflush</code>指令，这个指令由于要排序开销很大），从而完全避免了其最大的性能开销[10]</p><p> 有开源内核好像有一个选项可以计数随时间变化页面在层级间迁移的数量。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hybrid Memory Systems </tag>
            
            <tag> A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time Complexity Calculation</title>
      <link href="/2021/06/06/Time-Complexity-Calculation/"/>
      <url>/2021/06/06/Time-Complexity-Calculation/</url>
      
        <content type="html"><![CDATA[<h2 id="一、常见阶大小比较"><a href="#一、常见阶大小比较" class="headerlink" title="一、常见阶大小比较"></a>一、常见阶大小比较</h2><p>从大到小：  </p><ul><li>超指数阶：$n^n$，$n!$</li><li>指数阶：$9^{n/2}$,  $2^n$</li><li>多项式阶：$n^3$,   $n*log(n)$, $n^{1/2}$</li><li>对数阶：$log^2(n)$, $log(n)$, $log(log(n))$</li><li>常数阶：100, 1<br>下题需要保留阶最高的部分：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220748744.jpg"></li></ul><h2 id="二、算法复杂性估计函数"><a href="#二、算法复杂性估计函数" class="headerlink" title="二、算法复杂性估计函数"></a>二、算法复杂性估计函数</h2>$$\lim_{n \to \infty} \frac{f(n)}{g(n)}  =\begin{cases}(大于0的常数或)0       &amp;&amp;&amp; f(n)=O(g(n))上界&amp;-----f(n)\le cg(n)\\(大于0的常数或)无穷    &amp;&amp;&amp; f(n)= \Omega(g(n))下界&amp;-----f(n)\ge cg(n)\\大于0的常数            &amp;&amp;&amp; f(n)= \Theta(g(n))确切界&amp;-----f(n)=cg(n)\\0      &amp;&amp;&amp;f(n)=o(g(n))上界&amp;-----f(n)&lt; cg(n)\end{cases}$$<p>可以发现都是针对f(n)在讨论，很容易得出下题答案：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220628312.jpg"></p><h2 id="三、几个常用替换的式子"><a href="#三、几个常用替换的式子" class="headerlink" title="三、几个常用替换的式子"></a>三、几个常用替换的式子</h2><h3 id="1-Stirling公式："><a href="#1-Stirling公式：" class="headerlink" title="1.Stirling公式："></a>1.Stirling公式：</h3>$$n! \approx {(2 \pi n)}^{1/2}{(n/e)}^n$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605222019202.jpg"></p><h3 id="2-阶乘和二项式系数"><a href="#2-阶乘和二项式系数" class="headerlink" title="2.阶乘和二项式系数"></a>2.阶乘和二项式系数</h3>$$C_n^k = C_n^{n-k} \\ C_n^n = C_n^0 = 1 \\C_n^k = C_{n-1}^k +C_{n-1}^{k-1} $$<p>帕斯卡三角形可以辅助记二项式系数：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060522532381.jpg"></p>$$\sum_{j=0}^{n}{C_{n}^j}x^j= {(1+x)}^n $$<h3 id="3-和式"><a href="#3-和式" class="headerlink" title="3.和式"></a>3.和式</h3>$$\sum_{j=1}^{n}{a_{n-j}} = \sum_{j=0}^{n-1}{a_j} \\\sum_{j=0}^n{j \over 2^j} = \sum_{j=1}^n{j \over 2^j} = 2-{{(n+2)} \over {n^2}} = \Theta(1) \\\sum_{j=0}^njc^j = \sum_{j=1}^njc^j = \Theta(nc^n)$$<h3 id="4-定积分与和式转换"><a href="#4-定积分与和式转换" class="headerlink" title="4.定积分与和式转换"></a>4.定积分与和式转换</h3>$$\int_m^{n+1} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m-1}^nf(x)dx 递减函数 \\\int_{m-1}^{n} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m}^{n+1}f(x)dx  递增函数$$<p>可以采用画图的方法辅助记忆它的上下界：<br>以一个递增的函数为例，我们要求1（m）~6（n）他的面积，每一个小矩形$1*f(j)$如果我们积分每个点左边的矩形，那么总面积就是偏小的，积分右边矩形就会稍微偏大，这就找到了上下界，当函数平行于X轴时就会有等号。递减也是一个道理。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606094351964.jpg"><br>但是，如果是logn等函数会遇到定义域不存在的情况。我们应该从和式中把（在积分中）没有定义的点先拿出来，再去积分。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060610042349.jpg"></p><p>用代数方法证明就是放大缩小去求，用积分方法证明就是用上面那个公式。可以看到积分方法求时等式左边按公式应该为$\int_0^n jlogj !\ dj$定义域<br>不存在，所以对于右边：</p>$$\sum_{j=1}^njlogj=\sum_{j=2}^njlogj+1log1=\int_{2-1}^njlogj \!\ dj+1log1$$<h2 id="四、计算次数"><a href="#四、计算次数" class="headerlink" title="四、计算次数"></a>四、计算次数</h2><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法:COUNT输入:n=2k,k为正整数。输出: count的值 。count=0while n&gt;=1for j=1 to ncount=count+1n=n/2return countend COUNT<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>while要执行k+1次，$k=log_2n$.for循环在每次while的基础上执行n次所以(k+1)n即$nlog_2n$次计算</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法: MERGE输入:数组A[1..m]和它的三个索引p, q, r, 1&lt;=p&lt;=q&lt;r&lt;=m。两个子数组A[p..q]和A[q+1..r]各自按升序排列。输出:合并两个子数组A[p..q]和A[q+1..r]的升序数组A[p..r]for(s=p, t=q+1, k=p; S&lt;=q and t&lt;=r; k++)if A[s]&lt;=A[t] //两个指针从两个头开始排序B[k]=A[s]; //B[p..r]是个辅助数组S=S+1;elseB[k]=A[t];t=t+1;if s=q+1 B[k..r]=A[q+1..r] elseB[k..r]=A[s..q]A[p..r]=B[p..r]end MERGE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两段相邻数组分别有序，两个指针将两段变为有序的。2(r-p+1)先遍历一次排序，在从排好序的辅助数组移回来。</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">void insertion_ sort(Type *a, int n){// 代价 次数// ti: for的第i次while的循环次数for (int i=1; i&lt;n; i++){// c1   n(比较语句)key=a[i];// c2   n-1int j=i-1;// c3   n-1while( j&gt;=0 &amp;&amp; a[j]&gt;key ){  // c4   sum tia[j+1] = a[j];// c5   sum of (ti-1)j--;// c6sum of (ti-1)}a[j+1]=key; // c7   n-1}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>插入排序的思想是左手为空，右手的牌按序插入。这里t是个不确定的数，但是还是可以得出计算次数为：</p><p>$$c_1n+c_2(n-1)+c_3(n-1)+c_4\sum_{i=1}^{n-1}{t_i}+c_5\sum_{i=1}^{n-1}{(t_i-1)}+c_6\sum_{i=1}^{n-1}{(t_i-1)}+c_7(n-1)$$</p><p>最好情况就是已经排好序了，c5与c6都是0，每次for的while都只跑一次。<br>$$c_1n+c_2(n-1)+c_3(n-1)+c_4(n-1)+c_7(n-1) = O(n)$$<br>最坏情况就是倒序排的，n张牌每次都比上一次多查找一个。</p><p>$$\sum_{i=1}^{n-1}{(t_i-1)} = n(n-1)/2$$</p><p>复杂度$O(n^2)$</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">1. COUNT42.count &lt;-- 03.for i ←- 1 to Llogn」4.for j ←- i to i+55.for k ←- 1 to i^26.         count ←- count +17.end for8.  end for9.end for(a)第6步执行了多少次?(b)要表示算法的时间复杂性，用0和O哪个符号更合适?为什么?(c)算法的时间复杂性是什么?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>a）思路是把每次for循环乘起来。<br>$$\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}\sum_{k=1}^{i^2}{1}$$内层指的是从1到$i^2$个1相加$=\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}{i^2} = 6\sum_{i=1}^{\lfloor logn \rfloor}{i^2}$<br>利用平方和求和公式$n(n+1)(2n+1)/6$进一步化简$=\lfloor logn \rfloor(\lfloor logn \rfloor +1)(2\lfloor logn \rfloor+1)$<br>b） O 因为对于算出的确切的计算次数，这个用于表示算法时间复杂性的函数是它上界。<br>c）$O(log^3n)$</p><h2 id="五、解递归方程式"><a href="#五、解递归方程式" class="headerlink" title="五、解递归方程式"></a>五、解递归方程式</h2><h3 id="1-线性齐次递推式（二阶）"><a href="#1-线性齐次递推式（二阶）" class="headerlink" title="1.线性齐次递推式（二阶）"></a>1.线性齐次递推式（二阶）</h3><p>对于递推式:$$f(n)=a_1f(n-1)+a_2f(n-2)+…+a_kf(n-k)$$我们想要得到$f(n)$的确切解，它的解往往是$x^n$于是我们可以把这个递推式的等价于:$$x_n=a_1x^{n-1}+a_2x^{n-2}+…+a_kx^{n-k}$$将两边同时除以$x^{n-k}$并且移项可以得到与n无关还能解出x的式子$$x^k-a_1x^{k-1}-a_2x^{k-2}-…-a_k = 0$$这个就是常说的特征方程。</p><table><thead><tr><th align="center">步骤</th><th align="center">例1</th><th align="center">例2</th></tr></thead><tbody><tr><td align="center">序列</td><td align="center">1,4,16,64,256</td><td align="center">1,1,2,3,5,8(斐波拉契)</td></tr><tr><td align="center">递推关系</td><td align="center">f(n)=3f(n-1)+4f(n-2)</td><td align="center">f(n)=f(n-1)+f(n-2)</td></tr><tr><td align="center">特征方程</td><td align="center">$x^2-3x-4=0$</td><td align="center">$x^2-x-1=0$</td></tr><tr><td align="center">特征根</td><td align="center">$x_1=-1,x_2=4$</td><td align="center">$x_1= { {1+\sqrt5} \over 2},x_2={ {1-\sqrt5} \over 2}$</td></tr><tr><td align="center">通解</td><td align="center">$f(n) = c_1{(-1)}^n+c_24^n$</td><td align="center">$f(n)=c_1\left( { {1+\sqrt5} \over 2} \right)^n+c_2\left( { {1-\sqrt5} \over 2} \right)^n$</td></tr><tr><td align="center">带入序列中的点</td><td align="center">$c_1=0,c_2=1$</td><td align="center">$c_1={1\over {\sqrt5}},c_2=-{1\over {\sqrt5}}$</td></tr><tr><td align="center">最终解</td><td align="center">$f(n)=4^n$</td><td align="center">由于n无穷大$c_2\left( { {1-\sqrt5} \over 2} \right)^n$趋于0,$f(n)={1\over {\sqrt5}}\left( { {1+\sqrt5} \over 2} \right)^n$</td></tr></tbody></table><p>还有种特殊情况：$x_1=x_2=x$时$f(n)=c_1nx^n+c_2x^n$</p><h3 id="2-非齐次递推式"><a href="#2-非齐次递推式" class="headerlink" title="2.非齐次递推式"></a>2.非齐次递推式</h3><h4 id="2-1-f-n-f-n-1-g-n"><a href="#2-1-f-n-f-n-1-g-n" class="headerlink" title="2.1  f(n)=f(n-1)+g(n)"></a>2.1  f(n)=f(n-1)+g(n)</h4><p>对于这一类g(n)是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)+g(n) = \big(f(n-2)+g(n-1)\big)+g(n) = f(0)+ \sum_{j=1}^ng(j)$$<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606120205290.jpg"><br>这道题麻烦点在于怎么把前面的系数3搞没了。由1我们知道，这种递推式是$f(n)=x^n$变形令$f(n)=3^nq(n), f(0)=q(0)=3$于是乎原式变为：</p>$$3^nq(n)=3*3^{n-1}q(n-1)+2^n \\ q(n)=q(n-1)+{(2/3)}^n\\ q(n)=q(0)+\sum_{j=1}^n{{(2/3)}^n}\\ f(n)=3^n*\big(3+{(2/3)(1-{(2/3)}^n) \over {1-(2/3)}}\big) \\ f(n)=5*3^n+2^{n+1}$$<h4 id="2-2-f-n-f-n-1-g-n"><a href="#2-2-f-n-f-n-1-g-n" class="headerlink" title="2.2  f(n)=f(n-1)*g(n)"></a>2.2  f(n)=f(n-1)*g(n)</h4><p>对于这一类g(n)也是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)*g(n) = \big(f(n-2)*g(n-1)\big)+g(n) = f(0)\prod_{i=1}^ng(i)$$</p><h4 id="2-3-f-n-f-n-1-g-n-h-n"><a href="#2-3-f-n-f-n-1-g-n-h-n" class="headerlink" title="2.3  f(n)=f(n-1)*g(n)+h(n)"></a>2.3  f(n)=f(n-1)*g(n)+h(n)</h4><p>可以直接推，也可以带点技巧推。结果：$$=\prod_{i=1}^ng(i)\big( f(0)+\sum_{j=1}^n{h(j) \over{\prod_{i=1}^ng(i)} } \big)$$</p><h4 id="2-4-f-n-af-n-c-g-n"><a href="#2-4-f-n-af-n-c-g-n" class="headerlink" title="2.4 f(n)=af(n/c)+g(n)"></a>2.4 f(n)=af(n/c)+g(n)</h4>$$f(n)=\begin{cases} d&amp; \text{n=1}\\af({n\over c})+bn^x&amp; \text{n &gt;= 2} \end{cases}$$<p>其中d非负常量，g(n)非负函数，a，c正数。设$n=c^k$</p>$$f(n)=\begin{cases} bn^x*log_cn^x+dn^x&amp;{a=c^x}\\ \big(d+{{bc^x}\over{a-c^x}}\big)n^{log_ca}-\big({{bc^x}\over{a-c^x}}\big)n^x&amp; {a\neq c^x} \end{cases}$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606123724891.jpg"><br>这道题就没有为难菜鸡，直接把方向给了。这种做法在遇到f(n/2)的情况下叫做<strong>更换变元法</strong><br>$$f(2^k)=f(2^{k-1})+2^k=f(2^0)+\sum_{i=1}^k2^i=2^{k+1}-1 =2n-1\<br>g(2^k)=2g(2^{k-1})+1=\sum_{i=0}^k2^i=2^{k+1}-1=2n-1$$<br>也可以直接套公式:<br>对于f   $d=1,a=1,c=2,b=1$<br>对于g  $a=2,c=2,b=1,d=1$<br>这些公式太惨绝人寰了，其他还有一些和2.4一样复杂的，等遇到了再补充上去。</p><h2 id="六、p、np、np-hard、np-complete问题"><a href="#六、p、np、np-hard、np-complete问题" class="headerlink" title="六、p、np、np-hard、np-complete问题"></a>六、p、np、np-hard、np-complete问题</h2><p><a href="https://blog.csdn.net/birduncle/article/details/94646993">p、np、np-hard、np-complete问题</a>这篇文章讲的很清晰，附上一张从文章里拿的<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210704092654261.png"></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Advanced Mathematics </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
