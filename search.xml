<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Coalescing for Multi-Grained Page Migration</title>
      <link href="/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/"/>
      <url>/2023/05/20/Coalescing-for-Multi-Grained-Page-Migration/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>华中科技大学</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>观察结果1，一些热页在虚拟地址和物理地址上都有连续性。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>Tamp使用DRAM作为数据缓冲器来缓存NVM中多种尺寸的热页(所谓的多粒度页)。相应地使用分裂的超级页TLB和多粒度TLB来分别加速NVM和DRAM的地址转换。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>每一个idea都有对应的问题要解决，最后嵌套太多了，所以投的刊物可能会不好。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Multi-grained </tag>
            
            <tag> Hybrid Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mem Alloc for C++ Server Workloads</title>
      <link href="/2023/05/07/Mem-Alloc-for-C-Server-Workloads/"/>
      <url>/2023/05/07/Mem-Alloc-for-C-Server-Workloads/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Trident all page size</title>
      <link href="/2023/05/07/Trident-all-page-size/"/>
      <url>/2023/05/07/Trident-all-page-size/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Superpages in Non-Contiguous Physical Memory</title>
      <link href="/2023/05/07/Supporting-Superpages-in-Non-Contiguous-Physical-Memory/"/>
      <url>/2023/05/07/Supporting-Superpages-in-Non-Contiguous-Physical-Memory/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Tailored Page Sizes</title>
      <link href="/2023/05/04/Tailored-Page-Sizes/"/>
      <url>/2023/05/04/Tailored-Page-Sizes/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自International Symposium on Computer Architecture, (ISCA), 2020 </div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Faruk Guvenilir, The University of Texas at Austin</li><li>Yale N. Patt, 德克萨斯州大学奥斯汀分校</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> XXX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Perforated Page: Supporting Fragmented Memory Allocation for Large Pages</title>
      <link href="/2023/05/04/Perforated-Page-Supporting-Fragmented-Memory-Allocation-for-Large-Pages/"/>
      <url>/2023/05/04/Perforated-Page-Supporting-Fragmented-Memory-Allocation-for-Large-Pages/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自International Symposium on Computer Architecture, (ISCA), 2020 </div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Chang Hyun Park∗, Sanghoon Cha†, Bokyeong Kim†, Youngjin Kwon†, David Black-Schaffer∗, Jaehyuk Huh†</li><li>∗Department of Information Technology, Uppsala University</li><li>†School of Computing, KAIST</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> XXX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thermostat: Application-transparent Page Management for Two-tiered Main Memory</title>
      <link href="/2023/05/04/Thermostat-Application-transparent-Page-Management-for-Two-tiered-Main-Memory/"/>
      <url>/2023/05/04/Thermostat-Application-transparent-Page-Management-for-Two-tiered-Main-Memory/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自Architectural Support for Programming Languages and Operating Systems, (ASPLOS), 2017 </div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Neha Agarwal, 密歇根大学</li><li>Thomas F. Wenisch, University of Michigan</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> Tierd Memory </tag>
            
            <tag> Page Migration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Dynamic Translation Granularity for Hybrid Memory Systems</title>
      <link href="/2023/05/04/Supporting-Dynamic-Translation-Granularity-for-Hybrid-Memory-Systems/"/>
      <url>/2023/05/04/Supporting-Dynamic-Translation-Granularity-for-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自IEEE 40th International Conference on Computer Design, (ICCD), 2022 </div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Bokyeong Kim∗, Soojin Hwang†, Sanghoon Cha‡, Chang Hyun Park§, Jongse Park†, Jaehyuk Huh†</li><li>Samsung Research, †School of Computing, KAIST, ‡Samsung Advanced Institute of Technology, §Uppsala Universit bokyeong</li><li></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tierd Memory </tag>
            
            <tag> CCF-B Paper </tag>
            
            <tag> Page Migration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supporting Superpages and Lightweight Page Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/02/Supporting-Superpages-and-Lightweight-Page-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自ACM Transactions on Architecture and Code Optimization, (TACO), 2019</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>XIAOYUAN WANG, HAIKUN LIU, XIAOFEI LIAO, JI CHEN, HAI JIN, YU ZHANG, and LONG ZHENG, 华中科技大学</li><li>BINGSHENG HE, 新加坡国立大学</li><li>SONG JIANG, 德克萨斯大学阿灵顿分校(UTA)</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>在大内存系统中，<strong>超级页一直被用来减轻地址转换开销</strong>。 然而，在由DRAM和NVM组成的混合存储系统中，<strong>超页面往往会阻碍轻量级页面迁移，而轻量级页面迁移对性能和能量效率至关重要</strong>。 </p><blockquote><p>Superpages have long been used to mitigate address translation overhead in large-memory systems. However, superpages often preclude lightweight page migration, which is crucial for performance and energy efficiency in hybrid memory systems composed of DRAM and NVM.</p></blockquote><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><ol><li>如果大多数内存引用<strong>分布在超级页的一个小区域中</strong>，那么以超级页粒度（例如2MB）进行的页迁移会导致DRAM容量和带宽的巨大浪费，从而导致无法承受的性能开销。 成本可能比超级页面迁移的好处还要大。 这给超级页面的使用带来了一个困境，因为<strong>轻量级页面迁移可能会超过扩展TLB覆盖的好处</strong>。 </li></ol><blockquote><p>However, page migration at the superpage granularity (e.g., 2MB) can incur unbearable performance overhead due to a vast waste of DRAM capacity and bandwidth if most memory references are distributed in a small region ofthe superpage (see Section 2.2). The cost may be even larger than the benefit of superpage migration. This presents a dilemma for the use of superpages,<br>since the lightweight page migration can outweigh the benefits of extended TLB coverage.</p></blockquote><p><a href="https://www.jianshu.com/p/bea989a85a31">累积分布函数图怎么看</a><br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/6d3b808800554ed3b2d0a153353c4e26.png" alt="2MB超级页的累积分布函数与给定区间(108个周期)内一个超级页中被触及的4KB小页的数量"><br>从图中可以看到，很大一部分工作负载有80%以上的概率：2MB的页面中被访问的4kb页面只有12.5%。<br>还有一张表格来说明问题：<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/abbbcb642622478cb35fd087610a7f69.png" alt="4kb热页访问统计"><br>使用的工作负载：</p><ul><li><a href="https://www.spec.org/cpu2006">SPEC CPU2006</a></li><li><a href="http://parsec.cs.princeton.edu/index.htm">Parsec</a></li><li><a href="http://www.cs.cmu.edu/%E2%88%BCpbbs/">Problem Based Benchmarks Suit（PBBS）</a></li><li><a href="http://whitedb.org/">WhiteDB</a></li><li><a href="https://redis.io/">Redis</a></li><li><a href="http://graph500.org/">Graph500</a></li><li><a href="http://www.netlib.org/benchmark/">Linpack</a></li><li><a href="https://www.nas.nasa.gov/publications/npb.html">NPB-CG</a></li><li><a href="http://icl.cs.utk.edu/hpcc/">HPC Challenge Benchmark GUPS</a></li></ul><ol start="2"><li><strong>轻量级热页的标识</strong>：为了支持轻量级页迁移，大量工作提倡通过内存控制器监视内存访问。 然而，当<strong>主存容量变大时</strong>，以每页粒度（即4KB）使用<strong>访问计数器会导致高得令人望而却步的存储开销</strong>。</li></ol><blockquote><p>Identification oflightweight hot pages: to support lightweight page migration, a large body of work advocates monitoring memory accesses through the memory controller [55, 63]. However, using access counters at per-page granularity (i.e., 4KB) leads to prohibitively high storage overhead when the capacity of main memory becomes large.</p></blockquote><ol start="3"><li>轻量级页面迁移对TLB覆盖率的影响：页面迁移通常会<strong>分割超级页面</strong>，从而<strong>破坏物理地址的连续性</strong>。 </li></ol><blockquote><p>Impact oflightweight page migration on TLB coverage: page migrations often fragment superpages and thus break the physical address continuity.</p></blockquote><ol start="4"><li>热页寻址效率：由于热页占应用程序内存引用的主要部分，因此必须进一步<strong>减少DRAM中那些热页的地址转换开销</strong>。 </li></ol><blockquote><p>Efficiency of hot pages addressing:ashot pages<br>contribute to a major portion of applications’ memory references, it is essential to further reduce<br>the overhead of address translation for those hot pages in the DRAM.</p></blockquote><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的工作主张分割超级页面以实现轻量级内存管理，如页面迁移和共享，同时牺牲地址转换的性能[37,58]当<strong>超级页面中的热小页面迁移到DRAM时，保持改进的TLB覆盖率</strong>仍然是一个挑战。</p><blockquote><p>Previous work has advocated splintering superpages to enable lightweight memory management such as page migration and sharing, while sacrificing the performance of address translation [37,58]. It is still a challenge to retain the improved TLB coverage when the hot small pages within superpages are migrated to the DRAM.</p></blockquote><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>针对上述问题，提出了一种新的内存管理机制Rainbow, Rainbow在Superpage粒度上管理NVM，并使用DRAM在每个Superpage内缓存频繁访问（热）的小页面。相应地，Rainbow利用拆分TLB[2,7,30,52]的可用硬件特性来支持不同的页面大小，其中一个TLB用于寻址超级页面，另一个TLB用于寻址小页面。 Rainbow将SuperPage中的热小页迁移到DRAM中，而不会损害SuperPage TLB的完整性。 因此，Rainbow实际上将DRAM架构为NVM的缓存。</p><ul><li>为了减少细粒度页面访问计数的存储开销，分两个阶段进行计数。在<strong>给定的时间间隔内</strong>，Rainbow<strong>首先计算Superpage粒度下的NVM内存访问</strong>，然后选择<strong>前N个热门Superpage作为目标</strong>。 在第二阶段，我们<strong>只监视那些小页面</strong>(4KB)粒度的热点超页面，以识别热点小页面。 这种基于历史的策略避免了监视大量冷超页中的子块（4KB页），从而显著降低了热页识别的开销。 </li><li>我们采用<strong>拆分TLB</strong>来加速DRAM和NVM引用的地址转换性能。当一些小页迁移到DRAM时，为了保持SuperPages TLB的完整性，我们在内存控制器中使用位图来识别迁移的热页，而<strong>不会分裂SuperPages</strong>。</li><li>我们提出了一种<strong>物理地址重映射机制来访问DRAM中迁移的热页</strong>，而不必为寻址DRAM页而遭受昂贵的页表遍历。为了实现这一目标，我们将迁移的热点页面的目的地址存储在其原始住所（超级页面）中。 一旦热页对应的TLB未命中，DRAM页寻址应求助于对超级页的间接访问。这种设计在逻辑上利用了SuperPage TLBS作为4KB页面TLB的下一级缓存。 因为Superpage TLB命中率通常很高，所以Rainbow可以显著加快DRAM页面寻址的速度。 </li></ul><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>实验结果表明，与现有的内存迁移策略相比，在没有Superpage支持的情况下，Rainbow可以将应用程序的TLB丢失率降低99.9%，并将应用程序的性能（以OFIPC为标准）平均提高2.9×(45.3%)。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> Tierd Memory </tag>
            
            <tag> Page Migration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stealth-Persist: Architectural Support for Persistent Applications in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Stealth-Persist-Architectural-Support-for-Persistent-Applications-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自IEEE, (H), 2021</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Mazen Alwadi1, Vamsee Reddy Kommareddy1, Clayton Hughes2, Simon David Hammond2, Amro Awad3<br>University of Central Florida1, Sandia National Laboratories2, North Carolina State University3</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>它们存在着高写延迟和有限的写持久性。研究人员提出了结合DRAM和NVM的混合存储系统，利用DRAM的低延迟来掩盖NVM的一些缺点——通过在DRAM中缓存常驻NVM数据来提高系统性能。对于大容量的NVM快速和持久的缓存能力是有限的。利用DRAM作为NVM的一个快速持久的缓存，受到能源支持的限制。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>越来越多的应用程序将利用NVMs的持久性功能。因此，提高这类应用的性能，同时保证数据的持久性是一个关键的设计点。允许NVM的非常快速的持久性缓存，但不需要任何额外的能量支持能力来刷新DRAM缓存内容到NVM</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>现有的持久性存储器技术要么提供小容量但快速和基于电池支持的DRAM持久性区域，要么提供高容量的NVM (不需要电池支持)但缓慢的持久性区域。前者需要系统的支持，需要笨重的物品，并且会根据超级电容或电池的大小限制持久性DRAM的大小。此外，它需要改变某些DIMM来支持备份模式。同时，由于持久性对象的缓慢读取访问，后者会产生明显的性能下降。期待电池备份、有限的DRAM尺寸以及限制集成在系统中的DRAM模块的选择。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>实现NVM的快速持久的DRAM缓存，我们利用选择性的NVM镜像对缓存在DRAM中的持久页面进行了新的内存控制器。</p><p>在DRAM中缓存时将持久区域的更新镜像到NVM。 Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变。最后，为了支持对持久性页面的高性能访问，我们的方案从DRAM中提供对持久性对象的读取请求，如果在那里有缓存</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>写次数不变，对于写耐久性的破坏</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>虽然之前所有关于持久化应用的工作都探讨了对持久化对象的写的优化，但这是第一个探讨对持久化对象的读操作进行优化的工作。仅仅依靠对处理器芯片的微小改动来支持DRAM中持久性数据对象的缓存是非常重要的。Stealth-Persist的镜像操作发生在内存控制器上，不需要对应用程序或持久性编程库做任何改变</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> Tierd Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Object-Level Memory Allocation and Migration in Hybrid Memory Systems</title>
      <link href="/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/"/>
      <url>/2023/05/01/Object-Level-Memory-Allocation-and-Migration-in-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><blockquote><p><strong>文章来自IEEE(H)2022属于CCF-A</strong> </p></blockquote><h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>A, Haikun Liu, Renshan Liu, Xiaofei Liao, Hai Jin, and Yu ZhangBig Data Technology and<br>System, Service Computing Technology and System Lab, Cluster and Grid<br>Computing Lab, Huazhong<br>University of Science and Technology.<br>Bingsheng He is National University of Singapore</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>充分利用NVM和DRAM混合系统的优势，主要目标是将应用程序数据正确放置在混合存储器上。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>于是提出混合内存系统编程接口，<strong>对象粒度迁移，减少迁移开销</strong>。同时提高应用性能。</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>以前的研究主要集中在页面迁移方案上，以实现更高的性能和能源效率。但是，这些方案都依赖于在线页面访问监控成本高。并且还会由于多核时维护缓存/TLB一致性和dram带宽争用产生更多页粒度迁移的开销。</p><p>依赖于页面访问的recency and frequency来决定数据在DRAM或NVM上的位置。1一些用硬件辅助页迁移的方案由于目前硬件不支持页面访问技术，需要对硬件架构进行大量修改[5][6][11]。2用操作系统监控内存访问只能引用1个访问位不足以表达页面冷热度[12][Thermostat13]。3页面迁移通常需要一段时间来检测热页。4预测的页面访问模式可能与未来的访问行为不一致，导致不必要的页面迁移。5大页（huge page）已被越来越多地用于大数据应用和虚拟化平台[13][14]，由于对DRAM容量和带宽的低效利用，粗粒度的页面迁移甚至会降低系统性能。</p><p>采用离线剖析工具，应用对象的粒度来描述内存访问模式，然后指导它们在DRAM或NVM上的静态放置[15]。这个方案只考虑了内存访问行为的整体观点，而忽略了内存访问模式的潜在变化，即对象的访问频率(热度)可能在不同的执行阶段动态变化。</p><p>另一项工作将静态对象放置与选定页面的动态迁移相结合[16]，以处理这些波动。然而，它仍然依赖于内存控制器的硬件扩展来执行在线页面访问监控和迁移。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>OAM利用离线剖析方法来捕获对象级内存访问模式，并采用性能/能量模型来指导对象内存分配和迁移。更具体地说，我们分析每个对象的细粒度时间段的内存痕迹，并采用我们的性能/能量模型来识别对象内存访问模式变化的不同执行阶段。我们在应用程序的源代码中找出未来执行阶段会发生变化的位置，并通过静态代码工具自动注入对象迁移指令。当修改后的程序运行时，它通过考虑DRAM的使用和数据迁移的净收益，自己执行对象迁移。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>“有效的静态内存分配：在于2pp执行时间和性能差不多的情况下，与只使用DRAM的系统相比，””OAM w/o migration “”能够平均减少51%的内存能量消耗。这些结果表明，我们最初的OAM数据放置策略对提高混合内存系统的性能和能源效率是有效的。</p><p>在线内存迁移的有效性：与静态内存分配方案相比，对象迁移可以进一步提高应用性能，平均提高11%。没有迁移的OAM平均可以实现51%的EDP减少，而有迁移的OAM平均可以进一步减少10%的EDP</p><p>与一些页面迁移算法相比较：与CLOCK-DWF和2PP相比，OAM可以显著减少迁移流量，平均分别为42%和22%。开销也比他们都小。</p><p>适应不同数据和规模：执行时间都比不使用该方案要节省时间。</p><p>对不同NVM性能的敏感性：测试了目前的产品，对于Optance是由读密集引起的迁移，因为写延迟是差不多的。”</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>聚焦于更低的迟延和能耗。减少迁移性能开销</p><p>而我们的机制只为用C++编写的应用程序提供对象级迁移接口。</p><p>page-interleaving貌似是模拟和算法的基础。</p><p>也没有考虑断电后数据怎么办，而且会存在一致性问题。</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>提供了一个离线剖析工具来详细描述应用程序的内存访问模式，并提出了一个<strong>性能/能源模型</strong>来指导应用程序对象的初始内存分配和动态迁移<strong>从能源消耗上解决了读写不均衡</strong>，而不需要任何硬件修改和操作系统干预的在线内存监控。一个静态代码工具，用于自动转换应用程序源代码中的对象级内存分配和迁移，而不会给应用程序的程序员带来负担。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> Tierd Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/05/01/hello-world/"/>
      <url>/2023/05/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="1-Create-a-new-post"><a href="#1-Create-a-new-post" class="headerlink" title="1.Create a new post"></a>1.Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>执行命令会在/source/_posts下创建新文章，之后需要使用MarkDown语法编写该文章。  </p><p><code>---</code>包括起来的内容称之为<code>Front-matter</code>有很多配置选项可以添加。<br>更多的简单语法可以参考<a href="https://www.runoob.com/markdown/md-tutorial.html">菜鸟教程</a><br>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="2-清除旧数据"><a href="#2-清除旧数据" class="headerlink" title="2.清除旧数据"></a>2.清除旧数据</h3><p>文章写好之后，首先清除掉旧的数据</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">hexo clean <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会清除掉之前生成的网页，即站点根目录下的public文件夹</p><h3 id="3-Generate-static-files"><a href="#3-Generate-static-files" class="headerlink" title="3.Generate static files"></a>3.Generate static files</h3><p>然后使用如下命令生成新的页面：More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate 或者简写 hexo g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个命令会将source文件夹下所有的md文件进行渲染，生成HTML页面，存放在public文件夹下</p><h3 id="4-Run-server"><a href="#4-Run-server" class="headerlink" title="4.Run server"></a>4.Run server</h3><p>在本地开启服务器，预览一下文章是否满意</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server  <span class="token string">'hexo s'</span> <span class="token keyword">for</span> short<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="5-Deploy-to-remote-sites"><a href="#5-Deploy-to-remote-sites" class="headerlink" title="5.Deploy to remote sites"></a>5.Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy <span class="token string">'hexo d'</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h2 id="Blog-Template"><a href="#Blog-Template" class="headerlink" title="Blog Template"></a>Blog Template</h2><p>更高阶更详尽的Hexo Markdown教程参考<a href="https://blog.17lai.site/posts/cf0f47fd/#%E5%B8%B8%E7%94%A8%E6%A0%87%E8%AE%B0">夜法之书的博客</a><br>一些可以用到的LeTax数学公式编辑方式<a href="http://t.csdn.cn/VivVj">超详细 LaTex数学公式</a> || <a href="http://t.csdn.cn/iPVFt">LaTeX数学公式-详细教程</a></p><p>图床就是用csdn了 <span class="github-emoji"><span>😉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> 还有好多文章慢慢搬运过来，不急。<br>但是需要在每个csdn的图片的链接上加上<a href="https://images.weserv.nl/?url=%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%9B%BE%E7%89%87%E9%93%BE%E6%8E%A5LRU%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%89%8D%E7%BC%80%E3%80%82">https://images.weserv.nl/?url=真正的图片链接LRU，这个前缀。</a></p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token operator">!</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span>https://images.weserv.nl/?url<span class="token operator">=</span><span class="token punctuation">)</span><span class="token punctuation">{</span>% raw %<span class="token punctuation">}</span>在这之间写LaTex或者其他造成的符号转义冲突之类的报错<span class="token punctuation">{</span>% endraw %<span class="token punctuation">}</span>toc: <span class="token boolean">true</span>mathjax: <span class="token boolean">true</span>categories: Papertags:  - CCFA - Tierd Memoryimg: summary: hide: <span class="token boolean">false</span>---<span class="token comment">## 1. 论文信息</span><span class="token punctuation">{</span>% note primary %<span class="token punctuation">}</span><span class="token comment">#### Primary Header</span>**文章来自IEEE<span class="token punctuation">(</span>H<span class="token punctuation">)</span><span class="token number">2022</span>属于CCF-A** <span class="token punctuation">{</span>% endnote %<span class="token punctuation">}</span><span class="token comment">### 所有作者及单位</span> - A, 佛罗里达国际大学<span class="token punctuation">(</span>FIU<span class="token punctuation">)</span><span class="token comment">## 2. Background</span><span class="token comment">## 3. 解决了什么问题</span><span class="token comment">## 4. 其他学者解决这个问题的思路和缺陷</span><span class="token comment">## 5. 围绕该问题作者如何构建解决思路</span><span class="token comment">## 6. 从结果看，作者如何有力证明他解决了问题</span><span class="token comment">## 7. 缺陷和改进思路</span><span class="token comment">## 8. 创新点</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>每个标签结束后必须空一行:</p><div class="note info">这里是 info 标签样式</div> <div class="note info no-icon">这里是不带符号的 info 标签样式</div> <div class="note primary">这里是 primary 标签样式</div> <div class="note primary no-icon">这里是不带符号的 primary 标签样式</div> <div class="note warning">这里是 warning 标签样式</div> <div class="note warning no-icon">这里是不带符号的 warning 标签样式</div> <div class="note danger">这里是 danger 标签样式</div> <div class="note danger no-icon">这里是不带符号的 danger 标签样式</div><p>然后是行内标签，比加粗更能显示重点，Fulid移植的。<br><span class="label primary">Label primary</span></p><p><span class="label default">Label default</span></p><p><span class="label info">Label info</span></p><p><span class="label success">Label success</span></p><p><span class="label warning">Label warning</span></p><p><span class="label danger">Label danger</span></p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>this is a test for my first blog</title>
      <link href="/2023/04/30/this-is-a-test-for-my-first-blog/"/>
      <url>/2023/04/30/this-is-a-test-for-my-first-blog/</url>
      
        <content type="html"><![CDATA[<h2 id="詹青云2018年华语辩论世界杯决赛结辩"><a href="#詹青云2018年华语辩论世界杯决赛结辩" class="headerlink" title="詹青云2018年华语辩论世界杯决赛结辩"></a>詹青云2018年华语辩论世界杯决赛结辩</h2><p>大家好，我们今天和对方有三个根本的分歧。一是成功路径不同。我方承认，聚焦没有问题。如果一个年轻人在年轻的时代完全知道一生要什么，一生走下去从不后悔。没问题，挺幸福的。可是现实是，这个决定对于大多数人来说不应该在青年时代做。这个时候你的大脑没有发育完全、你的人生还在不停地变动、你的智识还有限，而这个世界在飞快的变化。很有可能你想要聚焦的东西有一天是你不适应、不喜欢或者被时代淘汰的东西。</p><p>这时候您方跳到了第二点告诉我说没关系，我只要坚定自己的内心我就没有问题了。这就是我们双方第二点分歧：幸福观的不同。您方的幸福观是一种妥协的幸福观，而我放的幸福观是进取的幸福观。您方的意思是不管我人生发挥得怎样，社会如何对待我，不用在乎！我妥协、我看开、我豁达，就可以幸福。对方辩友，那些历史上真正收获了豁达心态的人，杨慎“是非成败转头空”，王维“行到水穷处，坐看云起时”的时候，他们是在什么时候收获这种豁达，是在遍历人生的沧桑，经历了繁华，经过了奋斗，见识了人世中更深刻的道理，他可以领悟到繁华。就算我退一步，俗一点讲，我多读一点书，多看一点世界，对这个世界的理解和思考方式丰富一点，这种做加法的方法您才能收获真的豁达。</p><p>最后我们双方最根本的分歧是对这个时代理解不同。您告诉我说这个世界纷繁复杂，已经把太多选择推到年轻人的面前，所以我选择加就是在随大流。不是。我们仔细想一想，这个时代给我们多的选择不过是您方说的商品、营销课、成功学。可是人生加减法上，那些人生重大关头的选择是什么，这个社会真的在逼我们做加法吗？不是。我到了这个年纪就应该结婚生子，成家立业。在人生重大关头的选择上，这个社会是要求青年人割舍那些不切实际的幻想，割去那些错误的观念，回归一套社会范式，一套人生范式，是要求你做减法的。</p><p>这个时候真正追随自己内心是应该不顾这套范式的束缚，冲破束缚去追寻自己心中所爱，活出一个真正多元的世界。更重要的是，我们今天不是在替一个年轻人的幸福说话，是一代青年人。青年人拓宽人生边界的可能是在拓宽这个社会价值判断的可能。</p><p>既然这个世界号称它是多元而包容的，我们就应该去试，去让他实现这个诺言。</p><p>既然这个社会多元而包容，既然这个世界告诉我们“人不轻狂枉少年”，就没有人应该天然地觉得“轻狂”是一个贬义词。对方辩友一直在劝我们：人生选到自己最幸福的东西才是快乐的。对方辩友，各位，我们都是年轻人。在我们人生的这个阶段，有什么东西是唯一珍贵的？什么叫“欲买桂花同载酒，终不似，少年游”，什么叫“旧游无处不堪寻。无寻处，惟有少年心”。那个唯一带不走的东西，不就是青春本身吗？这一份机会你不珍惜，这一份可能你不珍惜，您跟我谈的是什么？是那一份安顿了的幸福，是那一份成熟了的幸福。可是这不是年轻人的幸福。因为年轻人不是在替你一个人谋幸福，不是你一个人看开了就可以。他是要为这个世界拓宽边界，是让所有的人都有机会把道路越走越宽，是</p><p>趁着年轻，我偏要勉强。</p><p>谢谢大家。</p>]]></content>
      
      
      <categories>
          
          <category> Markdown </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Perspective </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Survey of Non-Volatile Main Memory Technologies:State-of-the-Arts, Practices, and Future Direction</title>
      <link href="/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/"/>
      <url>/2023/03/08/A-Survey-of-Non-Volatile-Main-Memory-Technologies-State-of-the-Arts-Practices-and-Future-Direction/</url>
      
        <content type="html"><![CDATA[<blockquote><p>非易失性内存最近几年能在web of science查到的综述是寥寥无几的，虽然这篇的有些引用也是十多年前的数据，但是作为学习一个阶段的总结，和大佬对比一下在知识结构上的完整度，还有什么是不清楚的。还是挺有用的，不过这里没有写第三章节。欢迎大家分享批评指正这篇翻译。</p></blockquote><div class="note primary">文章来自Journal of Computer Science and Technology, (JCST), 2021</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Hai-Kun Liu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Di Chen, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Hai Jin, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Xiao-Fei Liao, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Binsheng He, 新加坡国立大学</li><li>Kan Hu, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li><li>Yu Zhang, 华中科技大学计科院,集群和网格计算实验室,服务计算技术与系统实验室,国家大数据技术与系统工程技术研究中心</li></ul><hr><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>非易失性主存储器(NVMM )最近已成为未来存储系统的一种有前途的技术。通常，NVMM具有许多理想的属性，例如高密度、字节寻址、非易失性、低成本和能效，但代价是高写入延迟、高写入功耗和有限的写入耐用性(写寿命短)。NVMM已经成为动态随机存取存储器(DRAM )的强有力的替代品，并将从根本上改变内存系统的格局。它们在系统架构设计、操作系统内存管理以及混合内存系统的编程模型方面带来了许多研究机会和挑战。在本文中，我们首先回顾了新兴NVMM技术的概况，然后对NVMM技术的最新研究进展进行了综述。我们根据不同的维度(如内存架构、数据持久性、性能改进、节能和磨损均衡)对这些研究进行分类。其次，为了展示构建NVMM系统的最佳实践，我们从架构、系统和应用的维度介绍了我们最近的混合存储系统设计工作。最后，我们对NVMM的未来研究方向提出了展望，并对设计挑战和机遇进行了阐述。</p><p><strong>关键词</strong>：非易失性存储器、持久性存储器、混合存储器系统、存储器层次结构<br>non-volatile memory, persistent memory, hybrid memory systems, memory hierarchy</p><hr><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>在大数据时代，内存计算越来越受到数据密集型应用的青睐。内存子系统对现代计算系统的功能和性能具有越来越大的影响。使用DRAM (动态随机存取存储器)的传统大内存系统[1 ,2]在功率和密度方面面临严峻的可扩展性挑战[3]。尽管DRAM规模从2013年的28nm持续到2016年的10+ nm[4，5] ，扩展已经放缓，变得越来越困难。此外,最近的研究[6-10]表明，基于DRAM的主存储器约占物理服务器总能耗的30%-40%。新兴的非易失性主存储器(NVMM )技术，如相变存储器(PCM)、自旋转移扭矩存储器(STT-RAM )和3D XPoint[11]通常提供比DRAM更高的内存密度、更低的每的比特成本和待机功耗。NVMM技术的出现有可能弥合慢速持久存储(即磁盘和SSD)与DRAM之间的差距，并将从根本上改变存储系统的格局。</p><p>表1显示了闪存SSD、DRAM、PCM、STT-RAM、ReRAM和Intel Optane DC持久内存模块(DCPMM)的不同内存特点，包括读/写延迟、写耐久性和待机功耗[7，12，13]。尽管NVMM在密度和能耗方面具有各种优势，但其写入延迟比DRAM高约6倍-30倍， 写入功耗比DRAM高约5倍- 10倍。此外，NVMM的写入耐久性非常有限(约$10^8$倍) ，而DRAM能够承受约$10^{16}$次的写入操作。这些缺点使得很难直接替代DRAM。使用NVMM的一种更实用的方法是混合内存架构，由DRAM和NVMM组成[15,16]。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/519ae688bfd74036bb3b66ae451bbc86.png"></p><p>为了充分发挥两者在混合内存系统中的优势，在性能提升、节能降耗、磨损均衡、数据持久性等方面存在许多开放的研究问题。为了解决这些问题，已经有许多关于内存层次结构的设计[15-18]、内存管理[19-21]和内存分配方案[22-24]的研究。这些研究成果导致了混合内存架构、操作系统和编程模型的创新。尽管学术界和工业界已经做出了大量工作来将新兴NVMM集成到存储器层次结构中，但仍有许多挑战需要解决。</p><p>另一方面，先前对NVMM技术的研究大多基于模拟/仿真NVMM器件，与真正非易失性(双列直插式内存模块Dual In-line Memory Modules)DIMM相比，NVMM设备承诺的性能可能存在各种偏差。最近宣布推出的Intel Optane DCPMM终于将NVMM DIMM商业化。真正的Intel Optane DCPMM与之前的研究预期承诺的功能相比，表现明显不同[18，20，26，28]。例如，如表1所示，Intel Optane DCPMM的读取延迟比DRAM高2倍-3倍,而其写入延迟甚至低于DRAM。单个Optane DCPMM DIMM的最大读写带宽分别为6.6GB/s和2.3 GB/s，而DRAM的读写带宽之间的差距要小得多(1.3x)。此外，随着系统中并行线程数量的增加，读/写性能是非单调的[25]。在他们的实验中，1个到4个线程之间达到了峰值性能，然后逐渐下降。由于Optane DCPMM DIMM的这些关键特性，以前关于持久性内存系统的研究应该重新审视和优化，以适应真正的NVMM DIMM。</p><p><strong>贡献</strong>。本文首先回顾关于混合内存架构、操作系统级混合内存管理和混合内存编程模型的最新研究现状。表2显示了NVMM技术的最新研究分类。我们根据不同维度对这些研究进行分类，包括内存架构、持久内存(PM)管理、性能改进、节能、磨损均衡、编程模型和应用程序。我们还讨论了它们的相似性和差异，以突出设计挑战和机遇。其次，为了展示构建NVMM系统的最佳实践，我们<strong>从架构、系统和应用的维度展示了我们在混合存储系统设计方面的努力</strong>。最后，我们提出了在实际应用场景中使用NVMM的未来研究方向，并对研究领域的设计挑战和机遇进行了一些说明。</p><p>尽管有其他关于NVMM的研究，但鉴于NVMM的快速发展，这篇综述从一个独特的角度对 NVMM进行了回顾。在[97]中, 作者介绍了PCM技术的最新研究。以解决有限的写入耐久性、潜在的长延迟、高能量写入、功耗等问题以及一些对内存隐私的担忧。在[98]中，作者对PCM设备及其架构和软件进行了全面的调查和回顾。其他一些有趣的调查侧重于在架构上将四种NVM技术(PCM、MRAM、FeRAM和ReRAM )集成到现有存储层次结构[99]中，或将NVM用于存储和主存储器系统的软件优化[100]。我们的调查与那些调查有三个不同之处。首先，先前的研究[97 ,98]从计算机架构的角度关注PCM设计。相比之下，我们的论文主要从存储器层次、系统软件和应用的维度来研究使用混合存储器的系统。其次，我们的论文包含了更多新发表的期刊/会议论文的评论。特别是，我们对新发布的Intel Optane DCPMM设备进行了更多研究。第三，我们介绍了最近关于存储器系统的近期经验，以阐明未来混合存储器系统的挑战和机遇。</p><p>本文的其余部分组织如下。第2节描述了现有的由DRAM和nvmm组成的混合内存架构。第3节介绍了NVMM中数据持久性保证的挑战和当前解决方案。4节介绍了混合存储器系统中性能优化和节能的最新研究。第5节介绍NVMM写耐久性的研究。第6节介绍了研究NVMM技术所做的努力和实践。在第7节中，我们讨论了NVMM的未来研究方向。我们在第8节结束本文。</p><h1 id="2-Hybrid-Memory-Architectures混合内存架构"><a href="#2-Hybrid-Memory-Architectures混合内存架构" class="headerlink" title="2. Hybrid Memory Architectures混合内存架构"></a>2. Hybrid Memory Architectures混合内存架构</h1><p>已经有很多关于混合存储器架构的研究。通常，主要有两种混合存储器架构，即水平和分层[18]，如图1所示。</p><h2 id="2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构"><a href="#2-1-Horizontal-Hybrid-Memory-Architectures水平混合存储器体系结构" class="headerlink" title="2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构"></a>2.1 Horizontal Hybrid Memory Architectures水平混合存储器体系结构</h2><p>许多DRAM/NVMM混合存储器系统[14 ,15 ,31]通过OS在平面(单个)存储器地址空间中管理DRAM和NVMM，并将它们两者用作主存储器[31 ,32]。为了提高数据访问性能，这些混合内存系统需要通过将频繁访问的(热)NVMM页面迁移到DRAM来克服NVMM的缺点，如图1 (a )所示。需要开发内存访问监控机制来指导页面迁移。</p><p><strong>内存访问监控</strong>。Zhang和Li[31]使用多队列算法对页面的热度进行分类，并将热页面和冷页面分别放置在DRAM和NVMM中。Park等人[32]也主张用于管理DRAM和NVMM的水平混合存储架构。此外，他们还提出了三种优化策略来降低混合存储系统的能耗。他们以非常细的 DRAM 行粒度监视内存数据，并定期检查每个DRAM行中的访问计数器。根据计数器将数据写回NVMM，以减少DRAM刷新的能耗。在再次访问数据之前，数据不会从NVMM缓存到DRAM。脏数据尽可能长时间地保存在DRAM中，以减少DRAM和NVMM之间的数据交换开销以及NVMM的昂贵写操作。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/e565e90afda244f6a8dcfbe3081068c1.png"></p><p><strong>页面迁移</strong>。针对不同的优化目标，已经提出了许多页面迁移算法。Soyoon等人[33]认为，在识别热页时，NVMM写入的频率比数据访问的最近性更重要，并提出了一种称为CLOCK with Dirty bits and Write frequency (CLOCK-DWF )的页替换算法。对于每个NVMM写入操作, CLOCK-DWF需要首先将相应的页面提取到DRAM，然后在DRAM中执行写入。这种方法可能会导致许多不必要的页面迁移，从而给NVMM带来更多的能耗和写回操作。Salkhordeh和Asadi[34]考虑了内存写入和读取，以迁移有利于性能和节能的热页面，并使用两个最近最少使用的(LRU)队列分别选择DRAM和NVM中的受害页面。Yoon等人基于行缓冲区位置进行了页面迁移，其中行缓冲区命中率低的页面被迁移到DRAM，而行缓冲区点击率高的页面仍保留在NVMM中。Li等人提出了一种实用模型，用于基于实用程序定义来指导页面迁移，该实用程序定义基于许多因素，如页面热性、内存级别并行性和行缓冲区位置。Khouzani等人考虑了程序的内存布局和内存级并行性，以迁移混合内存系统中的页面。</p><p><strong>架构限制</strong>。在水平混合存储器架构中管理NVMM和DRAM有几个挑战。</p><p>首先,页面级内存监控成本高昂。一方面，由于当今的商品x86系统不支持页面粒度的内存访问监控，因此硬件支持的页面迁移方案需要大量的硬件修改来监控内存访问统计[14,15,33]。另一方面, OS层的内存访问监控通常会导致显著的性能开销。许多操作系统在页面表条目(PTE)中为每个页面维护一个“已访问”位，以标识该页面是否被访问。然而，该位不能真实地反映页面访问的最近性和频率。因此，一些基于软件的方法将禁用Translation Lookaside Buffer (TLB )[102]来跟踪每个内存引用。这种页面访问监控机制通常会导致显著的性能开销，甚至抵消混合内存系统中页面迁移的好处。</p><p>第二，页面迁移成本也很高。一次页迁移可能导致多次页读/写操作(代价高昂)。由于页面可能只包含一小部分热数据，因此由于内存带宽和DRAM容量的浪费，页面粒度的迁移成本相对较高。</p><p>第三，热页面检测机制可能需要很长时间来预热页面，从而降低页面迁移的收益。此外，对于某些不规则的内存访问模式，热页面预测可能不准确，从而导致不必要的页面迁移。</p><h2 id="2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构"><a href="#2-2-Hierarchical-Hybrid-Memory-Architectures分层混合存储器体系结构" class="headerlink" title="2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构"></a>2.2 Hierarchical Hybrid Memory Architectures分层混合存储器体系结构</h2><p>许多研究建议通过分层缓存/内存架构来组织DRAM和NVMM[16,38,39]。他们使用DRAM作为NVMM的缓存，如图1 (b )所示。DRAM缓存对操作系统和应用程序是不可见的，完全由硬件管理。</p><p>Qureshi等人[16]提出了一种由大尺寸PCM和小尺寸DRAM组成的分层混合存储系统。DRAM缓存包含最近访问的数据，以减少最昂贵的NVMM访问，而大容量NVMM存储大部分所需数据，以避免在应用程序执行期间进行昂贵的I/O操作。类似地，Mladenov[38]设计了一个具有小容量DRAM缓存和大容量NVMM的混合存储系统，并基于应用程序数据的空间位置对其进行管理。DRAM作为按需缓存进行管理，并通过LRU算法进行替换。Loh和Hill[39]以缓存行的粒度管理DRAM，以提高DRAM缓存的效率，并使用组连接方式将NVMM数据映射到DRAM缓存。他们将元数据(标记)和数据放在同一个存储行中，以便可以快速访问缓存命中的数据，并减少标记查询的性能开销。</p><p>在这种内存结构中，由于DRAM被组织为N路集合关联缓存，因此需要额外的硬件来管理DRAM缓存。例如，需要SRAM存储器来存储DRAM高速缓存中数据块的元数据(即标签)，并且需要硬件查找电路来查找DRAM高速缓冲存储器中所请求的数据。因此，为了访问DRAM缓存中的数据，需要两个内存引用，一个用于访问元数据，另一个用于实际数据。为了加速元数据访问，Qureshi等人使用了高速SRAM来存储元数据。Meza等人[40]通过将元数据放在同一DRAM行中的数据块旁边，降低了标记存储的硬件成本。他们还建议使用片上元数据缓冲区将频繁访问的元数据缓存在小型SRAM中。</p><p><strong>架构限制</strong>。尽管分层混合存储器架构通常比单独访问NVMM中的数据的场景提供更好的性能，但在运行具有较差局部性的工作负载时，它可能会导致性能显著下降[103]。原因是大多数硬件管理的分层DRAM/NVMM系统为了简化而利用基于按需的数据提取策略，因此DRAM缓存位于存储器分层的关键数据路径中。如果数据块未命中DRAM缓存，则无论页面热度如何，都必须将其从NVMM提取到DRAM。这种缓存填充策略可能会导致DRAM和NVMM之间频繁的数据交换(类似于缓存抖动问题)。另一方面，硬件管理的缓存架构不能充分利用DRAM容量。由于DRAM缓存被设置为关联的，因此每个NVMM数据块被映射到一个固定的集合。当集合已满时,它必须在将新的NVMM数据块提取到DRAM之前驱逐数据块，即使其他缓存集合为空。</p><h2 id="2-3-Intel-Optane-DCPMM的体系结构"><a href="#2-3-Intel-Optane-DCPMM的体系结构" class="headerlink" title="2.3 Intel Optane DCPMM的体系结构"></a>2.3 Intel Optane DCPMM的体系结构</h2><p>最近发布的Intel Optane DCPMM与DRAM结合使用时支持水平和分层混合内存结构。OptaneDCPMM DIMM目前有两种操作模式:内存模式和应用程序直接模式[25]。这些模式中的每一种对于特定的用例都有其优点。</p><p><strong>内存模式</strong>。在这种模式下，DCPMM充当<strong>大容量的主存储器</strong>。操作系统(OS)将DCPMM识别为传统DRAM，<strong>并禁用DCPMM的持久性功能</strong>。如果将传统DRAM与DCPMM结合使用，它将隐藏在操作系统中，并充当DCPMM的缓存层。因此，DCPMM和DRAM实际上被组织在分层混合存储器架构中。内存模式的主要优点是提供在内存总线通道上使用的优越内存容量。这种模式强烈强调在不修改上层系统的情况下围绕内存空间构建大容量存储环境以及应用程序。推荐的用例是扩展主内存容量，以实现更好的基础设施扩展，例如用于大数据应用程序的并行计算平台(MapReduce、图形计算)。</p><p><strong>AD模式</strong>。在这种模式下，DCPMM为操作系统和应用程序提供了所有持久性特性。操作系统将DRAM和DCPMM分别作为主存储器和持久存储向应用程序公开。与DCPMM混合的传统DRAM仍然充当应用程序的标准DRAM，而DCPMM也被分配到存储器总线以实现更快的存储器访问。DCPMM用作两种名称空间之一:直接访问存取(DAX)和块存储。以前的namespace是字节可寻址的持久存储，应用程序通过特殊的 api 直接访问 。因此，DCPMM 和 DRAM 在这种模式下被逻辑地组织在一个水平混合内存架构中。后一个命名空间将DCPMM 作为区块存储设备提供给应用程序，类似于SSD，但是可以通过更快的内存总线访问。应用程序直接模式强调减少延迟和提高带宽的优势，比NVMe快 2.7倍。推荐的用例适用于大型内存数据库，这些数据库需要满足数据持久性的要求。</p><p>还有一种结合了内存模式和应用程序直接模式的混合内存模式。DCPMM的一部分容量用于内存模式操作,DCPMM剩余的容量用于应用程序直接模式操作。这种混合内存模式为管理不同应用场景的混合内存系统提供了一种更灵活的方法。</p><h2 id="2-4-Summary总结"><a href="#2-4-Summary总结" class="headerlink" title="2.4  Summary总结"></a>2.4  Summary总结</h2><p>上述两种混合存储器架构对于不同的场景有各自的优缺点。通常，分层架构更适合具有良好数据局部性的应用程序，而平面可寻址架构更适用于延迟不敏感或占空间较大的应用程序。关于哪种架构比另一种架构更好，目前尚无定论。实际上，Intel Optane DCPMM支持分层和平面可寻址混合内存架构。当前DCPMM的一个限制是，在重新配置DCPMM模式后，系统需要重新启动。如果一个可重新配置的混合内存系统能够以及时有效的方式动态地适应不同的场景，对于应用程序来说可能是有益的和灵活的。这可能是NVMM器件的一个有趣的研究方向。</p><h1 id="4-Performance-Improvement-and-Energy-Saving性能改进和节能"><a href="#4-Performance-Improvement-and-Energy-Saving性能改进和节能" class="headerlink" title="4 Performance Improvement and Energy Saving性能改进和节能"></a>4 Performance Improvement and Energy Saving性能改进和节能</h1><p>由于NVMMs显示出更高的访问延迟和写入能耗，已经有很多关于NVMMs的性能改进和节能的研究[32-34 ,63 ,65 ,66] 。这些研究可分为三类:减少NVMM写入次数、减少NVM写入本身的能耗以及通过页面迁移减少DRAM的能耗。</p><h2 id="4-1-NVMM-Write-Reduction-NVMM写入减少"><a href="#4-1-NVMM-Write-Reduction-NVMM写入减少" class="headerlink" title="4.1 NVMM Write Reduction NVMM写入减少"></a>4.1 NVMM Write Reduction NVMM写入减少</h2><p>为了减少NVMM写入,分层结构显然更合适,因为DRAM缓存减少了大量NVMM写入。为此,开发了两种主要技术,即页面迁移和绕过NVMM写入。</p><p><strong>页面迁移</strong>。页面迁移[14 ,15 ,33 ,58 ,59]策略主要基于写入次数和每个页面的最近访问频率来选择要迁移的页面。它们的主要区别在于触发页面迁移的条件。</p><p>PDRAM[15]根据写入次数将PCM页迁移到DRAM。在PDRAM中,存储器控制器维护一个表以记录每个PCM页的访问计数。如果写入PCM页的次数超过给定阈值,则触发页故障，然后将该页从PCM页迁移到DRAM。</p><p>CLOCK-DWF[33]将页面的写入历史集成到CLOCK算法中。当发生页面错误时，虚拟页面将从磁盘提取到PCM。否则, 该页将在DRAM中分配,因为该页可能是写密集型页。</p><p>RaPP[14]根据页面的等级在DRAM和PCM之间迁移页面。在RaPP中, 页面按访问频率和最近度排序。排名靠前的页面从PCM迁移到DRAM。因此，频繁写入的页面被放置在DRAM中,而很少写入的页面则被放置在PCM中。此外, RaPP还将任务关键页面放置在DRAM中，以提高应用程序性能。通过监视LLC中每个页面的写回操作的数量,存储器控制器能够跟踪每个页面的访问频率和最近性。RaPP根据多队列(MQ)算法对页面进行排序[118]。传统MQ定义了多个最近最少使用(LRU)队列。每个LRU队列是一个页面描述符队列, 其中包括参考计数器和逻辑过期时间。当第1次访问页面时,页面将移动到队列0的尾部。如果页面的引用计数达到$2^{i+1}$ ,则提示页面排队i+1。一旦PCM页面被移动到队列5,它就被迁移到DRAM。</p><p><strong>缓冲NVMM写入</strong>。在混合内存系统中, 缓存能够减少对NVMM的大量写入。适当的缓存替换策略不仅可以提高应用程序性能, 还可以降低NVMM的能耗。先前的研究[7,18]发现, 缓存中的许多块在被从缓存中逐出之前不会被再次使用。这些块称为死块，并消耗宝贵的缓存容量。DASCA[7]提出了一种死块预测方法,以减少STT-RAM缓存的能耗。退出这些死块将减少对STT-RAM缓存的写入,并且不会影响缓存命中率。WADE[62]进一步利用了NVMM读取和NVMM写入之间的能耗不对称性。由于NVMM写入操作比NVMM读取操作消耗更多的能量,因此频繁写入的块应保留在缓存中。WADE将缓存中的块分为两类:频繁回写的块和非频繁回写块。非频繁写回的块被替换,以提供更多机会将其他数据块保留在缓存中。</p><h2 id="4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低"><a href="#4-2-NVMM-Energy-Consumption-Reduction-NVMM能耗降低" class="headerlink" title="4.2 NVMM Energy Consumption Reduction NVMM能耗降低"></a>4.2 NVMM Energy Consumption Reduction NVMM能耗降低</h2><p>由于NVMM写入显示的能耗是NVMM读取的能耗的几倍，因此在降低NVMM写入的能耗方面已经做出了许多努力。这些方法可以分为两类:差分写入(仅写入脏位而不是整行)和在单个写入期间并行多个写入。</p><p>如果要写入的位数超过缓存行中总位数的一半，则Flip-N-Write[64]尝试通过翻转位来减少PCM写入能耗。在一次写入期间, 如果行中超过一半的位被写入 ,则每个位被翻转,因此位翻转不超过总位的50%。同时,设置标记位以识别行中的位是否被翻转。当读取行时,标记位用于确定行中的位是否应该翻转。与Flip-N-Write类似, Andrew等人[73]提倡细粒度写入。它只监视脏位而不是一行中的所有位。一个叫做PCM的新术语引入功率令牌以指示单次写入期间的电源。假设为每个芯片分配Plimit Watts功率, 并且每个位写入需要Pbit Watts , Plimitpbit可以同时写入位。在芯片内,可以同时写入银行。在单个写入期间,如果多个写入请求位于不同的存储库中,并且总功耗不超过Plimit ,则可以同时执行这些写入。因此，细粒度写入不仅减少了NVMM写入，而且通过实现更高的存储体并行性来提高系统性能。</p><p>一些研究[65 ,66]通过分离SET和RESET操作来提高NVMM的能量效率。由于NVMM写入1比写入0消耗更多的能量和时间，如果以正确的方式执行这些写入,则可以减少写入延迟和能耗。三阶段写入[65]将写入操作分为比较阶段、写入零阶段和写入一阶段。在比较阶段,利用Flip-N-Write机制来减少写入次数。零位和一位分别在写零级和写一级中被分别写入。因为在大多数工作负载中,零写操作占了大部分写操作，所以Tetris write[66]进一步考虑了SET和RESET操作的不对称性, 并行调度代价高昂的写操作。在功率约束下, 写零操作被插入到写操作的剩余间隔中。</p><p>CompEx[67]提出了一种压缩扩展编码机制,以减少MLC/TLC NVMM的能耗。为了提高MLC/TLC单元的寿命,首先压缩数据以减少数据冗余。然后将扩展码应用于压缩数据并写入物理NVMM单元。对于具有8个状态的TLC单元，状态0、1、6和7称为终端能量状态，而状态2、3、4.和5称为中心能量状态。中央能量状态消耗更多的时间和能量，因为它们需要更多的编程和验证迭代。CompEx利用扩展代码仅对NVMM电池使用终端能量状态。由于在编程MLC/TCL单元时，终端能量状态需要比中心能量状态更少的能量和时间，所以这一想法受到了激励。混合片上缓存也被提出以减少CPU的功耗。RHC[68]构建了一个混合缓存，其中SRAM和NVMM中的每种方式都可以独立地打开或关闭。如果一行很长时间没有被访问，该行将被关闭，而其标签仍处于打开状态，以跟踪该行的访问。当对标签的访问超过阈值时，该行将通电。为了更好地利用高性能SRAM和低动态功耗NVMM，RHC对SRAM和NVMM采用不同的阈值。</p><h2 id="4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低"><a href="#4-3-DRAM-Energy-Consumption-Reduction-DRAM能耗降低" class="headerlink" title="4.3 DRAM Energy Consumption Reduction DRAM能耗降低"></a>4.3 DRAM Energy Consumption Reduction DRAM能耗降低</h2><p>在只有DRAM的存储系统中, 静态能耗可以占存储系统总能耗的一半以上[69-71]。在混合存储器系统中,页面迁移技术被广泛用于减轻DRAM的能耗。非活动页面可以从DRAM迁移到NVMM，以便空闲的DRAM组可以断电。当页面稍后变为活动时, 它将再次迁移到DRAM。然而, 如果页面迁移没有正确执行,DRAM列组可能会频繁断电并重新激活。额外的能耗可能会抵消页面迁移带来的好处。</p><p>为了减少混合存储器系统的能耗，RAMZzz[8]揭示了高能耗的两个主要根源。一个是活动页面的稀疏分布,另一个是页面迁移可能不有效,因为DRAM的多能量状态之间的传输会引入额外的能量消耗。为了解决前一个问题, RAMZzz使用多个队列将具有类似活动的页面收集到同一个DRAM列中，从而避免频繁的能量状态转移。多个队列具有L个LRU队列来记录页面描述符。页面描述符包含一段时间内页面的ID和访问(读和写)计数。为了减少数据迁移的能量开销，将具有类似内存访问行为的页面重新组合在一起。这样，需要将页面分配给新的bank。RAMZzz在banks间并行迁移这些页面。</p><p>Refree[72]通过避免DRAM刷新，进一步降低了混合存储器系统中的DRAM能耗。当DRAM行需要刷新时,这意味着该行很长时间没有被访问。行中的数据已过时,不久以后不太可能再次访问。Refree将这些行逐出PCM，而不是在DRAM中刷新它们。在Refree中,所有行都会定期监视。此周期的间隔等于DRAM行自上次刷新以来的保留时间的一半。因此，行分为活动行和非活动行。激活行在访问。非活动行被逐出PCM，从而消除DRAM刷新。</p><h1 id="5-Write-Endurance-Improvement-写入耐久性改进"><a href="#5-Write-Endurance-Improvement-写入耐久性改进" class="headerlink" title="5. Write Endurance Improvement 写入耐久性改进"></a>5. Write Endurance Improvement 写入耐久性改进</h1><p>在混合存储器系统中,主要有两种策略来克服NVMM的有限写入耐久性。一个是减少NVMM写入，另一个是磨损均衡,它在所有NVMM单元之间均匀分布写入流量。</p><h2 id="5-1-Write-Reduction写入减少"><a href="#5-1-Write-Reduction写入减少" class="headerlink" title="5.1 Write Reduction写入减少"></a>5.1 Write Reduction写入减少</h2><p>已经提出了许多用于改善NVM寿命的写减少策略，包括数据迁移[8、14、15]、 缓存或缓冲[16]和内部NVM写减少[64、73、74]。</p><p>提出了一种延迟写入机制[16],以减少对PCM的写入。在分层混合存储器系统中,DRAM缓冲器用于隐藏高延迟PCM访问。当发生页面错误时, 数据将直接从磁盘提取到DRAM缓存中。在从DRAM高速缓存中逐出页面之前,页面不会写入PCM。行级写入还可以减轻NVMM上的写入操作, 从而减少NVMM的磨损[16]。对于内存密集型工作负载,写操作可能集中在几行中。通过跟踪DRAM中的缓存行，只有脏行被写回PCM,而不是页面的所有行。提出了内存压缩机制[67 ,75] ,以提高MLC/TLC NVMM的寿命。在写入NVMM单元之前, 首先压缩数据。因此,只有一小部分NVMM单元被写入。然而, 耐久性的提高是以性能适度下降为代价的。如果NVMM单元以较低的功耗写入, 则该单元可以以较高的写入延迟为代价维持更多的写入。具体地, 当写入NVMM单元的速度下降N倍时,单元的耐久性可以提高N到N3倍。Mellow Write[76]探索了这一功能，以提高NVMM的寿命。为了减轻性能下降, Mellow Write只采用只有一次写入操作的缓慢的存储体写入。</p><h2 id="5-2-Wear-Leveling磨损均衡"><a href="#5-2-Wear-Leveling磨损均衡" class="headerlink" title="5.2 Wear-Leveling磨损均衡"></a>5.2 Wear-Leveling磨损均衡</h2><p>与减少写入的方法不同, 磨损均衡在所有NVMM页面之间均匀分布写入。尽管写入总数没有减少,磨损均衡技术网以防业菊些页面因密集的写八雨快速磨损。</p><p>对于NVMM ,我们可以记录每行的写入计数,以指导磨损均衡策略。但是,不能忽略外部存储开销。Start Gap77]提出了一种细粒度磨损均衡方案。PCM页的行以旋转方式存储。在0和15之间随机生成旋转值,以指示移位的位置。对于具有16行的PCM页面,旋转值的范围可以从0到15。当旋转值为0时,页面存储在其原始地址中。如果旋转值为1 ,则第0行存储在第1行的物理地址中,并且每-行的地址都被旋转值移位。</p><p>在PDRAM1S]中,磨损均衡由写入计数阈值触发。当页面的写入计数超过给定阈值时,将触发页面交换中断以将页面迁移到DRAM。交换的PCM页面被添加到列表中， 这些页面将再次重新定位。</p><p>Zombie[78]提供了另一个实现福利水平的方向,并进一步延长了PCM的整体寿命。除了在PCM单元之间均匀分配写入的StartGap之外, Zombie利用禁用页面中的空闲块为工作内存提供更多的纠错资源。当PCM单元磨损时,它变得不可用。由于从软件的角度来看,内存占用被组织在页面中,因此包含故障单元的整个页面被禁用。但是，如果提供了一些备用单元格来替换出现故障的单元格,则可以再次使用该页面。这些备用单元称为纠错资源。当所有备用单元格耗尽时, 最终放弃包含失败单元格的页面。通常，当页面被禁用时,大约有99%的位可用。Zombie利用禁用页面中的大量好比特作为备用纠错资源,其中好比特被组织在细粒度块中。通过将工作页面与纠错资源配对, Zombie可以延长NVMM的使用寿命。</p><p>DRM[79]在虚拟地址空间和物理NVMM地址空间之间添加了中间映射层。在中间地址空间中，一个页面可能映射到PCM中的一个好页面或两个有故障的兼容PCM页面。兼容页面意味着一对具有错误字节的页面,但这些错误字节都不位于两个页面的同一位置。因此，两个兼容的页面可以被组合成一个新的好页面通过这种方式, DRM将PCM寿命显著提高了40倍。</p><h1 id="6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践"><a href="#6-Practices-of-Hybrid-Memory-System-Designs混合存储系统设计实践" class="headerlink" title="6 Practices of Hybrid Memory System Designs混合存储系统设计实践"></a>6 Practices of Hybrid Memory System Designs混合存储系统设计实践</h1><p>在本节中，我们从内存架构、OS支持的混合内存管理和NVMM支持的应用程序的角度介绍了我们最近在NVMM系统设计和优化方面的努力和实践，如图5所示。在下文中，我们将简要介绍我们的实践。</p><h2 id="6-1-Memory-Architectural-Designs记忆建筑设计"><a href="#6-1-Memory-Architectural-Designs记忆建筑设计" class="headerlink" title="6.1 Memory Architectural Designs记忆建筑设计"></a>6.1 Memory Architectural Designs记忆建筑设计</h2><p>在本小节中，我们将介绍我们对混合内存模拟和仿真、硬件/软件协同混合内存架构、细粒度NVM压缩和磨损均衡以及混合内存感知片上缓存管理的研究。</p><h3 id="6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真"><a href="#6-1-1-Hybrid-Memory-Architectural-Simulation混合存储器体系结构仿真" class="headerlink" title="6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真"></a>6.1.1 Hybrid Memory Architectural Simulation混合存储器体系结构仿真</h3><p>混合存储器体系结构仿真是研究混合存储器系统的先决条件。我们将zsim[27]与NVMain[20]集成, 以构建全系统架构模拟器。Zsim是用于x86-64多核架构的快速处理器模拟器。它能够对多核、片上缓存层次结构、缓存-致性协议(如MESI)、片上互连拓扑网络和物理内存接口进行建模。Zsim使用Intel Pin工具包收集进程的内存跟踪,然后回放内存跟踪以表征内存访问行为。NVMain是用于NVMM的架构级主存储器模拟器。它能够模拟不同的内存配置文件, 如读/写延迟、带宽、功耗等。它还支持子阵列级内存并行性和不同的内存地址编码方案。此外，NVMain还可以对混合存储器(如DRAM和存储器层次结构中的不同NVMM)进行建模。由于操作系统级内存管理不是由zsim模拟的,因此我们通过添加Translation Lookaside Buffer (TLB)和内存管理模块(如伙伴内存分配器和页表)来扩展zsim , 以支持全系统模拟。实施细节参考我们的开源软件。我们的工作为研究界提供了一个快速、完整的体系结构仿真框架。它可以帮助研究人员了解不同的NVMM特性,设计混合存储系统,并以简单高效的方式评估各种系统设计对应用程序性能的影响。</p><h3 id="6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器"><a href="#6-1-2-Lightweight-NVMM-Performance-Emulator轻量级NVMM性能仿真器" class="headerlink" title="6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器"></a>6.1.2 Lightweight NVMM Performance Emulator轻量级NVMM性能仿真器</h3><p>当前基于仿真的NVMM技术研究方法太慢,或者无法运行复杂的工作负载,例如并行和分布式应用程序。我们提出HME[28]，一种轻量级NVMM使用非统一内存访问(NUMA )架构的性能仿真器。HME利用商品Intel CPU中可用的硬件性能计数器来模拟较慢NVMM的性能特性。为了模拟NVMM的访问延迟,HME定期向远程NUMA节点上的DRAM访问注入软件生成的延迟。为了模拟NVMM带宽,HME利用DRAM热控制接口在短时间内限制对DRAM通道的内存请求量。不同于另一个NVMM仿真器Quartz[29] ,它不模拟NVMM的写入延迟,HME识别写直通和写回缓存逐出操作, 以分别模拟它们的延迟。通过这种方式,与Quartz相比, HME能够显著减少NVMM访问延迟的平均仿真误差[29]。在真正的NVMM设备Intel Optane DCPMM问世之前, 这项工作可以帮助研究人员和程序员评估NVMM性能特性对应用程序的影响, 并指导混合内存系统的系统设计和优化。</p><h3 id="6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存"><a href="#6-1-3-Hardware-Software-Cooperative-Caching硬件-软件协同缓存" class="headerlink" title="6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存"></a>6.1.3 Hardware/Software Cooperative Caching硬件/软件协同缓存</h3><p>基于我们的混合存储器模拟器， 我们提出了一种称为HSCC[18]的硬件/软件协同混合存储器架构。在HSCC中,DRAM和NVMM在物理上组织在单个存储器地址空间中，并且都用作主存储器。然而,DRAM在逻辑上可以用作NVMM的缓存,也可以由OS管理。图显示了HSCC的系统架构。我们扩展了页表和TLB，以维护NVMM到DRAM的物理地址映射, 从而以缓存/内存层次结构的形式管理DRAMNVMM。通过这种方式, HSCC能够像虚拟到NVMM地址转换一样高效地执行NVMM到DRAM地址转换。此外, 我们在每个TLB条目和页表条目中.添加一个访问计数器，以监视内存引用。与以前在内存控制器或操作系统中监视内存访问的方法不同,我们的设计可以精确地跟踪所有数据访问, 而无需额外的存储(SRAM)和性能开销。我们通过动态阈值调整策略识别频繁访问的(热)页面，以适应不同的应用程序,然后将NVMM中的热页面迁移到DRAM缓存，以获得更高的性能和能效。此外,我们开发了一种基于实用程序的DRAM缓存填充方案,以平衡DRAM缓存的效率和DRAM利用率。由于软件管理的DRAM顶面能够映射到任何NVMM页面,因此DRAM实际.上用作完全关联的缓存。这种方法可以显著提高DRAM缓存的利用率,并且还提供了根据应用程序的动态内存访问行为重新配置混合内存架构的机会。由于CPU可以绕过DRAM缓存直接访问NVMM中的冷数据，因此DRAM既可以用作平面可寻址混合存储器架构中的主存储器,也可以用作分层混合存储器架构。因此,与最先进的工作相比,HSCC可以将系统性能显著提高9.6倍，能耗降低34.3%[16]。我们的工作为实现可重构混合存储器系统提供了第一个架构解决方案,该系统可以在水平和分层存储器架构之间动态改变DRAMNVMM管理。</p><p>我们进一步在HSCC上提出了以下技术,以提高缓存性能并改进磨损均衡机制。</p><p>由于NVMM块的缓存未命中惩罚是DRAM块的几倍,因此在平面可寻址混合存储器体系结构中，缓存命中率不是唯一需要改进的性能指标。为了最好地利用昂贵的LLC，我们提出了一种新的度量，即平均存储器访问时间(AMAT),以评估混合存储器系统的总体性能。我们考虑了DRAM块和NVMM块的非对称缓存未命中惩罚，并提出了一种LLC未命中惩罚感知替换算法,称为MALRU[36 ,37] ,以改进混合存储器系统中的AMAT。MALRU动态地将LLC划分为保留区域和正常替换区域。MALRU优先替换LLC中的死DRAM块和冷DRAM块，使得NVMM块和热DRAM块保持在保留区域中。通过这种方式, 与LRU算法相比,MALRU实现了高达228%的应用程序性，能改进。这项工作展示了混合存储器系统如何影响片上缓存的架构设计。</p><p>为了提高NVMM的写入耐久性，我们提出了一种新的NVMM架构,以支持空间无关数据压缩和磨损均衡[119]。由于许多应用程序的内存块通常包含大量零字节和频繁值, 我们提出了零重复数据消除和频繁值压缩机制(称为ZD-FVC[119] ) , 以减少NVMM上的位写入。ZD-FVC可以集成到NVMM模块中，并完全由硬件实现, 无需任何操作系统的干预。我们在Gem5和NVMain模拟器中实现了ZD-FVC[119]，并使用SPEC CPU2006中的几个程序对其进行了评估。实验结果表明,ZD-FVC比几种最先进的方法要好得多。特别是, 与频繁值压缩相比, DZ-FVC可以将数据压缩比提高1.5倍。与数据比较写入相比，ZD-FVC能够将NVMM上的位写入减少30%，并将NVMM的寿命平均提高5.8倍。相应地, ZD-FVC还平均减少了43%的NVMM写入延迟和21%的能耗。我们的设计以简单高效的方式为NVMM提供了细粒度数据压缩和磨损均衡解决方案。它是其他磨损均衡方案的补充，以进一步提高NVMM寿命。</p><h1 id="6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件"><a href="#6-2-System-Software-for-Hybrid-Memories混合存储器的系统软件" class="headerlink" title="6.2 System Software for Hybrid Memories混合存储器的系统软件"></a>6.2 System Software for Hybrid Memories混合存储器的系统软件</h1><p>在本小节中,我们介绍了软件层混合内存系统的实践，包括对象级混合内存分配和迁移、NUMA感知页面迁移、超级页面支持和NVMM虚拟化机制。</p><h3 id="6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移"><a href="#6-2-1-Object-Migration-in-Hybrid-Memory-Systems混合存储系统中的对象迁移" class="headerlink" title="6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移"></a>6.2.1 Object Migration in Hybrid Memory Systems混合存储系统中的对象迁移</h3><p>页面迁移技术已被广泛用于改善混合存储器系统中的系统性能和能量效率。然而,以前的页面迁移方案都依赖于OS层中昂贵的在线页面访问监控方案来跟踪页面访问的最近性或频率。此外,由于额外的内存带宽消耗和缓存/TLB一致性保证机制,页面粒度上的数据迁移通常会导致非平凡的性能开销。</p><p>为了减轻混合内存系统中数据迁移的性能开销，我们提出了更轻量级的面向对象内存分配和迁移机制,称为OAM[120]。OAM的框架如图7所示。与之前的研究[44 ,121]不同, 我们进一步分析了细粒度时隙中的对象访问模式, 这些研究仅在静态对象放置的全局视图中描述了内存访问行为。OAM利用编译框架LLVM以对象粒度描述应用程序内存访问模式，然后将应用程序的执行分为不同阶段。OAM利用性能能量集成模型来指导不同执行阶段的初始内存分配和运行时对象迁移, 而无需对硬件和操作系统进行侵入性修改以进行在线页面访问监控。我们通过扩展Glibc库和Linux内核开发了新的内存分配和迁移API。基于这些API ,程序员能够将DRAM或NVMM显式分配给不同的对象, 然后迁移访问模式在DRAM和NVMM。我们开发了一个静态代码插入工具,可以自动修改遗留应用程序的源代码,而无需程序员重新设计应用程序。与最先进的页面迁移方法CLOCK-DWF[33]和2PP44相比, 实验结果表明,OAM可以分别显著降低83%和69%的数据迁移成本,并实现约22%和10%的应用程序性能改进。以前的持久内存管理方案通常依赖内存访问评测来指导静态数据放置,以及页面迁移(代价高昂)技术来适应运行时的动态内存访问模式。OAM提供了一种更轻量级的混合内存管理方案,支持细粒度对象级内存分配和迁移。</p><h3 id="6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA-感知混合内存管理"><a href="#6-2-2-NUMA-Aware-Hybrid-Memory-Management-NUMA-感知混合内存管理" class="headerlink" title="6.2.2 NUMA-Aware Hybrid Memory Management NUMA 感知混合内存管理"></a>6.2.2 NUMA-Aware Hybrid Memory Management NUMA 感知混合内存管理</h3><p>在非统一内存访问(NUMA )架构中，不同NUMA节点中应用程序观察到的内存访问延迟通常是不对称的。由于NVMM比DRAM慢几倍，混合存储器系统可以进一步扩大不同NUMA节点之间的性能差距。NUMA系统的传统内存管理机制在混合内存系统中不再有效,甚至可能降低应用程序性能。例如,自动NUMA平衡(ANB )策略总是将远程NUMA节点中的应用程序数据迁移到运行应用程序线程或进程的NUMA节点。然而，由于远程DRAM的访问性能可能甚至高于本地NVMM,ANB可能会错误地将应用数据移动到较慢的位置。为了解决这个问题，我们提出了HiNUMA[60]，这是一种用于混合内存管理的新NUMA抽象。当应用程序数据首次放置在混合存储器系统中时，HiNUMA将应用程序数据放置在NVMM和DRAM上，以分别平衡带宽敏感应用程序和延迟敏感应用程序的内存带宽利用率。总访问延迟。初始数据放置基于NUMA拓扑和混合内存访问性能。对于运行时混合内存管理,我们提出了一个新的NUMA平衡策略,名为HANB[60]，用于页面迁移。HANB能够通过考虑数据访问频率和内存带宽利用率来降低混合内存访问的总成本。我们在Linux内核中实现HiNUMA ,无需对硬件和应用程序进行任何修改。与NUMA架构中的传统内存管理策略和其他最先进的工作相比, HiNUMA可以通过有效利用混合内存来显著提高应用程序性能。从HiNUMA[60]中学到的经验教训也适用于配备真正IntelOptaneDCPMM设备的混合内存系统。</p><h3 id="6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级存储"><a href="#6-2-3-Supporting-Superpages-in-Hybrid-Memory-Systems支持混合存储系统中的超级存储" class="headerlink" title="6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级存储"></a>6.2.3 Supporting Superpages in Hybrid Memory Systems支持混合存储系统中的超级存储</h3><p>随着应用程序占地面积和相应内存容量的快速增长,虚拟到物理地址转换已成为混合内存系统的新的性能瓶颈。在大内存系统中, 超页已被广泛用于减轻地址转换开销。然而,使用超级页面的副作用是，它们通常会阻碍轻量级内存管理,例如页面迁移,而页面迁移在混合内存系统中被广泛用于提高系统性能和能效。不幸的是,同时拥有超级页面和轻量级页面迁移是一个挑战。</p><p>为了解决这个问题,我们提出了一种新的混合内存管理系统Rainbow[41]以弥合超级页面和轻量级页面迁移之间的根本冲突。如图8所示，Rainbow以超页(2MB)的粒度管理NVMM,并将DRAM作为缓存管理以基本页的粒度(4KB)将热数据块存储在超级页中。为了加快地址转换, Rainbow使用了拆分TLB的现有硬件功能来支持超级页面和普通页面。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/13d8d4f2c76f4dc28be42a4424d5faee.png"></p><p>我们提出了一种两阶段页面访问监控机制来识别超级页面中的热基页面。在第一阶段，Rainbow记录所有超级页面的访问计数以识别前N个热超级页面。在第二阶段,我们逻辑上将这些热超页分割成基本页(4KB ) , 并进一步监视它们以识别热基本页。这些方案显著减少了页访问计数器的SRAM存储开销和由于对热基页进行排序而导致的运行时性能开销。通过新的NVMM到DRAM地址重新映射机制, Rainbow能够将热基页迁移到DRAM,同时仍能保证超级页TLB的完整性。拆分的超页TLB和基本页TLB是并行查阅的。我们的地址重映射机制在逻辑上使用超页TLB作为基本页TLB的缓存。由于超级页TLB的命中率通常很高,Rainbow能够显著加快基本页地址转换。为了进一步提高TLB命中率, 我们还扩展Rainbow以支持多个页面大小,并一起迁移相邻的热基页面[42]。与不支持超级页面的最先进混合内存系统[18]相比,Rainbow通过同时使用超级页面和轻量级页面迁移的优势，可以将应用程序性能显著提高最多2.9倍。</p><p>这项工作提供了硬件/软件协同设计，以弥合超级页面和轻量级页面迁移技术之间的根本冲突。这可能是减轻大容量混合存储器系统中不断增加的虚拟到物理地址转换开销的一个有前途的解决方案。</p><h3 id="6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理"><a href="#6-2-4-NVMM-Management-in-Virtual-Machines虚拟机中的NVMM管理" class="headerlink" title="6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理"></a>6.2.4 NVMM Management in Virtual Machines虚拟机中的NVMM管理</h3><p>NVMM有望在云和数据中心环境中更受欢迎。然而,关于将NVMM用于虚拟机(VM)的研究很少。我们提出了HMvisor[61],一种管理程序/虚拟机协同混合内存管理系统, 以有效利用DRAM和NVMM。如图9所示, HMvisor利用伪NUMA机制来支持VM中的混合内存分配。由于VM中的虚拟NUMA节点可以映射到不同的物理NUMA节点, HMvisor可以将不同的内存区域映射到单个VM ,从而向VM暴露内存异质性。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/1402aec0f84e49268d98793dc3a37f4c.png"></p><p>为了支持VM中的轻量级页面迁移,HMvisor监控页面访问计数并分类然后VM通过域间通信机制周期性地收集热页面的信息。我们在VM中实现了一个可加载的驱动程序,以在DRAM和NVMM之间执行进程级页面迁移。由于HMvisor由VM本身执行页面迁移,因此HMvisor无需暂停VM进行页面迁移。HMvisor还提倡混合内存资源交易策略,以动态调整VM中NVMM和DRAM的大小。通过这种方式,HMvisor可以满足多样化应用程序的不同内存需求(容量或性能) ,同时保持VM的总货币成本不变。</p><p>HMvisor的原型在QEMU/KVM平台上实现。我们的评估表明,HMvisor能够以仅5%的性能开销为代价将NVMM写入流量减少50%。此外, 动态内存调整策略可以在VM承受高内存压力时显著减少VM中的主要页面错误,因此甚至可以将应用程序性能提高30倍。</p><p>这是在虚拟化环境中管理混合内存的早期系统工作。所提出的方案完全由软件实现, 因此也适用于新Intel Optane DCPMM设备的混合存储系统。</p><h2 id="6-3-NVMM-Supported-Applications-NVMM支持的应用程序"><a href="#6-3-NVMM-Supported-Applications-NVMM支持的应用程序" class="headerlink" title="6.3 NVMM-Supported Applications NVMM支持的应用程序"></a>6.3 NVMM-Supported Applications NVMM支持的应用程序</h2><p>由于混合存储器系统可以提供非常大容量的主存储器,因此它们已被广泛用于大数据应用,例如内存中的关键值(K-V)存储和图形计算。在本小节中,我们介绍了NVMM支持的针对这些应用程序的系统优化实践。</p><p>具有大容量内存的内存K-V存储可以在主内存中缓存更多的热数据,从而为应用程序提供更高的性能。然而,在混合内存系统中直接部署传统的K-V存储(如memcached)存在若干挑战。例如，如何有效地识别热K-V对象?如何重新设计NVMM友好的K-V索引以减少NVMM写入?如何重新设计缓存替换算法以平衡混合内存系统中的对象访问频率和最近性?如何解决板钙化问题[122] ,以在混合存储器系统中最佳地利用DRAM资源?</p><p>为了解决上述问题，我们提出HMCached[80] ,这是混合DRAMNVMM系统的K-V缓存(memcached)的扩展。图10显示了HMCached的系统架构。HM缓存跟踪K-V对象访问并记录每个K-V对的元数据结构中的进程计数, 因此HMCached可以轻松识别NVMM中频繁访问的(热)对象,并将它们迁移到DRAM。这样, 我们逻辑上将DRAM用作NVMM的专用缓存,以避免更昂贵的NVMM访问。此外, 我们通过拆分基于哈希的K-V索引来重新设计NVMM友好的K-V数据结构, 以进一步减少NVMM访问。我们将K-V对象的频繁更新元数据(例如,引用计数、时间戳和访问计数)放在DRAM中,其余部分(例如，键和值)放在NVMM中。我们利用多队列算法[118]来考虑DRAM缓存替换的对象访问频率和最近性。此外，我们建立了一个基于效用的性能模型来评估板类重新分配的效益。我们的动态slab重新分配策略能够有效解决slab calcification问题,并在数据访问模式发生变化时显著提高应用程序性能。与普通memcached相比HMCached可以显著减少70%的NVMM访问, 并实现大约50%的性能改进。此外，HMCached能够降低75%的DRAM成本,同时性能下降不到10%。</p><p>据我们所知,我们是第一个探索混合存储系统中K-V存储的对象级数据管理的人。我们基于Memcached实现HMCached并打开源代码。我们发现，后来的研究(如flatstore[90] )也有类似的想法来解耦KV存储的数据结构。</p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/b20aefdd545644598b482229209b0f46.png"></p><p>今天,我们已经看到了许多内存中的图形处理系统,其中应用程序的性能与主内存的容量高度相关。高密度和低成本NVMM技术对于降低图形处理的I/O成本至关重要。如图11所示, 与基于SSD的存储系统相比,混合存储系统可以显著提高应用程序性能。图12显示了混合存储器系统和仅DRAM系统之间的应用程序性能差距。我们提出了NGraph，一种新的图形处理框架，专门设计用于更好地利用混合存储器。我们基于不同图形数据的访问模式开发混合内存感知数据放置策略,以减轻对NVMM的随机和频繁访问。通常，图形结构数据占总图形数据的大部分。NGraph根据目标顶点划分图形数据,并采用任务分解机制来避免多个处理器之间的数据争用。此外,NGraph采用了工作窃取机制，以最小化多核系统上并行图形数据处理的最大时间。我们称为ReRAM技术的。</p><p>基于图形处理框架Ligra[123]实现NGraph。与最先进的Ligra相比,NGraph可以将应用程序性能提高48%。从这项工作中获得的经验教训[91]可用于在配备真实PM设备的图形处理平台中进一步提高大规模图形分析的性能。<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/76462968db584a9c8136d9c88d89346e.png"></p><p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/21935e2720af4afeae966b3ce6e3724d.png"></p><h1 id="7-Research-Directions研究方向"><a href="#7-Research-Directions研究方向" class="headerlink" title="7 Research Directions研究方向"></a>7 Research Directions研究方向</h1><p>NVMM技术的出现在材料、微电子、计算机架构、系统软件、编程模型和大数据应用领域引起了许多有趣的研究课题。随着IntelOptaneDCPMM等真正的NVMM设备越来越多地应用于数据中心环境,NVMM可能会改变数据中心的存储环境。我们的经<br>验和做法进行了一些初步和有趣的研究。在下文中,我们分享了NVMM未来研究方向的愿景,并分析了研究挑战和新机遇。图13说明了不同维度NVMM技术的未来趋势。</p><ol><li><p>3D堆叠NVMM技术的发展仍在继续。NVMM有望提供更高的集成密度以降低成本。目前, 高端NVDIMM对于企业应用来说仍然过于昂贵。NVMM与传统DRAM和NAND闪存竞争的关键挑战是存储密度或每字节成本。NVMM技术主要有两种单片3D集成机制[124]。一种是将例如Intel/Micron 3DX点。另一种是垂直3D堆叠结构。然而, 3D集成技术尚未成熟。仍然存在许多挑战,如制造成本、柱电极电阻和潜路径问题。</p></li><li><p>NVMM越来越多地用于分布式共享存储器系统。随着NVMM的密度不断增加，单个服务器中的主内存容量可以达到数百TB。为了提高大容量NVMM的利用率,必须通过远程直接内存访问(RDMA)技术在多个服务器之间共享它们。使用NVMM的典型方法是将来自多个服务器的所有可共享内存聚合到混合共享内存资源池中, 例如Hotpot[49 ,125 ,126]。所有内存资源在全局内存空间中共享。有一些关于在数据中心和云环境中使用NVMM的初步研究[49 ,126 ,127]。使用PM的一个新趋势是将其作为分类内存进行管理,就像传统的分类存储系统一样。此型号与以前的共享PM系统不同, 在该系统中，PMDIMM分布在多个服务器中，由用户级应用程序共享。这些计算内存紧密耦合的体系结构在可管理性、可扩展性和资源利用率方面有几个缺点。相比之下，在少数存储器节点中配备有大量PM的分解PM系统可以由计算节点通过高速结构连接。这种计算/内存分类架构可以更容易地减轻数据中心环境中的上述挑战。然而,仍然存在许多挑战。例如,NVMM的持久性特性也应在分布式环境中得到保证。传统的PM管理指令(如clflush和mfence) 只能保证数据在单个服务器中持久化，但不能保证数据通过RDMA网络持久化到远程服务器。对于每个RDMA操作,一旦数据到达远程服务器中的网络接口卡(NIC)，它就会向数据发送方发出确认。由于NIC中有数据缓冲区，数据不会立即存储到远程NVMM。如果此时发生电源故障,则无法保证数据持久性。因此,必须重新设计RDMA协议以支持flushing原语。此外,计算节点应支持对用户级应用程序透明的远程页面交换。为了支持这种机制应该重新设计传统的虚拟内存管理策略。另一方面,由于PM表现出类似内存的性能, 并且是字节可寻址的，因此需要对内存调度和管理进行新的设计，以适应分解的PM。</p></li></ol><p>3)基于NVMM的计算存储器集成计算机体系结构正在兴起。例如,新兴NVMM在存储器内处理(PIM)[95,96]和近数据处理(NDP)[128,129]架构中的应用正在兴起。PIM和NDP近年来已成为新的计算范式。NDP是指将处理器与存储器集成在单个芯片上,以便计算能够尽可能接近地访问存储器中的数据。NDP能够显著降低数据移动的成本。实现这一目标主要有两种方法。一种是将小型计算逻辑(如FPGA/ASIC)集成到存储器芯片中，以便在数据最终被提取到CPU之前对其进行预处理。另一种方法是将内存单元(HBM/HMC)集成到计算(CPU/GPGPU/FPGA)中。该模型通常用于许多处理器架构, 如IntelO Xeon PhiTM Knights Landing系列、NVIDIAO tesla V100和Google Tensor处理单元(TPU)。PIM指的是完全在计算机内存中处理数据。它通过在主存储器中执行计算,提供了高带宽、大规模并行性和高能量效率。使用NVMM (如ReRAM)的PIM通常可以并行计算两个或多个内存行的位逻辑, 并支持一步多行操作。该范例对于模拟计算方式中的矩阵向量乘法特别有效，并且可以实现极大程度的性能加速和节能。因此, PIM在加速机器学习算法(如卷积神经网络)中得到了广泛的研究(CNN)和深度神经网络(DNN)。尽管在PIM架构中.使用NVMM技术的兴趣越来越大[94-96 ,130]， 但目前的研究主要基于电模拟,没有一项研究可用于中型原型设计。</p><p>4)除了传统应用之外, 一些使用NVMM的新应用正在出现。尽管NVMM技术已初步应用于许多大数据应用, 如K-V存储、图形计算和机器学习,但大多数编程框架/模型和运行时系统都是为磁盘设备和基于DRAM的主存储器而设计的, 它们在混合存储系统中并不有效。例如, 这些系统中广泛使用缓冲和延迟写入机制来隐藏I/O操作的高延迟。然而,混合存储器系统中可能不需要这些机制,甚至可能会损害应用程序性能。应重新设计Hadoop/Spark/GraphChi/Tensorflow等大数据处理平台,以适应NVMM技术的特点。除了这些传统应用之外,一些基于NVMM的新型应用正在出现。例如,有一些建议通过利用NVMM的切换过程的内在变化,将NVMM用作硬件安全原语, 如物理不可克隆函数(PUF)[131]。 PUF通常用于具有高安全性要求的应用,例如密码学。最近,已经提出了许多基于NVMM技术的逻辑电路并将其原型化[132-134]。例如，ReRAM技术被提议用作基于ReRAM的FPGA的可重构开关[133]。此外，STT-RAM技术被提出用于设计非易失性缓存或寄存器[135]。</p><h1 id="8-Conclusions结论"><a href="#8-Conclusions结论" class="headerlink" title="8 Conclusions结论"></a>8 Conclusions结论</h1><p>与传统DRAM技术相比, 新兴的NVMM技术具有许多良好的特性。它们有可能从根本上改变存储系统的面貌，甚至为计算机系统添加新的功能和特性。现在有很多机会重新思考当今计算机系统的设计,以实现系统性能和能耗的数量级改进。本文从内存体系结构、操作系统级内存管理和应用程序优化的角度全面介绍了最新的工作和我们的实践。我们还分享了我们对NVMM技术未来研究方向的展望。通过利用NVMM的独特特性, 有巨大的机会来创新未来的计算范式, 开发NVMM的多种新颖应用。</p><hr>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-B Paper </tag>
            
            <tag> Review </tag>
            
            <tag> Hybrid Memory Systems </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MULTI-CLOCK: Dynamic Tiering for Hybrid Memory Systems</title>
      <link href="/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/"/>
      <url>/2023/02/06/MULTI-CLOCK-Dynamic-Tiering-for-Hybrid-Memory-Systems/</url>
      
        <content type="html"><![CDATA[<h2 id="1-论文信息"><a href="#1-论文信息" class="headerlink" title="1. 论文信息"></a>1. 论文信息</h2><div class="note primary">文章来自IEEE International Symposium on High-Performance Computer Architecture, (HPCA), 2022</div> <h3 id="所有作者及单位"><a href="#所有作者及单位" class="headerlink" title="所有作者及单位"></a>所有作者及单位</h3><ul><li>Adnan Maruf, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Ashikee Ghosh, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Janki Bhimani, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li><li>Daniel Campello, Google</li><li>Andy Rudoff, 英特尔公司</li><li>Raju Rangaswami, 佛罗里达国际大学(FIU)奈特基金会计算与信息科学学院</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><p>将PM作为第二级内存直接暴露给CPU是目前比较有希望的一个做法：如何把数据在正确时间放入正确分层中去。于是面临一个是大家关注的问题。</p><h2 id="3-解决了什么问题"><a href="#3-解决了什么问题" class="headerlink" title="3. 解决了什么问题"></a>3. 解决了什么问题</h2><p>动机：通过四个工作负载，统计归类50个采样页面的访问模式，得出层级友好页面是需要迁移的对象。然后说明了用frequency&amp;recency识别到的页面也具有层级友好的特征。相比于静态分层，说明了动态分层的必要。</p><p>最后把整个比较模糊的大问题转化为：试图解决分层系统中：如何根据frequency&amp;recency来识别升级的热点页?如何在内核中设计一个简单、低开销而又高效的系统？</p><h2 id="4-其他学者解决这个问题的思路和缺陷"><a href="#4-其他学者解决这个问题的思路和缺陷" class="headerlink" title="4. 其他学者解决这个问题的思路和缺陷"></a>4. 其他学者解决这个问题的思路和缺陷</h2><p>这些都是分层技术上的对比：<br>静态分层即一个内存页一旦被映射到一个分层，在其生命周期内就不会被重新分配到不同的分层。</p><p>[11]Nimble：只根据recency来选择页面，这篇的作者为了解决frequency决定把工作负载执行期分为观察窗口和性能窗口。得出观察窗口频率高的，在性能窗口概率也高。专注于透明大页(THP)迁移。应用程序需要通过Nimble的启动器运行以利用其页面迁移技术。MC在内置内核实现的功能。Nimble需要一个额外的启动器来运行内核上的任何工作负载。</p><p>[12]AutoTieringhint page fault的缺页异常来跟踪页面访问，并使用recency来识别热点页进行升级。尽管缺页异常可以提供高准确度的页面访问跟踪，但跟踪所有页面的成本很高，因为每一个页面故障都必须在访问页面之前进行处理。这是因为基于缺页的软件页面访问跟踪成本很高，而且跟踪页面历史位以识别冷页面的开销也很大。所以在后面工作负载测试中表现很差。</p><p>[19]Thermostat源代码不可用没有评估，通过poisoning页表项(PTE)和触发缺页异常来跟踪巨大的页面，并将冷页面迁移到较低的内存层。</p><p>[22]AMP(非统一内存访问架构NUMA，主板会分成不同的插槽，每一个插槽一组cpu，以及和这组cpu离得近的内存。)这在两个插座的NUMA机器中是不现实的，因为每个节点通常有自己的DRAM、PM和CPU。AMP使用一个节点，只用于DRAM的分配，其他节点只用于PM的分配。AMP是在Linux内核4.15版本上实现的，它不支持所需的KMEM DAX驱动(从内核v5.1开始提供)，以便PM作为主内存在分层系统中使用。AMP的核心设计原则要求它扫描和剖析来自DRAM和PM层的所有内存页，这在实际系统的内核中是不现实的，因为在我们评估的工作负载中，内存页的数量可以增长到数亿。<br>对比于PM当前这款硬件的两种用法。</p><p>[7]Memory-mode：数据不能持久化，dram作为缓存不透明。缓存是需要从高层获取数据的，而这里提出的分层是两层都能被直接访问的。</p><p>[44]对象级需要改变应用程序API，而内核级别的修改不需要应用程序有何变化。</p><p>[32]提出了一种有效使用持久性内存作为NUMA节点的设计。这个分层设计同时意识到了DRAM和PM节点 ,并且只通过NUMA平衡处理匿名页面的升级/移动。</p><p>[33-36]不需要硬件，而且都是主存没有缓冲。</p><h2 id="5-围绕该问题作者如何构建解决思路"><a href="#5-围绕该问题作者如何构建解决思路" class="headerlink" title="5. 围绕该问题作者如何构建解决思路"></a>5. 围绕该问题作者如何构建解决思路</h2><p>设计MULTI-CLOCK的主要假设是，最近被访问过一次以上的页面，在不久的将来更有可能被访问。</p><p>具体升级降级要求门槛就是那张图。（但是频繁程度不够吧？）及时更新页面引用状态的方式根据对内存页的访问模式不同而不同。无监督式：CPU在进程的页表入口中设置的页面引用位。</p><p>与CLOCK类似，升级是由每次系统守护程序kpromoted去完成的。它定期被唤醒，以扫描列表，更新它们，并将最近由无监督访问产生的升级列表中页迁移到更高层的页。</p><p>降级机制基于今天的虚拟内存系统中的页面驱逐技术。当内存达到内存压力时（该层与系统总内存量来计算）会去扫描每个列表。活动列表中的页面相对于非活动列表的比例超过了一个与该层可用内存量相关的阈值时，那么在活动列表中没有被标记为引用的页面将被移至非活动列表。最后，非活动列表被扫描，以寻找未被标记为引用的页面，并将其迁移到较低的层级。当没有更低层级列表可以迁移，就写回块存储。</p><h2 id="6-从结果看，作者如何有力证明他解决了问题"><a href="#6-从结果看，作者如何有力证明他解决了问题" class="headerlink" title="6. 从结果看，作者如何有力证明他解决了问题"></a>6. 从结果看，作者如何有力证明他解决了问题</h2><p>我们评估的目的是确定MULTI-CLOCK是否、何时以及如何能够提高应用工作负载的性能。</p><p>使用雅虎云服务基准( YCSB) [13]的六个不同的工作负载和GAP基准套件(GAPBS) [14] 的六个工作负载来讨论我们的结果。Memcached[3]，一个使用大量主内存来维护其数据的内存缓存服务，作为YCSB的键值存储后端。 配置时内存要被全部消耗，并且消耗一部分PM。</p><p>MULTI-CLOCk在所有YSCB工作负载上都优于静态分层、Nimble、AT-CPM和AT-OPM。对GAPBS的执行时间也比其他方案减少。</p><p>分析了MULT-CLOCK和Nimble所升级的页面数量。MULTI-CLOCK每次扫描平均升级758页，最多扫描1024。而N是把1024作为固定值。如果那些将来不会再被重新访问的页面被提升到DRAM中，那么提升这些页面的开销会损害系统性能。第二次再次被访问的百分比比N高15%</p><p>实现了目标，低开销，特别是密集型应用。</p><h2 id="7-缺陷和改进思路"><a href="#7-缺陷和改进思路" class="headerlink" title="7. 缺陷和改进思路"></a>7. 缺陷和改进思路</h2><p>工作重点在大容量和字节寻址上。</p><p>这个产品是放弃了持久性的。如果使用内存模式。</p><p>在决定页面留在上一层或者下一层时，考虑不对称的读写延迟，考虑读和写的访问类型。</p><p>能耗方面，目前这个产品的写是多大的消耗呢？(需要了解一下目前读写到底是啥情况)NVM的读和写的差别有多大</p><h2 id="8-创新点"><a href="#8-创新点" class="headerlink" title="8. 创新点"></a>8. 创新点</h2><p>在内核上修改代码，不需要其他程序有什么修改。使用升级列表和频率可以很好的利用层级友好页面的局部性。</p><h2 id="9-相关链接"><a href="#9-相关链接" class="headerlink" title="9. 相关链接"></a>9. 相关链接</h2><ul><li><a href="https://github.com/sylab/multi-clock">作者公布的源码</a></li><li><a href="https://docs.pmem.io/ndctl-user-guide/">ndctl用户手册</a></li><li><a href="https://pmem.io/blog/2020/01/memkind-support-for-kmem-dax-option/">PM的AD模式下的kmem模式设置</a></li><li><a href="https://github.com/brianfrankcooper/YCSB">YCSB工作负载源码</a></li><li><a href="https://cloud.tencent.com/developer/article/1004637">YCSB介绍与相关运行配置介绍</a></li><li><a href="https://github.com/pmem/ndctl/tree/main/Documentation/daxctl">ndctl下的daxctl安装源码下载</a></li><li><a href="https://memark.io/index.php/2021/04/09/pmem_intro/">持久内存开发资料汇总</a></li><li><a href="https://blog.csdn.net/qq_37858386/article/details/78444168">Linux驱动编程中EXPORT_SYMBOL介绍（因为作者公布的源码里有这个报错）</a></li><li><a href="https://github.com/pmem/ndctl/issues/108">daxctl fails to reconfigure to system-ram when DAX modules built-in</a></li><li><a href="https://www.intel.com/content/www/us/en/developer/articles/guide/qsg-intro-to-provisioning-pmem.html">持久内存配置简介，快速入门</a></li><li><a href="https://github.com/sbeamer/gapbs">GAPBS工作负载下载和使用</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CCF-A Paper </tag>
            
            <tag> Tierd Memory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Time Complexity Calculation</title>
      <link href="/2021/06/06/Time-Complexity-Calculation/"/>
      <url>/2021/06/06/Time-Complexity-Calculation/</url>
      
        <content type="html"><![CDATA[<h2 id="一、常见阶大小比较"><a href="#一、常见阶大小比较" class="headerlink" title="一、常见阶大小比较"></a>一、常见阶大小比较</h2><p>从大到小：  </p><ul><li>超指数阶：$n^n$，$n!$</li><li>指数阶：$9^{n/2}$,  $2^n$</li><li>多项式阶：$n^3$,   $n*log(n)$, $n^{1/2}$</li><li>对数阶：$log^2(n)$, $log(n)$, $log(log(n))$</li><li>常数阶：100, 1<br>下题需要保留阶最高的部分：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220748744.jpg"></li></ul><h2 id="二、算法复杂性估计函数"><a href="#二、算法复杂性估计函数" class="headerlink" title="二、算法复杂性估计函数"></a>二、算法复杂性估计函数</h2>$$\lim_{n \to \infty} \frac{f(n)}{g(n)}  =\begin{cases}(大于0的常数或)0       &amp;&amp;&amp; f(n)=O(g(n))上界&amp;-----f(n)\le cg(n)\\(大于0的常数或)无穷    &amp;&amp;&amp; f(n)= \Omega(g(n))下界&amp;-----f(n)\ge cg(n)\\大于0的常数            &amp;&amp;&amp; f(n)= \Theta(g(n))确切界&amp;-----f(n)=cg(n)\\0      &amp;&amp;&amp;f(n)=o(g(n))上界&amp;-----f(n)&lt; cg(n)\end{cases}$$<p>可以发现都是针对f(n)在讨论，很容易得出下题答案：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605220628312.jpg"></p><h2 id="三、几个常用替换的式子"><a href="#三、几个常用替换的式子" class="headerlink" title="三、几个常用替换的式子"></a>三、几个常用替换的式子</h2><h3 id="1-Stirling公式："><a href="#1-Stirling公式：" class="headerlink" title="1.Stirling公式："></a>1.Stirling公式：</h3>$$n! \approx {(2 \pi n)}^{1/2}{(n/e)}^n$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210605222019202.jpg"></p><h3 id="2-阶乘和二项式系数"><a href="#2-阶乘和二项式系数" class="headerlink" title="2.阶乘和二项式系数"></a>2.阶乘和二项式系数</h3>$$C_n^k = C_n^{n-k} \\ C_n^n = C_n^0 = 1 \\C_n^k = C_{n-1}^k +C_{n-1}^{k-1} $$<p>帕斯卡三角形可以辅助记二项式系数：<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060522532381.jpg"></p>$$\sum_{j=0}^{n}{C_{n}^j}x^j= {(1+x)}^n $$<h3 id="3-和式"><a href="#3-和式" class="headerlink" title="3.和式"></a>3.和式</h3>$$\sum_{j=1}^{n}{a_{n-j}} = \sum_{j=0}^{n-1}{a_j} \\\sum_{j=0}^n{j \over 2^j} = \sum_{j=1}^n{j \over 2^j} = 2-{{(n+2)} \over {n^2}} = \Theta(1) \\\sum_{j=0}^njc^j = \sum_{j=1}^njc^j = \Theta(nc^n)$$<h3 id="4-定积分与和式转换"><a href="#4-定积分与和式转换" class="headerlink" title="4.定积分与和式转换"></a>4.定积分与和式转换</h3>$$\int_m^{n+1} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m-1}^nf(x)dx 递减函数 \\\int_{m-1}^{n} f(x) dx  \le \sum_{j=m}^nf(j) \le \int_{m}^{n+1}f(x)dx  递增函数$$<p>可以采用画图的方法辅助记忆它的上下界：<br>以一个递增的函数为例，我们要求1（m）~6（n）他的面积，每一个小矩形$1*f(j)$如果我们积分每个点左边的矩形，那么总面积就是偏小的，积分右边矩形就会稍微偏大，这就找到了上下界，当函数平行于X轴时就会有等号。递减也是一个道理。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606094351964.jpg"><br>但是，如果是logn等函数会遇到定义域不存在的情况。我们应该从和式中把（在积分中）没有定义的点先拿出来，再去积分。<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/2021060610042349.jpg"></p><p>用代数方法证明就是放大缩小去求，用积分方法证明就是用上面那个公式。可以看到积分方法求时等式左边按公式应该为$\int_0^n jlogj !\ dj$定义域<br>不存在，所以对于右边：</p>$$\sum_{j=1}^njlogj=\sum_{j=2}^njlogj+1log1=\int_{2-1}^njlogj \!\ dj+1log1$$<h2 id="四、计算次数"><a href="#四、计算次数" class="headerlink" title="四、计算次数"></a>四、计算次数</h2><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法:COUNT输入:n=2k,k为正整数。输出: count的值 。count=0while n&gt;=1for j=1 to ncount=count+1n=n/2return countend COUNT<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>while要执行k+1次，$k=log_2n$.for循环在每次while的基础上执行n次所以(k+1)n即$nlog_2n$次计算</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">算法: MERGE输入:数组A[1..m]和它的三个索引p, q, r, 1&lt;=p&lt;=q&lt;r&lt;=m。两个子数组A[p..q]和A[q+1..r]各自按升序排列。输出:合并两个子数组A[p..q]和A[q+1..r]的升序数组A[p..r]for(s=p, t=q+1, k=p; S&lt;=q and t&lt;=r; k++)if A[s]&lt;=A[t] //两个指针从两个头开始排序B[k]=A[s]; //B[p..r]是个辅助数组S=S+1;elseB[k]=A[t];t=t+1;if s=q+1 B[k..r]=A[q+1..r] elseB[k..r]=A[s..q]A[p..r]=B[p..r]end MERGE<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>两段相邻数组分别有序，两个指针将两段变为有序的。2(r-p+1)先遍历一次排序，在从排好序的辅助数组移回来。</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">void insertion_ sort(Type *a, int n){// 代价 次数// ti: for的第i次while的循环次数for (int i=1; i&lt;n; i++){// c1   n(比较语句)key=a[i];// c2   n-1int j=i-1;// c3   n-1while( j&gt;=0 &amp;&amp; a[j]&gt;key ){  // c4   sum tia[j+1] = a[j];// c5   sum of (ti-1)j--;// c6sum of (ti-1)}a[j+1]=key; // c7   n-1}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>插入排序的思想是左手为空，右手的牌按序插入。这里t是个不确定的数，但是还是可以得出计算次数为：</p><p>$$c_1n+c_2(n-1)+c_3(n-1)+c_4\sum_{i=1}^{n-1}{t_i}+c_5\sum_{i=1}^{n-1}{(t_i-1)}+c_6\sum_{i=1}^{n-1}{(t_i-1)}+c_7(n-1)$$</p><p>最好情况就是已经排好序了，c5与c6都是0，每次for的while都只跑一次。<br>$$c_1n+c_2(n-1)+c_3(n-1)+c_4(n-1)+c_7(n-1) = O(n)$$<br>最坏情况就是倒序排的，n张牌每次都比上一次多查找一个。</p><p>$$\sum_{i=1}^{n-1}{(t_i-1)} = n(n-1)/2$$</p><p>复杂度$O(n^2)$</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup">1. COUNT42.count &lt;-- 03.for i ←- 1 to Llogn」4.for j ←- i to i+55.for k ←- 1 to i^26.         count ←- count +17.end for8.  end for9.end for(a)第6步执行了多少次?(b)要表示算法的时间复杂性，用0和O哪个符号更合适?为什么?(c)算法的时间复杂性是什么?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>a）思路是把每次for循环乘起来。<br>$$\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}\sum_{k=1}^{i^2}{1}$$内层指的是从1到$i^2$个1相加$=\sum_{i=1}^{\lfloor logn \rfloor}\sum_{j=i}^{i+5}{i^2} = 6\sum_{i=1}^{\lfloor logn \rfloor}{i^2}$<br>利用平方和求和公式$n(n+1)(2n+1)/6$进一步化简$=\lfloor logn \rfloor(\lfloor logn \rfloor +1)(2\lfloor logn \rfloor+1)$<br>b） O 因为对于算出的确切的计算次数，这个用于表示算法时间复杂性的函数是它上界。<br>c）$O(log^3n)$</p><h2 id="五、解递归方程式"><a href="#五、解递归方程式" class="headerlink" title="五、解递归方程式"></a>五、解递归方程式</h2><h3 id="1-线性齐次递推式（二阶）"><a href="#1-线性齐次递推式（二阶）" class="headerlink" title="1.线性齐次递推式（二阶）"></a>1.线性齐次递推式（二阶）</h3><p>对于递推式:$$f(n)=a_1f(n-1)+a_2f(n-2)+…+a_kf(n-k)$$我们想要得到$f(n)$的确切解，它的解往往是$x^n$于是我们可以把这个递推式的等价于:$$x_n=a_1x^{n-1}+a_2x^{n-2}+…+a_kx^{n-k}$$将两边同时除以$x^{n-k}$并且移项可以得到与n无关还能解出x的式子$$x^k-a_1x^{k-1}-a_2x^{k-2}-…-a_k = 0$$这个就是常说的特征方程。</p><table><thead><tr><th align="center">步骤</th><th align="center">例1</th><th align="center">例2</th></tr></thead><tbody><tr><td align="center">序列</td><td align="center">1,4,16,64,256</td><td align="center">1,1,2,3,5,8(斐波拉契)</td></tr><tr><td align="center">递推关系</td><td align="center">f(n)=3f(n-1)+4f(n-2)</td><td align="center">f(n)=f(n-1)+f(n-2)</td></tr><tr><td align="center">特征方程</td><td align="center">$x^2-3x-4=0$</td><td align="center">$x^2-x-1=0$</td></tr><tr><td align="center">特征根</td><td align="center">$x_1=-1,x_2=4$</td><td align="center">$x_1= { {1+\sqrt5} \over 2},x_2={ {1-\sqrt5} \over 2}$</td></tr><tr><td align="center">通解</td><td align="center">$f(n) = c_1{(-1)}^n+c_24^n$</td><td align="center">$f(n)=c_1\left( { {1+\sqrt5} \over 2} \right)^n+c_2\left( { {1-\sqrt5} \over 2} \right)^n$</td></tr><tr><td align="center">带入序列中的点</td><td align="center">$c_1=0,c_2=1$</td><td align="center">$c_1={1\over {\sqrt5}},c_2=-{1\over {\sqrt5}}$</td></tr><tr><td align="center">最终解</td><td align="center">$f(n)=4^n$</td><td align="center">由于n无穷大$c_2\left( { {1-\sqrt5} \over 2} \right)^n$趋于0,$f(n)={1\over {\sqrt5}}\left( { {1+\sqrt5} \over 2} \right)^n$</td></tr></tbody></table><p>还有种特殊情况：$x_1=x_2=x$时$f(n)=c_1nx^n+c_2x^n$</p><h3 id="2-非齐次递推式"><a href="#2-非齐次递推式" class="headerlink" title="2.非齐次递推式"></a>2.非齐次递推式</h3><h4 id="2-1-f-n-f-n-1-g-n"><a href="#2-1-f-n-f-n-1-g-n" class="headerlink" title="2.1  f(n)=f(n-1)+g(n)"></a>2.1  f(n)=f(n-1)+g(n)</h4><p>对于这一类g(n)是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)+g(n) = \big(f(n-2)+g(n-1)\big)+g(n) = f(0)+ \sum_{j=1}^ng(j)$$<br><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606120205290.jpg"><br>这道题麻烦点在于怎么把前面的系数3搞没了。由1我们知道，这种递推式是$f(n)=x^n$变形令$f(n)=3^nq(n), f(0)=q(0)=3$于是乎原式变为：</p>$$3^nq(n)=3*3^{n-1}q(n-1)+2^n \\ q(n)=q(n-1)+{(2/3)}^n\\ q(n)=q(0)+\sum_{j=1}^n{{(2/3)}^n}\\ f(n)=3^n*\big(3+{(2/3)(1-{(2/3)}^n) \over {1-(2/3)}}\big) \\ f(n)=5*3^n+2^{n+1}$$<h4 id="2-2-f-n-f-n-1-g-n"><a href="#2-2-f-n-f-n-1-g-n" class="headerlink" title="2.2  f(n)=f(n-1)*g(n)"></a>2.2  f(n)=f(n-1)*g(n)</h4><p>对于这一类g(n)也是一个已知的函数，推导可得：<br>$$f(n) = f(n-1)*g(n) = \big(f(n-2)*g(n-1)\big)+g(n) = f(0)\prod_{i=1}^ng(i)$$</p><h4 id="2-3-f-n-f-n-1-g-n-h-n"><a href="#2-3-f-n-f-n-1-g-n-h-n" class="headerlink" title="2.3  f(n)=f(n-1)*g(n)+h(n)"></a>2.3  f(n)=f(n-1)*g(n)+h(n)</h4><p>可以直接推，也可以带点技巧推。结果：$$=\prod_{i=1}^ng(i)\big( f(0)+\sum_{j=1}^n{h(j) \over{\prod_{i=1}^ng(i)} } \big)$$</p><h4 id="2-4-f-n-af-n-c-g-n"><a href="#2-4-f-n-af-n-c-g-n" class="headerlink" title="2.4 f(n)=af(n/c)+g(n)"></a>2.4 f(n)=af(n/c)+g(n)</h4>$$f(n)=\begin{cases} d&amp; \text{n=1}\\af({n\over c})+bn^x&amp; \text{n &gt;= 2} \end{cases}$$<p>其中d非负常量，g(n)非负函数，a，c正数。设$n=c^k$</p>$$f(n)=\begin{cases} bn^x*log_cn^x+dn^x&amp;{a=c^x}\\ \big(d+{{bc^x}\over{a-c^x}}\big)n^{log_ca}-\big({{bc^x}\over{a-c^x}}\big)n^x&amp; {a\neq c^x} \end{cases}$$<p><img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210606123724891.jpg"><br>这道题就没有为难菜鸡，直接把方向给了。这种做法在遇到f(n/2)的情况下叫做<strong>更换变元法</strong><br>$$f(2^k)=f(2^{k-1})+2^k=f(2^0)+\sum_{i=1}^k2^i=2^{k+1}-1 =2n-1\<br>g(2^k)=2g(2^{k-1})+1=\sum_{i=0}^k2^i=2^{k+1}-1=2n-1$$<br>也可以直接套公式:<br>对于f   $d=1,a=1,c=2,b=1$<br>对于g  $a=2,c=2,b=1,d=1$<br>这些公式太惨绝人寰了，其他还有一些和2.4一样复杂的，等遇到了再补充上去。</p><h2 id="六、p、np、np-hard、np-complete问题"><a href="#六、p、np、np-hard、np-complete问题" class="headerlink" title="六、p、np、np-hard、np-complete问题"></a>六、p、np、np-hard、np-complete问题</h2><p><a href="https://blog.csdn.net/birduncle/article/details/94646993">p、np、np-hard、np-complete问题</a>这篇文章讲的很清晰，附上一张从文章里拿的<img src="https://images.weserv.nl/?url=https://img-blog.csdnimg.cn/20210704092654261.png"></p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Advanced Mathematics </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
